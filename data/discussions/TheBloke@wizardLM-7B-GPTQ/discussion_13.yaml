!!python/object:huggingface_hub.community.DiscussionWithDetails
author: d3ztr0yur
conflicting_files: null
created_at: 2023-04-30 18:43:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9d47673a8d8cb97fedee65a32e6677c.svg
      fullname: Daniel Thanos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: d3ztr0yur
      type: user
    createdAt: '2023-04-30T19:43:57.000Z'
    data:
      edited: false
      editors:
      - d3ztr0yur
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9d47673a8d8cb97fedee65a32e6677c.svg
          fullname: Daniel Thanos
          isHf: false
          isPro: false
          name: d3ztr0yur
          type: user
        html: '<p>First of all thanks for all the work you are doing building these
          quantitated models, it is greatly appreciated! I am trying to load this
          using a HuggingFacePipeline in Langchain, that ultimately just uses the
          transformers library. I get the following error: </p>

          <p> OSError: TheBloke/wizardLM-7B-GPTQ does not appear to have a file named
          pytorch_model.bin, tf_model.h5, model.ckpt<br>or flax_model.msgpack.</p>

          <p>Is there something we need to do with the config files, model, or how
          we load it using transformers? I am trying to load this on a CUDA RTX GPU
          with 8GB of VRAM.  The code to use as an LLM in Langchain is:</p>

          <p>llm = HuggingFacePipeline.from_model_id(model_id="TheBloke/wizardLM-7B-GPTQ",
          task="text-generation", device=0, quantized=True, strict=False)</p>

          <p>The above code will ultimately setup the model like this:</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("TheBloke/wizardLM-7B-GPTQ")<br>model
          = AutoModelForCausalLM.from_pretrained("TheBloke/wizardLM-7B-GPTQ")</p>

          <p>I have had no luck loading any 7B 4bit model :( </p>

          '
        raw: "First of all thanks for all the work you are doing building these quantitated\
          \ models, it is greatly appreciated! I am trying to load this using a HuggingFacePipeline\
          \ in Langchain, that ultimately just uses the transformers library. I get\
          \ the following error: \r\n\r\n OSError: TheBloke/wizardLM-7B-GPTQ does\
          \ not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt\
          \ \r\nor flax_model.msgpack.\r\n\r\nIs there something we need to do with\
          \ the config files, model, or how we load it using transformers? I am trying\
          \ to load this on a CUDA RTX GPU with 8GB of VRAM.  The code to use as an\
          \ LLM in Langchain is:\r\n\r\nllm = HuggingFacePipeline.from_model_id(model_id=\"\
          TheBloke/wizardLM-7B-GPTQ\", task=\"text-generation\", device=0, quantized=True,\
          \ strict=False)\r\n\r\nThe above code will ultimately setup the model like\
          \ this:\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"TheBloke/wizardLM-7B-GPTQ\"\
          )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"TheBloke/wizardLM-7B-GPTQ\"\
          )\r\n\r\nI have had no luck loading any 7B 4bit model :( "
        updatedAt: '2023-04-30T19:43:57.196Z'
      numEdits: 0
      reactions: []
    id: 644ec4fd722da4dfd8726adb
    type: comment
  author: d3ztr0yur
  content: "First of all thanks for all the work you are doing building these quantitated\
    \ models, it is greatly appreciated! I am trying to load this using a HuggingFacePipeline\
    \ in Langchain, that ultimately just uses the transformers library. I get the\
    \ following error: \r\n\r\n OSError: TheBloke/wizardLM-7B-GPTQ does not appear\
    \ to have a file named pytorch_model.bin, tf_model.h5, model.ckpt \r\nor flax_model.msgpack.\r\
    \n\r\nIs there something we need to do with the config files, model, or how we\
    \ load it using transformers? I am trying to load this on a CUDA RTX GPU with\
    \ 8GB of VRAM.  The code to use as an LLM in Langchain is:\r\n\r\nllm = HuggingFacePipeline.from_model_id(model_id=\"\
    TheBloke/wizardLM-7B-GPTQ\", task=\"text-generation\", device=0, quantized=True,\
    \ strict=False)\r\n\r\nThe above code will ultimately setup the model like this:\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"TheBloke/wizardLM-7B-GPTQ\"\
    )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"TheBloke/wizardLM-7B-GPTQ\"\
    )\r\n\r\nI have had no luck loading any 7B 4bit model :( "
  created_at: 2023-04-30 18:43:57+00:00
  edited: false
  hidden: false
  id: 644ec4fd722da4dfd8726adb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-30T20:24:00.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I''m afraid you can''t just load these models in normal Transformers
          code. They''re quantised to use less VRAM, and therefore need special inference
          code.</p>

          <p>You have two options:</p>

          <ol>

          <li>Don''t load these quantised models, and instead load the original unquantised
          models in either fp16 or 8bit.  This will use much more VRAM than 4bit quantised
          and will provide slower inference, but can be done immediately with at most
          one simple dependency.</li>

          </ol>

          <p>You can load repos in HuggingFace format either in original 16bit, or
          in 8bit.  You almost certainly want 8bit, as otherwise even a 7B model will
          use 13GB VRAM, and a 13B needs 26GB -  and so can''t be loaded in full on
          any consumer card.  In 8bit those figures are halved.</p>

          <p>8bit inference requires the <a rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes
          pip installed</a> - <code>pip install bitsandbytes</code>.</p>

          <p>I have unquantised versions of all the models I''ve uploaded. Their repo
          names will end in <code>-HF</code>.  Eg <code>TheBloke/wizardLM-7B-HF</code></p>

          <p>To do 8bit inference, you simply add <code>load_in_8bit=True</code> to
          your  model load line, eg:</p>

          <pre><code>tokenizer = AutoTokenizer.from_pretrained("TheBloke/wizardLM-7B-HF")

          model = AutoModelForCausalLM.from_pretrained("TheBloke/wizardLM-7B-HF",
          load_in_8bit=True)

          </code></pre>

          <ol start="2">

          <li>The second option is to persevere with the 4bit quantised GPTQs, which
          will save on VRAM and therefore either provide faster inference, or allowed
          you to load larger models.  To do inference on GPTQs requires some special
          code; it''s not supported by the base Transformers library.</li>

          </ol>

          <p>To date I have made all my GPTQs using code called  <a rel="nofollow"
          href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">GPTQ-for-LLaMa</a>.
          Until recently this was the only method.  Unfortunately, doing inference
          with this code is not straightforward.  There is an example script in that
          repo that you can use, called <code>llama_inference.py</code>. So you could
          do tests with that and use its code as a base.  But as you''ll see, it''s
          complicated and quite messy.</p>

          <p>There should be a better solution: <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ">AutoGPTQ</a>.
          AutoGPTQ is much newer but is developing fast and I''m hopeful that this
          will become the future of GPTQ.</p>

          <p>As you''ll see from the AutoGPTQ repo, its aim is to make using GPTQ
          models nearly as simple as using a standard Transformers model. It''s named
          AutoGPTQ because it''s intended to work alongside the Transformers auto
          methods like AutoModelForCausalLM.</p>

          <p>Until a couple of days ago AutoGPTQ didn''t immediately support loading
          models made with GPTQ-for-LLaMa. But there''s just been <a rel="nofollow"
          href="https://github.com/PanQiWei/AutoGPTQ/pull/33">a PR merged that should
          support it</a>.  It''s so new I haven''t even tried it out myself yet, so
          if you give it a go let me know!</p>

          <p>Both GPTQ-for-LLaMa and AutoGPTQ can be run in one of two modes: Triton,
          or CUDA.  Triton is recommended, but only works on Linux or in WSL2.  CUDA
          requires compilation, meaning you may have to have a C/C++ compiler and
          the NVidia CUDA Toolkit installed. Although the AutoGPTQ dev recently added
          a PyPi package (<code>pip install auto-gptq</code>) so it may be he''s provided
          already-compiled binaries now.</p>

          <p>I recommend starting with the AutoGPTQ repo. It includes installation
          instructions and example inference code.</p>

          <p>Let me know how you get on!</p>

          '
        raw: 'Yeah I''m afraid you can''t just load these models in normal Transformers
          code. They''re quantised to use less VRAM, and therefore need special inference
          code.


          You have two options:

          1. Don''t load these quantised models, and instead load the original unquantised
          models in either fp16 or 8bit.  This will use much more VRAM than 4bit quantised
          and will provide slower inference, but can be done immediately with at most
          one simple dependency.


          You can load repos in HuggingFace format either in original 16bit, or in
          8bit.  You almost certainly want 8bit, as otherwise even a 7B model will
          use 13GB VRAM, and a 13B needs 26GB -  and so can''t be loaded in full on
          any consumer card.  In 8bit those figures are halved.


          8bit inference requires the [bitsandbytes pip installed](https://github.com/TimDettmers/bitsandbytes)
          - `pip install bitsandbytes`.


          I have unquantised versions of all the models I''ve uploaded. Their repo
          names will end in `-HF`.  Eg `TheBloke/wizardLM-7B-HF`


          To do 8bit inference, you simply add `load_in_8bit=True` to your  model
          load line, eg:

          ```

          tokenizer = AutoTokenizer.from_pretrained("TheBloke/wizardLM-7B-HF")

          model = AutoModelForCausalLM.from_pretrained("TheBloke/wizardLM-7B-HF",
          load_in_8bit=True)

          ```


          2. The second option is to persevere with the 4bit quantised GPTQs, which
          will save on VRAM and therefore either provide faster inference, or allowed
          you to load larger models.  To do inference on GPTQs requires some special
          code; it''s not supported by the base Transformers library.


          To date I have made all my GPTQs using code called  [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa).
          Until recently this was the only method.  Unfortunately, doing inference
          with this code is not straightforward.  There is an example script in that
          repo that you can use, called `llama_inference.py`. So you could do tests
          with that and use its code as a base.  But as you''ll see, it''s complicated
          and quite messy.


          There should be a better solution: [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ).
          AutoGPTQ is much newer but is developing fast and I''m hopeful that this
          will become the future of GPTQ.


          As you''ll see from the AutoGPTQ repo, its aim is to make using GPTQ models
          nearly as simple as using a standard Transformers model. It''s named AutoGPTQ
          because it''s intended to work alongside the Transformers auto methods like
          AutoModelForCausalLM.


          Until a couple of days ago AutoGPTQ didn''t immediately support loading
          models made with GPTQ-for-LLaMa. But there''s just been [a PR merged that
          should support it](https://github.com/PanQiWei/AutoGPTQ/pull/33).  It''s
          so new I haven''t even tried it out myself yet, so if you give it a go let
          me know!


          Both GPTQ-for-LLaMa and AutoGPTQ can be run in one of two modes: Triton,
          or CUDA.  Triton is recommended, but only works on Linux or in WSL2.  CUDA
          requires compilation, meaning you may have to have a C/C++ compiler and
          the NVidia CUDA Toolkit installed. Although the AutoGPTQ dev recently added
          a PyPi package (`pip install auto-gptq`) so it may be he''s provided already-compiled
          binaries now.


          I recommend starting with the AutoGPTQ repo. It includes installation instructions
          and example inference code.


          Let me know how you get on!'
        updatedAt: '2023-04-30T20:24:00.565Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - hussainwali1
        - ka0112
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jocastroc
    id: 644ece60ddf20748b05ee1d0
    type: comment
  author: TheBloke
  content: 'Yeah I''m afraid you can''t just load these models in normal Transformers
    code. They''re quantised to use less VRAM, and therefore need special inference
    code.


    You have two options:

    1. Don''t load these quantised models, and instead load the original unquantised
    models in either fp16 or 8bit.  This will use much more VRAM than 4bit quantised
    and will provide slower inference, but can be done immediately with at most one
    simple dependency.


    You can load repos in HuggingFace format either in original 16bit, or in 8bit.  You
    almost certainly want 8bit, as otherwise even a 7B model will use 13GB VRAM, and
    a 13B needs 26GB -  and so can''t be loaded in full on any consumer card.  In
    8bit those figures are halved.


    8bit inference requires the [bitsandbytes pip installed](https://github.com/TimDettmers/bitsandbytes)
    - `pip install bitsandbytes`.


    I have unquantised versions of all the models I''ve uploaded. Their repo names
    will end in `-HF`.  Eg `TheBloke/wizardLM-7B-HF`


    To do 8bit inference, you simply add `load_in_8bit=True` to your  model load line,
    eg:

    ```

    tokenizer = AutoTokenizer.from_pretrained("TheBloke/wizardLM-7B-HF")

    model = AutoModelForCausalLM.from_pretrained("TheBloke/wizardLM-7B-HF", load_in_8bit=True)

    ```


    2. The second option is to persevere with the 4bit quantised GPTQs, which will
    save on VRAM and therefore either provide faster inference, or allowed you to
    load larger models.  To do inference on GPTQs requires some special code; it''s
    not supported by the base Transformers library.


    To date I have made all my GPTQs using code called  [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa).
    Until recently this was the only method.  Unfortunately, doing inference with
    this code is not straightforward.  There is an example script in that repo that
    you can use, called `llama_inference.py`. So you could do tests with that and
    use its code as a base.  But as you''ll see, it''s complicated and quite messy.


    There should be a better solution: [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ).
    AutoGPTQ is much newer but is developing fast and I''m hopeful that this will
    become the future of GPTQ.


    As you''ll see from the AutoGPTQ repo, its aim is to make using GPTQ models nearly
    as simple as using a standard Transformers model. It''s named AutoGPTQ because
    it''s intended to work alongside the Transformers auto methods like AutoModelForCausalLM.


    Until a couple of days ago AutoGPTQ didn''t immediately support loading models
    made with GPTQ-for-LLaMa. But there''s just been [a PR merged that should support
    it](https://github.com/PanQiWei/AutoGPTQ/pull/33).  It''s so new I haven''t even
    tried it out myself yet, so if you give it a go let me know!


    Both GPTQ-for-LLaMa and AutoGPTQ can be run in one of two modes: Triton, or CUDA.  Triton
    is recommended, but only works on Linux or in WSL2.  CUDA requires compilation,
    meaning you may have to have a C/C++ compiler and the NVidia CUDA Toolkit installed.
    Although the AutoGPTQ dev recently added a PyPi package (`pip install auto-gptq`)
    so it may be he''s provided already-compiled binaries now.


    I recommend starting with the AutoGPTQ repo. It includes installation instructions
    and example inference code.


    Let me know how you get on!'
  created_at: 2023-04-30 19:24:00+00:00
  edited: false
  hidden: false
  id: 644ece60ddf20748b05ee1d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-30T20:35:48.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>PS. Sorry, having written all that I completely forgot you''re actually
          using LangChain.</p>

          <p>So firstly there is no official support for GPTQ in LangChain as yet.</p>

          <p>There should be support for 8bit, so that remains an option. A 7B model
          might just fit in 8GB in 8bit, though I don''t know for sure.</p>

          <p>Does LangChain have an option to set up the model first, then pass it
          that model? If so you could likely initialise the model with AutoGPTQ and
          then pass it to LangChain afterwards, if it supports that?</p>

          <p>Alternatively, I found this third party repo which says it has put GPTQ
          into LangChain for the purposes of making a chat bot: <a rel="nofollow"
          href="https://github.com/paolorechia/learn-langchain">https://github.com/paolorechia/learn-langchain</a></p>

          <p>I''ve not looked into how it does it exactly. My guess is it uses GPTQ-for-LLaMa,
          the original GPTQ code.  But it''d definitely be worth a look and maybe
          you can use some of their code.</p>

          <p>Or if neither of the above work for you, you could try making your own
          local edit of LangChain to incorporate AutoGPTQ which shouldn''t be too
          hard to do.</p>

          <p>Good luck!</p>

          '
        raw: 'PS. Sorry, having written all that I completely forgot you''re actually
          using LangChain.


          So firstly there is no official support for GPTQ in LangChain as yet.


          There should be support for 8bit, so that remains an option. A 7B model
          might just fit in 8GB in 8bit, though I don''t know for sure.


          Does LangChain have an option to set up the model first, then pass it that
          model? If so you could likely initialise the model with AutoGPTQ and then
          pass it to LangChain afterwards, if it supports that?


          Alternatively, I found this third party repo which says it has put GPTQ
          into LangChain for the purposes of making a chat bot: https://github.com/paolorechia/learn-langchain


          I''ve not looked into how it does it exactly. My guess is it uses GPTQ-for-LLaMa,
          the original GPTQ code.  But it''d definitely be worth a look and maybe
          you can use some of their code.


          Or if neither of the above work for you, you could try making your own local
          edit of LangChain to incorporate AutoGPTQ which shouldn''t be too hard to
          do.


          Good luck!'
        updatedAt: '2023-04-30T20:35:48.603Z'
      numEdits: 0
      reactions: []
    id: 644ed124a00f4b11d39a6088
    type: comment
  author: TheBloke
  content: 'PS. Sorry, having written all that I completely forgot you''re actually
    using LangChain.


    So firstly there is no official support for GPTQ in LangChain as yet.


    There should be support for 8bit, so that remains an option. A 7B model might
    just fit in 8GB in 8bit, though I don''t know for sure.


    Does LangChain have an option to set up the model first, then pass it that model?
    If so you could likely initialise the model with AutoGPTQ and then pass it to
    LangChain afterwards, if it supports that?


    Alternatively, I found this third party repo which says it has put GPTQ into LangChain
    for the purposes of making a chat bot: https://github.com/paolorechia/learn-langchain


    I''ve not looked into how it does it exactly. My guess is it uses GPTQ-for-LLaMa,
    the original GPTQ code.  But it''d definitely be worth a look and maybe you can
    use some of their code.


    Or if neither of the above work for you, you could try making your own local edit
    of LangChain to incorporate AutoGPTQ which shouldn''t be too hard to do.


    Good luck!'
  created_at: 2023-04-30 19:35:48+00:00
  edited: false
  hidden: false
  id: 644ed124a00f4b11d39a6088
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c58a12038261de466ab6fd21f319977.svg
      fullname: sieg chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cxfcxf
      type: user
    createdAt: '2023-05-01T00:31:03.000Z'
    data:
      edited: false
      editors:
      - cxfcxf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c58a12038261de466ab6fd21f319977.svg
          fullname: sieg chen
          isHf: false
          isPro: false
          name: cxfcxf
          type: user
        html: "<p>you can scape my code for it to work with langchain <a rel=\"nofollow\"\
          \ href=\"https://github.com/cxfcxf/embeddings\">https://github.com/cxfcxf/embeddings</a>,\
          \ it includes loading the model and use vector-store with qa_chain<br>this\
          \ model does seems better than vicuna-13b as use for embedding.</p>\n<pre><code\
          \ class=\"language-bash\">\u276F python embeddings.py --index-name state_of_the_union\
          \ run --model-dir /home/siegfried/model-gptq --model-name wizardlm-7b-4bits\
          \ --use-safetensors\nINFO    - Loading encoding model sentence-transformers/all-MiniLM-L6-v2...\n\
          INFO    - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n\
          INFO    - Index already exists\nINFO    - Loading Tokenizer from /home/siegfried/model-gptq/wizardlm-7b-4bits...\n\
          INFO    - Loading the model from /home/siegfried/model-gptq/wizardlm-7b-4bits...\n\
          INFO    - Loading gptq quantized models...\nWARNING - use_triton will force\
          \ moving the hole model to GPU, make sure you have enough VRAM.\nINFO  \
          \  - Found 3 unique KN Linear values.\nINFO    - Warming up autotune cache\
          \ ...\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:46&lt;00:00,  3.84s/it]\n\
          INFO    - creating transformer pipeline...\nThe model <span class=\"hljs-string\"\
          >'LlamaGPTQForCausalLM'</span> is not supported <span class=\"hljs-keyword\"\
          >for</span> text-generation. Supported models are [<span class=\"hljs-string\"\
          >'BartForCausalLM'</span>, <span class=\"hljs-string\">'BertLMHeadModel'</span>,\
          \ <span class=\"hljs-string\">'BertGenerationDecoder'</span>, <span class=\"\
          hljs-string\">'BigBirdForCausalLM'</span>, <span class=\"hljs-string\">'BigBirdPegasusForCausalLM'</span>,\
          \ <span class=\"hljs-string\">'BioGptForCausalLM'</span>, <span class=\"\
          hljs-string\">'BlenderbotForCausalLM'</span>, <span class=\"hljs-string\"\
          >'BlenderbotSmallForCausalLM'</span>, <span class=\"hljs-string\">'BloomForCausalLM'</span>,\
          \ <span class=\"hljs-string\">'CamembertForCausalLM'</span>, <span class=\"\
          hljs-string\">'CodeGenForCausalLM'</span>, <span class=\"hljs-string\">'CpmAntForCausalLM'</span>,\
          \ <span class=\"hljs-string\">'CTRLLMHeadModel'</span>, <span class=\"hljs-string\"\
          >'Data2VecTextForCausalLM'</span>, <span class=\"hljs-string\">'ElectraForCausalLM'</span>,\
          \ <span class=\"hljs-string\">'ErnieForCausalLM'</span>, <span class=\"\
          hljs-string\">'GitForCausalLM'</span>, <span class=\"hljs-string\">'GPT2LMHeadModel'</span>,\
          \ <span class=\"hljs-string\">'GPT2LMHeadModel'</span>, <span class=\"hljs-string\"\
          >'GPTBigCodeForCausalLM'</span>, <span class=\"hljs-string\">'GPTNeoForCausalLM'</span>,\
          \ <span class=\"hljs-string\">'GPTNeoXForCausalLM'</span>, <span class=\"\
          hljs-string\">'GPTNeoXJapaneseForCausalLM'</span>, <span class=\"hljs-string\"\
          >'GPTJForCausalLM'</span>, <span class=\"hljs-string\">'LlamaForCausalLM'</span>,\
          \ <span class=\"hljs-string\">'MarianForCausalLM'</span>, <span class=\"\
          hljs-string\">'MBartForCausalLM'</span>, <span class=\"hljs-string\">'MegaForCausalLM'</span>,\
          \ <span class=\"hljs-string\">'MegatronBertForCausalLM'</span>, <span class=\"\
          hljs-string\">'MvpForCausalLM'</span>, <span class=\"hljs-string\">'OpenAIGPTLMHeadModel'</span>,\
          \ <span class=\"hljs-string\">'OPTForCausalLM'</span>, <span class=\"hljs-string\"\
          >'PegasusForCausalLM'</span>, <span class=\"hljs-string\">'PLBartForCausalLM'</span>,\
          \ <span class=\"hljs-string\">'ProphetNetForCausalLM'</span>, <span class=\"\
          hljs-string\">'QDQBertLMHeadModel'</span>, <span class=\"hljs-string\">'ReformerModelWithLMHead'</span>,\
          \ <span class=\"hljs-string\">'RemBertForCausalLM'</span>, <span class=\"\
          hljs-string\">'RobertaForCausalLM'</span>, <span class=\"hljs-string\">'RobertaPreLayerNormForCausalLM'</span>,\
          \ <span class=\"hljs-string\">'RoCBertForCausalLM'</span>, <span class=\"\
          hljs-string\">'RoFormerForCausalLM'</span>, <span class=\"hljs-string\"\
          >'Speech2Text2ForCausalLM'</span>, <span class=\"hljs-string\">'TransfoXLLMHeadModel'</span>,\
          \ <span class=\"hljs-string\">'TrOCRForCausalLM'</span>, <span class=\"\
          hljs-string\">'XGLMForCausalLM'</span>, <span class=\"hljs-string\">'XLMWithLMHeadModel'</span>,\
          \ <span class=\"hljs-string\">'XLMProphetNetForCausalLM'</span>, <span class=\"\
          hljs-string\">'XLMRobertaForCausalLM'</span>, <span class=\"hljs-string\"\
          >'XLMRobertaXLForCausalLM'</span>, <span class=\"hljs-string\">'XLNetLMHeadModel'</span>,\
          \ <span class=\"hljs-string\">'XmodForCausalLM'</span>].\nINFO    - creating\
          \ chain...\nINFO    - Loading Q&amp;A chain...\nRunning on <span class=\"\
          hljs-built_in\">local</span> URL:  http://127.0.0.1:7860\n</code></pre>\n"
        raw: "you can scape my code for it to work with langchain https://github.com/cxfcxf/embeddings,\
          \ it includes loading the model and use vector-store with qa_chain\nthis\
          \ model does seems better than vicuna-13b as use for embedding.\n\n```bash\n\
          \u276F python embeddings.py --index-name state_of_the_union run --model-dir\
          \ /home/siegfried/model-gptq --model-name wizardlm-7b-4bits --use-safetensors\n\
          INFO    - Loading encoding model sentence-transformers/all-MiniLM-L6-v2...\n\
          INFO    - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n\
          INFO    - Index already exists\nINFO    - Loading Tokenizer from /home/siegfried/model-gptq/wizardlm-7b-4bits...\n\
          INFO    - Loading the model from /home/siegfried/model-gptq/wizardlm-7b-4bits...\n\
          INFO    - Loading gptq quantized models...\nWARNING - use_triton will force\
          \ moving the hole model to GPU, make sure you have enough VRAM.\nINFO  \
          \  - Found 3 unique KN Linear values.\nINFO    - Warming up autotune cache\
          \ ...\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:46<00:00,  3.84s/it]\n\
          INFO    - creating transformer pipeline...\nThe model 'LlamaGPTQForCausalLM'\
          \ is not supported for text-generation. Supported models are ['BartForCausalLM',\
          \ 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
          \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
          \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
          \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
          \ 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM',\
          \ 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM',\
          \ 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM',\
          \ 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
          \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
          \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
          \ 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
          \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
          \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel',\
          \ 'XmodForCausalLM'].\nINFO    - creating chain...\nINFO    - Loading Q&A\
          \ chain...\nRunning on local URL:  http://127.0.0.1:7860\n```"
        updatedAt: '2023-05-01T00:31:03.243Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 644f0847ddf20748b0636489
    type: comment
  author: cxfcxf
  content: "you can scape my code for it to work with langchain https://github.com/cxfcxf/embeddings,\
    \ it includes loading the model and use vector-store with qa_chain\nthis model\
    \ does seems better than vicuna-13b as use for embedding.\n\n```bash\n\u276F python\
    \ embeddings.py --index-name state_of_the_union run --model-dir /home/siegfried/model-gptq\
    \ --model-name wizardlm-7b-4bits --use-safetensors\nINFO    - Loading encoding\
    \ model sentence-transformers/all-MiniLM-L6-v2...\nINFO    - Load pretrained SentenceTransformer:\
    \ sentence-transformers/all-MiniLM-L6-v2\nINFO    - Index already exists\nINFO\
    \    - Loading Tokenizer from /home/siegfried/model-gptq/wizardlm-7b-4bits...\n\
    INFO    - Loading the model from /home/siegfried/model-gptq/wizardlm-7b-4bits...\n\
    INFO    - Loading gptq quantized models...\nWARNING - use_triton will force moving\
    \ the hole model to GPU, make sure you have enough VRAM.\nINFO    - Found 3 unique\
    \ KN Linear values.\nINFO    - Warming up autotune cache ...\n100%|\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:46<00:00,  3.84s/it]\nINFO\
    \    - creating transformer pipeline...\nThe model 'LlamaGPTQForCausalLM' is not\
    \ supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel',\
    \ 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
    \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
    \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
    \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
    \ 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM',\
    \ 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
    \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
    \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM',\
    \ 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel',\
    \ 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM',\
    \ 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
    \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
    \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n\
    INFO    - creating chain...\nINFO    - Loading Q&A chain...\nRunning on local\
    \ URL:  http://127.0.0.1:7860\n```"
  created_at: 2023-04-30 23:31:03+00:00
  edited: false
  hidden: false
  id: 644f0847ddf20748b0636489
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9d47673a8d8cb97fedee65a32e6677c.svg
      fullname: Daniel Thanos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: d3ztr0yur
      type: user
    createdAt: '2023-05-01T02:15:30.000Z'
    data:
      edited: false
      editors:
      - d3ztr0yur
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9d47673a8d8cb97fedee65a32e6677c.svg
          fullname: Daniel Thanos
          isHf: false
          isPro: false
          name: d3ztr0yur
          type: user
        html: '<p>Thank you both for the thoughtful responses and help. I will look
          over both approaches.</p>

          '
        raw: Thank you both for the thoughtful responses and help. I will look over
          both approaches.
        updatedAt: '2023-05-01T02:15:30.253Z'
      numEdits: 0
      reactions: []
    id: 644f20c2a00f4b11d3a12018
    type: comment
  author: d3ztr0yur
  content: Thank you both for the thoughtful responses and help. I will look over
    both approaches.
  created_at: 2023-05-01 01:15:30+00:00
  edited: false
  hidden: false
  id: 644f20c2a00f4b11d3a12018
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-01T09:21:57.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;cxfcxf&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cxfcxf\">@<span class=\"\
          underline\">cxfcxf</span></a></span>\n\n\t</span></span> that looks really\
          \ good!</p>\n<p>Maybe you should try a PR to LangChain to get AutoGPTQ support\
          \ added?</p>\n"
        raw: 'Hey @cxfcxf that looks really good!


          Maybe you should try a PR to LangChain to get AutoGPTQ support added?'
        updatedAt: '2023-05-01T09:21:57.003Z'
      numEdits: 0
      reactions: []
    id: 644f84b5577838187ef3fd91
    type: comment
  author: TheBloke
  content: 'Hey @cxfcxf that looks really good!


    Maybe you should try a PR to LangChain to get AutoGPTQ support added?'
  created_at: 2023-05-01 08:21:57+00:00
  edited: false
  hidden: false
  id: 644f84b5577838187ef3fd91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9d47673a8d8cb97fedee65a32e6677c.svg
      fullname: Daniel Thanos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: d3ztr0yur
      type: user
    createdAt: '2023-05-03T03:53:04.000Z'
    data:
      edited: false
      editors:
      - d3ztr0yur
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9d47673a8d8cb97fedee65a32e6677c.svg
          fullname: Daniel Thanos
          isHf: false
          isPro: false
          name: d3ztr0yur
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cxfcxf&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cxfcxf\">@<span class=\"\
          underline\">cxfcxf</span></a></span>\n\n\t</span></span> I looked at your\
          \ repo and used it to adapt for my use case. Thanks for sharing. However\
          \ I am still running into an error. FileNotFoundError: [Errno 2] No such\
          \ file or directory: '/wizardLM-7B-GPTQ/quantize_config.json'. </p>\n<p>This\
          \ happens when I use this code: model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ device=\"cuda:0\", use_triton=True, use_safetensors=True), where path_to_model\
          \ is the local model directory.</p>\n<p>It seems it is a looking a special\
          \ json file that stores some config information about the quantization.\
          \ <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> this is not in\
          \ your repo here. Would one of you know where I might find this json, or\
          \ where I can look up what needs to go into it so I can create it? I also\
          \ looked at the auto_gptq repo, and I couldn't find any obvious reference\
          \ to this json in their examples. <span data-props=\"{&quot;user&quot;:&quot;cxfcxf&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/cxfcxf\"\
          >@<span class=\"underline\">cxfcxf</span></a></span>\n\n\t</span></span>\
          \ I am curious to know how you got the model to run without that aforementioned\
          \ json, or if you did, if you might consider putting it in your repo?</p>\n\
          <p>Many Thanks!</p>\n"
        raw: "@cxfcxf I looked at your repo and used it to adapt for my use case.\
          \ Thanks for sharing. However I am still running into an error. FileNotFoundError:\
          \ [Errno 2] No such file or directory: '/wizardLM-7B-GPTQ/quantize_config.json'.\
          \ \n\nThis happens when I use this code: model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ device=\"cuda:0\", use_triton=True, use_safetensors=True), where path_to_model\
          \ is the local model directory.\n\nIt seems it is a looking a special json\
          \ file that stores some config information about the quantization. @TheBloke\
          \ this is not in your repo here. Would one of you know where I might find\
          \ this json, or where I can look up what needs to go into it so I can create\
          \ it? I also looked at the auto_gptq repo, and I couldn't find any obvious\
          \ reference to this json in their examples. @cxfcxf I am curious to know\
          \ how you got the model to run without that aforementioned json, or if you\
          \ did, if you might consider putting it in your repo?\n\nMany Thanks!"
        updatedAt: '2023-05-03T03:53:04.112Z'
      numEdits: 0
      reactions: []
    id: 6451daa0b3f75261a7e940ae
    type: comment
  author: d3ztr0yur
  content: "@cxfcxf I looked at your repo and used it to adapt for my use case. Thanks\
    \ for sharing. However I am still running into an error. FileNotFoundError: [Errno\
    \ 2] No such file or directory: '/wizardLM-7B-GPTQ/quantize_config.json'. \n\n\
    This happens when I use this code: model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
    \ device=\"cuda:0\", use_triton=True, use_safetensors=True), where path_to_model\
    \ is the local model directory.\n\nIt seems it is a looking a special json file\
    \ that stores some config information about the quantization. @TheBloke this is\
    \ not in your repo here. Would one of you know where I might find this json, or\
    \ where I can look up what needs to go into it so I can create it? I also looked\
    \ at the auto_gptq repo, and I couldn't find any obvious reference to this json\
    \ in their examples. @cxfcxf I am curious to know how you got the model to run\
    \ without that aforementioned json, or if you did, if you might consider putting\
    \ it in your repo?\n\nMany Thanks!"
  created_at: 2023-05-03 02:53:04+00:00
  edited: false
  hidden: false
  id: 6451daa0b3f75261a7e940ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c58a12038261de466ab6fd21f319977.svg
      fullname: sieg chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cxfcxf
      type: user
    createdAt: '2023-05-03T04:01:59.000Z'
    data:
      edited: false
      editors:
      - cxfcxf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c58a12038261de466ab6fd21f319977.svg
          fullname: sieg chen
          isHf: false
          isPro: false
          name: cxfcxf
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;cxfcxf&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/cxfcxf\"\
          >@<span class=\"underline\">cxfcxf</span></a></span>\n\n\t</span></span>\
          \ I looked at your repo and used it to adapt for my use case. Thanks for\
          \ sharing. However I am still running into an error. FileNotFoundError:\
          \ [Errno 2] No such file or directory: '/wizardLM-7B-GPTQ/quantize_config.json'.\
          \ </p>\n<p>This happens when I use this code: model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ device=\"cuda:0\", use_triton=True, use_safetensors=True), where path_to_model\
          \ is the local model directory.</p>\n<p>It seems it is a looking a special\
          \ json file that stores some config information about the quantization.\
          \ <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> this is not in\
          \ your repo here. Would one of you know where I might find this json, or\
          \ where I can look up what needs to go into it so I can create it? I also\
          \ looked at the auto_gptq repo, and I couldn't find any obvious reference\
          \ to this json in their examples. <span data-props=\"{&quot;user&quot;:&quot;cxfcxf&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/cxfcxf\"\
          >@<span class=\"underline\">cxfcxf</span></a></span>\n\n\t</span></span>\
          \ I am curious to know how you got the model to run without that aforementioned\
          \ json, or if you did, if you might consider putting it in your repo?</p>\n\
          <p>Many Thanks!</p>\n</blockquote>\n<p>yup you need this file cause i m\
          \ using the autogptq class to load it</p>\n<pre><code class=\"language-bash\"\
          >\u276F <span class=\"hljs-built_in\">cat</span> quantize_config.json\n\
          {\n  <span class=\"hljs-string\">\"bits\"</span>: 4,\n  <span class=\"hljs-string\"\
          >\"damp_percent\"</span>: 0.01,\n  <span class=\"hljs-string\">\"desc_act\"\
          </span>: <span class=\"hljs-literal\">true</span>,\n  <span class=\"hljs-string\"\
          >\"group_size\"</span>: 128\n}\n</code></pre>\n<p>you can just place this\
          \ file in</p>\n"
        raw: "> @cxfcxf I looked at your repo and used it to adapt for my use case.\
          \ Thanks for sharing. However I am still running into an error. FileNotFoundError:\
          \ [Errno 2] No such file or directory: '/wizardLM-7B-GPTQ/quantize_config.json'.\
          \ \n> \n> This happens when I use this code: model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ device=\"cuda:0\", use_triton=True, use_safetensors=True), where path_to_model\
          \ is the local model directory.\n> \n> It seems it is a looking a special\
          \ json file that stores some config information about the quantization.\
          \ @TheBloke this is not in your repo here. Would one of you know where I\
          \ might find this json, or where I can look up what needs to go into it\
          \ so I can create it? I also looked at the auto_gptq repo, and I couldn't\
          \ find any obvious reference to this json in their examples. @cxfcxf I am\
          \ curious to know how you got the model to run without that aforementioned\
          \ json, or if you did, if you might consider putting it in your repo?\n\
          > \n> Many Thanks!\n\nyup you need this file cause i m using the autogptq\
          \ class to load it\n```bash\n\u276F cat quantize_config.json\n{\n  \"bits\"\
          : 4,\n  \"damp_percent\": 0.01,\n  \"desc_act\": true,\n  \"group_size\"\
          : 128\n}\n```\nyou can just place this file in"
        updatedAt: '2023-05-03T04:01:59.616Z'
      numEdits: 0
      reactions: []
    id: 6451dcb75fb40b9f50bf977f
    type: comment
  author: cxfcxf
  content: "> @cxfcxf I looked at your repo and used it to adapt for my use case.\
    \ Thanks for sharing. However I am still running into an error. FileNotFoundError:\
    \ [Errno 2] No such file or directory: '/wizardLM-7B-GPTQ/quantize_config.json'.\
    \ \n> \n> This happens when I use this code: model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
    \ device=\"cuda:0\", use_triton=True, use_safetensors=True), where path_to_model\
    \ is the local model directory.\n> \n> It seems it is a looking a special json\
    \ file that stores some config information about the quantization. @TheBloke this\
    \ is not in your repo here. Would one of you know where I might find this json,\
    \ or where I can look up what needs to go into it so I can create it? I also looked\
    \ at the auto_gptq repo, and I couldn't find any obvious reference to this json\
    \ in their examples. @cxfcxf I am curious to know how you got the model to run\
    \ without that aforementioned json, or if you did, if you might consider putting\
    \ it in your repo?\n> \n> Many Thanks!\n\nyup you need this file cause i m using\
    \ the autogptq class to load it\n```bash\n\u276F cat quantize_config.json\n{\n\
    \  \"bits\": 4,\n  \"damp_percent\": 0.01,\n  \"desc_act\": true,\n  \"group_size\"\
    : 128\n}\n```\nyou can just place this file in"
  created_at: 2023-05-03 03:01:59+00:00
  edited: false
  hidden: false
  id: 6451dcb75fb40b9f50bf977f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9d47673a8d8cb97fedee65a32e6677c.svg
      fullname: Daniel Thanos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: d3ztr0yur
      type: user
    createdAt: '2023-05-03T13:50:50.000Z'
    data:
      edited: false
      editors:
      - d3ztr0yur
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9d47673a8d8cb97fedee65a32e6677c.svg
          fullname: Daniel Thanos
          isHf: false
          isPro: false
          name: d3ztr0yur
          type: user
        html: '<p>Many Thanks!</p>

          '
        raw: Many Thanks!
        updatedAt: '2023-05-03T13:50:50.986Z'
      numEdits: 0
      reactions: []
    id: 645266ba3a794b2d9b0bbd43
    type: comment
  author: d3ztr0yur
  content: Many Thanks!
  created_at: 2023-05-03 12:50:50+00:00
  edited: false
  hidden: false
  id: 645266ba3a794b2d9b0bbd43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b3aba52a787709d0a95e67d50f528428.svg
      fullname: aharon maslin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ahron-maslin
      type: user
    createdAt: '2023-06-01T19:11:22.000Z'
    data:
      edited: false
      editors:
      - ahron-maslin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b3aba52a787709d0a95e67d50f528428.svg
          fullname: aharon maslin
          isHf: false
          isPro: false
          name: ahron-maslin
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cxfcxf&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cxfcxf\">@<span class=\"\
          underline\">cxfcxf</span></a></span>\n\n\t</span></span> how were you able\
          \ to download the model? I can pull the files with <code>git clone https://huggingface.co/TheBloke/wizardLM-7B-GPTQ</code>,\
          \ but when I run it I get this <code>FileNotFoundError: Could not find model\
          \ at /content/wizardLM-7B-GPTQ/gptq_model-4bit-128g.safetensors</code>.\
          \ I'm assuming I need to somehow combine the shards into one file?<br>Thank\
          \ you</p>\n"
        raw: '@cxfcxf how were you able to download the model? I can pull the files
          with `git clone https://huggingface.co/TheBloke/wizardLM-7B-GPTQ`, but when
          I run it I get this `FileNotFoundError: Could not find model at /content/wizardLM-7B-GPTQ/gptq_model-4bit-128g.safetensors`.
          I''m assuming I need to somehow combine the shards into one file?

          Thank you'
        updatedAt: '2023-06-01T19:11:22.791Z'
      numEdits: 0
      reactions: []
    id: 6478ed5ac68a021fbba307ab
    type: comment
  author: ahron-maslin
  content: '@cxfcxf how were you able to download the model? I can pull the files
    with `git clone https://huggingface.co/TheBloke/wizardLM-7B-GPTQ`, but when I
    run it I get this `FileNotFoundError: Could not find model at /content/wizardLM-7B-GPTQ/gptq_model-4bit-128g.safetensors`.
    I''m assuming I need to somehow combine the shards into one file?

    Thank you'
  created_at: 2023-06-01 18:11:22+00:00
  edited: false
  hidden: false
  id: 6478ed5ac68a021fbba307ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-06-24T13:56:31.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6156427264213562
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>If you''re still working on this, I''ve added ooba support via the
          API to langchain.  You can follow the example notebook here:</p>

          <p><a rel="nofollow" href="https://github.com/hwchase17/langchain/blob/2518e6c95b37425230225d3c0f016ad79a0ef68d/docs/extras/modules/model_io/models/llms/integrations/textgen.ipynb#L8">https://github.com/hwchase17/langchain/blob/2518e6c95b37425230225d3c0f016ad79a0ef68d/docs/extras/modules/model_io/models/llms/integrations/textgen.ipynb#L8</a></p>

          <p>Or just use this code snippet:</p>

          <pre><code>from langchain.llms import TextGen

          llm = TextGen(model_url="http://localhost:5000")

          </code></pre>

          '
        raw: 'If you''re still working on this, I''ve added ooba support via the API
          to langchain.  You can follow the example notebook here:


          https://github.com/hwchase17/langchain/blob/2518e6c95b37425230225d3c0f016ad79a0ef68d/docs/extras/modules/model_io/models/llms/integrations/textgen.ipynb#L8


          Or just use this code snippet:

          ```

          from langchain.llms import TextGen

          llm = TextGen(model_url="http://localhost:5000")

          ```'
        updatedAt: '2023-06-24T13:56:31.866Z'
      numEdits: 0
      reactions: []
    id: 6496f60f6d6c7d9cb8aca0a3
    type: comment
  author: LoneStriker
  content: 'If you''re still working on this, I''ve added ooba support via the API
    to langchain.  You can follow the example notebook here:


    https://github.com/hwchase17/langchain/blob/2518e6c95b37425230225d3c0f016ad79a0ef68d/docs/extras/modules/model_io/models/llms/integrations/textgen.ipynb#L8


    Or just use this code snippet:

    ```

    from langchain.llms import TextGen

    llm = TextGen(model_url="http://localhost:5000")

    ```'
  created_at: 2023-06-24 12:56:31+00:00
  edited: false
  hidden: false
  id: 6496f60f6d6c7d9cb8aca0a3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: TheBloke/wizardLM-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: Error message when loading through HuggingFace Transformers/Langchain
