!!python/object:huggingface_hub.community.DiscussionWithDetails
author: simion314
conflicting_files: null
created_at: 2023-04-26 15:49:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c75c3aa5d5f0d8432a7bb43689d54b64.svg
      fullname: P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: simion314
      type: user
    createdAt: '2023-04-26T16:49:53.000Z'
    data:
      edited: false
      editors:
      - simion314
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c75c3aa5d5f0d8432a7bb43689d54b64.svg
          fullname: P
          isHf: false
          isPro: false
          name: simion314
          type: user
        html: "<p>I checked the hash of the model , and re-downloaded the json files\
          \ again.<br>Your vicuna model works fine for me. This is not a major issue\
          \ , but wanted to report it.</p>\n<p>Thee error is </p>\n<pre><code>\nLoading\
          \ wizard-llama-7B-GPTQ-4bit-128g.no-act-order...\nFound the following quantized\
          \ model: models/wizard-llama-7B-GPTQ-4bit-128g.no-act-order/wizard-llama-7B-GPTQ-4bit-128g.no-act-order.safetensors\n\
          Loading model ...\nDone.\nTraceback (most recent call last):\n  File \"\
          /giant/APPS/AI/oobabooga_linux/text-generation-webui/server.py\", line 914,\
          \ in \n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"/giant/APPS/AI/oobabooga_linux/text-generation-webui/modules/models.py\"\
          , line 240, in load_model\n    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\"), clean_up_tokenization_spaces=True)\n\
          \  File \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1811, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1965, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \  File \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\n    self.sp_model.Load(vocab_file)\n  File \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 905, in Load\n    return self.LoadFromFile(model_file)\n  File \"\
          /giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 310, in LoadFromFile\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto-&gt;ParseFromArray(serialized.data(),\
          \ serialized.size())] \n\nDone!\n</code></pre>"
        raw: "I checked the hash of the model , and re-downloaded the json files again.\
          \ \r\nYour vicuna model works fine for me. This is not a major issue , but\
          \ wanted to report it.\r\n\r\nThee error is \r\n<pre><code>\r\nLoading wizard-llama-7B-GPTQ-4bit-128g.no-act-order...\r\
          \nFound the following quantized model: models/wizard-llama-7B-GPTQ-4bit-128g.no-act-order/wizard-llama-7B-GPTQ-4bit-128g.no-act-order.safetensors\r\
          \nLoading model ...\r\nDone.\r\nTraceback (most recent call last):\r\n \
          \ File \"/giant/APPS/AI/oobabooga_linux/text-generation-webui/server.py\"\
          , line 914, in <module>\r\n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\
          \n  File \"/giant/APPS/AI/oobabooga_linux/text-generation-webui/modules/models.py\"\
          , line 240, in load_model\r\n    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\"), clean_up_tokenization_spaces=True)\r\
          \n  File \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1811, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \  File \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1965, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
          \n  File \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n  File \"\
          /giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n  File\
          \ \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
          \ serialized.size())] \r\n\r\nDone!\r\n</code></pre>"
        updatedAt: '2023-04-26T16:49:53.021Z'
      numEdits: 0
      reactions: []
    id: 64495631eb7db8f70fb5f48c
    type: comment
  author: simion314
  content: "I checked the hash of the model , and re-downloaded the json files again.\
    \ \r\nYour vicuna model works fine for me. This is not a major issue , but wanted\
    \ to report it.\r\n\r\nThee error is \r\n<pre><code>\r\nLoading wizard-llama-7B-GPTQ-4bit-128g.no-act-order...\r\
    \nFound the following quantized model: models/wizard-llama-7B-GPTQ-4bit-128g.no-act-order/wizard-llama-7B-GPTQ-4bit-128g.no-act-order.safetensors\r\
    \nLoading model ...\r\nDone.\r\nTraceback (most recent call last):\r\n  File \"\
    /giant/APPS/AI/oobabooga_linux/text-generation-webui/server.py\", line 914, in\
    \ <module>\r\n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\
    \n  File \"/giant/APPS/AI/oobabooga_linux/text-generation-webui/modules/models.py\"\
    , line 240, in load_model\r\n    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
    {shared.args.model_dir}/{model_name}/\"), clean_up_tokenization_spaces=True)\r\
    \n  File \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1811, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1965, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n  File \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py\"\
    , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n  File \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
    , line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n  File \"/giant/APPS/AI/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
    , line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
    \ serialized.size())] \r\n\r\nDone!\r\n</code></pre>"
  created_at: 2023-04-26 15:49:53+00:00
  edited: false
  hidden: false
  id: 64495631eb7db8f70fb5f48c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-26T16:53:08.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Hmm, can you show me the contents of your model directory with ls
          -al</p>

          <p>Maybe it''s a problem with one of the other files, like the tokenizer</p>

          '
        raw: 'Hmm, can you show me the contents of your model directory with ls -al


          Maybe it''s a problem with one of the other files, like the tokenizer'
        updatedAt: '2023-04-26T16:53:08.767Z'
      numEdits: 0
      reactions: []
    id: 644956f4111b3bf6877bdc5b
    type: comment
  author: TheBloke
  content: 'Hmm, can you show me the contents of your model directory with ls -al


    Maybe it''s a problem with one of the other files, like the tokenizer'
  created_at: 2023-04-26 15:53:08+00:00
  edited: false
  hidden: false
  id: 644956f4111b3bf6877bdc5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c75c3aa5d5f0d8432a7bb43689d54b64.svg
      fullname: P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: simion314
      type: user
    createdAt: '2023-04-26T16:56:39.000Z'
    data:
      edited: false
      editors:
      - simion314
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c75c3aa5d5f0d8432a7bb43689d54b64.svg
          fullname: P
          isHf: false
          isPro: false
          name: simion314
          type: user
        html: '<p>Sure, btw I renamed the files and folder to incldue the word llama
          so it is autodetected as a llama model</p>

          <pre><code>

          -rw-rw-r-- 1 simi simi         21 apr 26 19:32 added_tokens.json

          -rw-rw-r-- 1 simi simi        555 apr 26 19:32 config.json

          -rw-rw-r-- 1 simi simi        132 apr 26 19:32 generation_config.json

          -rw-rw-r-- 1 simi simi       4793 apr 26 16:11 README.md

          -rw-rw-r-- 1 simi simi        435 apr 26 19:32 special_tokens_map.json

          -rw-rw-r-- 1 simi simi        727 apr 26 19:32 tokenizer_config.json

          -rw-rw-r-- 1 simi simi    1842846 apr 26 19:32 tokenizer.json

          -rw-rw-r-- 1 simi simi        131 apr 26 19:32 tokenizer.model

          -rw-rw-r-- 1 simi simi 3893998440 apr 26 19:13 wizard-llama-7B-GPTQ-4bit-128g.no-act-order.safetensors

          -rw-rw-r-- 1 simi simi 4521170856 apr 26 16:22 wizard-llama-7B-GPTQ-4bit-128g.ooba.no-act-order.ptbakup

          </code></pre>'
        raw: 'Sure, btw I renamed the files and folder to incldue the word llama so
          it is autodetected as a llama model


          <pre><code>

          -rw-rw-r-- 1 simi simi         21 apr 26 19:32 added_tokens.json

          -rw-rw-r-- 1 simi simi        555 apr 26 19:32 config.json

          -rw-rw-r-- 1 simi simi        132 apr 26 19:32 generation_config.json

          -rw-rw-r-- 1 simi simi       4793 apr 26 16:11 README.md

          -rw-rw-r-- 1 simi simi        435 apr 26 19:32 special_tokens_map.json

          -rw-rw-r-- 1 simi simi        727 apr 26 19:32 tokenizer_config.json

          -rw-rw-r-- 1 simi simi    1842846 apr 26 19:32 tokenizer.json

          -rw-rw-r-- 1 simi simi        131 apr 26 19:32 tokenizer.model

          -rw-rw-r-- 1 simi simi 3893998440 apr 26 19:13 wizard-llama-7B-GPTQ-4bit-128g.no-act-order.safetensors

          -rw-rw-r-- 1 simi simi 4521170856 apr 26 16:22 wizard-llama-7B-GPTQ-4bit-128g.ooba.no-act-order.ptbakup

          </code></pre>'
        updatedAt: '2023-04-26T16:56:39.607Z'
      numEdits: 0
      reactions: []
    id: 644957c7b3cd701e0a043a9c
    type: comment
  author: simion314
  content: 'Sure, btw I renamed the files and folder to incldue the word llama so
    it is autodetected as a llama model


    <pre><code>

    -rw-rw-r-- 1 simi simi         21 apr 26 19:32 added_tokens.json

    -rw-rw-r-- 1 simi simi        555 apr 26 19:32 config.json

    -rw-rw-r-- 1 simi simi        132 apr 26 19:32 generation_config.json

    -rw-rw-r-- 1 simi simi       4793 apr 26 16:11 README.md

    -rw-rw-r-- 1 simi simi        435 apr 26 19:32 special_tokens_map.json

    -rw-rw-r-- 1 simi simi        727 apr 26 19:32 tokenizer_config.json

    -rw-rw-r-- 1 simi simi    1842846 apr 26 19:32 tokenizer.json

    -rw-rw-r-- 1 simi simi        131 apr 26 19:32 tokenizer.model

    -rw-rw-r-- 1 simi simi 3893998440 apr 26 19:13 wizard-llama-7B-GPTQ-4bit-128g.no-act-order.safetensors

    -rw-rw-r-- 1 simi simi 4521170856 apr 26 16:22 wizard-llama-7B-GPTQ-4bit-128g.ooba.no-act-order.ptbakup

    </code></pre>'
  created_at: 2023-04-26 15:56:39+00:00
  edited: false
  hidden: false
  id: 644957c7b3cd701e0a043a9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c75c3aa5d5f0d8432a7bb43689d54b64.svg
      fullname: P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: simion314
      type: user
    createdAt: '2023-05-01T08:37:54.000Z'
    data:
      edited: false
      editors:
      - simion314
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c75c3aa5d5f0d8432a7bb43689d54b64.svg
          fullname: P
          isHf: false
          isPro: false
          name: simion314
          type: user
        html: '<p>Hi, so it was my fault, the file tokenizer.model was not correct,
          I hope this will help others that hit this error.</p>

          '
        raw: Hi, so it was my fault, the file tokenizer.model was not correct, I hope
          this will help others that hit this error.
        updatedAt: '2023-05-01T08:37:54.156Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - derkmed
    id: 644f7a6220ba3e3e4be21c55
    type: comment
  author: simion314
  content: Hi, so it was my fault, the file tokenizer.model was not correct, I hope
    this will help others that hit this error.
  created_at: 2023-05-01 07:37:54+00:00
  edited: false
  hidden: false
  id: 644f7a6220ba3e3e4be21c55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-01T11:29:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry, I missed your response earlier - yeah only 131 bytes in tokenizer.model!</p>

          <p>Glad it''s sorted now.</p>

          '
        raw: 'Sorry, I missed your response earlier - yeah only 131 bytes in tokenizer.model!


          Glad it''s sorted now.'
        updatedAt: '2023-05-01T11:29:16.807Z'
      numEdits: 0
      reactions: []
      relatedEventId: 644fa28cd5f7dafcfa5c97a4
    id: 644fa28cd5f7dafcfa5c97a3
    type: comment
  author: TheBloke
  content: 'Sorry, I missed your response earlier - yeah only 131 bytes in tokenizer.model!


    Glad it''s sorted now.'
  created_at: 2023-05-01 10:29:16+00:00
  edited: false
  hidden: false
  id: 644fa28cd5f7dafcfa5c97a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-01T11:29:16.000Z'
    data:
      status: closed
    id: 644fa28cd5f7dafcfa5c97a4
    type: status-change
  author: TheBloke
  created_at: 2023-05-01 10:29:16+00:00
  id: 644fa28cd5f7dafcfa5c97a4
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/wizardLM-7B-GPTQ
repo_type: model
status: closed
target_branch: null
title: 'I get error RuntimeError: Internal: src/sentencepiece'
