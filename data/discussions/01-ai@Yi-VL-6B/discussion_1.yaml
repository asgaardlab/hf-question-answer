!!python/object:huggingface_hub.community.DiscussionWithDetails
author: teowu
conflicting_files: null
created_at: 2024-01-22 09:41:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/IfbYJI9vnmwR-NV44Dkqg.jpeg?w=200&h=200&f=face
      fullname: Haoning Wu @ VQAssessment Group
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teowu
      type: user
    createdAt: '2024-01-22T09:41:16.000Z'
    data:
      edited: false
      editors:
      - teowu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6714231967926025
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/IfbYJI9vnmwR-NV44Dkqg.jpeg?w=200&h=200&f=face
          fullname: Haoning Wu @ VQAssessment Group
          isHf: false
          isPro: false
          name: teowu
          type: user
        html: '<p>It seems Yi-VL-6B uses a <code>mlp2x_gelu_Norm</code> as multimodal
          projector. Will this be with any difference from the original llava''s <code>mlp2x_gelu</code>?
          Is the following implementation a correct one to load the model?</p>

          <p><code> Sequential(   (0): Linear(in_features=1280, out_features=4096,
          bias=True)   (1): GELU(approximate=''none'')   (2): LayerNorm((4096,), eps=1e-05,
          elementwise_affine=False)   (3): Linear(in_features=4096, out_features=4096,
          bias=True) )</code></p>

          '
        raw: "It seems Yi-VL-6B uses a `mlp2x_gelu_Norm` as multimodal projector.\
          \ Will this be with any difference from the original llava's `mlp2x_gelu`?\
          \ Is the following implementation a correct one to load the model?\r\n\r\
          \n```\r\nSequential(\r\n  (0): Linear(in_features=1280, out_features=4096,\
          \ bias=True)\r\n  (1): GELU(approximate='none')\r\n  (2): LayerNorm((4096,),\
          \ eps=1e-05, elementwise_affine=False)\r\n  (3): Linear(in_features=4096,\
          \ out_features=4096, bias=True)\r\n)```"
        updatedAt: '2024-01-22T09:41:16.947Z'
      numEdits: 0
      reactions: []
    id: 65ae383c81b87410112e3a32
    type: comment
  author: teowu
  content: "It seems Yi-VL-6B uses a `mlp2x_gelu_Norm` as multimodal projector. Will\
    \ this be with any difference from the original llava's `mlp2x_gelu`? Is the following\
    \ implementation a correct one to load the model?\r\n\r\n```\r\nSequential(\r\n\
    \  (0): Linear(in_features=1280, out_features=4096, bias=True)\r\n  (1): GELU(approximate='none')\r\
    \n  (2): LayerNorm((4096,), eps=1e-05, elementwise_affine=False)\r\n  (3): Linear(in_features=4096,\
    \ out_features=4096, bias=True)\r\n)```"
  created_at: 2024-01-22 09:41:16+00:00
  edited: false
  hidden: false
  id: 65ae383c81b87410112e3a32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg
      fullname: mengziyang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zylate
      type: user
    createdAt: '2024-01-22T10:01:33.000Z'
    data:
      edited: false
      editors:
      - zylate
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7897775173187256
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg
          fullname: mengziyang
          isHf: false
          isPro: false
          name: zylate
          type: user
        html: "<p>I think the weight is quite different compared with llava, llava\
          \ has 2 weight and 2 bias , yi has 4 bias and 4 weight<br><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/653614073f4248157d60ccdc/ZdA4I2Dnn-wb8FJK6g3D6.png\"\
          ><img alt=\"\u622A\u5C4F2024-01-22 17.59.13.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/653614073f4248157d60ccdc/ZdA4I2Dnn-wb8FJK6g3D6.png\"\
          ></a></p>\n"
        raw: "I think the weight is quite different compared with llava, llava has\
          \ 2 weight and 2 bias , yi has 4 bias and 4 weight \n![\u622A\u5C4F2024-01-22\
          \ 17.59.13.png](https://cdn-uploads.huggingface.co/production/uploads/653614073f4248157d60ccdc/ZdA4I2Dnn-wb8FJK6g3D6.png)\n"
        updatedAt: '2024-01-22T10:01:33.115Z'
      numEdits: 0
      reactions: []
    id: 65ae3cfd0ee3b941f59bc186
    type: comment
  author: zylate
  content: "I think the weight is quite different compared with llava, llava has 2\
    \ weight and 2 bias , yi has 4 bias and 4 weight \n![\u622A\u5C4F2024-01-22 17.59.13.png](https://cdn-uploads.huggingface.co/production/uploads/653614073f4248157d60ccdc/ZdA4I2Dnn-wb8FJK6g3D6.png)\n"
  created_at: 2024-01-22 10:01:33+00:00
  edited: false
  hidden: false
  id: 65ae3cfd0ee3b941f59bc186
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/IfbYJI9vnmwR-NV44Dkqg.jpeg?w=200&h=200&f=face
      fullname: Haoning Wu @ VQAssessment Group
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teowu
      type: user
    createdAt: '2024-01-22T10:07:44.000Z'
    data:
      edited: false
      editors:
      - teowu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9725555777549744
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/IfbYJI9vnmwR-NV44Dkqg.jpeg?w=200&h=200&f=face
          fullname: Haoning Wu @ VQAssessment Group
          isHf: false
          isPro: false
          name: teowu
          type: user
        html: '<p>Yes, this is quite strange and seems very different from LLaVA''s
          MLP2x-GeLU.<br>Hope Yi team can assist on providing some code demos for
          this.</p>

          '
        raw: 'Yes, this is quite strange and seems very different from LLaVA''s MLP2x-GeLU.

          Hope Yi team can assist on providing some code demos for this.'
        updatedAt: '2024-01-22T10:07:44.290Z'
      numEdits: 0
      reactions: []
    id: 65ae3e700db970cd1dd4ac27
    type: comment
  author: teowu
  content: 'Yes, this is quite strange and seems very different from LLaVA''s MLP2x-GeLU.

    Hope Yi team can assist on providing some code demos for this.'
  created_at: 2024-01-22 10:07:44+00:00
  edited: false
  hidden: false
  id: 65ae3e700db970cd1dd4ac27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/864f294a95def120f902a74ddd2e711f.svg
      fullname: Whild Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Whidbey
      type: user
    createdAt: '2024-01-22T11:15:32.000Z'
    data:
      edited: false
      editors:
      - Whidbey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.38334497809410095
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/864f294a95def120f902a74ddd2e711f.svg
          fullname: Whild Lee
          isHf: false
          isPro: false
          name: Whidbey
          type: user
        html: "<p>I fixed it just now.</p>\n<p>Firstly, add the codes below in  /LLaVA/llava/model/multimodal_projector/builder.py</p>\n\
          <p>class MLP2xGELUNorm(nn.Module):<br>    def <strong>init</strong>(self,\
          \ input_size, hidden_size):<br>        super().<strong>init</strong>()<br>\
          \        self.linear1 = nn.Linear(input_size, hidden_size)<br>        self.gelu1\
          \ = nn.GELU()<br>        self.linear2 = nn.Linear(hidden_size, hidden_size)<br>\
          \        self.gelu2 = nn.GELU()</p>\n<pre><code>def forward(self, x):\n\
          \    x = self.linear1(x)\n    x = self.gelu1(x)\n    x = self.linear2(x)\n\
          \    x = self.gelu2(x)\n    return x\n</code></pre>\n<p>Then, add the codes\
          \ below in build_vision_projector()<br>if projector_type == 'mlp2x_gelu_Norm':<br>\
          \        return MLP2xGELUNorm(config.mm_hidden_size, config.hidden_size)</p>\n\
          <p>DONE!</p>\n"
        raw: "I fixed it just now.\n\nFirstly, add the codes below in  /LLaVA/llava/model/multimodal_projector/builder.py\n\
          \nclass MLP2xGELUNorm(nn.Module):\n    def __init__(self, input_size, hidden_size):\n\
          \        super().__init__()\n        self.linear1 = nn.Linear(input_size,\
          \ hidden_size)\n        self.gelu1 = nn.GELU()\n        self.linear2 = nn.Linear(hidden_size,\
          \ hidden_size)\n        self.gelu2 = nn.GELU()\n\n    def forward(self,\
          \ x):\n        x = self.linear1(x)\n        x = self.gelu1(x)\n        x\
          \ = self.linear2(x)\n        x = self.gelu2(x)\n        return x\n\nThen,\
          \ add the codes below in build_vision_projector()\nif projector_type ==\
          \ 'mlp2x_gelu_Norm':\n        return MLP2xGELUNorm(config.mm_hidden_size,\
          \ config.hidden_size)\n\nDONE!"
        updatedAt: '2024-01-22T11:15:32.411Z'
      numEdits: 0
      reactions: []
    id: 65ae4e541dac7b5be457f860
    type: comment
  author: Whidbey
  content: "I fixed it just now.\n\nFirstly, add the codes below in  /LLaVA/llava/model/multimodal_projector/builder.py\n\
    \nclass MLP2xGELUNorm(nn.Module):\n    def __init__(self, input_size, hidden_size):\n\
    \        super().__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n\
    \        self.gelu1 = nn.GELU()\n        self.linear2 = nn.Linear(hidden_size,\
    \ hidden_size)\n        self.gelu2 = nn.GELU()\n\n    def forward(self, x):\n\
    \        x = self.linear1(x)\n        x = self.gelu1(x)\n        x = self.linear2(x)\n\
    \        x = self.gelu2(x)\n        return x\n\nThen, add the codes below in build_vision_projector()\n\
    if projector_type == 'mlp2x_gelu_Norm':\n        return MLP2xGELUNorm(config.mm_hidden_size,\
    \ config.hidden_size)\n\nDONE!"
  created_at: 2024-01-22 11:15:32+00:00
  edited: false
  hidden: false
  id: 65ae4e541dac7b5be457f860
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg
      fullname: mengziyang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zylate
      type: user
    createdAt: '2024-01-22T11:23:08.000Z'
    data:
      edited: false
      editors:
      - zylate
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9892809987068176
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg
          fullname: mengziyang
          isHf: false
          isPro: false
          name: zylate
          type: user
        html: '<p>but the structure is same as yi team used?<br>if the structure is
          not same as they used , the capability is not same as well</p>

          '
        raw: 'but the structure is same as yi team used?

          if the structure is not same as they used , the capability is not same as
          well'
        updatedAt: '2024-01-22T11:23:08.036Z'
      numEdits: 0
      reactions: []
    id: 65ae501cd0a5cc99d586aabb
    type: comment
  author: zylate
  content: 'but the structure is same as yi team used?

    if the structure is not same as they used , the capability is not same as well'
  created_at: 2024-01-22 11:23:08+00:00
  edited: false
  hidden: false
  id: 65ae501cd0a5cc99d586aabb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg
      fullname: mengziyang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zylate
      type: user
    createdAt: '2024-01-22T11:29:46.000Z'
    data:
      edited: false
      editors:
      - zylate
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.20308317244052887
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg
          fullname: mengziyang
          isHf: false
          isPro: false
          name: zylate
          type: user
        html: "<p>I fixed<br>use mlp2x_glue and add 2 norm layer<br>Pytorch will save\
          \ layer normal parameter so it will be 4 weight parameter<br><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/653614073f4248157d60ccdc/yWu3bbyyTjh77SPAS09jS.png\"\
          ><img alt=\"\u622A\u5C4F2024-01-22 19.29.17.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/653614073f4248157d60ccdc/yWu3bbyyTjh77SPAS09jS.png\"\
          ></a></p>\n<p>find from yi webset</p>\n"
        raw: "I fixed\nuse mlp2x_glue and add 2 norm layer \nPytorch will save layer\
          \ normal parameter so it will be 4 weight parameter\n![\u622A\u5C4F2024-01-22\
          \ 19.29.17.png](https://cdn-uploads.huggingface.co/production/uploads/653614073f4248157d60ccdc/yWu3bbyyTjh77SPAS09jS.png)\n\
          \nfind from yi webset"
        updatedAt: '2024-01-22T11:29:46.356Z'
      numEdits: 0
      reactions: []
    id: 65ae51aa11a0a3ff61d7afdb
    type: comment
  author: zylate
  content: "I fixed\nuse mlp2x_glue and add 2 norm layer \nPytorch will save layer\
    \ normal parameter so it will be 4 weight parameter\n![\u622A\u5C4F2024-01-22\
    \ 19.29.17.png](https://cdn-uploads.huggingface.co/production/uploads/653614073f4248157d60ccdc/yWu3bbyyTjh77SPAS09jS.png)\n\
    \nfind from yi webset"
  created_at: 2024-01-22 11:29:46+00:00
  edited: false
  hidden: false
  id: 65ae51aa11a0a3ff61d7afdb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5830421b18d77836042d8b758c35eaa0.svg
      fullname: wuqo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wunu
      type: user
    createdAt: '2024-01-23T06:20:02.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/5830421b18d77836042d8b758c35eaa0.svg
          fullname: wuqo
          isHf: false
          isPro: false
          name: wunu
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2024-01-23T06:29:33.832Z'
      numEdits: 0
      reactions: []
    id: 65af5a921dac7b5be4e98d20
    type: comment
  author: wunu
  content: This comment has been hidden
  created_at: 2024-01-23 06:20:02+00:00
  edited: true
  hidden: true
  id: 65af5a921dac7b5be4e98d20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/864f294a95def120f902a74ddd2e711f.svg
      fullname: Whild Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Whidbey
      type: user
    createdAt: '2024-01-23T06:24:04.000Z'
    data:
      edited: true
      editors:
      - Whidbey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9228715896606445
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/864f294a95def120f902a74ddd2e711f.svg
          fullname: Whild Lee
          isHf: false
          isPro: false
          name: Whidbey
          type: user
        html: '<p>Thanks, I fixed it too.</p>

          <p>I modified the system prompt, and encountered another issue during the
          inference.<br>Through CLI inference proviede by LLaVA, this model looks
          like a repeater, sometimes repeating the last token, and at other times
          repeating an entire sentence. so does 34B. Moreover, there is a multi-round
          dialogue when generating one response.<br>It looks so strange.</p>

          '
        raw: 'Thanks, I fixed it too.


          I modified the system prompt, and encountered another issue during the inference.

          Through CLI inference proviede by LLaVA, this model looks like a repeater,
          sometimes repeating the last token, and at other times repeating an entire
          sentence. so does 34B. Moreover, there is a multi-round dialogue when generating
          one response.

          It looks so strange.'
        updatedAt: '2024-01-23T06:25:25.956Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - codingl2k1
    id: 65af5b84c92e09d5cdd3d2fa
    type: comment
  author: Whidbey
  content: 'Thanks, I fixed it too.


    I modified the system prompt, and encountered another issue during the inference.

    Through CLI inference proviede by LLaVA, this model looks like a repeater, sometimes
    repeating the last token, and at other times repeating an entire sentence. so
    does 34B. Moreover, there is a multi-round dialogue when generating one response.

    It looks so strange.'
  created_at: 2024-01-23 06:24:04+00:00
  edited: true
  hidden: false
  id: 65af5b84c92e09d5cdd3d2fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5830421b18d77836042d8b758c35eaa0.svg
      fullname: wuqo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wunu
      type: user
    createdAt: '2024-01-23T07:58:11.000Z'
    data:
      edited: false
      editors:
      - wunu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8018931746482849
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5830421b18d77836042d8b758c35eaa0.svg
          fullname: wuqo
          isHf: false
          isPro: false
          name: wunu
          type: user
        html: "<blockquote>\n<p>Thanks, I fixed it too.</p>\n<p>I modified the system\
          \ prompt, and encountered another issue during the inference.<br>Through\
          \ CLI inference proviede by LLaVA, this model looks like a repeater, sometimes\
          \ repeating the last token, and at other times repeating an entire sentence.\
          \ so does 34B. Moreover, there is a multi-round dialogue when generating\
          \ one response.<br>It looks so strange.</p>\n</blockquote>\n<p>I change\
          \ the value of <code>conv-mode</code> to llava_v0 and modify the value of\
          \ <code>conv.system</code> to \"This is a chat between an inquisitive human\
          \ and an AI assistant. Assume the role of the AI assistant. Read all the\
          \ images carefully, and respond to the human's questions with informative,\
          \ helpful, detailed and polite answers. \u8FD9\u662F\u4E00\u4E2A\u597D\u5947\
          \u7684\u4EBA\u7C7B\u548C\u4E00\u4E2A\u4EBA\u5DE5\u667A\u80FD\u52A9\u624B\
          \u4E4B\u95F4\u7684\u5BF9\u8BDD\u3002\u5047\u8BBE\u4F60\u626E\u6F14\u8FD9\
          \u4E2AAI\u52A9\u624B\u7684\u89D2\u8272\u3002\u4ED4\u7EC6\u9605\u8BFB\u6240\
          \u6709\u7684\u56FE\u50CF\uFF0C\u5E76\u5BF9\u4EBA\u7C7B\u7684\u95EE\u9898\
          \u505A\u51FA\u4FE1\u606F\u4E30\u5BCC\u3001\u6709\u5E2E\u52A9\u3001\u8BE6\
          \u7EC6\u7684\u548C\u793C\u8C8C\u7684\u56DE\u7B54\u3002\", it looks normal.</p>\n"
        raw: "> Thanks, I fixed it too.\n> \n> I modified the system prompt, and encountered\
          \ another issue during the inference.\n> Through CLI inference proviede\
          \ by LLaVA, this model looks like a repeater, sometimes repeating the last\
          \ token, and at other times repeating an entire sentence. so does 34B. Moreover,\
          \ there is a multi-round dialogue when generating one response.\n> It looks\
          \ so strange.\n\nI change the value of `conv-mode` to llava_v0 and modify\
          \ the value of `conv.system` to \"This is a chat between an inquisitive\
          \ human and an AI assistant. Assume the role of the AI assistant. Read all\
          \ the images carefully, and respond to the human's questions with informative,\
          \ helpful, detailed and polite answers. \u8FD9\u662F\u4E00\u4E2A\u597D\u5947\
          \u7684\u4EBA\u7C7B\u548C\u4E00\u4E2A\u4EBA\u5DE5\u667A\u80FD\u52A9\u624B\
          \u4E4B\u95F4\u7684\u5BF9\u8BDD\u3002\u5047\u8BBE\u4F60\u626E\u6F14\u8FD9\
          \u4E2AAI\u52A9\u624B\u7684\u89D2\u8272\u3002\u4ED4\u7EC6\u9605\u8BFB\u6240\
          \u6709\u7684\u56FE\u50CF\uFF0C\u5E76\u5BF9\u4EBA\u7C7B\u7684\u95EE\u9898\
          \u505A\u51FA\u4FE1\u606F\u4E30\u5BCC\u3001\u6709\u5E2E\u52A9\u3001\u8BE6\
          \u7EC6\u7684\u548C\u793C\u8C8C\u7684\u56DE\u7B54\u3002\", it looks normal."
        updatedAt: '2024-01-23T07:58:11.277Z'
      numEdits: 0
      reactions: []
    id: 65af719369cd2991ef80f69b
    type: comment
  author: wunu
  content: "> Thanks, I fixed it too.\n> \n> I modified the system prompt, and encountered\
    \ another issue during the inference.\n> Through CLI inference proviede by LLaVA,\
    \ this model looks like a repeater, sometimes repeating the last token, and at\
    \ other times repeating an entire sentence. so does 34B. Moreover, there is a\
    \ multi-round dialogue when generating one response.\n> It looks so strange.\n\
    \nI change the value of `conv-mode` to llava_v0 and modify the value of `conv.system`\
    \ to \"This is a chat between an inquisitive human and an AI assistant. Assume\
    \ the role of the AI assistant. Read all the images carefully, and respond to\
    \ the human's questions with informative, helpful, detailed and polite answers.\
    \ \u8FD9\u662F\u4E00\u4E2A\u597D\u5947\u7684\u4EBA\u7C7B\u548C\u4E00\u4E2A\u4EBA\
    \u5DE5\u667A\u80FD\u52A9\u624B\u4E4B\u95F4\u7684\u5BF9\u8BDD\u3002\u5047\u8BBE\
    \u4F60\u626E\u6F14\u8FD9\u4E2AAI\u52A9\u624B\u7684\u89D2\u8272\u3002\u4ED4\u7EC6\
    \u9605\u8BFB\u6240\u6709\u7684\u56FE\u50CF\uFF0C\u5E76\u5BF9\u4EBA\u7C7B\u7684\
    \u95EE\u9898\u505A\u51FA\u4FE1\u606F\u4E30\u5BCC\u3001\u6709\u5E2E\u52A9\u3001\
    \u8BE6\u7EC6\u7684\u548C\u793C\u8C8C\u7684\u56DE\u7B54\u3002\", it looks normal."
  created_at: 2024-01-23 07:58:11+00:00
  edited: false
  hidden: false
  id: 65af719369cd2991ef80f69b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5830421b18d77836042d8b758c35eaa0.svg
      fullname: wuqo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wunu
      type: user
    createdAt: '2024-01-23T08:51:35.000Z'
    data:
      edited: false
      editors:
      - wunu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9386984705924988
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5830421b18d77836042d8b758c35eaa0.svg
          fullname: wuqo
          isHf: false
          isPro: false
          name: wunu
          type: user
        html: "<p>Another issue I\u2019ve encountered is that when I test the question,\
          \ \u201CIs anyone smoking?\u201D I experience numerous hallucinations. some\
          \ other setting might be incorrect. What other adjustments might I need\
          \ to make?</p>\n"
        raw: "Another issue I\u2019ve encountered is that when I test the question,\
          \ \u201CIs anyone smoking?\u201D I experience numerous hallucinations. some\
          \ other setting might be incorrect. What other adjustments might I need\
          \ to make?"
        updatedAt: '2024-01-23T08:51:35.966Z'
      numEdits: 0
      reactions: []
    id: 65af7e17a134c07ddef1a473
    type: comment
  author: wunu
  content: "Another issue I\u2019ve encountered is that when I test the question,\
    \ \u201CIs anyone smoking?\u201D I experience numerous hallucinations. some other\
    \ setting might be incorrect. What other adjustments might I need to make?"
  created_at: 2024-01-23 08:51:35+00:00
  edited: false
  hidden: false
  id: 65af7e17a134c07ddef1a473
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg
      fullname: mengziyang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zylate
      type: user
    createdAt: '2024-01-23T09:06:10.000Z'
    data:
      edited: false
      editors:
      - zylate
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.906360387802124
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg
          fullname: mengziyang
          isHf: false
          isPro: false
          name: zylate
          type: user
        html: '<blockquote>

          <p>Thanks, I fixed it too.</p>

          <p>I modified the system prompt, and encountered another issue during the
          inference.<br>Through CLI inference proviede by LLaVA, this model looks
          like a repeater, sometimes repeating the last token, and at other times
          repeating an entire sentence. so does 34B. Moreover, there is a multi-round
          dialogue when generating one response.<br>It looks so strange.</p>

          </blockquote>

          <p>not work for me , can you provide the mlp_projector and cli code ?</p>

          '
        raw: "> Thanks, I fixed it too.\n> \n> I modified the system prompt, and encountered\
          \ another issue during the inference.\n> Through CLI inference proviede\
          \ by LLaVA, this model looks like a repeater, sometimes repeating the last\
          \ token, and at other times repeating an entire sentence. so does 34B. Moreover,\
          \ there is a multi-round dialogue when generating one response.\n> It looks\
          \ so strange.\n\nnot work for me , can you provide the mlp_projector and\
          \ cli code ?\n"
        updatedAt: '2024-01-23T09:06:10.079Z'
      numEdits: 0
      reactions: []
    id: 65af81824a23c7b6b0f8b9e0
    type: comment
  author: zylate
  content: "> Thanks, I fixed it too.\n> \n> I modified the system prompt, and encountered\
    \ another issue during the inference.\n> Through CLI inference proviede by LLaVA,\
    \ this model looks like a repeater, sometimes repeating the last token, and at\
    \ other times repeating an entire sentence. so does 34B. Moreover, there is a\
    \ multi-round dialogue when generating one response.\n> It looks so strange.\n\
    \nnot work for me , can you provide the mlp_projector and cli code ?\n"
  created_at: 2024-01-23 09:06:10+00:00
  edited: false
  hidden: false
  id: 65af81824a23c7b6b0f8b9e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5830421b18d77836042d8b758c35eaa0.svg
      fullname: wuqo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wunu
      type: user
    createdAt: '2024-01-23T09:12:42.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/5830421b18d77836042d8b758c35eaa0.svg
          fullname: wuqo
          isHf: false
          isPro: false
          name: wunu
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2024-01-23T09:25:11.304Z'
      numEdits: 3
      reactions: []
    id: 65af830a06916708a93be96b
    type: comment
  author: wunu
  content: This comment has been hidden
  created_at: 2024-01-23 09:12:42+00:00
  edited: true
  hidden: true
  id: 65af830a06916708a93be96b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg
      fullname: mengziyang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zylate
      type: user
    createdAt: '2024-01-23T09:29:47.000Z'
    data:
      edited: false
      editors:
      - zylate
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3700260818004608
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg
          fullname: mengziyang
          isHf: false
          isPro: false
          name: zylate
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p>Thanks, I fixed it too.</p>\n\
          <p>I modified the system prompt, and encountered another issue during the\
          \ inference.<br>Through CLI inference proviede by LLaVA, this model looks\
          \ like a repeater, sometimes repeating the last token, and at other times\
          \ repeating an entire sentence. so does 34B. Moreover, there is a multi-round\
          \ dialogue when generating one response.<br>It looks so strange.</p>\n</blockquote>\n\
          <p>not work for me , can you provide the mlp_projector and cli code ?<br>my:\
          \ mlp_projector<br>mlp_gelu_norm_match = re.match(r'^mlp(\\d+)x_gelu_Norm$',\
          \ projector_type)<br>    if mlp_gelu_norm_match:<br>        mlp_depth =\
          \ int(mlp_gelu_norm_match.group(1))<br>        modules = [nn.Linear(config.mm_hidden_size,\
          \ config.hidden_size)]<br>        for _ in range(1, mlp_depth):<br>    \
          \        modules.append(nn.LayerNorm(config.hidden_size))<br>          \
          \  modules.append(nn.GELU())<br>            modules.append(nn.Linear(config.hidden_size,\
          \ config.hidden_size))<br>            modules.append(nn.LayerNorm(config.hidden_size))<br>\
          \        return nn.Sequential(*modules)</p>\n</blockquote>\n<p>i don't use\
          \ cli code, i use YI-VL in two method, eval/run_llava (anther change is\
          \ that, cause llava project bind with str <code>llava</code> in several\
          \ places , so i set a soft link,  named <code>Yi-VL-6B</code> to <code>llava_Yi-VL-6B</code>)\
          \ and test on gradio_demo:<br>def eval_model(args):<br>    # Model<br> \
          \   disable_torch_init()</p>\n<pre><code>device = \"5\"\nmodel_name = get_model_name_from_path(args.model_path)\n\
          tokenizer, model, image_processor, context_len = load_pretrained_model(\n\
          \    args.model_path, args.model_base, model_name, device_map=f\"cuda:{device}\"\
          , device=f\"cuda:{device}\"\n)\n\nqs = args.query\nimage_token_se = DEFAULT_IM_START_TOKEN\
          \ + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\nif IMAGE_PLACEHOLDER in\
          \ qs:\n    if model.config.mm_use_im_start_end:\n        qs = re.sub(IMAGE_PLACEHOLDER,\
          \ image_token_se, qs)\n    else:\n        qs = re.sub(IMAGE_PLACEHOLDER,\
          \ DEFAULT_IMAGE_TOKEN, qs)\nelse:\n    if model.config.mm_use_im_start_end:\n\
          \        qs = image_token_se + \"\\n\" + qs\n    else:\n        qs = DEFAULT_IMAGE_TOKEN\
          \ + \"\\n\" + qs\n\nif \"llama-2\" in model_name.lower():\n    conv_mode\
          \ = \"llava_llama_2\"\nelif \"v1\" in model_name.lower():\n    conv_mode\
          \ = \"llava_v1\"\nelif \"mpt\" in model_name.lower():\n    conv_mode = \"\
          mpt\"\nelse:\n    conv_mode = \"llava_v0\"\n\nif args.conv_mode is not None\
          \ and conv_mode != args.conv_mode:\n    print(\n        \"[WARNING] the\
          \ auto inferred conversation mode is {}, while `--conv-mode` is {}, using\
          \ {}\".format(\n            conv_mode, args.conv_mode, args.conv_mode\n\
          \        )\n    )\nelse:\n    args.conv_mode = conv_mode\n\nconv = conv_templates[args.conv_mode].copy()\n\
          conv.system = \"This is a chat between an inquisitive human and an AI assistant.\
          \ Assume the role of the AI assistant. Read all the images carefully, and\
          \ respond to the human's questions with informative, helpful, detailed and\
          \ polite answers. \u8FD9\u662F\u4E00\u4E2A\u597D\u5947\u7684\u4EBA\u7C7B\
          \u548C\u4E00\u4E2A\u4EBA\u5DE5\u667A\u80FD\u52A9\u624B\u4E4B\u95F4\u7684\
          \u5BF9\u8BDD\u3002\u5047\u8BBE\u4F60\u626E\u6F14\u8FD9\u4E2AAI\u52A9\u624B\
          \u7684\u89D2\u8272\u3002\u4ED4\u7EC6\u9605\u8BFB\u6240\u6709\u7684\u56FE\
          \u50CF\uFF0C\u5E76\u5BF9\u4EBA\u7C7B\u7684\u95EE\u9898\u505A\u51FA\u4FE1\
          \u606F\u4E30\u5BCC\u3001\u6709\u5E2E\u52A9\u3001\u8BE6\u7EC6\u7684\u548C\
          \u793C\u8C8C\u7684\u56DE\u7B54\u3002\"\nconv.append_message(conv.roles[0],\
          \ qs)\nconv.append_message(conv.roles[1], None)\nprompt = conv.get_prompt()\n\
          \nimage_files = image_parser(args)\nimages = load_images(image_files)\n\
          images_tensor = process_images(\n    images,\n    image_processor,\n   \
          \ model.config\n).to(model.device, dtype=torch.float16)\n\ninput_ids = (\n\
          \    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"\
          pt\")\n    .unsqueeze(0).to(model.device)\n)\n\nstop_str = conv.sep if conv.sep_style\
          \ != SeparatorStyle.TWO else conv.sep2\nkeywords = [stop_str]\nstopping_criteria\
          \ = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\nwith torch.inference_mode():\n\
          \    output_ids = model.generate(\n        input_ids,\n        images=images_tensor,\n\
          \        do_sample=True if args.temperature &gt; 0 else False,\n       \
          \ temperature=args.temperature,\n        top_p=args.top_p,\n        num_beams=args.num_beams,\n\
          \        max_new_tokens=args.max_new_tokens,\n        use_cache=True,\n\
          \        stopping_criteria=[stopping_criteria],\n    )\n\ninput_token_len\
          \ = input_ids.shape[1]\nn_diff_input_output = (input_ids != output_ids[:,\
          \ :input_token_len]).sum().item()\nif n_diff_input_output &gt; 0:\n    print(\n\
          \        f\"[Warning] {n_diff_input_output} output_ids are not the same\
          \ as the input_ids\"\n    )\noutputs = tokenizer.batch_decode(\n    output_ids[:,\
          \ input_token_len:], skip_special_tokens=True\n)[0]\noutputs = outputs.strip()\n\
          if outputs.endswith(stop_str):\n    outputs = outputs[: -len(stop_str)]\n\
          outputs = outputs.strip()\nprint(outputs)\n</code></pre>\n<p>if <strong>name</strong>\
          \ == \"<strong>main</strong>\":<br>    parser = argparse.ArgumentParser()<br>\
          \    parser.add_argument(\"--model-path\", type=str, default=\"/path_of_model/llava_Yi-VL-6B\"\
          )<br>    parser.add_argument(\"--model-base\", type=str, default=None)<br>\
          \    parser.add_argument(\"--image-file\", type=str, default=\"/path_of_image/llava_logo.png\"\
          )<br>    parser.add_argument(\"--query\", type=str,default=\"\u56FE\u4E2D\
          \u6709\u591A\u5C11\u5927\u8C61\uFF1F\")<br>    parser.add_argument(\"--conv-mode\"\
          , type=str, default=\"llava_v0\")<br>    parser.add_argument(\"--sep\",\
          \ type=str, default=\",\")<br>    parser.add_argument(\"--temperature\"\
          , type=float, default=0.2)<br>    parser.add_argument(\"--top_p\", type=float,\
          \ default=None)<br>    parser.add_argument(\"--num_beams\", type=int, default=1)<br>\
          \    parser.add_argument(\"--max_new_tokens\", type=int, default=512)<br>\
          \    args = parser.parse_args()</p>\n<pre><code>eval_model(args)\n</code></pre>\n\
          <p>i put layernorm in wrong place i think this maybe the point,<br>have\
          \ you notic that if you use llava_v0, the stopping_criteria'keystr would\
          \ be set as \"###\", but maybe is has no influence<br>and yi has provide\
          \ their generation config in generation_config.json<br>btw thaks for your\
          \ sharing, i'll share my progress when i make it work well.</p>\n</blockquote>\n"
        raw: "> > > Thanks, I fixed it too.\n> > > \n> > > I modified the system prompt,\
          \ and encountered another issue during the inference.\n> > > Through CLI\
          \ inference proviede by LLaVA, this model looks like a repeater, sometimes\
          \ repeating the last token, and at other times repeating an entire sentence.\
          \ so does 34B. Moreover, there is a multi-round dialogue when generating\
          \ one response.\n> > > It looks so strange.\n> > \n> > not work for me ,\
          \ can you provide the mlp_projector and cli code ?\n> my: mlp_projector\
          \ \n> mlp_gelu_norm_match = re.match(r'^mlp(\\d+)x_gelu_Norm$', projector_type)\n\
          >     if mlp_gelu_norm_match:\n>         mlp_depth = int(mlp_gelu_norm_match.group(1))\n\
          >         modules = [nn.Linear(config.mm_hidden_size, config.hidden_size)]\n\
          >         for _ in range(1, mlp_depth):\n>             modules.append(nn.LayerNorm(config.hidden_size))\n\
          >             modules.append(nn.GELU())\n>             modules.append(nn.Linear(config.hidden_size,\
          \ config.hidden_size))\n>             modules.append(nn.LayerNorm(config.hidden_size))\n\
          >         return nn.Sequential(*modules)\n> \n> i don't use cli code, i\
          \ use YI-VL in two method, eval/run_llava (anther change is that, cause\
          \ llava project bind with str `llava` in several places , so i set a soft\
          \ link,  named `Yi-VL-6B` to `llava_Yi-VL-6B`) and test on gradio_demo:\n\
          > def eval_model(args):\n>     # Model\n>     disable_torch_init()\n> \n\
          >     device = \"5\"\n>     model_name = get_model_name_from_path(args.model_path)\n\
          >     tokenizer, model, image_processor, context_len = load_pretrained_model(\n\
          >         args.model_path, args.model_base, model_name, device_map=f\"cuda:{device}\"\
          , device=f\"cuda:{device}\"\n>     )\n> \n>     qs = args.query\n>     image_token_se\
          \ = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n\
          >     if IMAGE_PLACEHOLDER in qs:\n>         if model.config.mm_use_im_start_end:\n\
          >             qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n>    \
          \     else:\n>             qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN,\
          \ qs)\n>     else:\n>         if model.config.mm_use_im_start_end:\n>  \
          \           qs = image_token_se + \"\\n\" + qs\n>         else:\n>     \
          \        qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n> \n>     if \"llama-2\"\
          \ in model_name.lower():\n>         conv_mode = \"llava_llama_2\"\n>   \
          \  elif \"v1\" in model_name.lower():\n>         conv_mode = \"llava_v1\"\
          \n>     elif \"mpt\" in model_name.lower():\n>         conv_mode = \"mpt\"\
          \n>     else:\n>         conv_mode = \"llava_v0\"\n> \n>     if args.conv_mode\
          \ is not None and conv_mode != args.conv_mode:\n>         print(\n>    \
          \         \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode`\
          \ is {}, using {}\".format(\n>                 conv_mode, args.conv_mode,\
          \ args.conv_mode\n>             )\n>         )\n>     else:\n>         args.conv_mode\
          \ = conv_mode\n>     \n>     conv = conv_templates[args.conv_mode].copy()\n\
          >     conv.system = \"This is a chat between an inquisitive human and an\
          \ AI assistant. Assume the role of the AI assistant. Read all the images\
          \ carefully, and respond to the human's questions with informative, helpful,\
          \ detailed and polite answers. \u8FD9\u662F\u4E00\u4E2A\u597D\u5947\u7684\
          \u4EBA\u7C7B\u548C\u4E00\u4E2A\u4EBA\u5DE5\u667A\u80FD\u52A9\u624B\u4E4B\
          \u95F4\u7684\u5BF9\u8BDD\u3002\u5047\u8BBE\u4F60\u626E\u6F14\u8FD9\u4E2A\
          AI\u52A9\u624B\u7684\u89D2\u8272\u3002\u4ED4\u7EC6\u9605\u8BFB\u6240\u6709\
          \u7684\u56FE\u50CF\uFF0C\u5E76\u5BF9\u4EBA\u7C7B\u7684\u95EE\u9898\u505A\
          \u51FA\u4FE1\u606F\u4E30\u5BCC\u3001\u6709\u5E2E\u52A9\u3001\u8BE6\u7EC6\
          \u7684\u548C\u793C\u8C8C\u7684\u56DE\u7B54\u3002\"\n>     conv.append_message(conv.roles[0],\
          \ qs)\n>     conv.append_message(conv.roles[1], None)\n>     prompt = conv.get_prompt()\n\
          > \n>     image_files = image_parser(args)\n>     images = load_images(image_files)\n\
          >     images_tensor = process_images(\n>         images,\n>         image_processor,\n\
          >         model.config\n>     ).to(model.device, dtype=torch.float16)\n\
          > \n>     input_ids = (\n>         tokenizer_image_token(prompt, tokenizer,\
          \ IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n>         .unsqueeze(0).to(model.device)\n\
          >     )\n> \n>     stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO\
          \ else conv.sep2\n>     keywords = [stop_str]\n>     stopping_criteria =\
          \ KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n> \n>     with\
          \ torch.inference_mode():\n>         output_ids = model.generate(\n>   \
          \          input_ids,\n>             images=images_tensor,\n>          \
          \   do_sample=True if args.temperature > 0 else False,\n>             temperature=args.temperature,\n\
          >             top_p=args.top_p,\n>             num_beams=args.num_beams,\n\
          >             max_new_tokens=args.max_new_tokens,\n>             use_cache=True,\n\
          >             stopping_criteria=[stopping_criteria],\n>         )\n> \n\
          >     input_token_len = input_ids.shape[1]\n>     n_diff_input_output =\
          \ (input_ids != output_ids[:, :input_token_len]).sum().item()\n>     if\
          \ n_diff_input_output > 0:\n>         print(\n>             f\"[Warning]\
          \ {n_diff_input_output} output_ids are not the same as the input_ids\"\n\
          >         )\n>     outputs = tokenizer.batch_decode(\n>         output_ids[:,\
          \ input_token_len:], skip_special_tokens=True\n>     )[0]\n>     outputs\
          \ = outputs.strip()\n>     if outputs.endswith(stop_str):\n>         outputs\
          \ = outputs[: -len(stop_str)]\n>     outputs = outputs.strip()\n>     print(outputs)\n\
          > \n> \n> if __name__ == \"__main__\":\n>     parser = argparse.ArgumentParser()\n\
          >     parser.add_argument(\"--model-path\", type=str, default=\"/path_of_model/llava_Yi-VL-6B\"\
          )\n>     parser.add_argument(\"--model-base\", type=str, default=None)\n\
          >     parser.add_argument(\"--image-file\", type=str, default=\"/path_of_image/llava_logo.png\"\
          )\n>     parser.add_argument(\"--query\", type=str,default=\"\u56FE\u4E2D\
          \u6709\u591A\u5C11\u5927\u8C61\uFF1F\")\n>     parser.add_argument(\"--conv-mode\"\
          , type=str, default=\"llava_v0\")\n>     parser.add_argument(\"--sep\",\
          \ type=str, default=\",\")\n>     parser.add_argument(\"--temperature\"\
          , type=float, default=0.2)\n>     parser.add_argument(\"--top_p\", type=float,\
          \ default=None)\n>     parser.add_argument(\"--num_beams\", type=int, default=1)\n\
          >     parser.add_argument(\"--max_new_tokens\", type=int, default=512)\n\
          >     args = parser.parse_args()\n> \n>     eval_model(args)\ni put layernorm\
          \ in wrong place i think this maybe the point,\nhave you notic that if you\
          \ use llava_v0, the stopping_criteria'keystr would be set as \"###\", but\
          \ maybe is has no influence\nand yi has provide their generation config\
          \ in generation_config.json\nbtw thaks for your sharing, i'll share my progress\
          \ when i make it work well.\n"
        updatedAt: '2024-01-23T09:29:47.334Z'
      numEdits: 0
      reactions: []
    id: 65af870be50627e40c92a90c
    type: comment
  author: zylate
  content: "> > > Thanks, I fixed it too.\n> > > \n> > > I modified the system prompt,\
    \ and encountered another issue during the inference.\n> > > Through CLI inference\
    \ proviede by LLaVA, this model looks like a repeater, sometimes repeating the\
    \ last token, and at other times repeating an entire sentence. so does 34B. Moreover,\
    \ there is a multi-round dialogue when generating one response.\n> > > It looks\
    \ so strange.\n> > \n> > not work for me , can you provide the mlp_projector and\
    \ cli code ?\n> my: mlp_projector \n> mlp_gelu_norm_match = re.match(r'^mlp(\\\
    d+)x_gelu_Norm$', projector_type)\n>     if mlp_gelu_norm_match:\n>         mlp_depth\
    \ = int(mlp_gelu_norm_match.group(1))\n>         modules = [nn.Linear(config.mm_hidden_size,\
    \ config.hidden_size)]\n>         for _ in range(1, mlp_depth):\n>           \
    \  modules.append(nn.LayerNorm(config.hidden_size))\n>             modules.append(nn.GELU())\n\
    >             modules.append(nn.Linear(config.hidden_size, config.hidden_size))\n\
    >             modules.append(nn.LayerNorm(config.hidden_size))\n>         return\
    \ nn.Sequential(*modules)\n> \n> i don't use cli code, i use YI-VL in two method,\
    \ eval/run_llava (anther change is that, cause llava project bind with str `llava`\
    \ in several places , so i set a soft link,  named `Yi-VL-6B` to `llava_Yi-VL-6B`)\
    \ and test on gradio_demo:\n> def eval_model(args):\n>     # Model\n>     disable_torch_init()\n\
    > \n>     device = \"5\"\n>     model_name = get_model_name_from_path(args.model_path)\n\
    >     tokenizer, model, image_processor, context_len = load_pretrained_model(\n\
    >         args.model_path, args.model_base, model_name, device_map=f\"cuda:{device}\"\
    , device=f\"cuda:{device}\"\n>     )\n> \n>     qs = args.query\n>     image_token_se\
    \ = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n>   \
    \  if IMAGE_PLACEHOLDER in qs:\n>         if model.config.mm_use_im_start_end:\n\
    >             qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n>         else:\n\
    >             qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n>     else:\n\
    >         if model.config.mm_use_im_start_end:\n>             qs = image_token_se\
    \ + \"\\n\" + qs\n>         else:\n>             qs = DEFAULT_IMAGE_TOKEN + \"\
    \\n\" + qs\n> \n>     if \"llama-2\" in model_name.lower():\n>         conv_mode\
    \ = \"llava_llama_2\"\n>     elif \"v1\" in model_name.lower():\n>         conv_mode\
    \ = \"llava_v1\"\n>     elif \"mpt\" in model_name.lower():\n>         conv_mode\
    \ = \"mpt\"\n>     else:\n>         conv_mode = \"llava_v0\"\n> \n>     if args.conv_mode\
    \ is not None and conv_mode != args.conv_mode:\n>         print(\n>          \
    \   \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode`\
    \ is {}, using {}\".format(\n>                 conv_mode, args.conv_mode, args.conv_mode\n\
    >             )\n>         )\n>     else:\n>         args.conv_mode = conv_mode\n\
    >     \n>     conv = conv_templates[args.conv_mode].copy()\n>     conv.system\
    \ = \"This is a chat between an inquisitive human and an AI assistant. Assume\
    \ the role of the AI assistant. Read all the images carefully, and respond to\
    \ the human's questions with informative, helpful, detailed and polite answers.\
    \ \u8FD9\u662F\u4E00\u4E2A\u597D\u5947\u7684\u4EBA\u7C7B\u548C\u4E00\u4E2A\u4EBA\
    \u5DE5\u667A\u80FD\u52A9\u624B\u4E4B\u95F4\u7684\u5BF9\u8BDD\u3002\u5047\u8BBE\
    \u4F60\u626E\u6F14\u8FD9\u4E2AAI\u52A9\u624B\u7684\u89D2\u8272\u3002\u4ED4\u7EC6\
    \u9605\u8BFB\u6240\u6709\u7684\u56FE\u50CF\uFF0C\u5E76\u5BF9\u4EBA\u7C7B\u7684\
    \u95EE\u9898\u505A\u51FA\u4FE1\u606F\u4E30\u5BCC\u3001\u6709\u5E2E\u52A9\u3001\
    \u8BE6\u7EC6\u7684\u548C\u793C\u8C8C\u7684\u56DE\u7B54\u3002\"\n>     conv.append_message(conv.roles[0],\
    \ qs)\n>     conv.append_message(conv.roles[1], None)\n>     prompt = conv.get_prompt()\n\
    > \n>     image_files = image_parser(args)\n>     images = load_images(image_files)\n\
    >     images_tensor = process_images(\n>         images,\n>         image_processor,\n\
    >         model.config\n>     ).to(model.device, dtype=torch.float16)\n> \n> \
    \    input_ids = (\n>         tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX,\
    \ return_tensors=\"pt\")\n>         .unsqueeze(0).to(model.device)\n>     )\n\
    > \n>     stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n\
    >     keywords = [stop_str]\n>     stopping_criteria = KeywordsStoppingCriteria(keywords,\
    \ tokenizer, input_ids)\n> \n>     with torch.inference_mode():\n>         output_ids\
    \ = model.generate(\n>             input_ids,\n>             images=images_tensor,\n\
    >             do_sample=True if args.temperature > 0 else False,\n>          \
    \   temperature=args.temperature,\n>             top_p=args.top_p,\n>        \
    \     num_beams=args.num_beams,\n>             max_new_tokens=args.max_new_tokens,\n\
    >             use_cache=True,\n>             stopping_criteria=[stopping_criteria],\n\
    >         )\n> \n>     input_token_len = input_ids.shape[1]\n>     n_diff_input_output\
    \ = (input_ids != output_ids[:, :input_token_len]).sum().item()\n>     if n_diff_input_output\
    \ > 0:\n>         print(\n>             f\"[Warning] {n_diff_input_output} output_ids\
    \ are not the same as the input_ids\"\n>         )\n>     outputs = tokenizer.batch_decode(\n\
    >         output_ids[:, input_token_len:], skip_special_tokens=True\n>     )[0]\n\
    >     outputs = outputs.strip()\n>     if outputs.endswith(stop_str):\n>     \
    \    outputs = outputs[: -len(stop_str)]\n>     outputs = outputs.strip()\n> \
    \    print(outputs)\n> \n> \n> if __name__ == \"__main__\":\n>     parser = argparse.ArgumentParser()\n\
    >     parser.add_argument(\"--model-path\", type=str, default=\"/path_of_model/llava_Yi-VL-6B\"\
    )\n>     parser.add_argument(\"--model-base\", type=str, default=None)\n>    \
    \ parser.add_argument(\"--image-file\", type=str, default=\"/path_of_image/llava_logo.png\"\
    )\n>     parser.add_argument(\"--query\", type=str,default=\"\u56FE\u4E2D\u6709\
    \u591A\u5C11\u5927\u8C61\uFF1F\")\n>     parser.add_argument(\"--conv-mode\",\
    \ type=str, default=\"llava_v0\")\n>     parser.add_argument(\"--sep\", type=str,\
    \ default=\",\")\n>     parser.add_argument(\"--temperature\", type=float, default=0.2)\n\
    >     parser.add_argument(\"--top_p\", type=float, default=None)\n>     parser.add_argument(\"\
    --num_beams\", type=int, default=1)\n>     parser.add_argument(\"--max_new_tokens\"\
    , type=int, default=512)\n>     args = parser.parse_args()\n> \n>     eval_model(args)\n\
    i put layernorm in wrong place i think this maybe the point,\nhave you notic that\
    \ if you use llava_v0, the stopping_criteria'keystr would be set as \"###\", but\
    \ maybe is has no influence\nand yi has provide their generation config in generation_config.json\n\
    btw thaks for your sharing, i'll share my progress when i make it work well.\n"
  created_at: 2024-01-23 09:29:47+00:00
  edited: false
  hidden: false
  id: 65af870be50627e40c92a90c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5830421b18d77836042d8b758c35eaa0.svg
      fullname: wuqo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wunu
      type: user
    createdAt: '2024-01-23T09:32:31.000Z'
    data:
      edited: true
      editors:
      - wunu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8923397064208984
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5830421b18d77836042d8b758c35eaa0.svg
          fullname: wuqo
          isHf: false
          isPro: false
          name: wunu
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Thanks, I fixed it too.</p>

          <p>I modified the system prompt, and encountered another issue during the
          inference.<br>Through CLI inference proviede by LLaVA, this model looks
          like a repeater, sometimes repeating the last token, and at other times
          repeating an entire sentence. so does 34B. Moreover, there is a multi-round
          dialogue when generating one response.<br>It looks so strange.</p>

          </blockquote>

          <p>not work for me , can you provide the mlp_projector and cli code ?</p>

          </blockquote>

          <p>It seems an offcial version has  been provided,  <a rel="nofollow" href="https://github.com/01-ai/Yi/blob/liuyudong/yi_vl/VL/single_inference.py">https://github.com/01-ai/Yi/blob/liuyudong/yi_vl/VL/single_inference.py</a></p>

          '
        raw: "> > Thanks, I fixed it too.\n> > \n> > I modified the system prompt,\
          \ and encountered another issue during the inference.\n> > Through CLI inference\
          \ proviede by LLaVA, this model looks like a repeater, sometimes repeating\
          \ the last token, and at other times repeating an entire sentence. so does\
          \ 34B. Moreover, there is a multi-round dialogue when generating one response.\n\
          > > It looks so strange.\n> \n> not work for me , can you provide the mlp_projector\
          \ and cli code ?\n\nIt seems an offcial version has  been provided,  https://github.com/01-ai/Yi/blob/liuyudong/yi_vl/VL/single_inference.py"
        updatedAt: '2024-01-23T09:32:45.471Z'
      numEdits: 1
      reactions: []
    id: 65af87affd56b86c9ce2dbf5
    type: comment
  author: wunu
  content: "> > Thanks, I fixed it too.\n> > \n> > I modified the system prompt, and\
    \ encountered another issue during the inference.\n> > Through CLI inference proviede\
    \ by LLaVA, this model looks like a repeater, sometimes repeating the last token,\
    \ and at other times repeating an entire sentence. so does 34B. Moreover, there\
    \ is a multi-round dialogue when generating one response.\n> > It looks so strange.\n\
    > \n> not work for me , can you provide the mlp_projector and cli code ?\n\nIt\
    \ seems an offcial version has  been provided,  https://github.com/01-ai/Yi/blob/liuyudong/yi_vl/VL/single_inference.py"
  created_at: 2024-01-23 09:32:31+00:00
  edited: true
  hidden: false
  id: 65af87affd56b86c9ce2dbf5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg
      fullname: mengziyang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zylate
      type: user
    createdAt: '2024-01-23T09:43:33.000Z'
    data:
      edited: false
      editors:
      - zylate
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8937340974807739
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg
          fullname: mengziyang
          isHf: false
          isPro: false
          name: zylate
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>Thanks, I fixed it too.</p>

          <p>I modified the system prompt, and encountered another issue during the
          inference.<br>Through CLI inference proviede by LLaVA, this model looks
          like a repeater, sometimes repeating the last token, and at other times
          repeating an entire sentence. so does 34B. Moreover, there is a multi-round
          dialogue when generating one response.<br>It looks so strange.</p>

          </blockquote>

          <p>not work for me , can you provide the mlp_projector and cli code ?</p>

          </blockquote>

          <p>It seems an offcial version has  been provided,  <a rel="nofollow" href="https://github.com/01-ai/Yi/blob/liuyudong/yi_vl/VL/single_inference.py">https://github.com/01-ai/Yi/blob/liuyudong/yi_vl/VL/single_inference.py</a><br>I
          see! now the work change to how to fine-tune it</p>

          </blockquote>

          '
        raw: "> > > Thanks, I fixed it too.\n> > > \n> > > I modified the system prompt,\
          \ and encountered another issue during the inference.\n> > > Through CLI\
          \ inference proviede by LLaVA, this model looks like a repeater, sometimes\
          \ repeating the last token, and at other times repeating an entire sentence.\
          \ so does 34B. Moreover, there is a multi-round dialogue when generating\
          \ one response.\n> > > It looks so strange.\n> > \n> > not work for me ,\
          \ can you provide the mlp_projector and cli code ?\n> \n> It seems an offcial\
          \ version has  been provided,  https://github.com/01-ai/Yi/blob/liuyudong/yi_vl/VL/single_inference.py\n\
          I see! now the work change to how to fine-tune it\n"
        updatedAt: '2024-01-23T09:43:33.238Z'
      numEdits: 0
      reactions: []
    id: 65af8a451f418c744953f1ca
    type: comment
  author: zylate
  content: "> > > Thanks, I fixed it too.\n> > > \n> > > I modified the system prompt,\
    \ and encountered another issue during the inference.\n> > > Through CLI inference\
    \ proviede by LLaVA, this model looks like a repeater, sometimes repeating the\
    \ last token, and at other times repeating an entire sentence. so does 34B. Moreover,\
    \ there is a multi-round dialogue when generating one response.\n> > > It looks\
    \ so strange.\n> > \n> > not work for me , can you provide the mlp_projector and\
    \ cli code ?\n> \n> It seems an offcial version has  been provided,  https://github.com/01-ai/Yi/blob/liuyudong/yi_vl/VL/single_inference.py\n\
    I see! now the work change to how to fine-tune it\n"
  created_at: 2024-01-23 09:43:33+00:00
  edited: false
  hidden: false
  id: 65af8a451f418c744953f1ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/edc9396ea6c04bb2ae55bc418d6fcda8.svg
      fullname: Potted Rose Petal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PottedRosePetal
      type: user
    createdAt: '2024-01-23T11:13:22.000Z'
    data:
      edited: false
      editors:
      - PottedRosePetal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6706868410110474
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/edc9396ea6c04bb2ae55bc418d6fcda8.svg
          fullname: Potted Rose Petal
          isHf: false
          isPro: false
          name: PottedRosePetal
          type: user
        html: '<p>As the above link does not work, here a correct one in the yi repo:<br><a
          rel="nofollow" href="https://github.com/01-ai/Yi/blob/main/VL/single_inference.py">https://github.com/01-ai/Yi/blob/main/VL/single_inference.py</a></p>

          '
        raw: 'As the above link does not work, here a correct one in the yi repo:

          https://github.com/01-ai/Yi/blob/main/VL/single_inference.py'
        updatedAt: '2024-01-23T11:13:22.788Z'
      numEdits: 0
      reactions: []
    id: 65af9f52da1c18bd6d2b2ed2
    type: comment
  author: PottedRosePetal
  content: 'As the above link does not work, here a correct one in the yi repo:

    https://github.com/01-ai/Yi/blob/main/VL/single_inference.py'
  created_at: 2024-01-23 11:13:22+00:00
  edited: false
  hidden: false
  id: 65af9f52da1c18bd6d2b2ed2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: 01-ai/Yi-VL-6B
repo_type: model
status: open
target_branch: null
title: How to use the "mlp2x_gelu_Norm"?
