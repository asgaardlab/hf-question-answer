!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jackboot
conflicting_files: null
created_at: 2024-01-07 17:47:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
      fullname: Jack Boot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jackboot
      type: user
    createdAt: '2024-01-07T17:47:08.000Z'
    data:
      edited: false
      editors:
      - jackboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6811536550521851
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
          fullname: Jack Boot
          isHf: false
          isPro: false
          name: jackboot
          type: user
        html: '<p>I''ve been perplexity testing some of these on different datasets.
          For any chat type log, OG mixtral-instruct does best with more experts,
          even up to 8. Other tunes it''s 3 or bust, no matter what you feed them,
          it''s the exact same pattern. This one however, is all over the place.</p>

          <p>Chatlogs - textgen:</p>

          <p>2 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 3.8989474773406982<br>3 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 3.8434770107269287<br>4 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 3.8221964836120605<br>5 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 3.814671277999878<br>6 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 3.814753293991089<br>7 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 3.819715738296509<br>8 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 3.8253870010375977</p>

          <p>ptb_new (199) - textgen: </p>

          <p>2 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 19.62730598449707<br>3 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 18.78565788269043<br>4 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 18.902372360229492<br>5 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 18.98589515686035<br>6 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 20.042049407958984<br>7 experts -<br>8 experts - </p>

          <p>GU_small (134) - textgen:</p>

          <p>2 experts -<br>3 experts -<br>4 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 2.887450695037842<br>5 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 2.8757941722869873<br>6 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 2.8695790767669678</p>

          <p>roleplay_half (402) - textgen:</p>

          <p>2 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 3.2560477256774902<br>3 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 3.2203352451324463<br>4 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 3.217078685760498<br>5 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 3.217573642730713<br>6 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal
          is: 3.2202346324920654</p>

          <p>Didn''t finish some yet, once I saw where the trend was going.</p>

          <p>4 experts is what I''m going with until I run more d/s. Its definitely
          keeping up with original instruct in terms of how it plays characters. In
          all the other community tunes I tried, none of them  ever picked up accents
          from the card on their own.  Thus far, it seems to break less and repeat
          loop a bit less too. Even at the nuclear option of 4 temp and .1 minP. </p>

          '
        raw: "I've been perplexity testing some of these on different datasets. For\
          \ any chat type log, OG mixtral-instruct does best with more experts, even\
          \ up to 8. Other tunes it's 3 or bust, no matter what you feed them, it's\
          \ the exact same pattern. This one however, is all over the place.\r\n\r\
          \n\r\nChatlogs - textgen:\r\n\r\n2 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 3.8989474773406982\r\n3 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 3.8434770107269287\r\n4 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 3.8221964836120605\r\n5 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 3.814671277999878\r\n6 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 3.814753293991089\r\n7 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 3.819715738296509\r\n8 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 3.8253870010375977\r\n\r\n\r\nptb_new (199) - textgen: \r\n\r\n2 experts\
          \ - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal is: 19.62730598449707\r\
          \n3 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 18.78565788269043\r\n4 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 18.902372360229492\r\n5 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 18.98589515686035\r\n6 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 20.042049407958984\r\n7 experts - \r\n8 experts - \r\n\r\n\r\nGU_small\
          \ (134) - textgen:\r\n\r\n2 experts - \r\n3 experts - \r\n4 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 2.887450695037842\r\n5 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 2.8757941722869873\r\n6 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 2.8695790767669678\r\n\r\n\r\nroleplay_half (402) - textgen:\r\n\r\
          \n2 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 3.2560477256774902\r\n3 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 3.2203352451324463 \r\n4 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 3.217078685760498\r\n5 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 3.217573642730713\r\n6 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
          \ is: 3.2202346324920654\r\n\r\nDidn't finish some yet, once I saw where\
          \ the trend was going.\r\n\r\n4 experts is what I'm going with until I run\
          \ more d/s. Its definitely keeping up with original instruct in terms of\
          \ how it plays characters. In all the other community tunes I tried, none\
          \ of them  ever picked up accents from the card on their own.  Thus far,\
          \ it seems to break less and repeat loop a bit less too. Even at the nuclear\
          \ option of 4 temp and .1 minP. \r\n\r\n\r\n"
        updatedAt: '2024-01-07T17:47:08.594Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Nexesenex
        - Ekirs
        - TheLonelyDevil
    id: 659ae39cd3f21374153851b2
    type: comment
  author: jackboot
  content: "I've been perplexity testing some of these on different datasets. For\
    \ any chat type log, OG mixtral-instruct does best with more experts, even up\
    \ to 8. Other tunes it's 3 or bust, no matter what you feed them, it's the exact\
    \ same pattern. This one however, is all over the place.\r\n\r\n\r\nChatlogs -\
    \ textgen:\r\n\r\n2 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 3.8989474773406982\r\n3 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 3.8434770107269287\r\n4 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 3.8221964836120605\r\n5 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 3.814671277999878\r\n6 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 3.814753293991089\r\n7 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 3.819715738296509\r\n8 experts -  Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 3.8253870010375977\r\n\r\n\r\nptb_new (199) - textgen: \r\n\r\n2 experts\
    \ - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal is: 19.62730598449707\r\
    \n3 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal is:\
    \ 18.78565788269043\r\n4 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 18.902372360229492\r\n5 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 18.98589515686035\r\n6 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 20.042049407958984\r\n7 experts - \r\n8 experts - \r\n\r\n\r\nGU_small (134)\
    \ - textgen:\r\n\r\n2 experts - \r\n3 experts - \r\n4 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 2.887450695037842\r\n5 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 2.8757941722869873\r\n6 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 2.8695790767669678\r\n\r\n\r\nroleplay_half (402) - textgen:\r\n\r\n2 experts\
    \ - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal is: 3.2560477256774902\r\
    \n3 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal is:\
    \ 3.2203352451324463 \r\n4 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 3.217078685760498\r\n5 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 3.217573642730713\r\n6 experts - Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-6.0bpw-h6-exl2-rpcal\
    \ is: 3.2202346324920654\r\n\r\nDidn't finish some yet, once I saw where the trend\
    \ was going.\r\n\r\n4 experts is what I'm going with until I run more d/s. Its\
    \ definitely keeping up with original instruct in terms of how it plays characters.\
    \ In all the other community tunes I tried, none of them  ever picked up accents\
    \ from the card on their own.  Thus far, it seems to break less and repeat loop\
    \ a bit less too. Even at the nuclear option of 4 temp and .1 minP. \r\n\r\n\r\
    \n"
  created_at: 2024-01-07 17:47:08+00:00
  edited: false
  hidden: false
  id: 659ae39cd3f21374153851b2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Doctor-Shotgun/Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss
repo_type: model
status: open
target_branch: null
title: Finally decent results.
