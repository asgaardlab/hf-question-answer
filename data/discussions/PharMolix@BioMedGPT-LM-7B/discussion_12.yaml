!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ruihealthcare
conflicting_files: null
created_at: 2023-10-02 21:18:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d4dd2b2246819ccc39d05472015ff89.svg
      fullname: Rui Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ruihealthcare
      type: user
    createdAt: '2023-10-02T22:18:51.000Z'
    data:
      edited: false
      editors:
      - ruihealthcare
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6210697889328003
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d4dd2b2246819ccc39d05472015ff89.svg
          fullname: Rui Li
          isHf: false
          isPro: false
          name: ruihealthcare
          type: user
        html: '<p>Thanks for sharing the brilliant work! I am new to nlp and want
          to ask some simple questions.<br>How could I use the model? Is the following
          code correct?</p>

          <p>self.llama = LlamaForCausalLM(<br>    self.config<br>).from_pretrained(model_path,
          config=self.config)<br>self.tokenizer = LlamaTokenizer.from_pretrained(model_path,
          **kwargs)<br>pad_sequence = torch.nn.utils.rnn.pad_sequence(<br>    sequence,
          batch_first=True, padding_value=self.tokenizer.pad_token_id<br>)</p>

          '
        raw: "Thanks for sharing the brilliant work! I am new to nlp and want to ask\
          \ some simple questions.\r\nHow could I use the model? Is the following\
          \ code correct?\r\n\r\nself.llama = LlamaForCausalLM(\r\n    self.config\r\
          \n).from_pretrained(model_path, config=self.config)\r\nself.tokenizer =\
          \ LlamaTokenizer.from_pretrained(model_path, **kwargs)\r\npad_sequence =\
          \ torch.nn.utils.rnn.pad_sequence(\r\n    sequence, batch_first=True, padding_value=self.tokenizer.pad_token_id\r\
          \n)"
        updatedAt: '2023-10-02T22:18:51.002Z'
      numEdits: 0
      reactions: []
    id: 651b41cbfa4bf59ced73c430
    type: comment
  author: ruihealthcare
  content: "Thanks for sharing the brilliant work! I am new to nlp and want to ask\
    \ some simple questions.\r\nHow could I use the model? Is the following code correct?\r\
    \n\r\nself.llama = LlamaForCausalLM(\r\n    self.config\r\n).from_pretrained(model_path,\
    \ config=self.config)\r\nself.tokenizer = LlamaTokenizer.from_pretrained(model_path,\
    \ **kwargs)\r\npad_sequence = torch.nn.utils.rnn.pad_sequence(\r\n    sequence,\
    \ batch_first=True, padding_value=self.tokenizer.pad_token_id\r\n)"
  created_at: 2023-10-02 21:18:51+00:00
  edited: false
  hidden: false
  id: 651b41cbfa4bf59ced73c430
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d4dd2b2246819ccc39d05472015ff89.svg
      fullname: Rui Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ruihealthcare
      type: user
    createdAt: '2023-10-03T16:11:24.000Z'
    data:
      edited: false
      editors:
      - ruihealthcare
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.22774164378643036
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d4dd2b2246819ccc39d05472015ff89.svg
          fullname: Rui Li
          isHf: false
          isPro: false
          name: ruihealthcare
          type: user
        html: '<p>I tried to load the model via the above code, it shows:</p>

          <p>Some weights of LlamaForCausalLM were not initialized from the model
          checkpoint at /content/drive/MyDrive/mimic/BioMedGPT-LM-7B and are newly
          initialized: [''model.layers.13.mlp.down_proj.weight'', ''model.layers.7.self_attn.q_proj.weight'',
          ''model.layers.13.self_attn.k_proj.weight'', ''model.layers.10.self_attn.k_proj.weight'',
          ''model.layers.0.post_attention_layernorm.weight'', ''model.layers.22.self_attn.v_proj.weight'',
          ''model.layers.11.self_attn.v_proj.weight'', ''model.layers.3.self_attn.k_proj.weight'',
          ''model.layers.16.input_layernorm.weight'', ''model.layers.17.self_attn.o_proj.weight'',
          ''model.layers.6.self_attn.o_proj.weight'', ''model.layers.0.self_attn.k_proj.weight'',
          ''model.layers.4.self_attn.o_proj.weight'', ''model.layers.7.self_attn.o_proj.weight'',
          ''model.layers.9.self_attn.k_proj.weight'', ''model.layers.1.mlp.up_proj.weight'',
          ''model.layers.15.self_attn.v_proj.weight'', ''model.layers.1.self_attn.q_proj.weight'',
          ''model.layers.19.mlp.gate_proj.weight'', ''model.layers.17.mlp.gate_proj.weight'',
          ''model.layers.2.self_attn.q_proj.weight'', ''model.embed_tokens.weight'',
          ''model.layers.18.mlp.up_proj.weight'', ''model.layers.9.self_attn.q_proj.weight'',
          ''model.layers.19.self_attn.o_proj.weight'', ''model.layers.20.self_attn.q_proj.weight'',
          ''model.layers.16.self_attn.v_proj.weight'', ''model.layers.20.self_attn.k_proj.weight'',
          ''model.layers.14.post_attention_layernorm.weight'', ''model.layers.20.self_attn.o_proj.weight'',
          ''model.layers.18.self_attn.q_proj.weight'', ''model.layers.12.input_layernorm.weight'',
          ''model.layers.22.mlp.up_proj.weight'', ''model.layers.23.self_attn.o_proj.weight'',
          ''model.layers.23.mlp.down_proj.weight'', ''model.layers.9.post_attention_layernorm.weight'',
          ''model.layers.6.post_attention_layernorm.weight'', ''model.layers.0.self_attn.v_proj.weight'',
          ''model.layers.21.mlp.down_proj.weight'', ''model.layers.3.post_attention_layernorm.weight'',
          ''model.layers.7.self_attn.k_proj.weight'', ''model.layers.7.mlp.up_proj.weight'',
          ''model.layers.21.mlp.up_proj.weight'', ''model.layers.7.mlp.gate_proj.weight'',
          ''model.layers.9.mlp.up_proj.weight'', ''model.layers.7.input_layernorm.weight'',
          ''model.layers.15.mlp.up_proj.weight'', ''model.layers.8.self_attn.q_proj.weight'',
          ''model.layers.17.post_attention_layernorm.weight'', ''model.layers.13.self_attn.q_proj.weight'',
          ''model.layers.14.self_attn.k_proj.weight'', ''model.layers.4.mlp.gate_proj.weight'',
          ''model.layers.6.mlp.gate_proj.weight'', ''model.layers.6.self_attn.q_proj.weight'',
          ''model.layers.8.mlp.up_proj.weight'', ''model.layers.21.self_attn.q_proj.weight'',
          ''model.layers.12.self_attn.q_proj.weight'', ''model.layers.4.input_layernorm.weight'',
          ''model.layers.11.mlp.gate_proj.weight'', ''model.layers.2.input_layernorm.weight'',
          ''model.layers.18.post_attention_layernorm.weight'', ''model.layers.17.self_attn.q_proj.weight'',
          ''model.layers.20.mlp.gate_proj.weight'', ''model.layers.0.input_layernorm.weight'',
          ''model.layers.23.self_attn.q_proj.weight'', ''model.layers.18.mlp.down_proj.weight'',
          ''model.layers.5.self_attn.q_proj.weight'', ''model.layers.0.mlp.up_proj.weight'',
          ''model.layers.9.self_attn.o_proj.weight'', ''model.layers.1.input_layernorm.weight'',
          ''model.layers.0.mlp.down_proj.weight'', ''model.layers.5.mlp.down_proj.weight'',
          ''model.layers.7.mlp.down_proj.weight'', ''model.layers.17.input_layernorm.weight'',
          ''model.layers.8.mlp.gate_proj.weight'', ''model.layers.16.self_attn.q_proj.weight'',
          ''model.layers.4.mlp.up_proj.weight'', ''model.layers.19.mlp.down_proj.weight'',
          ''model.layers.9.mlp.down_proj.weight'', ''model.layers.2.self_attn.v_proj.weight'',
          ''model.layers.11.input_layernorm.weight'', ''model.layers.19.mlp.up_proj.weight'',
          ''model.layers.22.mlp.down_proj.weight'', ''model.layers.2.mlp.down_proj.weight'',
          ''model.layers.4.mlp.down_proj.weight'', ''model.layers.4.self_attn.k_proj.weight'',
          ''model.layers.6.mlp.down_proj.weight'', ''model.layers.10.post_attention_layernorm.weight'',
          ''model.layers.6.mlp.up_proj.weight'', ''model.layers.17.mlp.down_proj.weight'',
          ''model.layers.12.mlp.down_proj.weight'', ''model.layers.14.mlp.up_proj.weight'',
          ''model.layers.14.mlp.down_proj.weight'', ''model.layers.21.post_attention_layernorm.weight'',
          ''model.layers.0.mlp.gate_proj.weight'', ''model.layers.2.self_attn.o_proj.weight'',
          ''model.layers.10.mlp.up_proj.weight'', ''model.layers.2.mlp.gate_proj.weight'',
          ''model.layers.12.post_attention_layernorm.weight'', ''model.layers.18.self_attn.k_proj.weight'',
          ''model.layers.11.mlp.up_proj.weight'', ''model.layers.3.mlp.down_proj.weight'',
          ''model.layers.12.self_attn.o_proj.weight'', ''model.layers.20.input_layernorm.weight'',
          ''model.layers.18.self_attn.v_proj.weight'', ''model.layers.2.post_attention_layernorm.weight'',
          ''model.layers.15.self_attn.q_proj.weight'', ''model.layers.16.post_attention_layernorm.weight'',
          ''model.layers.21.self_attn.o_proj.weight'', ''model.layers.19.self_attn.q_proj.weight'',
          ''model.layers.19.self_attn.k_proj.weight'', ''model.layers.14.mlp.gate_proj.weight'',
          ''model.layers.15.self_attn.o_proj.weight'', ''model.layers.22.post_attention_layernorm.weight'',
          ''model.layers.14.self_attn.q_proj.weight'', ''model.layers.6.self_attn.v_proj.weight'',
          ''model.layers.23.self_attn.v_proj.weight'', ''model.layers.10.input_layernorm.weight'',
          ''model.layers.22.self_attn.o_proj.weight'', ''model.layers.11.post_attention_layernorm.weight'',
          ''model.layers.2.self_attn.k_proj.weight'', ''model.layers.12.mlp.gate_proj.weight'',
          ''model.layers.11.self_attn.q_proj.weight'', ''model.layers.4.self_attn.q_proj.weight'',
          ''model.layers.9.mlp.gate_proj.weight'', ''model.layers.16.mlp.down_proj.weight'',
          ''model.layers.20.self_attn.v_proj.weight'', ''model.layers.22.self_attn.k_proj.weight'',
          ''model.layers.13.self_attn.o_proj.weight'', ''model.layers.1.self_attn.k_proj.weight'',
          ''model.layers.3.mlp.gate_proj.weight'', ''model.layers.3.input_layernorm.weight'',
          ''model.layers.8.post_attention_layernorm.weight'', ''model.layers.19.input_layernorm.weight'',
          ''model.layers.21.input_layernorm.weight'', ''model.layers.0.self_attn.o_proj.weight'',
          ''model.layers.14.input_layernorm.weight'', ''model.layers.10.mlp.down_proj.weight'',
          ''model.layers.19.self_attn.v_proj.weight'', ''model.layers.5.self_attn.v_proj.weight'',
          ''model.layers.20.mlp.down_proj.weight'', ''model.layers.6.self_attn.k_proj.weight'',
          ''model.layers.23.mlp.gate_proj.weight'', ''model.layers.21.self_attn.v_proj.weight'',
          ''model.layers.15.self_attn.k_proj.weight'', ''model.layers.4.post_attention_layernorm.weight'',
          ''model.layers.5.mlp.up_proj.weight'', ''model.layers.20.post_attention_layernorm.weight'',
          ''model.layers.15.mlp.gate_proj.weight'', ''model.layers.3.mlp.up_proj.weight'',
          ''model.layers.15.mlp.down_proj.weight'', ''model.layers.1.self_attn.v_proj.weight'',
          ''model.layers.2.mlp.up_proj.weight'', ''model.layers.21.mlp.gate_proj.weight'',
          ''model.layers.11.self_attn.k_proj.weight'', ''model.layers.16.mlp.gate_proj.weight'',
          ''model.layers.5.mlp.gate_proj.weight'', ''model.layers.4.self_attn.v_proj.weight'',
          ''model.layers.1.mlp.down_proj.weight'', ''model.layers.3.self_attn.o_proj.weight'',
          ''model.layers.16.self_attn.k_proj.weight'', ''model.layers.18.input_layernorm.weight'',
          ''model.layers.5.self_attn.k_proj.weight'', ''model.layers.17.self_attn.k_proj.weight'',
          ''model.layers.19.post_attention_layernorm.weight'', ''model.layers.5.post_attention_layernorm.weight'',
          ''model.layers.1.mlp.gate_proj.weight'', ''model.layers.23.self_attn.k_proj.weight'',
          ''model.layers.0.self_attn.q_proj.weight'', ''model.layers.9.self_attn.v_proj.weight'',
          ''model.layers.1.self_attn.o_proj.weight'', ''model.layers.8.mlp.down_proj.weight'',
          ''model.layers.22.mlp.gate_proj.weight'', ''model.layers.6.input_layernorm.weight'',
          ''model.layers.11.self_attn.o_proj.weight'', ''model.layers.12.self_attn.v_proj.weight'',
          ''model.layers.5.self_attn.o_proj.weight'', ''model.layers.13.mlp.gate_proj.weight'',
          ''model.layers.17.self_attn.v_proj.weight'', ''model.layers.10.self_attn.q_proj.weight'',
          ''model.layers.11.mlp.down_proj.weight'', ''model.layers.12.mlp.up_proj.weight'',
          ''model.layers.13.self_attn.v_proj.weight'', ''model.layers.13.input_layernorm.weight'',
          ''model.layers.8.self_attn.k_proj.weight'', ''model.layers.13.mlp.up_proj.weight'',
          ''model.layers.10.mlp.gate_proj.weight'', ''model.layers.7.self_attn.v_proj.weight'',
          ''model.layers.5.input_layernorm.weight'', ''model.layers.22.self_attn.q_proj.weight'',
          ''model.layers.3.self_attn.q_proj.weight'', ''model.layers.16.mlp.up_proj.weight'',
          ''model.layers.3.self_attn.v_proj.weight'', ''model.layers.18.self_attn.o_proj.weight'',
          ''model.layers.20.mlp.up_proj.weight'', ''model.layers.10.self_attn.v_proj.weight'',
          ''model.layers.17.mlp.up_proj.weight'', ''model.layers.21.self_attn.k_proj.weight'',
          ''model.layers.15.input_layernorm.weight'', ''model.layers.22.input_layernorm.weight'',
          ''model.layers.12.self_attn.k_proj.weight'', ''model.layers.14.self_attn.v_proj.weight'',
          ''model.layers.18.mlp.gate_proj.weight'', ''model.layers.16.self_attn.o_proj.weight'',
          ''model.layers.10.self_attn.o_proj.weight'', ''model.layers.8.self_attn.v_proj.weight'',
          ''model.layers.14.self_attn.o_proj.weight'', ''model.layers.8.input_layernorm.weight'',
          ''model.layers.9.input_layernorm.weight'', ''model.layers.8.self_attn.o_proj.weight'',
          ''model.layers.1.post_attention_layernorm.weight'', ''model.layers.15.post_attention_layernorm.weight'',
          ''model.layers.7.post_attention_layernorm.weight'', ''model.layers.13.post_attention_layernorm.weight'']<br>You
          should probably TRAIN this model on a down-stream task to be able to use
          it for predictions and inference.</p>

          '
        raw: 'I tried to load the model via the above code, it shows:


          Some weights of LlamaForCausalLM were not initialized from the model checkpoint
          at /content/drive/MyDrive/mimic/BioMedGPT-LM-7B and are newly initialized:
          [''model.layers.13.mlp.down_proj.weight'', ''model.layers.7.self_attn.q_proj.weight'',
          ''model.layers.13.self_attn.k_proj.weight'', ''model.layers.10.self_attn.k_proj.weight'',
          ''model.layers.0.post_attention_layernorm.weight'', ''model.layers.22.self_attn.v_proj.weight'',
          ''model.layers.11.self_attn.v_proj.weight'', ''model.layers.3.self_attn.k_proj.weight'',
          ''model.layers.16.input_layernorm.weight'', ''model.layers.17.self_attn.o_proj.weight'',
          ''model.layers.6.self_attn.o_proj.weight'', ''model.layers.0.self_attn.k_proj.weight'',
          ''model.layers.4.self_attn.o_proj.weight'', ''model.layers.7.self_attn.o_proj.weight'',
          ''model.layers.9.self_attn.k_proj.weight'', ''model.layers.1.mlp.up_proj.weight'',
          ''model.layers.15.self_attn.v_proj.weight'', ''model.layers.1.self_attn.q_proj.weight'',
          ''model.layers.19.mlp.gate_proj.weight'', ''model.layers.17.mlp.gate_proj.weight'',
          ''model.layers.2.self_attn.q_proj.weight'', ''model.embed_tokens.weight'',
          ''model.layers.18.mlp.up_proj.weight'', ''model.layers.9.self_attn.q_proj.weight'',
          ''model.layers.19.self_attn.o_proj.weight'', ''model.layers.20.self_attn.q_proj.weight'',
          ''model.layers.16.self_attn.v_proj.weight'', ''model.layers.20.self_attn.k_proj.weight'',
          ''model.layers.14.post_attention_layernorm.weight'', ''model.layers.20.self_attn.o_proj.weight'',
          ''model.layers.18.self_attn.q_proj.weight'', ''model.layers.12.input_layernorm.weight'',
          ''model.layers.22.mlp.up_proj.weight'', ''model.layers.23.self_attn.o_proj.weight'',
          ''model.layers.23.mlp.down_proj.weight'', ''model.layers.9.post_attention_layernorm.weight'',
          ''model.layers.6.post_attention_layernorm.weight'', ''model.layers.0.self_attn.v_proj.weight'',
          ''model.layers.21.mlp.down_proj.weight'', ''model.layers.3.post_attention_layernorm.weight'',
          ''model.layers.7.self_attn.k_proj.weight'', ''model.layers.7.mlp.up_proj.weight'',
          ''model.layers.21.mlp.up_proj.weight'', ''model.layers.7.mlp.gate_proj.weight'',
          ''model.layers.9.mlp.up_proj.weight'', ''model.layers.7.input_layernorm.weight'',
          ''model.layers.15.mlp.up_proj.weight'', ''model.layers.8.self_attn.q_proj.weight'',
          ''model.layers.17.post_attention_layernorm.weight'', ''model.layers.13.self_attn.q_proj.weight'',
          ''model.layers.14.self_attn.k_proj.weight'', ''model.layers.4.mlp.gate_proj.weight'',
          ''model.layers.6.mlp.gate_proj.weight'', ''model.layers.6.self_attn.q_proj.weight'',
          ''model.layers.8.mlp.up_proj.weight'', ''model.layers.21.self_attn.q_proj.weight'',
          ''model.layers.12.self_attn.q_proj.weight'', ''model.layers.4.input_layernorm.weight'',
          ''model.layers.11.mlp.gate_proj.weight'', ''model.layers.2.input_layernorm.weight'',
          ''model.layers.18.post_attention_layernorm.weight'', ''model.layers.17.self_attn.q_proj.weight'',
          ''model.layers.20.mlp.gate_proj.weight'', ''model.layers.0.input_layernorm.weight'',
          ''model.layers.23.self_attn.q_proj.weight'', ''model.layers.18.mlp.down_proj.weight'',
          ''model.layers.5.self_attn.q_proj.weight'', ''model.layers.0.mlp.up_proj.weight'',
          ''model.layers.9.self_attn.o_proj.weight'', ''model.layers.1.input_layernorm.weight'',
          ''model.layers.0.mlp.down_proj.weight'', ''model.layers.5.mlp.down_proj.weight'',
          ''model.layers.7.mlp.down_proj.weight'', ''model.layers.17.input_layernorm.weight'',
          ''model.layers.8.mlp.gate_proj.weight'', ''model.layers.16.self_attn.q_proj.weight'',
          ''model.layers.4.mlp.up_proj.weight'', ''model.layers.19.mlp.down_proj.weight'',
          ''model.layers.9.mlp.down_proj.weight'', ''model.layers.2.self_attn.v_proj.weight'',
          ''model.layers.11.input_layernorm.weight'', ''model.layers.19.mlp.up_proj.weight'',
          ''model.layers.22.mlp.down_proj.weight'', ''model.layers.2.mlp.down_proj.weight'',
          ''model.layers.4.mlp.down_proj.weight'', ''model.layers.4.self_attn.k_proj.weight'',
          ''model.layers.6.mlp.down_proj.weight'', ''model.layers.10.post_attention_layernorm.weight'',
          ''model.layers.6.mlp.up_proj.weight'', ''model.layers.17.mlp.down_proj.weight'',
          ''model.layers.12.mlp.down_proj.weight'', ''model.layers.14.mlp.up_proj.weight'',
          ''model.layers.14.mlp.down_proj.weight'', ''model.layers.21.post_attention_layernorm.weight'',
          ''model.layers.0.mlp.gate_proj.weight'', ''model.layers.2.self_attn.o_proj.weight'',
          ''model.layers.10.mlp.up_proj.weight'', ''model.layers.2.mlp.gate_proj.weight'',
          ''model.layers.12.post_attention_layernorm.weight'', ''model.layers.18.self_attn.k_proj.weight'',
          ''model.layers.11.mlp.up_proj.weight'', ''model.layers.3.mlp.down_proj.weight'',
          ''model.layers.12.self_attn.o_proj.weight'', ''model.layers.20.input_layernorm.weight'',
          ''model.layers.18.self_attn.v_proj.weight'', ''model.layers.2.post_attention_layernorm.weight'',
          ''model.layers.15.self_attn.q_proj.weight'', ''model.layers.16.post_attention_layernorm.weight'',
          ''model.layers.21.self_attn.o_proj.weight'', ''model.layers.19.self_attn.q_proj.weight'',
          ''model.layers.19.self_attn.k_proj.weight'', ''model.layers.14.mlp.gate_proj.weight'',
          ''model.layers.15.self_attn.o_proj.weight'', ''model.layers.22.post_attention_layernorm.weight'',
          ''model.layers.14.self_attn.q_proj.weight'', ''model.layers.6.self_attn.v_proj.weight'',
          ''model.layers.23.self_attn.v_proj.weight'', ''model.layers.10.input_layernorm.weight'',
          ''model.layers.22.self_attn.o_proj.weight'', ''model.layers.11.post_attention_layernorm.weight'',
          ''model.layers.2.self_attn.k_proj.weight'', ''model.layers.12.mlp.gate_proj.weight'',
          ''model.layers.11.self_attn.q_proj.weight'', ''model.layers.4.self_attn.q_proj.weight'',
          ''model.layers.9.mlp.gate_proj.weight'', ''model.layers.16.mlp.down_proj.weight'',
          ''model.layers.20.self_attn.v_proj.weight'', ''model.layers.22.self_attn.k_proj.weight'',
          ''model.layers.13.self_attn.o_proj.weight'', ''model.layers.1.self_attn.k_proj.weight'',
          ''model.layers.3.mlp.gate_proj.weight'', ''model.layers.3.input_layernorm.weight'',
          ''model.layers.8.post_attention_layernorm.weight'', ''model.layers.19.input_layernorm.weight'',
          ''model.layers.21.input_layernorm.weight'', ''model.layers.0.self_attn.o_proj.weight'',
          ''model.layers.14.input_layernorm.weight'', ''model.layers.10.mlp.down_proj.weight'',
          ''model.layers.19.self_attn.v_proj.weight'', ''model.layers.5.self_attn.v_proj.weight'',
          ''model.layers.20.mlp.down_proj.weight'', ''model.layers.6.self_attn.k_proj.weight'',
          ''model.layers.23.mlp.gate_proj.weight'', ''model.layers.21.self_attn.v_proj.weight'',
          ''model.layers.15.self_attn.k_proj.weight'', ''model.layers.4.post_attention_layernorm.weight'',
          ''model.layers.5.mlp.up_proj.weight'', ''model.layers.20.post_attention_layernorm.weight'',
          ''model.layers.15.mlp.gate_proj.weight'', ''model.layers.3.mlp.up_proj.weight'',
          ''model.layers.15.mlp.down_proj.weight'', ''model.layers.1.self_attn.v_proj.weight'',
          ''model.layers.2.mlp.up_proj.weight'', ''model.layers.21.mlp.gate_proj.weight'',
          ''model.layers.11.self_attn.k_proj.weight'', ''model.layers.16.mlp.gate_proj.weight'',
          ''model.layers.5.mlp.gate_proj.weight'', ''model.layers.4.self_attn.v_proj.weight'',
          ''model.layers.1.mlp.down_proj.weight'', ''model.layers.3.self_attn.o_proj.weight'',
          ''model.layers.16.self_attn.k_proj.weight'', ''model.layers.18.input_layernorm.weight'',
          ''model.layers.5.self_attn.k_proj.weight'', ''model.layers.17.self_attn.k_proj.weight'',
          ''model.layers.19.post_attention_layernorm.weight'', ''model.layers.5.post_attention_layernorm.weight'',
          ''model.layers.1.mlp.gate_proj.weight'', ''model.layers.23.self_attn.k_proj.weight'',
          ''model.layers.0.self_attn.q_proj.weight'', ''model.layers.9.self_attn.v_proj.weight'',
          ''model.layers.1.self_attn.o_proj.weight'', ''model.layers.8.mlp.down_proj.weight'',
          ''model.layers.22.mlp.gate_proj.weight'', ''model.layers.6.input_layernorm.weight'',
          ''model.layers.11.self_attn.o_proj.weight'', ''model.layers.12.self_attn.v_proj.weight'',
          ''model.layers.5.self_attn.o_proj.weight'', ''model.layers.13.mlp.gate_proj.weight'',
          ''model.layers.17.self_attn.v_proj.weight'', ''model.layers.10.self_attn.q_proj.weight'',
          ''model.layers.11.mlp.down_proj.weight'', ''model.layers.12.mlp.up_proj.weight'',
          ''model.layers.13.self_attn.v_proj.weight'', ''model.layers.13.input_layernorm.weight'',
          ''model.layers.8.self_attn.k_proj.weight'', ''model.layers.13.mlp.up_proj.weight'',
          ''model.layers.10.mlp.gate_proj.weight'', ''model.layers.7.self_attn.v_proj.weight'',
          ''model.layers.5.input_layernorm.weight'', ''model.layers.22.self_attn.q_proj.weight'',
          ''model.layers.3.self_attn.q_proj.weight'', ''model.layers.16.mlp.up_proj.weight'',
          ''model.layers.3.self_attn.v_proj.weight'', ''model.layers.18.self_attn.o_proj.weight'',
          ''model.layers.20.mlp.up_proj.weight'', ''model.layers.10.self_attn.v_proj.weight'',
          ''model.layers.17.mlp.up_proj.weight'', ''model.layers.21.self_attn.k_proj.weight'',
          ''model.layers.15.input_layernorm.weight'', ''model.layers.22.input_layernorm.weight'',
          ''model.layers.12.self_attn.k_proj.weight'', ''model.layers.14.self_attn.v_proj.weight'',
          ''model.layers.18.mlp.gate_proj.weight'', ''model.layers.16.self_attn.o_proj.weight'',
          ''model.layers.10.self_attn.o_proj.weight'', ''model.layers.8.self_attn.v_proj.weight'',
          ''model.layers.14.self_attn.o_proj.weight'', ''model.layers.8.input_layernorm.weight'',
          ''model.layers.9.input_layernorm.weight'', ''model.layers.8.self_attn.o_proj.weight'',
          ''model.layers.1.post_attention_layernorm.weight'', ''model.layers.15.post_attention_layernorm.weight'',
          ''model.layers.7.post_attention_layernorm.weight'', ''model.layers.13.post_attention_layernorm.weight'']

          You should probably TRAIN this model on a down-stream task to be able to
          use it for predictions and inference.

          '
        updatedAt: '2023-10-03T16:11:24.629Z'
      numEdits: 0
      reactions: []
    id: 651c3d2c30627528c85f8ca5
    type: comment
  author: ruihealthcare
  content: 'I tried to load the model via the above code, it shows:


    Some weights of LlamaForCausalLM were not initialized from the model checkpoint
    at /content/drive/MyDrive/mimic/BioMedGPT-LM-7B and are newly initialized: [''model.layers.13.mlp.down_proj.weight'',
    ''model.layers.7.self_attn.q_proj.weight'', ''model.layers.13.self_attn.k_proj.weight'',
    ''model.layers.10.self_attn.k_proj.weight'', ''model.layers.0.post_attention_layernorm.weight'',
    ''model.layers.22.self_attn.v_proj.weight'', ''model.layers.11.self_attn.v_proj.weight'',
    ''model.layers.3.self_attn.k_proj.weight'', ''model.layers.16.input_layernorm.weight'',
    ''model.layers.17.self_attn.o_proj.weight'', ''model.layers.6.self_attn.o_proj.weight'',
    ''model.layers.0.self_attn.k_proj.weight'', ''model.layers.4.self_attn.o_proj.weight'',
    ''model.layers.7.self_attn.o_proj.weight'', ''model.layers.9.self_attn.k_proj.weight'',
    ''model.layers.1.mlp.up_proj.weight'', ''model.layers.15.self_attn.v_proj.weight'',
    ''model.layers.1.self_attn.q_proj.weight'', ''model.layers.19.mlp.gate_proj.weight'',
    ''model.layers.17.mlp.gate_proj.weight'', ''model.layers.2.self_attn.q_proj.weight'',
    ''model.embed_tokens.weight'', ''model.layers.18.mlp.up_proj.weight'', ''model.layers.9.self_attn.q_proj.weight'',
    ''model.layers.19.self_attn.o_proj.weight'', ''model.layers.20.self_attn.q_proj.weight'',
    ''model.layers.16.self_attn.v_proj.weight'', ''model.layers.20.self_attn.k_proj.weight'',
    ''model.layers.14.post_attention_layernorm.weight'', ''model.layers.20.self_attn.o_proj.weight'',
    ''model.layers.18.self_attn.q_proj.weight'', ''model.layers.12.input_layernorm.weight'',
    ''model.layers.22.mlp.up_proj.weight'', ''model.layers.23.self_attn.o_proj.weight'',
    ''model.layers.23.mlp.down_proj.weight'', ''model.layers.9.post_attention_layernorm.weight'',
    ''model.layers.6.post_attention_layernorm.weight'', ''model.layers.0.self_attn.v_proj.weight'',
    ''model.layers.21.mlp.down_proj.weight'', ''model.layers.3.post_attention_layernorm.weight'',
    ''model.layers.7.self_attn.k_proj.weight'', ''model.layers.7.mlp.up_proj.weight'',
    ''model.layers.21.mlp.up_proj.weight'', ''model.layers.7.mlp.gate_proj.weight'',
    ''model.layers.9.mlp.up_proj.weight'', ''model.layers.7.input_layernorm.weight'',
    ''model.layers.15.mlp.up_proj.weight'', ''model.layers.8.self_attn.q_proj.weight'',
    ''model.layers.17.post_attention_layernorm.weight'', ''model.layers.13.self_attn.q_proj.weight'',
    ''model.layers.14.self_attn.k_proj.weight'', ''model.layers.4.mlp.gate_proj.weight'',
    ''model.layers.6.mlp.gate_proj.weight'', ''model.layers.6.self_attn.q_proj.weight'',
    ''model.layers.8.mlp.up_proj.weight'', ''model.layers.21.self_attn.q_proj.weight'',
    ''model.layers.12.self_attn.q_proj.weight'', ''model.layers.4.input_layernorm.weight'',
    ''model.layers.11.mlp.gate_proj.weight'', ''model.layers.2.input_layernorm.weight'',
    ''model.layers.18.post_attention_layernorm.weight'', ''model.layers.17.self_attn.q_proj.weight'',
    ''model.layers.20.mlp.gate_proj.weight'', ''model.layers.0.input_layernorm.weight'',
    ''model.layers.23.self_attn.q_proj.weight'', ''model.layers.18.mlp.down_proj.weight'',
    ''model.layers.5.self_attn.q_proj.weight'', ''model.layers.0.mlp.up_proj.weight'',
    ''model.layers.9.self_attn.o_proj.weight'', ''model.layers.1.input_layernorm.weight'',
    ''model.layers.0.mlp.down_proj.weight'', ''model.layers.5.mlp.down_proj.weight'',
    ''model.layers.7.mlp.down_proj.weight'', ''model.layers.17.input_layernorm.weight'',
    ''model.layers.8.mlp.gate_proj.weight'', ''model.layers.16.self_attn.q_proj.weight'',
    ''model.layers.4.mlp.up_proj.weight'', ''model.layers.19.mlp.down_proj.weight'',
    ''model.layers.9.mlp.down_proj.weight'', ''model.layers.2.self_attn.v_proj.weight'',
    ''model.layers.11.input_layernorm.weight'', ''model.layers.19.mlp.up_proj.weight'',
    ''model.layers.22.mlp.down_proj.weight'', ''model.layers.2.mlp.down_proj.weight'',
    ''model.layers.4.mlp.down_proj.weight'', ''model.layers.4.self_attn.k_proj.weight'',
    ''model.layers.6.mlp.down_proj.weight'', ''model.layers.10.post_attention_layernorm.weight'',
    ''model.layers.6.mlp.up_proj.weight'', ''model.layers.17.mlp.down_proj.weight'',
    ''model.layers.12.mlp.down_proj.weight'', ''model.layers.14.mlp.up_proj.weight'',
    ''model.layers.14.mlp.down_proj.weight'', ''model.layers.21.post_attention_layernorm.weight'',
    ''model.layers.0.mlp.gate_proj.weight'', ''model.layers.2.self_attn.o_proj.weight'',
    ''model.layers.10.mlp.up_proj.weight'', ''model.layers.2.mlp.gate_proj.weight'',
    ''model.layers.12.post_attention_layernorm.weight'', ''model.layers.18.self_attn.k_proj.weight'',
    ''model.layers.11.mlp.up_proj.weight'', ''model.layers.3.mlp.down_proj.weight'',
    ''model.layers.12.self_attn.o_proj.weight'', ''model.layers.20.input_layernorm.weight'',
    ''model.layers.18.self_attn.v_proj.weight'', ''model.layers.2.post_attention_layernorm.weight'',
    ''model.layers.15.self_attn.q_proj.weight'', ''model.layers.16.post_attention_layernorm.weight'',
    ''model.layers.21.self_attn.o_proj.weight'', ''model.layers.19.self_attn.q_proj.weight'',
    ''model.layers.19.self_attn.k_proj.weight'', ''model.layers.14.mlp.gate_proj.weight'',
    ''model.layers.15.self_attn.o_proj.weight'', ''model.layers.22.post_attention_layernorm.weight'',
    ''model.layers.14.self_attn.q_proj.weight'', ''model.layers.6.self_attn.v_proj.weight'',
    ''model.layers.23.self_attn.v_proj.weight'', ''model.layers.10.input_layernorm.weight'',
    ''model.layers.22.self_attn.o_proj.weight'', ''model.layers.11.post_attention_layernorm.weight'',
    ''model.layers.2.self_attn.k_proj.weight'', ''model.layers.12.mlp.gate_proj.weight'',
    ''model.layers.11.self_attn.q_proj.weight'', ''model.layers.4.self_attn.q_proj.weight'',
    ''model.layers.9.mlp.gate_proj.weight'', ''model.layers.16.mlp.down_proj.weight'',
    ''model.layers.20.self_attn.v_proj.weight'', ''model.layers.22.self_attn.k_proj.weight'',
    ''model.layers.13.self_attn.o_proj.weight'', ''model.layers.1.self_attn.k_proj.weight'',
    ''model.layers.3.mlp.gate_proj.weight'', ''model.layers.3.input_layernorm.weight'',
    ''model.layers.8.post_attention_layernorm.weight'', ''model.layers.19.input_layernorm.weight'',
    ''model.layers.21.input_layernorm.weight'', ''model.layers.0.self_attn.o_proj.weight'',
    ''model.layers.14.input_layernorm.weight'', ''model.layers.10.mlp.down_proj.weight'',
    ''model.layers.19.self_attn.v_proj.weight'', ''model.layers.5.self_attn.v_proj.weight'',
    ''model.layers.20.mlp.down_proj.weight'', ''model.layers.6.self_attn.k_proj.weight'',
    ''model.layers.23.mlp.gate_proj.weight'', ''model.layers.21.self_attn.v_proj.weight'',
    ''model.layers.15.self_attn.k_proj.weight'', ''model.layers.4.post_attention_layernorm.weight'',
    ''model.layers.5.mlp.up_proj.weight'', ''model.layers.20.post_attention_layernorm.weight'',
    ''model.layers.15.mlp.gate_proj.weight'', ''model.layers.3.mlp.up_proj.weight'',
    ''model.layers.15.mlp.down_proj.weight'', ''model.layers.1.self_attn.v_proj.weight'',
    ''model.layers.2.mlp.up_proj.weight'', ''model.layers.21.mlp.gate_proj.weight'',
    ''model.layers.11.self_attn.k_proj.weight'', ''model.layers.16.mlp.gate_proj.weight'',
    ''model.layers.5.mlp.gate_proj.weight'', ''model.layers.4.self_attn.v_proj.weight'',
    ''model.layers.1.mlp.down_proj.weight'', ''model.layers.3.self_attn.o_proj.weight'',
    ''model.layers.16.self_attn.k_proj.weight'', ''model.layers.18.input_layernorm.weight'',
    ''model.layers.5.self_attn.k_proj.weight'', ''model.layers.17.self_attn.k_proj.weight'',
    ''model.layers.19.post_attention_layernorm.weight'', ''model.layers.5.post_attention_layernorm.weight'',
    ''model.layers.1.mlp.gate_proj.weight'', ''model.layers.23.self_attn.k_proj.weight'',
    ''model.layers.0.self_attn.q_proj.weight'', ''model.layers.9.self_attn.v_proj.weight'',
    ''model.layers.1.self_attn.o_proj.weight'', ''model.layers.8.mlp.down_proj.weight'',
    ''model.layers.22.mlp.gate_proj.weight'', ''model.layers.6.input_layernorm.weight'',
    ''model.layers.11.self_attn.o_proj.weight'', ''model.layers.12.self_attn.v_proj.weight'',
    ''model.layers.5.self_attn.o_proj.weight'', ''model.layers.13.mlp.gate_proj.weight'',
    ''model.layers.17.self_attn.v_proj.weight'', ''model.layers.10.self_attn.q_proj.weight'',
    ''model.layers.11.mlp.down_proj.weight'', ''model.layers.12.mlp.up_proj.weight'',
    ''model.layers.13.self_attn.v_proj.weight'', ''model.layers.13.input_layernorm.weight'',
    ''model.layers.8.self_attn.k_proj.weight'', ''model.layers.13.mlp.up_proj.weight'',
    ''model.layers.10.mlp.gate_proj.weight'', ''model.layers.7.self_attn.v_proj.weight'',
    ''model.layers.5.input_layernorm.weight'', ''model.layers.22.self_attn.q_proj.weight'',
    ''model.layers.3.self_attn.q_proj.weight'', ''model.layers.16.mlp.up_proj.weight'',
    ''model.layers.3.self_attn.v_proj.weight'', ''model.layers.18.self_attn.o_proj.weight'',
    ''model.layers.20.mlp.up_proj.weight'', ''model.layers.10.self_attn.v_proj.weight'',
    ''model.layers.17.mlp.up_proj.weight'', ''model.layers.21.self_attn.k_proj.weight'',
    ''model.layers.15.input_layernorm.weight'', ''model.layers.22.input_layernorm.weight'',
    ''model.layers.12.self_attn.k_proj.weight'', ''model.layers.14.self_attn.v_proj.weight'',
    ''model.layers.18.mlp.gate_proj.weight'', ''model.layers.16.self_attn.o_proj.weight'',
    ''model.layers.10.self_attn.o_proj.weight'', ''model.layers.8.self_attn.v_proj.weight'',
    ''model.layers.14.self_attn.o_proj.weight'', ''model.layers.8.input_layernorm.weight'',
    ''model.layers.9.input_layernorm.weight'', ''model.layers.8.self_attn.o_proj.weight'',
    ''model.layers.1.post_attention_layernorm.weight'', ''model.layers.15.post_attention_layernorm.weight'',
    ''model.layers.7.post_attention_layernorm.weight'', ''model.layers.13.post_attention_layernorm.weight'']

    You should probably TRAIN this model on a down-stream task to be able to use it
    for predictions and inference.

    '
  created_at: 2023-10-03 15:11:24+00:00
  edited: false
  hidden: false
  id: 651c3d2c30627528c85f8ca5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37c76e5163a686b08ff4cdabb43d7046.svg
      fullname: kui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kuikui
      type: user
    createdAt: '2023-11-07T12:05:43.000Z'
    data:
      edited: false
      editors:
      - kuikui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9899812340736389
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37c76e5163a686b08ff4cdabb43d7046.svg
          fullname: kui
          isHf: false
          isPro: false
          name: kuikui
          type: user
        html: '<p>Hi, Did you successfully run it?</p>

          '
        raw: Hi, Did you successfully run it?
        updatedAt: '2023-11-07T12:05:43.844Z'
      numEdits: 0
      reactions: []
    id: 654a28176ae7f66eeba7a705
    type: comment
  author: kuikui
  content: Hi, Did you successfully run it?
  created_at: 2023-11-07 12:05:43+00:00
  edited: false
  hidden: false
  id: 654a28176ae7f66eeba7a705
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: PharMolix/BioMedGPT-LM-7B
repo_type: model
status: open
target_branch: null
title: how to use the model
