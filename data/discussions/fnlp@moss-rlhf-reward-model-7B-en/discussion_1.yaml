!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AndyWodecki
conflicting_files: null
created_at: 2023-10-23 08:04:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2e34caf35ed817bc0f14aa225cfd1c3.svg
      fullname: Andrzej Wodecki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AndyWodecki
      type: user
    createdAt: '2023-10-23T09:04:53.000Z'
    data:
      edited: false
      editors:
      - AndyWodecki
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9298722147941589
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2e34caf35ed817bc0f14aa225cfd1c3.svg
          fullname: Andrzej Wodecki
          isHf: false
          isPro: false
          name: AndyWodecki
          type: user
        html: '<p>I want to train the reward model on my dataset (not HH-RLHF from
          Anthropic). At the moment, I use TRL library for that.<br>I am truly impressed
          by Your approach, specifically the training setup specified in section 4.2.
          of Your paper.<br>Would you be so kind as to provide the code for that part?
          Or some technical tips on how to create it?<br>Many thanks in advance :)</p>

          '
        raw: "I want to train the reward model on my dataset (not HH-RLHF from Anthropic).\
          \ At the moment, I use TRL library for that.\r\nI am truly impressed by\
          \ Your approach, specifically the training setup specified in section 4.2.\
          \ of Your paper.\r\nWould you be so kind as to provide the code for that\
          \ part? Or some technical tips on how to create it?\r\nMany thanks in advance\
          \ :)"
        updatedAt: '2023-10-23T09:04:53.693Z'
      numEdits: 0
      reactions: []
    id: 65363735ae42162a1e1a2c38
    type: comment
  author: AndyWodecki
  content: "I want to train the reward model on my dataset (not HH-RLHF from Anthropic).\
    \ At the moment, I use TRL library for that.\r\nI am truly impressed by Your approach,\
    \ specifically the training setup specified in section 4.2. of Your paper.\r\n\
    Would you be so kind as to provide the code for that part? Or some technical tips\
    \ on how to create it?\r\nMany thanks in advance :)"
  created_at: 2023-10-23 08:04:53+00:00
  edited: false
  hidden: false
  id: 65363735ae42162a1e1a2c38
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: fnlp/moss-rlhf-reward-model-7B-en
repo_type: model
status: open
target_branch: null
title: The code for training the reward model?
