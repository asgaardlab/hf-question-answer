!!python/object:huggingface_hub.community.DiscussionWithDetails
author: iguana0335
conflicting_files: null
created_at: 2023-04-11 19:28:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5ebcd2775959e77452ceec8aea070014.svg
      fullname: Larry Muniz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iguana0335
      type: user
    createdAt: '2023-04-11T20:28:19.000Z'
    data:
      edited: false
      editors:
      - iguana0335
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5ebcd2775959e77452ceec8aea070014.svg
          fullname: Larry Muniz
          isHf: false
          isPro: false
          name: iguana0335
          type: user
        html: '<p>Hello. I would like to transform this model to ggml format to be
          able to test it on a machine with lower resources. Is there any procedure
          or scripts to do it? Thank you.</p>

          '
        raw: Hello. I would like to transform this model to ggml format to be able
          to test it on a machine with lower resources. Is there any procedure or
          scripts to do it? Thank you.
        updatedAt: '2023-04-11T20:28:19.980Z'
      numEdits: 0
      reactions: []
    id: 6435c2e3a4bd75c62cbf9ca8
    type: comment
  author: iguana0335
  content: Hello. I would like to transform this model to ggml format to be able to
    test it on a machine with lower resources. Is there any procedure or scripts to
    do it? Thank you.
  created_at: 2023-04-11 19:28:19+00:00
  edited: false
  hidden: false
  id: 6435c2e3a4bd75c62cbf9ca8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
      fullname: Javier de la Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: versae
      type: user
    createdAt: '2023-04-12T11:09:21.000Z'
    data:
      edited: true
      editors:
      - versae
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
          fullname: Javier de la Rosa
          isHf: false
          isPro: false
          name: versae
          type: user
        html: "<p>I tried to convert it to ggml but had some errors due to a missing\
          \ <code>added_tokens.json</code> in the tokenizer since the original model\
          \ uses GPT2's instead of GPT-J-6B's. I just pushed a couple of versions,\
          \ in <code>float16</code> and <code>float32</code>, so the smaller model\
          \ in <code>float16</code> should be able to run in CPU using <a rel=\"nofollow\"\
          \ href=\"https://github.com/ggerganov/ggml/tree/master/examples/gpt-j\"\
          >https://github.com/ggerganov/ggml/tree/master/examples/gpt-j</a>. Just\
          \ remember that the prompt needs to follow the Spanish version of the Alpaca\
          \ prompt. For example, you could create a <code>prompt.txt</code> file:</p>\n\
          <pre><code>A continuaci\xF3n hay una instrucci\xF3n que describe una tarea.\
          \ Escribe una respuesta que complete adecuadamente lo que se pide.\n\n###\
          \ Instrucci\xF3n:\n$TASK\n\n### Respuesta:\n</code></pre>\n<p>Download the\
          \ model, compile and install ggml, and then invoke it:</p>\n<pre><code class=\"\
          language-bash\">$ TASK=<span class=\"hljs-string\">\"Escribe un email poniendo\
          \ una excusa para faltar a la reuni\xF3n\"</span> envsubst &lt; prompt.txt\
          \ | ./bin/gpt-j -m /path/to/bertin-gpt-j-6B-alpaca/ggml-model-f16.bin\n\
          </code></pre>\n"
        raw: "I tried to convert it to ggml but had some errors due to a missing `added_tokens.json`\
          \ in the tokenizer since the original model uses GPT2's instead of GPT-J-6B's.\
          \ I just pushed a couple of versions, in `float16` and `float32`, so the\
          \ smaller model in `float16` should be able to run in CPU using https://github.com/ggerganov/ggml/tree/master/examples/gpt-j.\
          \ Just remember that the prompt needs to follow the Spanish version of the\
          \ Alpaca prompt. For example, you could create a `prompt.txt` file:\n\n\
          ```\nA continuaci\xF3n hay una instrucci\xF3n que describe una tarea. Escribe\
          \ una respuesta que complete adecuadamente lo que se pide.\n\n### Instrucci\xF3\
          n:\n$TASK\n\n### Respuesta:\n```\n\nDownload the model, compile and install\
          \ ggml, and then invoke it:\n\n```bash\n$ TASK=\"Escribe un email poniendo\
          \ una excusa para faltar a la reuni\xF3n\" envsubst < prompt.txt | ./bin/gpt-j\
          \ -m /path/to/bertin-gpt-j-6B-alpaca/ggml-model-f16.bin\n```"
        updatedAt: '2023-04-12T11:17:55.942Z'
      numEdits: 3
      reactions: []
    id: 64369161adb1d6b4f1f80ef9
    type: comment
  author: versae
  content: "I tried to convert it to ggml but had some errors due to a missing `added_tokens.json`\
    \ in the tokenizer since the original model uses GPT2's instead of GPT-J-6B's.\
    \ I just pushed a couple of versions, in `float16` and `float32`, so the smaller\
    \ model in `float16` should be able to run in CPU using https://github.com/ggerganov/ggml/tree/master/examples/gpt-j.\
    \ Just remember that the prompt needs to follow the Spanish version of the Alpaca\
    \ prompt. For example, you could create a `prompt.txt` file:\n\n```\nA continuaci\xF3\
    n hay una instrucci\xF3n que describe una tarea. Escribe una respuesta que complete\
    \ adecuadamente lo que se pide.\n\n### Instrucci\xF3n:\n$TASK\n\n### Respuesta:\n\
    ```\n\nDownload the model, compile and install ggml, and then invoke it:\n\n```bash\n\
    $ TASK=\"Escribe un email poniendo una excusa para faltar a la reuni\xF3n\" envsubst\
    \ < prompt.txt | ./bin/gpt-j -m /path/to/bertin-gpt-j-6B-alpaca/ggml-model-f16.bin\n\
    ```"
  created_at: 2023-04-12 10:09:21+00:00
  edited: true
  hidden: false
  id: 64369161adb1d6b4f1f80ef9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5ebcd2775959e77452ceec8aea070014.svg
      fullname: Larry Muniz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iguana0335
      type: user
    createdAt: '2023-04-14T20:24:23.000Z'
    data:
      edited: true
      editors:
      - iguana0335
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5ebcd2775959e77452ceec8aea070014.svg
          fullname: Larry Muniz
          isHf: false
          isPro: false
          name: iguana0335
          type: user
        html: "<p>Hello. I'm testing the ggml-model-f16.bin, and I have this \"promp.txt\"\
          \ file :</p>\n<p>A continuaci\xF3n hay una instrucci\xF3n que describe una\
          \ tarea. Escribe una respuesta que complete adecuadamente lo que se pide.</p>\n\
          <p>### Instrucci\xF3n:<br>\"Cuentame un cuento.\"</p>\n<p>### Respuesta:</p>\n\
          <p>But when I launch the model there are many \"gpt_tokenize: unknown token\"\
          \ and the text generate is not good:</p>\n<p>$ cat prompt.txt | ./gpt-j\
          \ -m /bertin-gpt-j-6b-alpaca/ggml-bertin-gpt-j-6B-alpaca/ggml-model-f16.bin<br>main:\
          \ seed = 1681503060<br>gptj_model_load: loading model from '/bertin-gpt-j-6b-alpaca/ggml-bertin-gpt-j-6B-alpaca/ggml-model-f16.bin'\
          \ - please wait ...<br>gptj_model_load: n_vocab = 50400<br>gptj_model_load:\
          \ n_ctx   = 2048<br>gptj_model_load: n_embd  = 4096<br>gptj_model_load:\
          \ n_head  = 16<br>gptj_model_load: n_layer = 28<br>gptj_model_load: n_rot\
          \   = 64<br>gptj_model_load: f16     = 1<br>gptj_model_load: ggml ctx size\
          \ = 12438.86 MB<br>gptj_model_load: memory_size =   896.00 MB, n_mem = 57344<br>gptj_model_load:\
          \ ................................... done<br>gptj_model_load: model size\
          \ = 11542.79 MB / num tensors = 285<br>gpt_tokenize: unknown token '<br>'<br>gpt_tokenize:\
          \ unknown token 'A'<br>gpt_tokenize: unknown token 'a'<br>gpt_tokenize:\
          \ unknown token 'c'<br>gpt_tokenize: unknown token 'i'<br>gpt_tokenize:\
          \ unknown token '\u2592'<br>...<br>...<br>gpt_tokenize: unknown token ':'<br>'pt_tokenize:\
          \ unknown token '<br>main: number of tokens in prompt = 32</p>\n<p> continu\
          \ hayna instrcci que describena tar Escibena respesta que complete aduaamen\
          \ que se pid### InstrcciCuta cue.\"###RespestaCu\xE1\u2592 es el cle fist\
          \ x si 4\u2592 - 6 = 18\u2592\u2592\u2592\u2592### knee\u2592estaEl cle\
          \ fist x es 6\u2592&lt;|endoftext|&gt;</p>\n<p>main: mem per token = 15488240\
          \ bytes<br>main:     load time = 18915.16 ms<br>main:   sample time =  \
          \  98.78 ms<br>main:  predict time = 17135.78 ms / 267.75 ms per token<br>main:\
          \    total time = 43800.53 ms</p>\n<p>I'm probably doing something wrong\
          \ or I need to configure something...<br>Thank you.</p>\n"
        raw: "Hello. I'm testing the ggml-model-f16.bin, and I have this \"promp.txt\"\
          \ file :\n\nA continuaci\xF3n hay una instrucci\xF3n que describe una tarea.\
          \ Escribe una respuesta que complete adecuadamente lo que se pide.\n\n\\\
          #\\#\\# Instrucci\xF3n:\n\"Cuentame un cuento.\"\n\n\\#\\#\\# Respuesta:\n\
          \n\nBut when I launch the model there are many \"gpt_tokenize: unknown token\"\
          \ and the text generate is not good:\n\n$ cat prompt.txt | ./gpt-j -m /bertin-gpt-j-6b-alpaca/ggml-bertin-gpt-j-6B-alpaca/ggml-model-f16.bin\n\
          main: seed = 1681503060\ngptj_model_load: loading model from '/bertin-gpt-j-6b-alpaca/ggml-bertin-gpt-j-6B-alpaca/ggml-model-f16.bin'\
          \ - please wait ...\ngptj_model_load: n_vocab = 50400\ngptj_model_load:\
          \ n_ctx   = 2048\ngptj_model_load: n_embd  = 4096\ngptj_model_load: n_head\
          \  = 16\ngptj_model_load: n_layer = 28\ngptj_model_load: n_rot   = 64\n\
          gptj_model_load: f16     = 1\ngptj_model_load: ggml ctx size = 12438.86\
          \ MB\ngptj_model_load: memory_size =   896.00 MB, n_mem = 57344\ngptj_model_load:\
          \ ................................... done\ngptj_model_load: model size\
          \ = 11542.79 MB / num tensors = 285\ngpt_tokenize: unknown token '\n'\n\
          gpt_tokenize: unknown token 'A'\ngpt_tokenize: unknown token 'a'\ngpt_tokenize:\
          \ unknown token 'c'\ngpt_tokenize: unknown token 'i'\ngpt_tokenize: unknown\
          \ token '\u2592'\n...\n...\ngpt_tokenize: unknown token ':'\n'pt_tokenize:\
          \ unknown token '\nmain: number of tokens in prompt = 32\n\n continu hayna\
          \ instrcci que describena tar Escibena respesta que complete aduaamen que\
          \ se pid### InstrcciCuta cue.\"###RespestaCu\xE1\u2592 es el cle fist x\
          \ si 4\u2592 - 6 = 18\u2592\u2592\u2592\u2592### knee\u2592estaEl cle fist\
          \ x es 6\u2592<|endoftext|>\n\nmain: mem per token = 15488240 bytes\nmain:\
          \     load time = 18915.16 ms\nmain:   sample time =    98.78 ms\nmain:\
          \  predict time = 17135.78 ms / 267.75 ms per token\nmain:    total time\
          \ = 43800.53 ms\n\n\n\nI'm probably doing something wrong or I need to configure\
          \ something...\nThank you."
        updatedAt: '2023-04-14T20:27:33.404Z'
      numEdits: 2
      reactions: []
    id: 6439b67768228e8b33453a06
    type: comment
  author: iguana0335
  content: "Hello. I'm testing the ggml-model-f16.bin, and I have this \"promp.txt\"\
    \ file :\n\nA continuaci\xF3n hay una instrucci\xF3n que describe una tarea. Escribe\
    \ una respuesta que complete adecuadamente lo que se pide.\n\n\\#\\#\\# Instrucci\xF3\
    n:\n\"Cuentame un cuento.\"\n\n\\#\\#\\# Respuesta:\n\n\nBut when I launch the\
    \ model there are many \"gpt_tokenize: unknown token\" and the text generate is\
    \ not good:\n\n$ cat prompt.txt | ./gpt-j -m /bertin-gpt-j-6b-alpaca/ggml-bertin-gpt-j-6B-alpaca/ggml-model-f16.bin\n\
    main: seed = 1681503060\ngptj_model_load: loading model from '/bertin-gpt-j-6b-alpaca/ggml-bertin-gpt-j-6B-alpaca/ggml-model-f16.bin'\
    \ - please wait ...\ngptj_model_load: n_vocab = 50400\ngptj_model_load: n_ctx\
    \   = 2048\ngptj_model_load: n_embd  = 4096\ngptj_model_load: n_head  = 16\ngptj_model_load:\
    \ n_layer = 28\ngptj_model_load: n_rot   = 64\ngptj_model_load: f16     = 1\n\
    gptj_model_load: ggml ctx size = 12438.86 MB\ngptj_model_load: memory_size = \
    \  896.00 MB, n_mem = 57344\ngptj_model_load: ...................................\
    \ done\ngptj_model_load: model size = 11542.79 MB / num tensors = 285\ngpt_tokenize:\
    \ unknown token '\n'\ngpt_tokenize: unknown token 'A'\ngpt_tokenize: unknown token\
    \ 'a'\ngpt_tokenize: unknown token 'c'\ngpt_tokenize: unknown token 'i'\ngpt_tokenize:\
    \ unknown token '\u2592'\n...\n...\ngpt_tokenize: unknown token ':'\n'pt_tokenize:\
    \ unknown token '\nmain: number of tokens in prompt = 32\n\n continu hayna instrcci\
    \ que describena tar Escibena respesta que complete aduaamen que se pid### InstrcciCuta\
    \ cue.\"###RespestaCu\xE1\u2592 es el cle fist x si 4\u2592 - 6 = 18\u2592\u2592\
    \u2592\u2592### knee\u2592estaEl cle fist x es 6\u2592<|endoftext|>\n\nmain: mem\
    \ per token = 15488240 bytes\nmain:     load time = 18915.16 ms\nmain:   sample\
    \ time =    98.78 ms\nmain:  predict time = 17135.78 ms / 267.75 ms per token\n\
    main:    total time = 43800.53 ms\n\n\n\nI'm probably doing something wrong or\
    \ I need to configure something...\nThank you."
  created_at: 2023-04-14 19:24:23+00:00
  edited: true
  hidden: false
  id: 6439b67768228e8b33453a06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
      fullname: Javier de la Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: versae
      type: user
    createdAt: '2023-04-18T09:11:36.000Z'
    data:
      edited: true
      editors:
      - versae
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
          fullname: Javier de la Rosa
          isHf: false
          isPro: false
          name: versae
          type: user
        html: "<p>Strange. Just run the same prompt and got this</p>\n<pre><code class=\"\
          language-bash\">$ TASK=<span class=\"hljs-string\">\"Cu\xE9ntame un cuento\"\
          </span> envsubst &lt; es.txt | ./gpt-j -m /../bertin-gpt-j-6B-alpaca/ggml-model-f16.bin\
          \ \nmain: seed = 1681808791\ngptj_model_load: loading model from <span class=\"\
          hljs-string\">'/../bertin-gpt-j-6B-alpaca/ggml-model-f16.bin'</span> - please\
          \ <span class=\"hljs-built_in\">wait</span> ...\ngptj_model_load: n_vocab\
          \ = 50400\ngptj_model_load: n_ctx   = 2048\ngptj_model_load: n_embd  = 4096\n\
          gptj_model_load: n_head  = 16\ngptj_model_load: n_layer = 28\ngptj_model_load:\
          \ n_rot   = 64\ngptj_model_load: f16     = 1\ngptj_model_load: ggml ctx\
          \ size = 12438.86 MB\ngptj_model_load: memory_size =   896.00 MB, n_mem\
          \ = 57344\ngptj_model_load: ................................... <span class=\"\
          hljs-keyword\">done</span>\ngptj_model_load: model size = 11542.79 MB /\
          \ num tensors = 285\nmain: number of tokens <span class=\"hljs-keyword\"\
          >in</span> prompt = 73\n\n\nA continuaci\xF3n hay una instrucci\xF3n que\
          \ describe una tarea. Escribe una respuesta que complete adecuadamente lo\
          \ que se pide.\n\n<span class=\"hljs-comment\">### Instrucci\xF3n:</span>\n\
          Cu\xE9ntame un cuento\n\n<span class=\"hljs-comment\">### Respuesta:</span>\n\
          Hab\xEDa una vez un peque\xF1o rat\xF3n de campo que viv\xEDa en un peque\xF1\
          o agujero en el tronco de un \xE1rbol alto. El rat\xF3n era muy astuto,\
          \ y a menudo se le ve\xEDa resolviendo problemas para los dem\xE1s animales\
          \ del bosque. Un d\xEDa, el rat\xF3n escuch\xF3 hablar de un gran concurso\
          \ en el que se ofrec\xEDa una gran recompensa al mejor cuentacuentos. El\
          \ rat\xF3n decidi\xF3 participar. \n\nEl rat\xF3n comenz\xF3 a practicar\
          \ su cuento todos los d\xEDas, pero ninguno de sus intentos parec\xEDa bueno.\
          \ Un d\xEDa, el rat\xF3n escuch\xF3 un ruido fuera del agujero en el tronco.\
          \ Mir\xF3 por el agujero y\n\nmain: mem per token = 15488240 bytes\nmain:\
          \     load time = 24641.92 ms\nmain:   sample time =    71.52 ms\nmain:\
          \  predict time = 90674.88 ms / 333.36 ms per token\nmain:    total time\
          \ = 117546.53 ms\n</code></pre>\n<p>Maybe try downloaidng the model again?\
          \ The commit hash of the gpt-j binary I'm building is <a rel=\"nofollow\"\
          \ href=\"https://github.com/ggerganov/ggml/tree/75824e76bd41818ff8902d244feec1ec6c1d2c86\"\
          >75824e76bd41818ff8902d244feec1ec6c1d2c86</a>.</p>\n"
        raw: "Strange. Just run the same prompt and got this\n\n```bash\n$ TASK=\"\
          Cu\xE9ntame un cuento\" envsubst < es.txt | ./gpt-j -m /../bertin-gpt-j-6B-alpaca/ggml-model-f16.bin\
          \ \nmain: seed = 1681808791\ngptj_model_load: loading model from '/../bertin-gpt-j-6B-alpaca/ggml-model-f16.bin'\
          \ - please wait ...\ngptj_model_load: n_vocab = 50400\ngptj_model_load:\
          \ n_ctx   = 2048\ngptj_model_load: n_embd  = 4096\ngptj_model_load: n_head\
          \  = 16\ngptj_model_load: n_layer = 28\ngptj_model_load: n_rot   = 64\n\
          gptj_model_load: f16     = 1\ngptj_model_load: ggml ctx size = 12438.86\
          \ MB\ngptj_model_load: memory_size =   896.00 MB, n_mem = 57344\ngptj_model_load:\
          \ ................................... done\ngptj_model_load: model size\
          \ = 11542.79 MB / num tensors = 285\nmain: number of tokens in prompt =\
          \ 73\n\n\nA continuaci\xF3n hay una instrucci\xF3n que describe una tarea.\
          \ Escribe una respuesta que complete adecuadamente lo que se pide.\n\n###\
          \ Instrucci\xF3n:\nCu\xE9ntame un cuento\n\n### Respuesta:\nHab\xEDa una\
          \ vez un peque\xF1o rat\xF3n de campo que viv\xEDa en un peque\xF1o agujero\
          \ en el tronco de un \xE1rbol alto. El rat\xF3n era muy astuto, y a menudo\
          \ se le ve\xEDa resolviendo problemas para los dem\xE1s animales del bosque.\
          \ Un d\xEDa, el rat\xF3n escuch\xF3 hablar de un gran concurso en el que\
          \ se ofrec\xEDa una gran recompensa al mejor cuentacuentos. El rat\xF3n\
          \ decidi\xF3 participar. \n\nEl rat\xF3n comenz\xF3 a practicar su cuento\
          \ todos los d\xEDas, pero ninguno de sus intentos parec\xEDa bueno. Un d\xED\
          a, el rat\xF3n escuch\xF3 un ruido fuera del agujero en el tronco. Mir\xF3\
          \ por el agujero y\n\nmain: mem per token = 15488240 bytes\nmain:     load\
          \ time = 24641.92 ms\nmain:   sample time =    71.52 ms\nmain:  predict\
          \ time = 90674.88 ms / 333.36 ms per token\nmain:    total time = 117546.53\
          \ ms\n```\n\nMaybe try downloaidng the model again? The commit hash of the\
          \ gpt-j binary I'm building is [75824e76bd41818ff8902d244feec1ec6c1d2c86](https://github.com/ggerganov/ggml/tree/75824e76bd41818ff8902d244feec1ec6c1d2c86)."
        updatedAt: '2023-04-20T11:51:25.871Z'
      numEdits: 2
      reactions: []
    id: 643e5ec8fa5a9e14c73cd2c4
    type: comment
  author: versae
  content: "Strange. Just run the same prompt and got this\n\n```bash\n$ TASK=\"Cu\xE9\
    ntame un cuento\" envsubst < es.txt | ./gpt-j -m /../bertin-gpt-j-6B-alpaca/ggml-model-f16.bin\
    \ \nmain: seed = 1681808791\ngptj_model_load: loading model from '/../bertin-gpt-j-6B-alpaca/ggml-model-f16.bin'\
    \ - please wait ...\ngptj_model_load: n_vocab = 50400\ngptj_model_load: n_ctx\
    \   = 2048\ngptj_model_load: n_embd  = 4096\ngptj_model_load: n_head  = 16\ngptj_model_load:\
    \ n_layer = 28\ngptj_model_load: n_rot   = 64\ngptj_model_load: f16     = 1\n\
    gptj_model_load: ggml ctx size = 12438.86 MB\ngptj_model_load: memory_size = \
    \  896.00 MB, n_mem = 57344\ngptj_model_load: ...................................\
    \ done\ngptj_model_load: model size = 11542.79 MB / num tensors = 285\nmain: number\
    \ of tokens in prompt = 73\n\n\nA continuaci\xF3n hay una instrucci\xF3n que describe\
    \ una tarea. Escribe una respuesta que complete adecuadamente lo que se pide.\n\
    \n### Instrucci\xF3n:\nCu\xE9ntame un cuento\n\n### Respuesta:\nHab\xEDa una vez\
    \ un peque\xF1o rat\xF3n de campo que viv\xEDa en un peque\xF1o agujero en el\
    \ tronco de un \xE1rbol alto. El rat\xF3n era muy astuto, y a menudo se le ve\xED\
    a resolviendo problemas para los dem\xE1s animales del bosque. Un d\xEDa, el rat\xF3\
    n escuch\xF3 hablar de un gran concurso en el que se ofrec\xEDa una gran recompensa\
    \ al mejor cuentacuentos. El rat\xF3n decidi\xF3 participar. \n\nEl rat\xF3n comenz\xF3\
    \ a practicar su cuento todos los d\xEDas, pero ninguno de sus intentos parec\xED\
    a bueno. Un d\xEDa, el rat\xF3n escuch\xF3 un ruido fuera del agujero en el tronco.\
    \ Mir\xF3 por el agujero y\n\nmain: mem per token = 15488240 bytes\nmain:    \
    \ load time = 24641.92 ms\nmain:   sample time =    71.52 ms\nmain:  predict time\
    \ = 90674.88 ms / 333.36 ms per token\nmain:    total time = 117546.53 ms\n```\n\
    \nMaybe try downloaidng the model again? The commit hash of the gpt-j binary I'm\
    \ building is [75824e76bd41818ff8902d244feec1ec6c1d2c86](https://github.com/ggerganov/ggml/tree/75824e76bd41818ff8902d244feec1ec6c1d2c86)."
  created_at: 2023-04-18 08:11:36+00:00
  edited: true
  hidden: false
  id: 643e5ec8fa5a9e14c73cd2c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5ebcd2775959e77452ceec8aea070014.svg
      fullname: Larry Muniz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iguana0335
      type: user
    createdAt: '2023-04-19T19:46:38.000Z'
    data:
      edited: false
      editors:
      - iguana0335
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5ebcd2775959e77452ceec8aea070014.svg
          fullname: Larry Muniz
          isHf: false
          isPro: false
          name: iguana0335
          type: user
        html: '<p>Hello.<br>The model that i have been downloaded has the same hash
          and size  </p>

          <p>$ certutil -hashfile ggml-model-f16.bin sha256<br>SHA256 hash de ggml-model-f16.bin:<br>8672f372dae6bb7660f070355adb44dbb35747746525192b89a24466aabebbb4</p>

          <p>$ ls -l  ggml-model-f16.bin<br>-rw-r--r-- 1 root root 12104027673 Apr
          12 23:17 ggml-model-f16.bin</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/642ca6b4fc341371b02f18a8/85D1PDTrLYx7hFJIP4Oq5.jpeg"><img
          alt="sha256_size_from_huggingface.jpg" src="https://cdn-uploads.huggingface.co/production/uploads/642ca6b4fc341371b02f18a8/85D1PDTrLYx7hFJIP4Oq5.jpeg"></a></p>

          <p>Thank you.</p>

          '
        raw: "Hello.\nThe model that i have been downloaded has the same hash and\
          \ size  \n\n$ certutil -hashfile ggml-model-f16.bin sha256\nSHA256 hash\
          \ de ggml-model-f16.bin:\n8672f372dae6bb7660f070355adb44dbb35747746525192b89a24466aabebbb4\n\
          \n$ ls -l  ggml-model-f16.bin\n-rw-r--r-- 1 root root 12104027673 Apr 12\
          \ 23:17 ggml-model-f16.bin\n\n![sha256_size_from_huggingface.jpg](https://cdn-uploads.huggingface.co/production/uploads/642ca6b4fc341371b02f18a8/85D1PDTrLYx7hFJIP4Oq5.jpeg)\n\
          \nThank you."
        updatedAt: '2023-04-19T19:46:38.855Z'
      numEdits: 0
      reactions: []
    id: 6440451e2113f7dfcb561e52
    type: comment
  author: iguana0335
  content: "Hello.\nThe model that i have been downloaded has the same hash and size\
    \  \n\n$ certutil -hashfile ggml-model-f16.bin sha256\nSHA256 hash de ggml-model-f16.bin:\n\
    8672f372dae6bb7660f070355adb44dbb35747746525192b89a24466aabebbb4\n\n$ ls -l  ggml-model-f16.bin\n\
    -rw-r--r-- 1 root root 12104027673 Apr 12 23:17 ggml-model-f16.bin\n\n![sha256_size_from_huggingface.jpg](https://cdn-uploads.huggingface.co/production/uploads/642ca6b4fc341371b02f18a8/85D1PDTrLYx7hFJIP4Oq5.jpeg)\n\
    \nThank you."
  created_at: 2023-04-19 18:46:38+00:00
  edited: false
  hidden: false
  id: 6440451e2113f7dfcb561e52
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
      fullname: Javier de la Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: versae
      type: user
    createdAt: '2023-04-20T12:08:04.000Z'
    data:
      edited: true
      editors:
      - versae
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
          fullname: Javier de la Rosa
          isHf: false
          isPro: false
          name: versae
          type: user
        html: "<p>Just tried it again on a totally new VM from GCP using Ubuntu 20.04.01.\
          \ It all went fine. I'm don't know how to reproduce. You get the same errors\
          \ with the f32 model? Do you have an up to date version of ggml?</p>\n<p>This\
          \ are the steps I followed:</p>\n<pre><code class=\"language-bash\">apt\
          \ install cmake\ngit <span class=\"hljs-built_in\">clone</span> https://github.com/ggerganov/ggml\n\
          <span class=\"hljs-built_in\">cd</span> ggml/\n<span class=\"hljs-built_in\"\
          >mkdir</span> build &amp;&amp; <span class=\"hljs-built_in\">cd</span> build\n\
          cmake ..\nmake -j16 gpt-2 gpt-j\n<span class=\"hljs-built_in\">cd</span>\
          \ ..\n<span class=\"hljs-comment\"># create prompt file es.txt</span>\n\
          wget https://huggingface.co/bertin-project/bertin-gpt-j-6B-alpaca/resolve/main/ggml-model-f16.bin\n\
          TASK=<span class=\"hljs-string\">\"Escribe un email poniendo una excusa\
          \ para faltar a la reuni\xF3n\"</span> envsubst &lt; es.txt | ./build/bin/gpt-j\
          \ -m ggml-model-f16.bin\n</code></pre>\n"
        raw: "Just tried it again on a totally new VM from GCP using Ubuntu 20.04.01.\
          \ It all went fine. I'm don't know how to reproduce. You get the same errors\
          \ with the f32 model? Do you have an up to date version of ggml?\n\nThis\
          \ are the steps I followed:\n```bash\napt install cmake\ngit clone https://github.com/ggerganov/ggml\n\
          cd ggml/\nmkdir build && cd build\ncmake ..\nmake -j16 gpt-2 gpt-j\ncd ..\n\
          # create prompt file es.txt\nwget https://huggingface.co/bertin-project/bertin-gpt-j-6B-alpaca/resolve/main/ggml-model-f16.bin\n\
          TASK=\"Escribe un email poniendo una excusa para faltar a la reuni\xF3n\"\
          \ envsubst < es.txt | ./build/bin/gpt-j -m ggml-model-f16.bin\n```"
        updatedAt: '2023-04-20T12:11:45.562Z'
      numEdits: 3
      reactions: []
    id: 64412b24006550f1ed6a7949
    type: comment
  author: versae
  content: "Just tried it again on a totally new VM from GCP using Ubuntu 20.04.01.\
    \ It all went fine. I'm don't know how to reproduce. You get the same errors with\
    \ the f32 model? Do you have an up to date version of ggml?\n\nThis are the steps\
    \ I followed:\n```bash\napt install cmake\ngit clone https://github.com/ggerganov/ggml\n\
    cd ggml/\nmkdir build && cd build\ncmake ..\nmake -j16 gpt-2 gpt-j\ncd ..\n# create\
    \ prompt file es.txt\nwget https://huggingface.co/bertin-project/bertin-gpt-j-6B-alpaca/resolve/main/ggml-model-f16.bin\n\
    TASK=\"Escribe un email poniendo una excusa para faltar a la reuni\xF3n\" envsubst\
    \ < es.txt | ./build/bin/gpt-j -m ggml-model-f16.bin\n```"
  created_at: 2023-04-20 11:08:04+00:00
  edited: true
  hidden: false
  id: 64412b24006550f1ed6a7949
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5ebcd2775959e77452ceec8aea070014.svg
      fullname: Larry Muniz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iguana0335
      type: user
    createdAt: '2023-04-20T23:37:52.000Z'
    data:
      edited: true
      editors:
      - iguana0335
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5ebcd2775959e77452ceec8aea070014.svg
          fullname: Larry Muniz
          isHf: false
          isPro: false
          name: iguana0335
          type: user
        html: '<p>Hello.<br>Yes, I''m having a problem with gpt-j.exe in my computer.<br>I''m
          using " <a rel="nofollow" href="https://github.com/LostRuins/koboldcpp">https://github.com/LostRuins/koboldcpp</a>
          " utility with your model, and it''s working fine!!<br>Here is an example:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/642ca6b4fc341371b02f18a8/gx3q-69kbPAyd_vISD_mr.jpeg"><img
          alt="WithKoboldcpp.jpg" src="https://cdn-uploads.huggingface.co/production/uploads/642ca6b4fc341371b02f18a8/gx3q-69kbPAyd_vISD_mr.jpeg"></a></p>

          <p>I''m trying to solve that problem with the generation of gpt-j.exe (no
          errors...but not working); I''ll try in another PC.</p>

          <p>Thank you very much!! :)</p>

          '
        raw: 'Hello.

          Yes, I''m having a problem with gpt-j.exe in my computer.

          I''m using " https://github.com/LostRuins/koboldcpp " utility with your
          model, and it''s working fine!!

          Here is an example:


          ![WithKoboldcpp.jpg](https://cdn-uploads.huggingface.co/production/uploads/642ca6b4fc341371b02f18a8/gx3q-69kbPAyd_vISD_mr.jpeg)


          I''m trying to solve that problem with the generation of gpt-j.exe (no errors...but
          not working); I''ll try in another PC.


          Thank you very much!! :)'
        updatedAt: '2023-04-22T12:43:34.797Z'
      numEdits: 1
      reactions: []
    id: 6441ccd0a839ee80331f2ddd
    type: comment
  author: iguana0335
  content: 'Hello.

    Yes, I''m having a problem with gpt-j.exe in my computer.

    I''m using " https://github.com/LostRuins/koboldcpp " utility with your model,
    and it''s working fine!!

    Here is an example:


    ![WithKoboldcpp.jpg](https://cdn-uploads.huggingface.co/production/uploads/642ca6b4fc341371b02f18a8/gx3q-69kbPAyd_vISD_mr.jpeg)


    I''m trying to solve that problem with the generation of gpt-j.exe (no errors...but
    not working); I''ll try in another PC.


    Thank you very much!! :)'
  created_at: 2023-04-20 22:37:52+00:00
  edited: true
  hidden: false
  id: 6441ccd0a839ee80331f2ddd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
      fullname: Javier de la Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: versae
      type: user
    createdAt: '2023-04-21T11:07:41.000Z'
    data:
      edited: false
      editors:
      - versae
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
          fullname: Javier de la Rosa
          isHf: false
          isPro: false
          name: versae
          type: user
        html: '<p>Awesome! I''m closing the issue then :) </p>

          '
        raw: 'Awesome! I''m closing the issue then :) '
        updatedAt: '2023-04-21T11:07:41.316Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64426e7d752c03e8d2d81ff3
    id: 64426e7d752c03e8d2d81ff2
    type: comment
  author: versae
  content: 'Awesome! I''m closing the issue then :) '
  created_at: 2023-04-21 10:07:41+00:00
  edited: false
  hidden: false
  id: 64426e7d752c03e8d2d81ff2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
      fullname: Javier de la Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: versae
      type: user
    createdAt: '2023-04-21T11:07:41.000Z'
    data:
      status: closed
    id: 64426e7d752c03e8d2d81ff3
    type: status-change
  author: versae
  created_at: 2023-04-21 10:07:41+00:00
  id: 64426e7d752c03e8d2d81ff3
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6f85105ae0ea068ddaa9f555fd630f2.svg
      fullname: Guillermo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hwpoison89
      type: user
    createdAt: '2023-04-29T02:47:09.000Z'
    data:
      edited: false
      editors:
      - hwpoison89
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6f85105ae0ea068ddaa9f555fd630f2.svg
          fullname: Guillermo
          isHf: false
          isPro: false
          name: hwpoison89
          type: user
        html: "<p>Es posible una cuantizaci\xF3n a 4bits?</p>\n"
        raw: "Es posible una cuantizaci\xF3n a 4bits?"
        updatedAt: '2023-04-29T02:47:09.382Z'
      numEdits: 0
      reactions: []
    id: 644c852d45e79023c7ec1a9f
    type: comment
  author: hwpoison89
  content: "Es posible una cuantizaci\xF3n a 4bits?"
  created_at: 2023-04-29 01:47:09+00:00
  edited: false
  hidden: false
  id: 644c852d45e79023c7ec1a9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
      fullname: Javier de la Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: versae
      type: user
    createdAt: '2023-04-29T08:45:28.000Z'
    data:
      edited: false
      editors:
      - versae
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
          fullname: Javier de la Rosa
          isHf: false
          isPro: false
          name: versae
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;hwpoison89&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/hwpoison89\">@<span class=\"\
          underline\">hwpoison89</span></a></span>\n\n\t</span></span> yes, I believe\
          \ it's possible now using ggml, but they had some issues in the past depending\
          \ on how that quantization was performed.</p>\n"
        raw: '@hwpoison89 yes, I believe it''s possible now using ggml, but they had
          some issues in the past depending on how that quantization was performed.'
        updatedAt: '2023-04-29T08:45:28.325Z'
      numEdits: 0
      reactions: []
    id: 644cd92897a3b0904a498ebd
    type: comment
  author: versae
  content: '@hwpoison89 yes, I believe it''s possible now using ggml, but they had
    some issues in the past depending on how that quantization was performed.'
  created_at: 2023-04-29 07:45:28+00:00
  edited: false
  hidden: false
  id: 644cd92897a3b0904a498ebd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: bertin-project/bertin-gpt-j-6B-alpaca
repo_type: model
status: closed
target_branch: null
title: Convert to ggml format
