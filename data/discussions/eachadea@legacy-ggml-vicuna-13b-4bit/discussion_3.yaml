!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zbruceli
conflicting_files: null
created_at: 2023-04-06 17:02:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-04-06T18:02:46.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: '<p>Hi,</p>

          <p>First thank you for the effort to do the 4-bit quantization so it can
          be run with llama.cpp on local computers.</p>

          <p>I''m using it with the latest llama.cpp code, on a 16GB RAM macbook with
          M1 Pro chip. It was able to run the Alpaca-7b-4bit model easily, using about
          2GB of RAM and 2-3 cores, and the token generating speed is about 6 tokens
          per second.</p>

          <p>However, running the vicuna-13b-4bit model, the token generating speed
          is almost 1 token per minute (not a typo, and not per second) which renders
          it almost useless. I checked Mac OS''s activity monitor, surprisingly RAM
          seems still around 2GB, the CPU usage increased to about 4 cores. I''m not
          sure if you have tested on Mac as well, and what could be the issue.</p>

          <p>Thanks<br>Bruce</p>

          '
        raw: "Hi,\r\n\r\nFirst thank you for the effort to do the 4-bit quantization\
          \ so it can be run with llama.cpp on local computers.\r\n\r\nI'm using it\
          \ with the latest llama.cpp code, on a 16GB RAM macbook with M1 Pro chip.\
          \ It was able to run the Alpaca-7b-4bit model easily, using about 2GB of\
          \ RAM and 2-3 cores, and the token generating speed is about 6 tokens per\
          \ second.\r\n\r\nHowever, running the vicuna-13b-4bit model, the token generating\
          \ speed is almost 1 token per minute (not a typo, and not per second) which\
          \ renders it almost useless. I checked Mac OS's activity monitor, surprisingly\
          \ RAM seems still around 2GB, the CPU usage increased to about 4 cores.\
          \ I'm not sure if you have tested on Mac as well, and what could be the\
          \ issue.\r\n\r\nThanks\r\nBruce"
        updatedAt: '2023-04-06T18:02:46.811Z'
      numEdits: 0
      reactions: []
    id: 642f094670daaa6e7206bb3b
    type: comment
  author: zbruceli
  content: "Hi,\r\n\r\nFirst thank you for the effort to do the 4-bit quantization\
    \ so it can be run with llama.cpp on local computers.\r\n\r\nI'm using it with\
    \ the latest llama.cpp code, on a 16GB RAM macbook with M1 Pro chip. It was able\
    \ to run the Alpaca-7b-4bit model easily, using about 2GB of RAM and 2-3 cores,\
    \ and the token generating speed is about 6 tokens per second.\r\n\r\nHowever,\
    \ running the vicuna-13b-4bit model, the token generating speed is almost 1 token\
    \ per minute (not a typo, and not per second) which renders it almost useless.\
    \ I checked Mac OS's activity monitor, surprisingly RAM seems still around 2GB,\
    \ the CPU usage increased to about 4 cores. I'm not sure if you have tested on\
    \ Mac as well, and what could be the issue.\r\n\r\nThanks\r\nBruce"
  created_at: 2023-04-06 17:02:46+00:00
  edited: false
  hidden: false
  id: 642f094670daaa6e7206bb3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd12a8358b7d9aa82d849f76c57bd526.svg
      fullname: kname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kname
      type: user
    createdAt: '2023-04-07T14:59:35.000Z'
    data:
      edited: false
      editors:
      - kname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd12a8358b7d9aa82d849f76c57bd526.svg
          fullname: kname
          isHf: false
          isPro: false
          name: kname
          type: user
        html: '<p>This works fine on my m1 studio. My guess is you''re loading past
          your ram capacity and using your ssd as a swap or page file. This causes
          a dramatic shift in speed. You could try using --mlock in your ./main command
          which will attempt to use only ram. My guess is you''ll probably see segmentation
          faults and other crashes of the model. </p>

          <p>Another modifier you can use, but maybe only useful on your 7b models
          is -t # which is how many threads you''d like the model to use. If you have
          a 8c thread m1 mb pro I''d say 6 is the highest you should go. Or if you
          have 10c you could use -t 8</p>

          <p>The -t command I don''t think will speed up your model on this 13b as
          your most likely issue is the model (or parts of the model) being sent to
          your "virtual ram" aka swap aka page file in the ssd.</p>

          '
        raw: "This works fine on my m1 studio. My guess is you're loading past your\
          \ ram capacity and using your ssd as a swap or page file. This causes a\
          \ dramatic shift in speed. You could try using --mlock in your ./main command\
          \ which will attempt to use only ram. My guess is you'll probably see segmentation\
          \ faults and other crashes of the model. \n\nAnother modifier you can use,\
          \ but maybe only useful on your 7b models is -t # which is how many threads\
          \ you'd like the model to use. If you have a 8c thread m1 mb pro I'd say\
          \ 6 is the highest you should go. Or if you have 10c you could use -t 8\n\
          \nThe -t command I don't think will speed up your model on this 13b as your\
          \ most likely issue is the model (or parts of the model) being sent to your\
          \ \"virtual ram\" aka swap aka page file in the ssd."
        updatedAt: '2023-04-07T14:59:35.877Z'
      numEdits: 0
      reactions: []
    id: 64302fd7ec768944a8bf402a
    type: comment
  author: kname
  content: "This works fine on my m1 studio. My guess is you're loading past your\
    \ ram capacity and using your ssd as a swap or page file. This causes a dramatic\
    \ shift in speed. You could try using --mlock in your ./main command which will\
    \ attempt to use only ram. My guess is you'll probably see segmentation faults\
    \ and other crashes of the model. \n\nAnother modifier you can use, but maybe\
    \ only useful on your 7b models is -t # which is how many threads you'd like the\
    \ model to use. If you have a 8c thread m1 mb pro I'd say 6 is the highest you\
    \ should go. Or if you have 10c you could use -t 8\n\nThe -t command I don't think\
    \ will speed up your model on this 13b as your most likely issue is the model\
    \ (or parts of the model) being sent to your \"virtual ram\" aka swap aka page\
    \ file in the ssd."
  created_at: 2023-04-07 13:59:35+00:00
  edited: false
  hidden: false
  id: 64302fd7ec768944a8bf402a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-04-07T15:20:16.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: '<p>Thank you for the answer. And I''m monitoring the recent mmap change
          on llama.cpp.</p>

          <p>I will also try on another Linux machine which has 32GB physical RAM
          to isolate the issue.</p>

          '
        raw: 'Thank you for the answer. And I''m monitoring the recent mmap change
          on llama.cpp.


          I will also try on another Linux machine which has 32GB physical RAM to
          isolate the issue.'
        updatedAt: '2023-04-07T15:20:16.974Z'
      numEdits: 0
      reactions: []
    id: 643034b0bd622dfbdf4b9bae
    type: comment
  author: zbruceli
  content: 'Thank you for the answer. And I''m monitoring the recent mmap change on
    llama.cpp.


    I will also try on another Linux machine which has 32GB physical RAM to isolate
    the issue.'
  created_at: 2023-04-07 14:20:16+00:00
  edited: false
  hidden: false
  id: 643034b0bd622dfbdf4b9bae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2d512601c486a98818caade6280784a4.svg
      fullname: Billy Ballo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Billyballo
      type: user
    createdAt: '2023-04-07T21:19:37.000Z'
    data:
      edited: false
      editors:
      - Billyballo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2d512601c486a98818caade6280784a4.svg
          fullname: Billy Ballo
          isHf: false
          isPro: false
          name: Billyballo
          type: user
        html: '<p>Relevant issue on GitHub: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/767">https://github.com/ggerganov/llama.cpp/issues/767</a></p>

          '
        raw: 'Relevant issue on GitHub: https://github.com/ggerganov/llama.cpp/issues/767'
        updatedAt: '2023-04-07T21:19:37.624Z'
      numEdits: 0
      reactions: []
    id: 643088e9036373277e858143
    type: comment
  author: Billyballo
  content: 'Relevant issue on GitHub: https://github.com/ggerganov/llama.cpp/issues/767'
  created_at: 2023-04-07 20:19:37+00:00
  edited: false
  hidden: false
  id: 643088e9036373277e858143
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-04-07T21:36:44.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: '<p>Indeed it is due to the recent mmap change. Here is what I found:</p>

          <p>On 16GB Macbook M1 Pro, use --mlock solve the problem. it is generating
          tokens quite fast, and stable (for now)</p>

          '
        raw: 'Indeed it is due to the recent mmap change. Here is what I found:


          On 16GB Macbook M1 Pro, use --mlock solve the problem. it is generating
          tokens quite fast, and stable (for now)'
        updatedAt: '2023-04-07T21:36:44.129Z'
      numEdits: 0
      reactions: []
    id: 64308cec036373277e859b53
    type: comment
  author: zbruceli
  content: 'Indeed it is due to the recent mmap change. Here is what I found:


    On 16GB Macbook M1 Pro, use --mlock solve the problem. it is generating tokens
    quite fast, and stable (for now)'
  created_at: 2023-04-07 20:36:44+00:00
  edited: false
  hidden: false
  id: 64308cec036373277e859b53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c89d703de8644be9516479444204a34.svg
      fullname: Adam Timur Aslan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamaslan
      type: user
    createdAt: '2023-12-03T18:53:14.000Z'
    data:
      edited: false
      editors:
      - adamaslan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9710597395896912
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c89d703de8644be9516479444204a34.svg
          fullname: Adam Timur Aslan
          isHf: false
          isPro: false
          name: adamaslan
          type: user
        html: '<p>these are some great tips for those of us traveling about or yet
          to fully invest in larger more capable machines :)</p>

          <p>what would you say is the most efficient model to run locally with only
          8 - 16 gb of ram and 256 - 512 ssd? also do you think adding an external
          ssd would help at all?</p>

          '
        raw: 'these are some great tips for those of us traveling about or yet to
          fully invest in larger more capable machines :)


          what would you say is the most efficient model to run locally with only
          8 - 16 gb of ram and 256 - 512 ssd? also do you think adding an external
          ssd would help at all?'
        updatedAt: '2023-12-03T18:53:14.055Z'
      numEdits: 0
      reactions: []
    id: 656cce9a1f8d9b618dc67482
    type: comment
  author: adamaslan
  content: 'these are some great tips for those of us traveling about or yet to fully
    invest in larger more capable machines :)


    what would you say is the most efficient model to run locally with only 8 - 16
    gb of ram and 256 - 512 ssd? also do you think adding an external ssd would help
    at all?'
  created_at: 2023-12-03 18:53:14+00:00
  edited: false
  hidden: false
  id: 656cce9a1f8d9b618dc67482
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
      fullname: Bruce Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zbruceli
      type: user
    createdAt: '2023-12-04T19:19:23.000Z'
    data:
      edited: false
      editors:
      - zbruceli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9647773504257202
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zjZanVr7LG7UbvuOUfmX_.jpeg?w=200&h=200&f=face
          fullname: Bruce Li
          isHf: false
          isPro: false
          name: zbruceli
          type: user
        html: '<blockquote>

          <p>these are some great tips for those of us traveling about or yet to fully
          invest in larger more capable machines :)</p>

          <p>what would you say is the most efficient model to run locally with only
          8 - 16 gb of ram and 256 - 512 ssd? also do you think adding an external
          ssd would help at all?</p>

          </blockquote>

          <p>You can start with a good 7B model (e.g. Zephyr), 4bit quantization,
          GGUF model. That should fit into your RAM fine. SSD does not really help,
          since the model needs to fit into RAM to make it run fast.</p>

          <p>If you close your other apps and free up enough RAM, then maybe 13B will
          run as well. But it might be quite a bit slower.</p>

          '
        raw: "> these are some great tips for those of us traveling about or yet to\
          \ fully invest in larger more capable machines :)\n> \n> what would you\
          \ say is the most efficient model to run locally with only 8 - 16 gb of\
          \ ram and 256 - 512 ssd? also do you think adding an external ssd would\
          \ help at all?\n\nYou can start with a good 7B model (e.g. Zephyr), 4bit\
          \ quantization, GGUF model. That should fit into your RAM fine. SSD does\
          \ not really help, since the model needs to fit into RAM to make it run\
          \ fast.\n\nIf you close your other apps and free up enough RAM, then maybe\
          \ 13B will run as well. But it might be quite a bit slower."
        updatedAt: '2023-12-04T19:19:23.220Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - adamaslan
    id: 656e263b90d556ffa65725f3
    type: comment
  author: zbruceli
  content: "> these are some great tips for those of us traveling about or yet to\
    \ fully invest in larger more capable machines :)\n> \n> what would you say is\
    \ the most efficient model to run locally with only 8 - 16 gb of ram and 256 -\
    \ 512 ssd? also do you think adding an external ssd would help at all?\n\nYou\
    \ can start with a good 7B model (e.g. Zephyr), 4bit quantization, GGUF model.\
    \ That should fit into your RAM fine. SSD does not really help, since the model\
    \ needs to fit into RAM to make it run fast.\n\nIf you close your other apps and\
    \ free up enough RAM, then maybe 13B will run as well. But it might be quite a\
    \ bit slower."
  created_at: 2023-12-04 19:19:23+00:00
  edited: false
  hidden: false
  id: 656e263b90d556ffa65725f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c89d703de8644be9516479444204a34.svg
      fullname: Adam Timur Aslan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamaslan
      type: user
    createdAt: '2023-12-04T19:39:30.000Z'
    data:
      edited: false
      editors:
      - adamaslan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.991901159286499
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c89d703de8644be9516479444204a34.svg
          fullname: Adam Timur Aslan
          isHf: false
          isPro: false
          name: adamaslan
          type: user
        html: '<p>Okay very cool! Will give that model a try! what do you think some
          of the cooler things I could do are with that model?</p>

          '
        raw: Okay very cool! Will give that model a try! what do you think some of
          the cooler things I could do are with that model?
        updatedAt: '2023-12-04T19:39:30.670Z'
      numEdits: 0
      reactions: []
    id: 656e2af202a56b531af2ceac
    type: comment
  author: adamaslan
  content: Okay very cool! Will give that model a try! what do you think some of the
    cooler things I could do are with that model?
  created_at: 2023-12-04 19:39:30+00:00
  edited: false
  hidden: false
  id: 656e2af202a56b531af2ceac
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: eachadea/legacy-ggml-vicuna-13b-4bit
repo_type: model
status: open
target_branch: null
title: Extremely slow on 16GB Macbook M1 Pro
