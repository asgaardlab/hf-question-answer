!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ravikiran3690
conflicting_files: null
created_at: 2023-07-12 21:08:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/463aaf1beab6fe65415edcaa3daaf992.svg
      fullname: Ravi Kiran Dendukuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ravikiran3690
      type: user
    createdAt: '2023-07-12T22:08:02.000Z'
    data:
      edited: false
      editors:
      - ravikiran3690
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.880729079246521
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/463aaf1beab6fe65415edcaa3daaf992.svg
          fullname: Ravi Kiran Dendukuri
          isHf: false
          isPro: false
          name: ravikiran3690
          type: user
        html: '<p>tiiuae/falcon-7b Has changed the file configuration_RW.py to configuration_falcon.py
          in their newest commit and that''s causing the error.</p>

          '
        raw: tiiuae/falcon-7b Has changed the file configuration_RW.py to configuration_falcon.py
          in their newest commit and that's causing the error.
        updatedAt: '2023-07-12T22:08:02.150Z'
      numEdits: 0
      reactions:
      - count: 9
        reaction: "\U0001F614"
        users:
        - envizzion
        - darthhexx
        - yuezhaoesri
        - mrmllm
        - DipanAI
        - timnon
        - avnishkr
        - prakash8486
        - ChinniAjay
      - count: 1
        reaction: "\U0001F44D"
        users:
        - avnishkr
    id: 64af2442f164159c85e5a63a
    type: comment
  author: ravikiran3690
  content: tiiuae/falcon-7b Has changed the file configuration_RW.py to configuration_falcon.py
    in their newest commit and that's causing the error.
  created_at: 2023-07-12 21:08:02+00:00
  edited: false
  hidden: false
  id: 64af2442f164159c85e5a63a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2b3bb9cb5dc20bce5c4d59953d44dec6.svg
      fullname: Akshay Nambiar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: axay
      type: user
    createdAt: '2023-07-13T12:34:17.000Z'
    data:
      edited: false
      editors:
      - axay
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.32505354285240173
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2b3bb9cb5dc20bce5c4d59953d44dec6.svg
          fullname: Akshay Nambiar
          isHf: false
          isPro: false
          name: axay
          type: user
        html: '<p>The Falcon model has been updated. Change the config.json to the
          below format. That worked for me. </p>

          <p>{<br>  "_name_or_path": "tiiuae/falcon-7b",<br>  "alibi": false,<br>  "apply_residual_connection_post_layernorm":
          false,<br>  "architectures": [<br>    "FalconForCausalLM"<br>  ],<br>  "attention_dropout":
          0.0,<br>  "auto_map": {<br>    "AutoConfig": "tiiuae/falcon-7b--configuration_falcon.FalconConfig",<br>    "AutoModel":
          "tiiuae/falcon-7b--modeling_falcon.FalconModel",<br>    "AutoModelForCausalLM":
          "tiiuae/falcon-7b--modeling_falcon.FalconForCausalLM",<br>    "AutoModelForQuestionAnswering":
          "tiiuae/falcon-7b--modeling_falcon.FalconForQuestionAnswering",<br>    "AutoModelForSequenceClassification":
          "tiiuae/falcon-7b--modeling_falcon.FalconForSequenceClassification",<br>    "AutoModelForTokenClassification":
          "tiiuae/falcon-7b--modeling_falcon.FalonForTokenClassification"<br>  },<br>  "bias":
          false,<br>  "bos_token_id": 11,<br>  "eos_token_id": 11,<br>  "hidden_dropout":
          0.0,<br>  "hidden_size": 4544,<br>  "initializer_range": 0.02,<br>  "layer_norm_epsilon":
          1e-05,<br>  "model_type": "RefinedWebModel",<br>  "multi_query": true,<br>  "n_head":
          71,<br>  "n_layer": 32,<br>  "parallel_attn": true,<br>  "torch_dtype":
          "bfloat16",<br>  "transformers_version": "4.30.0.dev0",<br>  "use_cache":
          true,<br>  "vocab_size": 65024<br>}</p>

          '
        raw: "The Falcon model has been updated. Change the config.json to the below\
          \ format. That worked for me. \n\n\n{\n  \"_name_or_path\": \"tiiuae/falcon-7b\"\
          ,\n  \"alibi\": false,\n  \"apply_residual_connection_post_layernorm\":\
          \ false,\n  \"architectures\": [\n    \"FalconForCausalLM\"\n  ],\n  \"\
          attention_dropout\": 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"tiiuae/falcon-7b--configuration_falcon.FalconConfig\"\
          ,\n    \"AutoModel\": \"tiiuae/falcon-7b--modeling_falcon.FalconModel\"\
          ,\n    \"AutoModelForCausalLM\": \"tiiuae/falcon-7b--modeling_falcon.FalconForCausalLM\"\
          ,\n    \"AutoModelForQuestionAnswering\": \"tiiuae/falcon-7b--modeling_falcon.FalconForQuestionAnswering\"\
          ,\n    \"AutoModelForSequenceClassification\": \"tiiuae/falcon-7b--modeling_falcon.FalconForSequenceClassification\"\
          ,\n    \"AutoModelForTokenClassification\": \"tiiuae/falcon-7b--modeling_falcon.FalonForTokenClassification\"\
          \n  },\n  \"bias\": false,\n  \"bos_token_id\": 11,\n  \"eos_token_id\"\
          : 11,\n  \"hidden_dropout\": 0.0,\n  \"hidden_size\": 4544,\n  \"initializer_range\"\
          : 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"RefinedWebModel\"\
          ,\n  \"multi_query\": true,\n  \"n_head\": 71,\n  \"n_layer\": 32,\n  \"\
          parallel_attn\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\"\
          : \"4.30.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 65024\n}\n\
          \n"
        updatedAt: '2023-07-13T12:34:17.597Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - ravikiran3690
        - AbdurRahim
        - NuriaAldamaG
    id: 64afef491e39d3cd14eeb05b
    type: comment
  author: axay
  content: "The Falcon model has been updated. Change the config.json to the below\
    \ format. That worked for me. \n\n\n{\n  \"_name_or_path\": \"tiiuae/falcon-7b\"\
    ,\n  \"alibi\": false,\n  \"apply_residual_connection_post_layernorm\": false,\n\
    \  \"architectures\": [\n    \"FalconForCausalLM\"\n  ],\n  \"attention_dropout\"\
    : 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"tiiuae/falcon-7b--configuration_falcon.FalconConfig\"\
    ,\n    \"AutoModel\": \"tiiuae/falcon-7b--modeling_falcon.FalconModel\",\n   \
    \ \"AutoModelForCausalLM\": \"tiiuae/falcon-7b--modeling_falcon.FalconForCausalLM\"\
    ,\n    \"AutoModelForQuestionAnswering\": \"tiiuae/falcon-7b--modeling_falcon.FalconForQuestionAnswering\"\
    ,\n    \"AutoModelForSequenceClassification\": \"tiiuae/falcon-7b--modeling_falcon.FalconForSequenceClassification\"\
    ,\n    \"AutoModelForTokenClassification\": \"tiiuae/falcon-7b--modeling_falcon.FalonForTokenClassification\"\
    \n  },\n  \"bias\": false,\n  \"bos_token_id\": 11,\n  \"eos_token_id\": 11,\n\
    \  \"hidden_dropout\": 0.0,\n  \"hidden_size\": 4544,\n  \"initializer_range\"\
    : 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"RefinedWebModel\"\
    ,\n  \"multi_query\": true,\n  \"n_head\": 71,\n  \"n_layer\": 32,\n  \"parallel_attn\"\
    : true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.30.0.dev0\"\
    ,\n  \"use_cache\": true,\n  \"vocab_size\": 65024\n}\n\n"
  created_at: 2023-07-13 11:34:17+00:00
  edited: false
  hidden: false
  id: 64afef491e39d3cd14eeb05b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/463aaf1beab6fe65415edcaa3daaf992.svg
      fullname: Ravi Kiran Dendukuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ravikiran3690
      type: user
    createdAt: '2023-07-13T16:27:57.000Z'
    data:
      status: closed
    id: 64b0260d9275953512b3df79
    type: status-change
  author: ravikiran3690
  created_at: 2023-07-13 15:27:57+00:00
  id: 64b0260d9275953512b3df79
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/463aaf1beab6fe65415edcaa3daaf992.svg
      fullname: Ravi Kiran Dendukuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ravikiran3690
      type: user
    createdAt: '2023-07-13T16:28:02.000Z'
    data:
      status: open
    id: 64b02612bf3f6d8228ca032d
    type: status-change
  author: ravikiran3690
  created_at: 2023-07-13 15:28:02+00:00
  id: 64b02612bf3f6d8228ca032d
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/463aaf1beab6fe65415edcaa3daaf992.svg
      fullname: Ravi Kiran Dendukuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ravikiran3690
      type: user
    createdAt: '2023-07-13T16:28:15.000Z'
    data:
      status: closed
    id: 64b0261f4e89123e13d32ed0
    type: status-change
  author: ravikiran3690
  created_at: 2023-07-13 15:28:15+00:00
  id: 64b0261f4e89123e13d32ed0
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: ybelkada/falcon-7b-sharded-bf16
repo_type: model
status: closed
target_branch: null
title: 'OSError: tiiuae/falcon-7b does not appear to have a file named configuration_RW.py.
  Checkout ''https://huggingface.co/tiiuae/falcon-7b/main'' for available files.'
