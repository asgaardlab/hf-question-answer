!!python/object:huggingface_hub.community.DiscussionWithDetails
author: grimulkan
conflicting_files: null
created_at: 2023-09-23 05:33:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2023-09-23T06:33:09.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9678035974502563
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Hi &amp; thanks for a very nice paper. From the paper, it appears
          you managed to achieve an effective batch size of 64 on 8xA100, described
          as gradient accumulation = 8 x batch size = 8. Was that statement in the
          paper only for the smaller models, or did it apply to the 70B model @ 32K
          context as well? </p>

          <p>It also looks like you used LORA (as opposed to QLORA or GPTQ-LORA),
          and it doesn''t seem like you used bitsandbytes (load_in_8_bit), so I''m
          curious what batch size you achieved with 8xA100 using your method. If you
          really managed batch size = 8 that''s very impressive and significantly
          cheaper than any other long-context training method around today, including
          quantization-based methods!</p>

          <p>I also see no reason why your method cannot be combined with quantization
          for even more efficient training.</p>

          '
        raw: "Hi & thanks for a very nice paper. From the paper, it appears you managed\
          \ to achieve an effective batch size of 64 on 8xA100, described as gradient\
          \ accumulation = 8 x batch size = 8. Was that statement in the paper only\
          \ for the smaller models, or did it apply to the 70B model @ 32K context\
          \ as well? \n\nIt also looks like you used LORA (as opposed to QLORA or\
          \ GPTQ-LORA), and it doesn't seem like you used bitsandbytes (load_in_8_bit),\
          \ so I'm curious what batch size you achieved with 8xA100 using your method.\
          \ If you really managed batch size = 8 that's very impressive and significantly\
          \ cheaper than any other long-context training method around today, including\
          \ quantization-based methods!\n\nI also see no reason why your method cannot\
          \ be combined with quantization for even more efficient training."
        updatedAt: '2023-09-23T06:35:47.975Z'
      numEdits: 1
      reactions: []
    id: 650e86a5f874d950df26779c
    type: comment
  author: grimulkan
  content: "Hi & thanks for a very nice paper. From the paper, it appears you managed\
    \ to achieve an effective batch size of 64 on 8xA100, described as gradient accumulation\
    \ = 8 x batch size = 8. Was that statement in the paper only for the smaller models,\
    \ or did it apply to the 70B model @ 32K context as well? \n\nIt also looks like\
    \ you used LORA (as opposed to QLORA or GPTQ-LORA), and it doesn't seem like you\
    \ used bitsandbytes (load_in_8_bit), so I'm curious what batch size you achieved\
    \ with 8xA100 using your method. If you really managed batch size = 8 that's very\
    \ impressive and significantly cheaper than any other long-context training method\
    \ around today, including quantization-based methods!\n\nI also see no reason\
    \ why your method cannot be combined with quantization for even more efficient\
    \ training."
  created_at: 2023-09-23 05:33:09+00:00
  edited: true
  hidden: false
  id: 650e86a5f874d950df26779c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653710384819-62919485a29097b211bc7b83.png?w=200&h=200&f=face
      fullname: YukangChen
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Yukang
      type: user
    createdAt: '2023-09-23T07:17:23.000Z'
    data:
      edited: false
      editors:
      - Yukang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.36506208777427673
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653710384819-62919485a29097b211bc7b83.png?w=200&h=200&f=face
          fullname: YukangChen
          isHf: false
          isPro: false
          name: Yukang
          type: user
        html: '<p>Hi, </p>

          <p>Thanks for your question. </p>

          <p>Yes, it is. To be more clear, batch_size_per_gpu is 1. gradient_acc_steps
          is 8. We use 8x A100 GPUs. Thus, the global effective batch size is 1x8x8=64.
          We use deespeed with stage3 and flash-attn2.</p>

          <p>The training script is just like:<br>torchrun --nproc_per_node=8 --master_port=6034
          fine-tune.py  <br>        --model_name_or_path /dataset/pretrained-models/Llama-2-70b-hf
          <br>        --bf16 True <br>        --output_dir /dataset/models/   <br>        --cache_dir
          /dataset/datasets/redpajama <br>        --num_train_epochs 1  <br>        --model_max_length
          32768 <br>        --use_flash_attn True <br>        --low_rank_training
          True <br>        --per_device_train_batch_size 1     <br>        --per_device_eval_batch_size
          2     <br>        --gradient_accumulation_steps 8     <br>        --evaluation_strategy
          "no"     <br>        --save_strategy "steps"     <br>        --save_steps
          1000     <br>        --save_total_limit 16     <br>        --learning_rate
          2e-5     <br>        --weight_decay 0.0     <br>        --warmup_steps 20     <br>        --lr_scheduler_type
          "constant_with_warmup"     <br>        --logging_steps 1     <br>        --deepspeed
          configs/default_offload_opt_param.json <br>        --tf32 True <br>        --max_steps
          1000</p>

          <p>The GPU memory cost on our machine is<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62919485a29097b211bc7b83/48HHYSlhOw255_E3RSgEM.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/62919485a29097b211bc7b83/48HHYSlhOw255_E3RSgEM.png"></a></p>

          <p>Regards,<br>Yukang Chen</p>

          '
        raw: "Hi, \n\nThanks for your question. \n\nYes, it is. To be more clear,\
          \ batch_size_per_gpu is 1. gradient_acc_steps is 8. We use 8x A100 GPUs.\
          \ Thus, the global effective batch size is 1x8x8=64. We use deespeed with\
          \ stage3 and flash-attn2.\n\nThe training script is just like:\ntorchrun\
          \ --nproc_per_node=8 --master_port=6034 fine-tune.py  \\\n        --model_name_or_path\
          \ /dataset/pretrained-models/Llama-2-70b-hf \\\n        --bf16 True \\\n\
          \        --output_dir /dataset/models/   \\\n        --cache_dir /dataset/datasets/redpajama\
          \ \\\n        --num_train_epochs 1  \\\n        --model_max_length 32768\
          \ \\\n        --use_flash_attn True \\\n        --low_rank_training True\
          \ \\\n        --per_device_train_batch_size 1     \\\n        --per_device_eval_batch_size\
          \ 2     \\\n        --gradient_accumulation_steps 8     \\\n        --evaluation_strategy\
          \ \"no\"     \\\n        --save_strategy \"steps\"     \\\n        --save_steps\
          \ 1000     \\\n        --save_total_limit 16     \\\n        --learning_rate\
          \ 2e-5     \\\n        --weight_decay 0.0     \\\n        --warmup_steps\
          \ 20     \\\n        --lr_scheduler_type \"constant_with_warmup\"     \\\
          \n        --logging_steps 1     \\\n        --deepspeed configs/default_offload_opt_param.json\
          \ \\\n        --tf32 True \\\n        --max_steps 1000\n\nThe GPU memory\
          \ cost on our machine is\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62919485a29097b211bc7b83/48HHYSlhOw255_E3RSgEM.png)\n\
          \n\nRegards,\nYukang Chen"
        updatedAt: '2023-09-23T07:17:23.647Z'
      numEdits: 0
      reactions: []
    id: 650e91035877b1c0770230d6
    type: comment
  author: Yukang
  content: "Hi, \n\nThanks for your question. \n\nYes, it is. To be more clear, batch_size_per_gpu\
    \ is 1. gradient_acc_steps is 8. We use 8x A100 GPUs. Thus, the global effective\
    \ batch size is 1x8x8=64. We use deespeed with stage3 and flash-attn2.\n\nThe\
    \ training script is just like:\ntorchrun --nproc_per_node=8 --master_port=6034\
    \ fine-tune.py  \\\n        --model_name_or_path /dataset/pretrained-models/Llama-2-70b-hf\
    \ \\\n        --bf16 True \\\n        --output_dir /dataset/models/   \\\n   \
    \     --cache_dir /dataset/datasets/redpajama \\\n        --num_train_epochs 1\
    \  \\\n        --model_max_length 32768 \\\n        --use_flash_attn True \\\n\
    \        --low_rank_training True \\\n        --per_device_train_batch_size 1\
    \     \\\n        --per_device_eval_batch_size 2     \\\n        --gradient_accumulation_steps\
    \ 8     \\\n        --evaluation_strategy \"no\"     \\\n        --save_strategy\
    \ \"steps\"     \\\n        --save_steps 1000     \\\n        --save_total_limit\
    \ 16     \\\n        --learning_rate 2e-5     \\\n        --weight_decay 0.0 \
    \    \\\n        --warmup_steps 20     \\\n        --lr_scheduler_type \"constant_with_warmup\"\
    \     \\\n        --logging_steps 1     \\\n        --deepspeed configs/default_offload_opt_param.json\
    \ \\\n        --tf32 True \\\n        --max_steps 1000\n\nThe GPU memory cost\
    \ on our machine is\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62919485a29097b211bc7b83/48HHYSlhOw255_E3RSgEM.png)\n\
    \n\nRegards,\nYukang Chen"
  created_at: 2023-09-23 06:17:23+00:00
  edited: false
  hidden: false
  id: 650e91035877b1c0770230d6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Yukang/Llama-2-70b-longlora-32k
repo_type: model
status: open
target_branch: null
title: Training VRAM for 70B 32K
