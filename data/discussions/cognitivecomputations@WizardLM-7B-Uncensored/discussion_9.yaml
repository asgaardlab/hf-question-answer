!!python/object:huggingface_hub.community.DiscussionWithDetails
author: scribematic
conflicting_files: null
created_at: 2023-05-09 17:53:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ee30555c0755df4d46c0b5eedd53862c.svg
      fullname: Alexander Sheppert
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: scribematic
      type: user
    createdAt: '2023-05-09T18:53:51.000Z'
    data:
      edited: true
      editors:
      - julien-c
      - scribematic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p>Hi, I am trying to deploy on sagemaker and am running into some\
          \ issues I don't get on other models</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> sagemaker.huggingface <span class=\"\
          hljs-keyword\">import</span> HuggingFaceModel\n<span class=\"hljs-keyword\"\
          >import</span> boto3\n<span class=\"hljs-keyword\">from</span> sagemaker\
          \ <span class=\"hljs-keyword\">import</span> Session\n\n<span class=\"hljs-comment\"\
          ># Replace with your access key and secret key</span>\naccess_key = <span\
          \ class=\"hljs-string\">\"key\"</span>\nsecret_key = <span class=\"hljs-string\"\
          >\"key\"</span>\n\n<span class=\"hljs-comment\"># Create a boto3 session\
          \ with the specified access key and secret key</span>\nboto3_session = boto3.Session(\n\
          \    aws_access_key_id=access_key,\n    aws_secret_access_key=secret_key,\n\
          \    region_name=<span class=\"hljs-string\">\"us-east-1\"</span>\n)\n\n\
          <span class=\"hljs-comment\"># Use the boto3 session to create the IAM client</span>\n\
          iam_client = boto3_session.client(<span class=\"hljs-string\">'iam'</span>)\n\
          \n<span class=\"hljs-comment\"># Create a SageMaker session with the custom\
          \ boto3 session</span>\nsagemaker_session = Session(boto_session=boto3_session)\n\
          \nrole = iam_client.get_role(RoleName=<span class=\"hljs-string\">'ROLE'</span>)[<span\
          \ class=\"hljs-string\">'Role'</span>][<span class=\"hljs-string\">'Arn'</span>]\n\
          <span class=\"hljs-comment\"># Hub Model configuration. https://huggingface.co/models</span>\n\
          hub = {\n    <span class=\"hljs-string\">'HF_MODEL_ID'</span>:<span class=\"\
          hljs-string\">'TheBloke/WizardLM-7B-uncensored-GPTQ'</span>,\n    <span\
          \ class=\"hljs-string\">'HF_TASK'</span>:<span class=\"hljs-string\">'text-generation'</span>\n\
          }\n\n<span class=\"hljs-comment\"># create Hugging Face Model Class</span>\n\
          huggingface_model = HuggingFaceModel(\n    transformers_version=<span class=\"\
          hljs-string\">'4.17.0'</span>,\n    pytorch_version=<span class=\"hljs-string\"\
          >'1.10.2'</span>,\n    py_version=<span class=\"hljs-string\">'py38'</span>,\n\
          \    env=hub,\n    role=role,\n    sagemaker_session=sagemaker_session \
          \ <span class=\"hljs-comment\"># Pass the custom SageMaker session</span>\n\
          )\n\n<span class=\"hljs-comment\"># deploy model to SageMaker Inference</span>\n\
          predictor = huggingface_model.deploy(\n    initial_instance_count=<span\
          \ class=\"hljs-number\">1</span>, <span class=\"hljs-comment\"># number\
          \ of instances</span>\n    instance_type=<span class=\"hljs-string\">'ml.g4dn.2xlarge'</span>\
          \ <span class=\"hljs-comment\"># ec2 instance type</span>\n)\n</code></pre>\n\
          <p>I am getting the following error trying to query the endpoint after deployment:</p>\n\
          <pre><code>{\n  \"code\": 400,\n  \"type\": \"InternalServerException\"\
          ,\n  \"message\": \"\\u0027llama\\u0027\"\n}\n</code></pre>\n<p>Is this\
          \ a library that it doesn't import? Do I need to custom set this up instead\
          \ of just deploying to sagemaker? The inference huggingface export doesn't\
          \ work for the same reason, probably worth bringing to your attention.</p>\n\
          <p>Thank you</p>\n"
        raw: "Hi, I am trying to deploy on sagemaker and am running into some issues\
          \ I don't get on other models\n\n```python\nfrom sagemaker.huggingface import\
          \ HuggingFaceModel\nimport boto3\nfrom sagemaker import Session\n\n# Replace\
          \ with your access key and secret key\naccess_key = \"key\"\nsecret_key\
          \ = \"key\"\n\n# Create a boto3 session with the specified access key and\
          \ secret key\nboto3_session = boto3.Session(\n    aws_access_key_id=access_key,\n\
          \    aws_secret_access_key=secret_key,\n    region_name=\"us-east-1\"\n\
          )\n\n# Use the boto3 session to create the IAM client\niam_client = boto3_session.client('iam')\n\
          \n# Create a SageMaker session with the custom boto3 session\nsagemaker_session\
          \ = Session(boto_session=boto3_session)\n\nrole = iam_client.get_role(RoleName='ROLE')['Role']['Arn']\n\
          # Hub Model configuration. https://huggingface.co/models\nhub = {\n    'HF_MODEL_ID':'TheBloke/WizardLM-7B-uncensored-GPTQ',\n\
          \    'HF_TASK':'text-generation'\n}\n\n# create Hugging Face Model Class\n\
          huggingface_model = HuggingFaceModel(\n    transformers_version='4.17.0',\n\
          \    pytorch_version='1.10.2',\n    py_version='py38',\n    env=hub,\n \
          \   role=role,\n    sagemaker_session=sagemaker_session  # Pass the custom\
          \ SageMaker session\n)\n\n# deploy model to SageMaker Inference\npredictor\
          \ = huggingface_model.deploy(\n    initial_instance_count=1, # number of\
          \ instances\n    instance_type='ml.g4dn.2xlarge' # ec2 instance type\n)\n\
          ```\n\nI am getting the following error trying to query the endpoint after\
          \ deployment:\n```\n{\n  \"code\": 400,\n  \"type\": \"InternalServerException\"\
          ,\n  \"message\": \"\\u0027llama\\u0027\"\n}\n```\n\nIs this a library that\
          \ it doesn't import? Do I need to custom set this up instead of just deploying\
          \ to sagemaker? The inference huggingface export doesn't work for the same\
          \ reason, probably worth bringing to your attention.\n\nThank you"
        updatedAt: '2023-05-11T10:55:16.612Z'
      numEdits: 5
      reactions: []
    id: 645a96bf5e6871b4b2d5598c
    type: comment
  author: scribematic
  content: "Hi, I am trying to deploy on sagemaker and am running into some issues\
    \ I don't get on other models\n\n```python\nfrom sagemaker.huggingface import\
    \ HuggingFaceModel\nimport boto3\nfrom sagemaker import Session\n\n# Replace with\
    \ your access key and secret key\naccess_key = \"key\"\nsecret_key = \"key\"\n\
    \n# Create a boto3 session with the specified access key and secret key\nboto3_session\
    \ = boto3.Session(\n    aws_access_key_id=access_key,\n    aws_secret_access_key=secret_key,\n\
    \    region_name=\"us-east-1\"\n)\n\n# Use the boto3 session to create the IAM\
    \ client\niam_client = boto3_session.client('iam')\n\n# Create a SageMaker session\
    \ with the custom boto3 session\nsagemaker_session = Session(boto_session=boto3_session)\n\
    \nrole = iam_client.get_role(RoleName='ROLE')['Role']['Arn']\n# Hub Model configuration.\
    \ https://huggingface.co/models\nhub = {\n    'HF_MODEL_ID':'TheBloke/WizardLM-7B-uncensored-GPTQ',\n\
    \    'HF_TASK':'text-generation'\n}\n\n# create Hugging Face Model Class\nhuggingface_model\
    \ = HuggingFaceModel(\n    transformers_version='4.17.0',\n    pytorch_version='1.10.2',\n\
    \    py_version='py38',\n    env=hub,\n    role=role,\n    sagemaker_session=sagemaker_session\
    \  # Pass the custom SageMaker session\n)\n\n# deploy model to SageMaker Inference\n\
    predictor = huggingface_model.deploy(\n    initial_instance_count=1, # number\
    \ of instances\n    instance_type='ml.g4dn.2xlarge' # ec2 instance type\n)\n```\n\
    \nI am getting the following error trying to query the endpoint after deployment:\n\
    ```\n{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\"\
    : \"\\u0027llama\\u0027\"\n}\n```\n\nIs this a library that it doesn't import?\
    \ Do I need to custom set this up instead of just deploying to sagemaker? The\
    \ inference huggingface export doesn't work for the same reason, probably worth\
    \ bringing to your attention.\n\nThank you"
  created_at: 2023-05-09 17:53:51+00:00
  edited: true
  hidden: false
  id: 645a96bf5e6871b4b2d5598c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/abb35e82b57a1df2d296dab274d04372.svg
      fullname: Aaron McClendon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: magicsquares137
      type: user
    createdAt: '2023-05-11T03:37:14.000Z'
    data:
      edited: false
      editors:
      - magicsquares137
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/abb35e82b57a1df2d296dab274d04372.svg
          fullname: Aaron McClendon
          isHf: false
          isPro: false
          name: magicsquares137
          type: user
        html: '<p>I am getting the same error, any suggestions? Im simply using the
          sagemaker deployment code listed above</p>

          '
        raw: I am getting the same error, any suggestions? Im simply using the sagemaker
          deployment code listed above
        updatedAt: '2023-05-11T03:37:14.106Z'
      numEdits: 0
      reactions: []
    id: 645c62ea0f9f526e8d041b54
    type: comment
  author: magicsquares137
  content: I am getting the same error, any suggestions? Im simply using the sagemaker
    deployment code listed above
  created_at: 2023-05-11 02:37:14+00:00
  edited: false
  hidden: false
  id: 645c62ea0f9f526e8d041b54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-05-11T08:57:29.000Z'
    data:
      edited: true
      editors:
      - ehartford
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>I''m afraid I don''t know anything about sagemaker.  But I''m happy
          to take pull requests if anyone figures out what''s wrong</p>

          '
        raw: I'm afraid I don't know anything about sagemaker.  But I'm happy to take
          pull requests if anyone figures out what's wrong
        updatedAt: '2023-05-11T08:57:43.713Z'
      numEdits: 1
      reactions: []
    id: 645cadf9f1e3b219cb075676
    type: comment
  author: ehartford
  content: I'm afraid I don't know anything about sagemaker.  But I'm happy to take
    pull requests if anyone figures out what's wrong
  created_at: 2023-05-11 07:57:29+00:00
  edited: true
  hidden: false
  id: 645cadf9f1e3b219cb075676
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/abb35e82b57a1df2d296dab274d04372.svg
      fullname: Aaron McClendon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: magicsquares137
      type: user
    createdAt: '2023-05-18T04:02:43.000Z'
    data:
      edited: false
      editors:
      - magicsquares137
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/abb35e82b57a1df2d296dab274d04372.svg
          fullname: Aaron McClendon
          isHf: false
          isPro: false
          name: magicsquares137
          type: user
        html: '<p>I figured out the error, unfortunately dont see an immediate solution
          to deploy this as a sagemaker endpoint. The sagemaker env only supports
          HF transformers versions up to 4.7 or something, and this model is a fine
          tuned llama model, which was done on 4.28: <a href="https://huggingface.co/decapoda-research/llama-7b-hf/discussions/39">https://huggingface.co/decapoda-research/llama-7b-hf/discussions/39</a></p>

          <p>not sure when support  will be available</p>

          '
        raw: 'I figured out the error, unfortunately dont see an immediate solution
          to deploy this as a sagemaker endpoint. The sagemaker env only supports
          HF transformers versions up to 4.7 or something, and this model is a fine
          tuned llama model, which was done on 4.28: https://huggingface.co/decapoda-research/llama-7b-hf/discussions/39


          not sure when support  will be available'
        updatedAt: '2023-05-18T04:02:43.835Z'
      numEdits: 0
      reactions: []
    id: 6465a3636ceebdc7fd9448fe
    type: comment
  author: magicsquares137
  content: 'I figured out the error, unfortunately dont see an immediate solution
    to deploy this as a sagemaker endpoint. The sagemaker env only supports HF transformers
    versions up to 4.7 or something, and this model is a fine tuned llama model, which
    was done on 4.28: https://huggingface.co/decapoda-research/llama-7b-hf/discussions/39


    not sure when support  will be available'
  created_at: 2023-05-18 03:02:43+00:00
  edited: false
  hidden: false
  id: 6465a3636ceebdc7fd9448fe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: cognitivecomputations/WizardLM-7B-Uncensored
repo_type: model
status: open
target_branch: null
title: Issue with deploy on sagemaker -
