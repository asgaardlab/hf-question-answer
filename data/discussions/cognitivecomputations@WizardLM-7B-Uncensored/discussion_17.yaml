!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GaaraOtheSand
conflicting_files: null
created_at: 2023-05-25 17:42:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
      fullname: Kalvin Tipton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaaraOtheSand
      type: user
    createdAt: '2023-05-25T18:42:50.000Z'
    data:
      edited: false
      editors:
      - GaaraOtheSand
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
          fullname: Kalvin Tipton
          isHf: false
          isPro: false
          name: GaaraOtheSand
          type: user
        html: "<p>I'm finding myself stuck on how to implement this into my script,\
          \ I figure that you could simply do it like other models but I'd like to\
          \ make sure, this is how I'd imagine you can run it:</p>\n<pre><code>config\
          \ = transformers.AutoConfig.from_pretrained(\n  'ehartford/WizardLM-7B-Uncensored',\n\
          \  trust_remote_code=True\n)\n\nconfig.attn_config['attn_impl'] = 'torch'\n\
          \nconfig.update({\"max_seq_len\": 8192})\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \  'ehartford/WizardLM-7B-Uncensored',\n  config=config,\n  torch_dtype=torch.bfloat16,\n\
          \  trust_remote_code=True\n)\n</code></pre>\n<p>Although, I'm uncertain\
          \ about things like if I can actually change the sequence length on this\
          \ one, and if I need to add the attn_config part. Any help would greatly\
          \ be appreciated.</p>\n"
        raw: "I'm finding myself stuck on how to implement this into my script, I\
          \ figure that you could simply do it like other models but I'd like to make\
          \ sure, this is how I'd imagine you can run it:\r\n\r\n    config = transformers.AutoConfig.from_pretrained(\r\
          \n      'ehartford/WizardLM-7B-Uncensored',\r\n      trust_remote_code=True\r\
          \n    )\r\n\r\n    config.attn_config['attn_impl'] = 'torch'\r\n\r\n   \
          \ config.update({\"max_seq_len\": 8192})\r\n\r\n    model = transformers.AutoModelForCausalLM.from_pretrained(\r\
          \n      'ehartford/WizardLM-7B-Uncensored',\r\n      config=config,\r\n\
          \      torch_dtype=torch.bfloat16,\r\n      trust_remote_code=True\r\n \
          \   )\r\n\r\nAlthough, I'm uncertain about things like if I can actually\
          \ change the sequence length on this one, and if I need to add the attn_config\
          \ part. Any help would greatly be appreciated."
        updatedAt: '2023-05-25T18:42:50.457Z'
      numEdits: 0
      reactions: []
    id: 646fac2a799a974be31a6377
    type: comment
  author: GaaraOtheSand
  content: "I'm finding myself stuck on how to implement this into my script, I figure\
    \ that you could simply do it like other models but I'd like to make sure, this\
    \ is how I'd imagine you can run it:\r\n\r\n    config = transformers.AutoConfig.from_pretrained(\r\
    \n      'ehartford/WizardLM-7B-Uncensored',\r\n      trust_remote_code=True\r\n\
    \    )\r\n\r\n    config.attn_config['attn_impl'] = 'torch'\r\n\r\n    config.update({\"\
    max_seq_len\": 8192})\r\n\r\n    model = transformers.AutoModelForCausalLM.from_pretrained(\r\
    \n      'ehartford/WizardLM-7B-Uncensored',\r\n      config=config,\r\n      torch_dtype=torch.bfloat16,\r\
    \n      trust_remote_code=True\r\n    )\r\n\r\nAlthough, I'm uncertain about things\
    \ like if I can actually change the sequence length on this one, and if I need\
    \ to add the attn_config part. Any help would greatly be appreciated."
  created_at: 2023-05-25 17:42:50+00:00
  edited: false
  hidden: false
  id: 646fac2a799a974be31a6377
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: cognitivecomputations/WizardLM-7B-Uncensored
repo_type: model
status: open
target_branch: null
title: Am I overthinking this?
