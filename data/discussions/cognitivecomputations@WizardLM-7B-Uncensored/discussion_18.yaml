!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aris-T
conflicting_files: null
created_at: 2023-05-30 19:19:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/864340522fb37bd52f30d29968b107b6.svg
      fullname: Christopher Brown
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aris-T
      type: user
    createdAt: '2023-05-30T20:19:27.000Z'
    data:
      edited: false
      editors:
      - aris-T
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/864340522fb37bd52f30d29968b107b6.svg
          fullname: Christopher Brown
          isHf: false
          isPro: false
          name: aris-T
          type: user
        html: "<p>How does one go about training these models. I am looking to incorporate\
          \ an internal codebase into the LLM. Here is my training code. I can't seem\
          \ to get this to work as it always maxes out my GPU memory. I have even\
          \ tried a H100 80 GB instance. I have tried the basics of reduced batch\
          \ size. How does one go about training one of these from a checkpoint? Thanks.</p>\n\
          <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM,\
          \ TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom datasets\
          \ import load_dataset\n\nprint(\"Initializing a tokenizer\")\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"ehartford/WizardLM-7B-Uncensored\")\n\
          \nprint(\"Loading and preprocessing the dataset\")\ndatasets = load_dataset('text',\
          \ data_files='./codebase.txt')\n\ndef tokenize_function(examples):\n   \
          \ return tokenizer(examples[\"text\"])\n\ntokenized_datasets = datasets.map(tokenize_function,\
          \ batched=True, remove_columns=[\"text\"])\n\nblock_size = 128\ndef group_texts(examples):\n\
          \    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n\
          \    total_length = len(concatenated_examples[list(examples.keys())[0]])\n\
          \    total_length = (total_length // block_size) * block_size\n    result\
          \ = {\n        k: [t[i : i + block_size] for i in range(0, total_length,\
          \ block_size)]\n        for k, t in concatenated_examples.items()\n    }\n\
          \    return result\n\nlm_datasets = tokenized_datasets.map(\n    group_texts,\n\
          \    batched=True,\n    batch_size=1000,\n)\n\nprint(\"Initializing a model\"\
          )\nmodel = AutoModelForCausalLM.from_pretrained(\"ehartford/WizardLM-7B-Uncensored\"\
          )\n\n# Defining the training arguments\ntraining_args = TrainingArguments(\n\
          \   output_dir=\"./results\",  # The output directory\n   overwrite_output_dir=True,\
          \  # overwrite the content of the output directory\n   num_train_epochs=3,\
          \  # number of training epochs\n   per_device_train_batch_size=1,  # batch\
          \ size for training\n   per_device_eval_batch_size=1,  # batch size for\
          \ evaluation\n   eval_steps = 400,  # Number of update steps between two\
          \ evaluations.\n   save_steps=800,  # after # steps model is saved \n  \
          \ warmup_steps=500,  # number of warmup steps for learning rate scheduler\n\
          )\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\
          \ mlm=False,\n)\n\n# Initializing a Trainer\ntrainer = Trainer(\n   model=model,\n\
          \   args=training_args,\n   train_dataset=lm_datasets[\"train\"],\n   eval_dataset=lm_datasets[\"\
          train\"],\n   data_collator=data_collator,\n)\n\n# Training\ntrainer.train()\n\
          </code></pre>\n"
        raw: "How does one go about training these models. I am looking to incorporate\
          \ an internal codebase into the LLM. Here is my training code. I can't seem\
          \ to get this to work as it always maxes out my GPU memory. I have even\
          \ tried a H100 80 GB instance. I have tried the basics of reduced batch\
          \ size. How does one go about training one of these from a checkpoint? Thanks.\r\
          \n\r\n```\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
          \ TrainingArguments, Trainer, DataCollatorForLanguageModeling\r\nfrom datasets\
          \ import load_dataset\r\n\r\nprint(\"Initializing a tokenizer\")\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"ehartford/WizardLM-7B-Uncensored\")\r\
          \n\r\nprint(\"Loading and preprocessing the dataset\")\r\ndatasets = load_dataset('text',\
          \ data_files='./codebase.txt')\r\n\r\ndef tokenize_function(examples):\r\
          \n    return tokenizer(examples[\"text\"])\r\n\r\ntokenized_datasets = datasets.map(tokenize_function,\
          \ batched=True, remove_columns=[\"text\"])\r\n\r\nblock_size = 128\r\ndef\
          \ group_texts(examples):\r\n    concatenated_examples = {k: sum(examples[k],\
          \ []) for k in examples.keys()}\r\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\r\
          \n    total_length = (total_length // block_size) * block_size\r\n    result\
          \ = {\r\n        k: [t[i : i + block_size] for i in range(0, total_length,\
          \ block_size)]\r\n        for k, t in concatenated_examples.items()\r\n\
          \    }\r\n    return result\r\n\r\nlm_datasets = tokenized_datasets.map(\r\
          \n    group_texts,\r\n    batched=True,\r\n    batch_size=1000,\r\n)\r\n\
          \r\nprint(\"Initializing a model\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          ehartford/WizardLM-7B-Uncensored\")\r\n\r\n# Defining the training arguments\r\
          \ntraining_args = TrainingArguments(\r\n   output_dir=\"./results\",  #\
          \ The output directory\r\n   overwrite_output_dir=True,  # overwrite the\
          \ content of the output directory\r\n   num_train_epochs=3,  # number of\
          \ training epochs\r\n   per_device_train_batch_size=1,  # batch size for\
          \ training\r\n   per_device_eval_batch_size=1,  # batch size for evaluation\r\
          \n   eval_steps = 400,  # Number of update steps between two evaluations.\r\
          \n   save_steps=800,  # after # steps model is saved \r\n   warmup_steps=500,\
          \  # number of warmup steps for learning rate scheduler\r\n)\r\n\r\ndata_collator\
          \ = DataCollatorForLanguageModeling(\r\n    tokenizer=tokenizer, mlm=False,\r\
          \n)\r\n\r\n# Initializing a Trainer\r\ntrainer = Trainer(\r\n   model=model,\r\
          \n   args=training_args,\r\n   train_dataset=lm_datasets[\"train\"],\r\n\
          \   eval_dataset=lm_datasets[\"train\"],\r\n   data_collator=data_collator,\r\
          \n)\r\n\r\n# Training\r\ntrainer.train()\r\n\r\n```"
        updatedAt: '2023-05-30T20:19:27.712Z'
      numEdits: 0
      reactions: []
    id: 64765a4f8153ce239948c2f6
    type: comment
  author: aris-T
  content: "How does one go about training these models. I am looking to incorporate\
    \ an internal codebase into the LLM. Here is my training code. I can't seem to\
    \ get this to work as it always maxes out my GPU memory. I have even tried a H100\
    \ 80 GB instance. I have tried the basics of reduced batch size. How does one\
    \ go about training one of these from a checkpoint? Thanks.\r\n\r\n```\r\nfrom\
    \ transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments,\
    \ Trainer, DataCollatorForLanguageModeling\r\nfrom datasets import load_dataset\r\
    \n\r\nprint(\"Initializing a tokenizer\")\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    ehartford/WizardLM-7B-Uncensored\")\r\n\r\nprint(\"Loading and preprocessing the\
    \ dataset\")\r\ndatasets = load_dataset('text', data_files='./codebase.txt')\r\
    \n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"\
    ])\r\n\r\ntokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=[\"\
    text\"])\r\n\r\nblock_size = 128\r\ndef group_texts(examples):\r\n    concatenated_examples\
    \ = {k: sum(examples[k], []) for k in examples.keys()}\r\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\r\
    \n    total_length = (total_length // block_size) * block_size\r\n    result =\
    \ {\r\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\r\
    \n        for k, t in concatenated_examples.items()\r\n    }\r\n    return result\r\
    \n\r\nlm_datasets = tokenized_datasets.map(\r\n    group_texts,\r\n    batched=True,\r\
    \n    batch_size=1000,\r\n)\r\n\r\nprint(\"Initializing a model\")\r\nmodel =\
    \ AutoModelForCausalLM.from_pretrained(\"ehartford/WizardLM-7B-Uncensored\")\r\
    \n\r\n# Defining the training arguments\r\ntraining_args = TrainingArguments(\r\
    \n   output_dir=\"./results\",  # The output directory\r\n   overwrite_output_dir=True,\
    \  # overwrite the content of the output directory\r\n   num_train_epochs=3, \
    \ # number of training epochs\r\n   per_device_train_batch_size=1,  # batch size\
    \ for training\r\n   per_device_eval_batch_size=1,  # batch size for evaluation\r\
    \n   eval_steps = 400,  # Number of update steps between two evaluations.\r\n\
    \   save_steps=800,  # after # steps model is saved \r\n   warmup_steps=500, \
    \ # number of warmup steps for learning rate scheduler\r\n)\r\n\r\ndata_collator\
    \ = DataCollatorForLanguageModeling(\r\n    tokenizer=tokenizer, mlm=False,\r\n\
    )\r\n\r\n# Initializing a Trainer\r\ntrainer = Trainer(\r\n   model=model,\r\n\
    \   args=training_args,\r\n   train_dataset=lm_datasets[\"train\"],\r\n   eval_dataset=lm_datasets[\"\
    train\"],\r\n   data_collator=data_collator,\r\n)\r\n\r\n# Training\r\ntrainer.train()\r\
    \n\r\n```"
  created_at: 2023-05-30 19:19:27+00:00
  edited: false
  hidden: false
  id: 64765a4f8153ce239948c2f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-05-31T21:24:54.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>I used exactly the same method as WizardLM.<br>Using LlamaX.<br><a
          rel="nofollow" href="https://github.com/nlpxucan/WizardLM#fine-tuning">https://github.com/nlpxucan/WizardLM#fine-tuning</a></p>

          '
        raw: 'I used exactly the same method as WizardLM.

          Using LlamaX.

          https://github.com/nlpxucan/WizardLM#fine-tuning'
        updatedAt: '2023-05-31T21:24:54.979Z'
      numEdits: 0
      reactions: []
    id: 6477bb26bb7681ad670e3817
    type: comment
  author: ehartford
  content: 'I used exactly the same method as WizardLM.

    Using LlamaX.

    https://github.com/nlpxucan/WizardLM#fine-tuning'
  created_at: 2023-05-31 20:24:54+00:00
  edited: false
  hidden: false
  id: 6477bb26bb7681ad670e3817
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: cognitivecomputations/WizardLM-7B-Uncensored
repo_type: model
status: open
target_branch: null
title: Training Approach?
