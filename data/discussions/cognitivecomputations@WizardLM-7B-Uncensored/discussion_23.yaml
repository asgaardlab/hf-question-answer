!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SteveC
conflicting_files: null
created_at: 2023-11-21 04:20:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/254a5a40facc3d8f4dc7c6cc651577e0.svg
      fullname: S Charlesworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SteveC
      type: user
    createdAt: '2023-11-21T04:20:32.000Z'
    data:
      edited: false
      editors:
      - SteveC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5091986656188965
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/254a5a40facc3d8f4dc7c6cc651577e0.svg
          fullname: S Charlesworth
          isHf: false
          isPro: false
          name: SteveC
          type: user
        html: '<p>I''m tryna use it like this:</p>

          <p>model_id = ''ehartford/WizardLM-7B-Uncensored'' </p>

          <p>device = f''cuda:{cuda.current_device()}'' if cuda.is_available() else
          ''cpu''</p>

          <h1 id="set-quantization-configuration-to-load-large-model-with-less-gpu-memory">set
          quantization configuration to load large model with less GPU memory</h1>

          <h1 id="this-requires-the-bitsandbytes-library">this requires the <code>bitsandbytes</code>
          library</h1>

          <p>bnb_config = transformers.BitsAndBytesConfig(<br>    load_in_4bit=True,<br>    bnb_4bit_quant_type=''nf4'',<br>    bnb_4bit_use_double_quant=True,<br>    bnb_4bit_compute_dtype=bfloat16<br>)</p>

          <h1 id="begin-initializing-hf-items-need-auth-token-for-these">begin initializing
          HF items, need auth token for these</h1>

          <p>hf_auth = ''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''<br>model_config =
          transformers.AutoConfig.from_pretrained(<br>    model_id,<br>    use_auth_token=hf_auth<br>)</p>

          <p>model = transformers.AutoModelForCausalLM.from_pretrained(<br>    model_id,<br>    trust_remote_code=True,<br>    config=model_config,<br>    quantization_config=bnb_config,<br>    device_map=''auto'',<br>    use_auth_token=hf_auth<br>)</p>

          <h1 id="need-tokenizer">need tokenizer:</h1>

          <p>tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)</p>

          <p>model.eval()<br>print(f"Model loaded on {device}")</p>

          <p>And then....</p>

          <p>generate_text = transformers.pipeline(<br>    model=model,<br>    tokenizer=tokenizer,<br>    return_full_text=True,  #
          langchain expects the full text<br>    task=''text-generation'',<br>    #
          we pass model parameters here too<br>    temperature=0.9,  # ''randomness''
          of outputs, 0.0 is the min and 1.0 the max<br>    max_new_tokens=256 #,  #
          mex number of tokens to generate in the output</p>

          <h1 id="repetition_penalty11---without-this-output-begins-repeating">repetition_penalty=1.1  #
          without this output begins repeating</h1>

          <p>)</p>

          <p>and then:</p>

          <p>generate_text("""Yo T how''s it goin? You got any a dem no show jobs?</p>

          <h3 id="response">Response:""")</h3>

          <p>Gives:</p>

          <p>[{''generated_text'': "Yo T how''s it goin? You got any a dem no show
          jobs?\n\n### Response:H"}]</p>

          <p>I''m probably doing a new very dumb thing, I had 13b working a while
          back....</p>

          '
        raw: "I'm tryna use it like this:\r\n\r\nmodel_id = 'ehartford/WizardLM-7B-Uncensored'\
          \ \r\n\r\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available()\
          \ else 'cpu'\r\n\r\n# set quantization configuration to load large model\
          \ with less GPU memory\r\n# this requires the `bitsandbytes` library\r\n\
          bnb_config = transformers.BitsAndBytesConfig(\r\n    load_in_4bit=True,\r\
          \n    bnb_4bit_quant_type='nf4',\r\n    bnb_4bit_use_double_quant=True,\r\
          \n    bnb_4bit_compute_dtype=bfloat16\r\n)\r\n\r\n# begin initializing HF\
          \ items, need auth token for these\r\nhf_auth = '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'\r\
          \nmodel_config = transformers.AutoConfig.from_pretrained(\r\n    model_id,\r\
          \n    use_auth_token=hf_auth\r\n)\r\n\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\
          \n    model_id,\r\n    trust_remote_code=True,\r\n    config=model_config,\r\
          \n    quantization_config=bnb_config,\r\n    device_map='auto',\r\n    use_auth_token=hf_auth\r\
          \n)\r\n\r\n# need tokenizer:\r\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id,\
          \ trust_remote_code=True)\r\n\r\nmodel.eval()\r\nprint(f\"Model loaded on\
          \ {device}\")\r\n\r\nAnd then....\r\n\r\n\r\ngenerate_text = transformers.pipeline(\r\
          \n    model=model, \r\n    tokenizer=tokenizer,\r\n    return_full_text=True,\
          \  # langchain expects the full text\r\n    task='text-generation',\r\n\
          \    # we pass model parameters here too\r\n    temperature=0.9,  # 'randomness'\
          \ of outputs, 0.0 is the min and 1.0 the max\r\n    max_new_tokens=256 #,\
          \  # mex number of tokens to generate in the output\r\n   # repetition_penalty=1.1\
          \  # without this output begins repeating\r\n)\r\n\r\nand then:\r\n\r\n\
          generate_text(\"\"\"Yo T how's it goin? You got any a dem no show jobs?\r\
          \n\r\n### Response:\"\"\")\r\n\r\nGives:\r\n\r\n[{'generated_text': \"Yo\
          \ T how's it goin? You got any a dem no show jobs?\\n\\n### Response:H\"\
          }]\r\n\r\n\r\nI'm probably doing a new very dumb thing, I had 13b working\
          \ a while back....\r\n"
        updatedAt: '2023-11-21T04:20:32.052Z'
      numEdits: 0
      reactions: []
    id: 655c3010f97967fef422e00e
    type: comment
  author: SteveC
  content: "I'm tryna use it like this:\r\n\r\nmodel_id = 'ehartford/WizardLM-7B-Uncensored'\
    \ \r\n\r\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else\
    \ 'cpu'\r\n\r\n# set quantization configuration to load large model with less\
    \ GPU memory\r\n# this requires the `bitsandbytes` library\r\nbnb_config = transformers.BitsAndBytesConfig(\r\
    \n    load_in_4bit=True,\r\n    bnb_4bit_quant_type='nf4',\r\n    bnb_4bit_use_double_quant=True,\r\
    \n    bnb_4bit_compute_dtype=bfloat16\r\n)\r\n\r\n# begin initializing HF items,\
    \ need auth token for these\r\nhf_auth = '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'\r\
    \nmodel_config = transformers.AutoConfig.from_pretrained(\r\n    model_id,\r\n\
    \    use_auth_token=hf_auth\r\n)\r\n\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\
    \n    model_id,\r\n    trust_remote_code=True,\r\n    config=model_config,\r\n\
    \    quantization_config=bnb_config,\r\n    device_map='auto',\r\n    use_auth_token=hf_auth\r\
    \n)\r\n\r\n# need tokenizer:\r\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id,\
    \ trust_remote_code=True)\r\n\r\nmodel.eval()\r\nprint(f\"Model loaded on {device}\"\
    )\r\n\r\nAnd then....\r\n\r\n\r\ngenerate_text = transformers.pipeline(\r\n  \
    \  model=model, \r\n    tokenizer=tokenizer,\r\n    return_full_text=True,  #\
    \ langchain expects the full text\r\n    task='text-generation',\r\n    # we pass\
    \ model parameters here too\r\n    temperature=0.9,  # 'randomness' of outputs,\
    \ 0.0 is the min and 1.0 the max\r\n    max_new_tokens=256 #,  # mex number of\
    \ tokens to generate in the output\r\n   # repetition_penalty=1.1  # without this\
    \ output begins repeating\r\n)\r\n\r\nand then:\r\n\r\ngenerate_text(\"\"\"Yo\
    \ T how's it goin? You got any a dem no show jobs?\r\n\r\n### Response:\"\"\"\
    )\r\n\r\nGives:\r\n\r\n[{'generated_text': \"Yo T how's it goin? You got any a\
    \ dem no show jobs?\\n\\n### Response:H\"}]\r\n\r\n\r\nI'm probably doing a new\
    \ very dumb thing, I had 13b working a while back....\r\n"
  created_at: 2023-11-21 04:20:32+00:00
  edited: false
  hidden: false
  id: 655c3010f97967fef422e00e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: cognitivecomputations/WizardLM-7B-Uncensored
repo_type: model
status: open
target_branch: null
title: I get one letter responses....
