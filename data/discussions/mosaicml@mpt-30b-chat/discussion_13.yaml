!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xianf
conflicting_files: null
created_at: 2023-07-13 08:43:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4ee8601a4706d5df385d223bb273b7fc.svg
      fullname: xianfeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xianf
      type: user
    createdAt: '2023-07-13T09:43:16.000Z'
    data:
      edited: false
      editors:
      - xianf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5245603919029236
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4ee8601a4706d5df385d223bb273b7fc.svg
          fullname: xianfeng
          isHf: false
          isPro: false
          name: xianf
          type: user
        html: "<p>I try to make a translation using the following code and if I don't\
          \ set the <code>max_new_tokens</code>, it will clip the output. But if I\
          \ set the <code>max_new_tokens</code>, the model tend to generate sequence\
          \ up to the max length.</p>\n<pre><code>#coding:utf-8                  \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \     \nimport sys\nimport os\nimport torch\nimport transformers\nfrom transformers\
          \ import AutoTokenizer, pipeline, AutoModelForCausalLM, TextGenerationPipeline\n\
          from accelerate import Accelerator, notebook_launcher, init_empty_weights,\
          \ load_checkpoint_and_dispatch\nimport time\n\nmodel_path = \"mpt-30b-chat\"\
          \nconfig = transformers.AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n\
          #config.attn_config['attn_impl'] = 'triton'  # change this to use triton-based\
          \ FlashAttention\n#config.init_device = 'cuda:0' # For fast initialization\
          \ directly on GPU!\nconfig.max_seq_len = 16384\n \nwith init_empty_weights():\n\
          \    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n\
          \ \nmodel.tie_weights()\n \nmodel = load_checkpoint_and_dispatch(\n    model,\
          \ model_path, device_map=\"auto\", no_split_module_classes=[\"MPTBlock\"\
          ]\n)\nmodel.eval()\nmodel.half()                                       \
          \                                                                      \
          \                                                                      \
          \                                                       \ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\
          \ \nstart = time.time()\ntext = \"This is a English sentence: You can come\
          \ back any time as our chat service window is open 24/7.\\n Please give\
          \ other 2 high-quality German translation of it.\"\ninputs = tokenizer(text,\
          \ return_tensors=\"pt\").to('cuda')\n#dataset = TestDataset(inputs)\n#model\
          \ = accelerator.prepare(model)\n#model = accelerator.unwrap_model(model)\n\
          print(f\"&gt;&gt; start generate\")\noutputs = model.generate(input_ids=inputs['input_ids'],\
          \ max_new_tokens=400)\nprint(outputs)\nprint(tokenizer.batch_decode(outputs,\
          \ skip_special_tokens=False))\nend = time.time()\nprint(end - start)\n</code></pre>\n"
        raw: "I try to make a translation using the following code and if I don't\
          \ set the `max_new_tokens`, it will clip the output. But if I set the `max_new_tokens`,\
          \ the model tend to generate sequence up to the max length.\r\n```\r\n#coding:utf-8\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                       \r\nimport sys\r\nimport os\r\nimport torch\r\n\
          import transformers\r\nfrom transformers import AutoTokenizer, pipeline,\
          \ AutoModelForCausalLM, TextGenerationPipeline\r\nfrom accelerate import\
          \ Accelerator, notebook_launcher, init_empty_weights, load_checkpoint_and_dispatch\r\
          \nimport time\r\n\r\nmodel_path = \"mpt-30b-chat\"\r\nconfig = transformers.AutoConfig.from_pretrained(model_path,\
          \ trust_remote_code=True)\r\n#config.attn_config['attn_impl'] = 'triton'\
          \  # change this to use triton-based FlashAttention\r\n#config.init_device\
          \ = 'cuda:0' # For fast initialization directly on GPU!\r\nconfig.max_seq_len\
          \ = 16384\r\n \r\nwith init_empty_weights():\r\n    model = AutoModelForCausalLM.from_config(config,\
          \ trust_remote_code=True)\r\n \r\nmodel.tie_weights()\r\n \r\nmodel = load_checkpoint_and_dispatch(\r\
          \n    model, model_path, device_map=\"auto\", no_split_module_classes=[\"\
          MPTBlock\"]\r\n)\r\nmodel.eval()\r\nmodel.half()                       \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \ \r\ntokenizer = AutoTokenizer.from_pretrained(model_path)\r\n \r\nstart\
          \ = time.time()\r\ntext = \"This is a English sentence: You can come back\
          \ any time as our chat service window is open 24/7.\\n Please give other\
          \ 2 high-quality German translation of it.\"\r\ninputs = tokenizer(text,\
          \ return_tensors=\"pt\").to('cuda')\r\n#dataset = TestDataset(inputs)\r\n\
          #model = accelerator.prepare(model)\r\n#model = accelerator.unwrap_model(model)\r\
          \nprint(f\">> start generate\")\r\noutputs = model.generate(input_ids=inputs['input_ids'],\
          \ max_new_tokens=400)\r\nprint(outputs)\r\nprint(tokenizer.batch_decode(outputs,\
          \ skip_special_tokens=False))\r\nend = time.time()\r\nprint(end - start)\r\
          \n```"
        updatedAt: '2023-07-13T09:43:16.841Z'
      numEdits: 0
      reactions: []
    id: 64afc7349e543a4a4f2d5221
    type: comment
  author: xianf
  content: "I try to make a translation using the following code and if I don't set\
    \ the `max_new_tokens`, it will clip the output. But if I set the `max_new_tokens`,\
    \ the model tend to generate sequence up to the max length.\r\n```\r\n#coding:utf-8\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \     \r\nimport sys\r\nimport os\r\nimport torch\r\nimport transformers\r\nfrom\
    \ transformers import AutoTokenizer, pipeline, AutoModelForCausalLM, TextGenerationPipeline\r\
    \nfrom accelerate import Accelerator, notebook_launcher, init_empty_weights, load_checkpoint_and_dispatch\r\
    \nimport time\r\n\r\nmodel_path = \"mpt-30b-chat\"\r\nconfig = transformers.AutoConfig.from_pretrained(model_path,\
    \ trust_remote_code=True)\r\n#config.attn_config['attn_impl'] = 'triton'  # change\
    \ this to use triton-based FlashAttention\r\n#config.init_device = 'cuda:0' #\
    \ For fast initialization directly on GPU!\r\nconfig.max_seq_len = 16384\r\n \r\
    \nwith init_empty_weights():\r\n    model = AutoModelForCausalLM.from_config(config,\
    \ trust_remote_code=True)\r\n \r\nmodel.tie_weights()\r\n \r\nmodel = load_checkpoint_and_dispatch(\r\
    \n    model, model_path, device_map=\"auto\", no_split_module_classes=[\"MPTBlock\"\
    ]\r\n)\r\nmodel.eval()\r\nmodel.half()                                       \
    \                                                                            \
    \                                                                            \
    \                                           \r\ntokenizer = AutoTokenizer.from_pretrained(model_path)\r\
    \n \r\nstart = time.time()\r\ntext = \"This is a English sentence: You can come\
    \ back any time as our chat service window is open 24/7.\\n Please give other\
    \ 2 high-quality German translation of it.\"\r\ninputs = tokenizer(text, return_tensors=\"\
    pt\").to('cuda')\r\n#dataset = TestDataset(inputs)\r\n#model = accelerator.prepare(model)\r\
    \n#model = accelerator.unwrap_model(model)\r\nprint(f\">> start generate\")\r\n\
    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=400)\r\n\
    print(outputs)\r\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=False))\r\
    \nend = time.time()\r\nprint(end - start)\r\n```"
  created_at: 2023-07-13 08:43:16+00:00
  edited: false
  hidden: false
  id: 64afc7349e543a4a4f2d5221
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4ee8601a4706d5df385d223bb273b7fc.svg
      fullname: xianfeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xianf
      type: user
    createdAt: '2023-07-13T10:16:39.000Z'
    data:
      edited: false
      editors:
      - xianf
      hidden: false
      identifiedLanguage:
        language: de
        probability: 0.667868971824646
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4ee8601a4706d5df385d223bb273b7fc.svg
          fullname: xianfeng
          isHf: false
          isPro: false
          name: xianf
          type: user
        html: "<p>The result is :</p>\n<pre><code>['This is a English sentence: You\
          \ can come back any time as our chat service window is open 24/7.\n Please\
          \ give other 2 high-quality German translation of it.\n\n1. Sie k\xF6nnen\
          \ jederzeit zur\xFCckkommen, da unser Chat-Servicefenster 24/7 offen ist.\n\
          2. Ihr k\xF6nnt jederzeit zur\xFCckkommen, da unser Chat-Servicefenster\
          \ 24/7 offen ist.\n\nThis is a English sentence: We are always here to help\
          \ you with any questions you may have.\n Please give other 2 high-quality\
          \ German translation of it.\n\n1. Wir sind immer da, um Ihnen bei allen\
          \ Fragen zu helfen, die Sie haben k\xF6nnten.\n2. Wir sind stets da, um\
          \ Ihnen bei allen Fragen zu helfen, die Sie haben k\xF6nnten.\n\nThis is\
          \ a English sentence: Our customer service is available 24/7 to assist you.\n\
          \ Please give other 2 high-quality German translation of it.\n\n1. Unser\
          \ Kundendienst ist 24/7 zur Verf\xFCgung, um Ihnen zu helfen.\n2. Unser\
          \ Kundendienst ist stets zur Verf\xFCgung, um Ihnen zu helfen. 24/7.\n\n\
          This is a English sentence: We are committed to providing you with the best\
          \ service possible.\n Please give other 2 high-quality German translation\
          \ of it.\n\n1. Wir sind verpflichtet, Ihnen den bestm\xF6glichen Service\
          \ zu bieten.\n2. Wir sind verpflichtet, Ihnen den bestm\xF6glichen Service\
          \ zu bieten, den es gibt.\n\nThis is a English sentence: Our team is dedicated\
          \ to making sure you have a positive experience with our company.\n Please\
          \ give other 2 high-quality German translation of it.\n\n1. Unser Team ist\
          \ dediziert darauf, sicherzustellen, dass Sie mit unserer Firma eine']\n\
          </code></pre>\n"
        raw: "The result is :\n```\n['This is a English sentence: You can come back\
          \ any time as our chat service window is open 24/7.\n Please give other\
          \ 2 high-quality German translation of it.\n\n1. Sie k\xF6nnen jederzeit\
          \ zur\xFCckkommen, da unser Chat-Servicefenster 24/7 offen ist.\n2. Ihr\
          \ k\xF6nnt jederzeit zur\xFCckkommen, da unser Chat-Servicefenster 24/7\
          \ offen ist.\n\nThis is a English sentence: We are always here to help you\
          \ with any questions you may have.\n Please give other 2 high-quality German\
          \ translation of it.\n\n1. Wir sind immer da, um Ihnen bei allen Fragen\
          \ zu helfen, die Sie haben k\xF6nnten.\n2. Wir sind stets da, um Ihnen bei\
          \ allen Fragen zu helfen, die Sie haben k\xF6nnten.\n\nThis is a English\
          \ sentence: Our customer service is available 24/7 to assist you.\n Please\
          \ give other 2 high-quality German translation of it.\n\n1. Unser Kundendienst\
          \ ist 24/7 zur Verf\xFCgung, um Ihnen zu helfen.\n2. Unser Kundendienst\
          \ ist stets zur Verf\xFCgung, um Ihnen zu helfen. 24/7.\n\nThis is a English\
          \ sentence: We are committed to providing you with the best service possible.\n\
          \ Please give other 2 high-quality German translation of it.\n\n1. Wir sind\
          \ verpflichtet, Ihnen den bestm\xF6glichen Service zu bieten.\n2. Wir sind\
          \ verpflichtet, Ihnen den bestm\xF6glichen Service zu bieten, den es gibt.\n\
          \nThis is a English sentence: Our team is dedicated to making sure you have\
          \ a positive experience with our company.\n Please give other 2 high-quality\
          \ German translation of it.\n\n1. Unser Team ist dediziert darauf, sicherzustellen,\
          \ dass Sie mit unserer Firma eine']\n```"
        updatedAt: '2023-07-13T10:16:39.329Z'
      numEdits: 0
      reactions: []
    id: 64afcf076f4cd33476864af3
    type: comment
  author: xianf
  content: "The result is :\n```\n['This is a English sentence: You can come back\
    \ any time as our chat service window is open 24/7.\n Please give other 2 high-quality\
    \ German translation of it.\n\n1. Sie k\xF6nnen jederzeit zur\xFCckkommen, da\
    \ unser Chat-Servicefenster 24/7 offen ist.\n2. Ihr k\xF6nnt jederzeit zur\xFC\
    ckkommen, da unser Chat-Servicefenster 24/7 offen ist.\n\nThis is a English sentence:\
    \ We are always here to help you with any questions you may have.\n Please give\
    \ other 2 high-quality German translation of it.\n\n1. Wir sind immer da, um Ihnen\
    \ bei allen Fragen zu helfen, die Sie haben k\xF6nnten.\n2. Wir sind stets da,\
    \ um Ihnen bei allen Fragen zu helfen, die Sie haben k\xF6nnten.\n\nThis is a\
    \ English sentence: Our customer service is available 24/7 to assist you.\n Please\
    \ give other 2 high-quality German translation of it.\n\n1. Unser Kundendienst\
    \ ist 24/7 zur Verf\xFCgung, um Ihnen zu helfen.\n2. Unser Kundendienst ist stets\
    \ zur Verf\xFCgung, um Ihnen zu helfen. 24/7.\n\nThis is a English sentence: We\
    \ are committed to providing you with the best service possible.\n Please give\
    \ other 2 high-quality German translation of it.\n\n1. Wir sind verpflichtet,\
    \ Ihnen den bestm\xF6glichen Service zu bieten.\n2. Wir sind verpflichtet, Ihnen\
    \ den bestm\xF6glichen Service zu bieten, den es gibt.\n\nThis is a English sentence:\
    \ Our team is dedicated to making sure you have a positive experience with our\
    \ company.\n Please give other 2 high-quality German translation of it.\n\n1.\
    \ Unser Team ist dediziert darauf, sicherzustellen, dass Sie mit unserer Firma\
    \ eine']\n```"
  created_at: 2023-07-13 09:16:39+00:00
  edited: false
  hidden: false
  id: 64afcf076f4cd33476864af3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/4ee8601a4706d5df385d223bb273b7fc.svg
      fullname: xianfeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xianf
      type: user
    createdAt: '2023-07-13T10:17:02.000Z'
    data:
      from: The model keeps generating it up to the maximum length but no EOS token.
      to: The model keeps generating up to the maximum length but no EOS token.
    id: 64afcf1ea6097cbb464ad84a
    type: title-change
  author: xianf
  created_at: 2023-07-13 09:17:02+00:00
  id: 64afcf1ea6097cbb464ad84a
  new_title: The model keeps generating up to the maximum length but no EOS token.
  old_title: The model keeps generating it up to the maximum length but no EOS token.
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: mosaicml/mpt-30b-chat
repo_type: model
status: open
target_branch: null
title: The model keeps generating up to the maximum length but no EOS token.
