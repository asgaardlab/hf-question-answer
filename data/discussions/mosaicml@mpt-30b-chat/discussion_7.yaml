!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zokica
conflicting_files: null
created_at: 2023-06-29 07:14:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-06-29T08:14:03.000Z'
    data:
      edited: true
      editors:
      - zokica
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42977944016456604
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: "<p>I load it on RTX 3090 in 4 bits, and I get 1 tokens per second on\
          \ a GPU which is slower than a CPU. For example llama 7b is around 30 tokens/second\
          \ on teh same GPU. And the speed should be around 10 tokens/second.</p>\n\
          <p>I show only load code</p>\n<p>if 1==1:</p>\n<pre><code>import time\n\
          timea = time.time()\nimport torch\nimport transformers\nfrom transformers\
          \ import BitsAndBytesConfig\nfrom transformers import AutoTokenizer\ntokenizer\
          \ = AutoTokenizer.from_pretrained('mosaicml/mpt-30b')\nbase_model = \"mosaicml/mpt-30b-chat\"\
          \nconfig = transformers.AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n\
          config.max_seq_len = 16384 # (input + output) tokens can now be up to 16384\n\
          \nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n\
          \    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n\
          )\n\n\n#print(\"device_map\",device_map) \nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \        base_model,\n        config=config,\n        quantization_config=bnb_config,\n\
          \        device_map=\"auto\",\n        trust_remote_code=True\n    )  \n\
          \nmodel.eval()\n\nprint(\"load time\",-timea + time.time())\n</code></pre>\n"
        raw: "I load it on RTX 3090 in 4 bits, and I get 1 tokens per second on a\
          \ GPU which is slower than a CPU. For example llama 7b is around 30 tokens/second\
          \ on teh same GPU. And the speed should be around 10 tokens/second.\n\n\
          I show only load code\n\nif 1==1:\n\n    import time\n    timea = time.time()\n\
          \    import torch\n    import transformers\n    from transformers import\
          \ BitsAndBytesConfig\n    from transformers import AutoTokenizer\n    tokenizer\
          \ = AutoTokenizer.from_pretrained('mosaicml/mpt-30b')\n    base_model =\
          \ \"mosaicml/mpt-30b-chat\"\n    config = transformers.AutoConfig.from_pretrained(base_model,\
          \ trust_remote_code=True)\n    config.max_seq_len = 16384 # (input + output)\
          \ tokens can now be up to 16384\n    \n    bnb_config = BitsAndBytesConfig(\n\
          \        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n \
          \       bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n\
          \    )\n\n\n    #print(\"device_map\",device_map) \n    model = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \            base_model,\n            config=config,\n            quantization_config=bnb_config,\n\
          \            device_map=\"auto\",\n            trust_remote_code=True\n\
          \        )  \n\n    model.eval()\n\n    print(\"load time\",-timea + time.time())"
        updatedAt: '2023-06-29T08:31:07.002Z'
      numEdits: 2
      reactions: []
    id: 649d3d4b1684bb856062622b
    type: comment
  author: zokica
  content: "I load it on RTX 3090 in 4 bits, and I get 1 tokens per second on a GPU\
    \ which is slower than a CPU. For example llama 7b is around 30 tokens/second\
    \ on teh same GPU. And the speed should be around 10 tokens/second.\n\nI show\
    \ only load code\n\nif 1==1:\n\n    import time\n    timea = time.time()\n   \
    \ import torch\n    import transformers\n    from transformers import BitsAndBytesConfig\n\
    \    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained('mosaicml/mpt-30b')\n\
    \    base_model = \"mosaicml/mpt-30b-chat\"\n    config = transformers.AutoConfig.from_pretrained(base_model,\
    \ trust_remote_code=True)\n    config.max_seq_len = 16384 # (input + output) tokens\
    \ can now be up to 16384\n    \n    bnb_config = BitsAndBytesConfig(\n       \
    \ load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"\
    nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n\n\n    #print(\"\
    device_map\",device_map) \n    model = transformers.AutoModelForCausalLM.from_pretrained(\n\
    \            base_model,\n            config=config,\n            quantization_config=bnb_config,\n\
    \            device_map=\"auto\",\n            trust_remote_code=True\n      \
    \  )  \n\n    model.eval()\n\n    print(\"load time\",-timea + time.time())"
  created_at: 2023-06-29 07:14:03+00:00
  edited: true
  hidden: false
  id: 649d3d4b1684bb856062622b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: mosaicml/mpt-30b-chat
repo_type: model
status: open
target_branch: null
title: 'The model is extremelly slow in 4bit, is my code for loading ok? '
