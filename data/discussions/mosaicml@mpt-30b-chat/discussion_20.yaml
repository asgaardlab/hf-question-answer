!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lalit34
conflicting_files: null
created_at: 2023-09-01 06:07:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e858c1e12e6cd5f2a41b63353982ce1b.svg
      fullname: Lalit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lalit34
      type: user
    createdAt: '2023-09-01T07:07:45.000Z'
    data:
      edited: true
      editors:
      - lalit34
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9509473443031311
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e858c1e12e6cd5f2a41b63353982ce1b.svg
          fullname: Lalit
          isHf: false
          isPro: false
          name: lalit34
          type: user
        html: "<p>I am not able to get this work. All i am getting is empty responses\
          \ i have tried playing with parameters but still no luck any advise what\
          \ to modify .</p>\n<p>app = Flask(<strong>name</strong>)</p>\n<p>@dataclass<br>class\
          \ GenerationConfig:<br>    temperature: float<br>    top_k: int<br>    top_p:\
          \ float<br>    repetition_penalty: float<br>    max_new_tokens: int<br>\
          \    seed: int<br>    reset: bool<br>    stream: bool<br>    threads: int<br>\
          \    stop: list</p>\n<p>def format_prompt(system_prompt: str, user_prompt:\
          \ str):<br>    system_prompt = f\"system\\n{system_prompt}\\n\"<br>    user_prompt\
          \ = f\"user\\n{user_prompt}\\n\"<br>    assistant_prompt = f\"assistant\\\
          n\"<br>    return f\"{system_prompt}{user_prompt}{assistant_prompt}\"</p>\n\
          <p>def generate(<br>    llm: AutoModelForCausalLM,<br>    generation_config:\
          \ GenerationConfig,<br>    system_prompt: str,<br>    user_input: str,<br>):<br>\
          \    # return llm(<br>    #     format_prompt(<br>    #         system_prompt,<br>\
          \    #         user_prompt,<br>    #     ),<br>    #     **asdict(generation_config),<br>\
          \    # )<br>    model_output = llm(<br>    format_prompt(system_prompt,\
          \ user_input.strip()),<br>    **asdict(generation_config),<br>    )<br>\
          \    print(\"Model output:\", model_output)<br>    return model_output</p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;app&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/app\">@<span class=\"\
          underline\">app</span></a></span>\n\n\t</span></span>.route('/generate',\
          \ methods=['GET','POST'])<br>def generate_response_endpoint():<br>    #user_input\
          \ = request.data.decode('utf-8')<br>    # Load the model and configuration<br>\
          \    if request.method == 'GET':<br>        user_input = request.args.get('user_input',\
          \ '')  # Get input from query parameter<br>    elif request.method == 'POST':<br>\
          \        user_input = request.data.decode('utf-8')</p>\n<pre><code>print(\"\
          Loading model...\")\n#config = AutoConfig.from_pretrained(\"mosaicml/mpt-30b-chat\"\
          , context_length=8192)\nllm = AutoModelForCausalLM.from_pretrained(\n  \
          \  \"/home/azureuser/mpt-30B-inference/models/mpt-30b-chat.ggmlv0.q4_1.bin\"\
          ,\n    model_type=\"mpt\"\n)\nprint(\"model Loaded\")\n\nsystem_prompt =\
          \ \"Reply.\"\n\ngeneration_config = GenerationConfig(\n    temperature=0.2,\n\
          \    top_k=0,\n    top_p=0.9,\n    repetition_penalty=1.0,\n    max_new_tokens=512,\
          \ \n    seed=42,\n    reset=False,  \n    stream=False, \n    threads=int(os.cpu_count()\
          \ / 2),  # adjust for your CPU\n    stop=[\"\", \"|&lt;\"],\n)\n\ngenerator\
          \ = generate(llm, generation_config, system_prompt, user_input.strip())\n\
          #time.sleep(60)\n\nprint(generator)\n\nresponse = generator\n\nprint(response)\n\
          return Response(response, content_type='text/plain; charset=utf-8')\n</code></pre>\n\
          <p>if <strong>name</strong> == \"<strong>main</strong>\":<br>    app.run(host='0.0.0.0',\
          \ port=3002)</p>\n"
        raw: "I am not able to get this work. All i am getting is empty responses\
          \ i have tried playing with parameters but still no luck any advise what\
          \ to modify .\n\napp = Flask(__name__)\n\n@dataclass\nclass GenerationConfig:\n\
          \    temperature: float\n    top_k: int\n    top_p: float\n    repetition_penalty:\
          \ float\n    max_new_tokens: int\n    seed: int\n    reset: bool\n    stream:\
          \ bool\n    threads: int\n    stop: list\n\ndef format_prompt(system_prompt:\
          \ str, user_prompt: str):\n    system_prompt = f\"system\\n{system_prompt}\\\
          n\"\n    user_prompt = f\"user\\n{user_prompt}\\n\"\n    assistant_prompt\
          \ = f\"assistant\\n\"\n    return f\"{system_prompt}{user_prompt}{assistant_prompt}\"\
          \n\ndef generate(\n    llm: AutoModelForCausalLM,\n    generation_config:\
          \ GenerationConfig,\n    system_prompt: str,\n    user_input: str,\n):\n\
          \    # return llm(\n    #     format_prompt(\n    #         system_prompt,\n\
          \    #         user_prompt,\n    #     ),\n    #     **asdict(generation_config),\n\
          \    # )\n    model_output = llm(\n    format_prompt(system_prompt, user_input.strip()),\n\
          \    **asdict(generation_config),\n    )\n    print(\"Model output:\", model_output)\n\
          \    return model_output\n\n@app.route('/generate', methods=['GET','POST'])\n\
          def generate_response_endpoint():\n    #user_input = request.data.decode('utf-8')\n\
          \    # Load the model and configuration\n    if request.method == 'GET':\n\
          \        user_input = request.args.get('user_input', '')  # Get input from\
          \ query parameter\n    elif request.method == 'POST':\n        user_input\
          \ = request.data.decode('utf-8')\n\n    print(\"Loading model...\")\n  \
          \  #config = AutoConfig.from_pretrained(\"mosaicml/mpt-30b-chat\", context_length=8192)\n\
          \    llm = AutoModelForCausalLM.from_pretrained(\n        \"/home/azureuser/mpt-30B-inference/models/mpt-30b-chat.ggmlv0.q4_1.bin\"\
          ,\n        model_type=\"mpt\"\n    )\n    print(\"model Loaded\")\n\n  \
          \  system_prompt = \"Reply.\"\n\n    generation_config = GenerationConfig(\n\
          \        temperature=0.2,\n        top_k=0,\n        top_p=0.9,\n      \
          \  repetition_penalty=1.0,\n        max_new_tokens=512, \n        seed=42,\n\
          \        reset=False,  \n        stream=False, \n        threads=int(os.cpu_count()\
          \ / 2),  # adjust for your CPU\n        stop=[\"\", \"|<\"],\n    )\n\n\
          \    generator = generate(llm, generation_config, system_prompt, user_input.strip())\n\
          \    #time.sleep(60)\n\n    print(generator)\n    \n    response = generator\n\
          \n    print(response)\n    return Response(response, content_type='text/plain;\
          \ charset=utf-8')\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0',\
          \ port=3002)"
        updatedAt: '2023-09-01T07:19:29.074Z'
      numEdits: 1
      reactions: []
    id: 64f18dc1a71373f2a6ee86b7
    type: comment
  author: lalit34
  content: "I am not able to get this work. All i am getting is empty responses i\
    \ have tried playing with parameters but still no luck any advise what to modify\
    \ .\n\napp = Flask(__name__)\n\n@dataclass\nclass GenerationConfig:\n    temperature:\
    \ float\n    top_k: int\n    top_p: float\n    repetition_penalty: float\n   \
    \ max_new_tokens: int\n    seed: int\n    reset: bool\n    stream: bool\n    threads:\
    \ int\n    stop: list\n\ndef format_prompt(system_prompt: str, user_prompt: str):\n\
    \    system_prompt = f\"system\\n{system_prompt}\\n\"\n    user_prompt = f\"user\\\
    n{user_prompt}\\n\"\n    assistant_prompt = f\"assistant\\n\"\n    return f\"\
    {system_prompt}{user_prompt}{assistant_prompt}\"\n\ndef generate(\n    llm: AutoModelForCausalLM,\n\
    \    generation_config: GenerationConfig,\n    system_prompt: str,\n    user_input:\
    \ str,\n):\n    # return llm(\n    #     format_prompt(\n    #         system_prompt,\n\
    \    #         user_prompt,\n    #     ),\n    #     **asdict(generation_config),\n\
    \    # )\n    model_output = llm(\n    format_prompt(system_prompt, user_input.strip()),\n\
    \    **asdict(generation_config),\n    )\n    print(\"Model output:\", model_output)\n\
    \    return model_output\n\n@app.route('/generate', methods=['GET','POST'])\n\
    def generate_response_endpoint():\n    #user_input = request.data.decode('utf-8')\n\
    \    # Load the model and configuration\n    if request.method == 'GET':\n   \
    \     user_input = request.args.get('user_input', '')  # Get input from query\
    \ parameter\n    elif request.method == 'POST':\n        user_input = request.data.decode('utf-8')\n\
    \n    print(\"Loading model...\")\n    #config = AutoConfig.from_pretrained(\"\
    mosaicml/mpt-30b-chat\", context_length=8192)\n    llm = AutoModelForCausalLM.from_pretrained(\n\
    \        \"/home/azureuser/mpt-30B-inference/models/mpt-30b-chat.ggmlv0.q4_1.bin\"\
    ,\n        model_type=\"mpt\"\n    )\n    print(\"model Loaded\")\n\n    system_prompt\
    \ = \"Reply.\"\n\n    generation_config = GenerationConfig(\n        temperature=0.2,\n\
    \        top_k=0,\n        top_p=0.9,\n        repetition_penalty=1.0,\n     \
    \   max_new_tokens=512, \n        seed=42,\n        reset=False,  \n        stream=False,\
    \ \n        threads=int(os.cpu_count() / 2),  # adjust for your CPU\n        stop=[\"\
    \", \"|<\"],\n    )\n\n    generator = generate(llm, generation_config, system_prompt,\
    \ user_input.strip())\n    #time.sleep(60)\n\n    print(generator)\n    \n   \
    \ response = generator\n\n    print(response)\n    return Response(response, content_type='text/plain;\
    \ charset=utf-8')\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0',\
    \ port=3002)"
  created_at: 2023-09-01 06:07:45+00:00
  edited: true
  hidden: false
  id: 64f18dc1a71373f2a6ee86b7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: mosaicml/mpt-30b-chat
repo_type: model
status: open
target_branch: null
title: Response
