!!python/object:huggingface_hub.community.DiscussionWithDetails
author: markdoucette
conflicting_files: null
created_at: 2023-07-31 20:12:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/720d2f4b1c76dd5f88d4bd8a3ee536c9.svg
      fullname: Mark Doucette
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: markdoucette
      type: user
    createdAt: '2023-07-31T21:12:34.000Z'
    data:
      edited: true
      editors:
      - markdoucette
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6276468634605408
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/720d2f4b1c76dd5f88d4bd8a3ee536c9.svg
          fullname: Mark Doucette
          isHf: false
          isPro: false
          name: markdoucette
          type: user
        html: "<p>I'm trying to get MPT-30b-Chat running on a ml.p3.8xlarge instance\
          \ and I'm gettin an error that says I'm out of disc space \"[Errno 28] No\
          \ space left on device\". I've tweaked the code from this post (<a rel=\"\
          nofollow\" href=\"https://hackernoon.com/how-to-run-mpt-7b-on-aws-sagemaker-mosaicmls-chatgpt-competitor\"\
          >https://hackernoon.com/how-to-run-mpt-7b-on-aws-sagemaker-mosaicmls-chatgpt-competitor</a>)\
          \ which can be found here (<a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1kJr2LHHLKYkbnNutVYEkt2vrYsbO38aw?ref=hackernoon.com\"\
          >https://colab.research.google.com/drive/1kJr2LHHLKYkbnNutVYEkt2vrYsbO38aw?ref=hackernoon.com</a>).\
          \ Here is my current code: </p>\n<pre><code>!pip install -qU transformers\
          \ accelerate einops langchain xformers\nfrom torch import cuda, bfloat16\n\
          from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n\
          import transformers\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available()\
          \ else 'cpu'\n\nname = 'mosaicml/mpt-30b-chat'\n\ntokenizer = AutoTokenizer.from_pretrained(name,\
          \ trust_remote_code=True)\n\nconfig = transformers.AutoConfig.from_pretrained(name,\
          \ trust_remote_code=True)\nconfig.init_device = 'cuda:0'\n\nmodel = AutoModelForCausalLM.from_pretrained(name,\n\
          \                                             trust_remote_code=True,\n\
          \                                             config=config,\n         \
          \                                    torch_dtype=bfloat16)\n</code></pre>\n\
          <p>Would really appreciate any help on this.</p>\n"
        raw: "I'm trying to get MPT-30b-Chat running on a ml.p3.8xlarge instance and\
          \ I'm gettin an error that says I'm out of disc space \"[Errno 28] No space\
          \ left on device\". I've tweaked the code from this post (https://hackernoon.com/how-to-run-mpt-7b-on-aws-sagemaker-mosaicmls-chatgpt-competitor)\
          \ which can be found here (https://colab.research.google.com/drive/1kJr2LHHLKYkbnNutVYEkt2vrYsbO38aw?ref=hackernoon.com).\
          \ Here is my current code: \n\n```\n!pip install -qU transformers accelerate\
          \ einops langchain xformers\nfrom torch import cuda, bfloat16\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM, AutoConfig\nimport transformers\n\
          \ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\
          \nname = 'mosaicml/mpt-30b-chat'\n\ntokenizer = AutoTokenizer.from_pretrained(name,\
          \ trust_remote_code=True)\n\nconfig = transformers.AutoConfig.from_pretrained(name,\
          \ trust_remote_code=True)\nconfig.init_device = 'cuda:0'\n\nmodel = AutoModelForCausalLM.from_pretrained(name,\n\
          \                                             trust_remote_code=True,\n\
          \                                             config=config,\n         \
          \                                    torch_dtype=bfloat16)\n```\n\nWould\
          \ really appreciate any help on this."
        updatedAt: '2023-07-31T21:27:33.728Z'
      numEdits: 1
      reactions: []
    id: 64c823c2120a85440be5a492
    type: comment
  author: markdoucette
  content: "I'm trying to get MPT-30b-Chat running on a ml.p3.8xlarge instance and\
    \ I'm gettin an error that says I'm out of disc space \"[Errno 28] No space left\
    \ on device\". I've tweaked the code from this post (https://hackernoon.com/how-to-run-mpt-7b-on-aws-sagemaker-mosaicmls-chatgpt-competitor)\
    \ which can be found here (https://colab.research.google.com/drive/1kJr2LHHLKYkbnNutVYEkt2vrYsbO38aw?ref=hackernoon.com).\
    \ Here is my current code: \n\n```\n!pip install -qU transformers accelerate einops\
    \ langchain xformers\nfrom torch import cuda, bfloat16\nfrom transformers import\
    \ AutoTokenizer, AutoModelForCausalLM, AutoConfig\nimport transformers\n\ndevice\
    \ = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\nname\
    \ = 'mosaicml/mpt-30b-chat'\n\ntokenizer = AutoTokenizer.from_pretrained(name,\
    \ trust_remote_code=True)\n\nconfig = transformers.AutoConfig.from_pretrained(name,\
    \ trust_remote_code=True)\nconfig.init_device = 'cuda:0'\n\nmodel = AutoModelForCausalLM.from_pretrained(name,\n\
    \                                             trust_remote_code=True,\n      \
    \                                       config=config,\n                     \
    \                        torch_dtype=bfloat16)\n```\n\nWould really appreciate\
    \ any help on this."
  created_at: 2023-07-31 20:12:34+00:00
  edited: true
  hidden: false
  id: 64c823c2120a85440be5a492
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4e12580b126032c2f9c65c45f959492.svg
      fullname: Praveen Sahu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: praveensahu
      type: user
    createdAt: '2023-09-16T08:21:29.000Z'
    data:
      edited: true
      editors:
      - praveensahu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8666768670082092
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4e12580b126032c2f9c65c45f959492.svg
          fullname: Praveen Sahu
          isHf: false
          isPro: false
          name: praveensahu
          type: user
        html: '<p>I am trying the deploy script instead,  and even for me I get the
          following error on AWS Sagemaker<br>code: 28, kind: StorageFull, message:
          "No space left on device"</p>

          <p>It always fails for me to download "pytorch_model-00004-of-00007.bin"</p>

          <p>I tried many things. Creating a new notebook instance, increasing the
          EBS storage size to about 120 GB. But somehow the same error remains.<br>Not
          sure what is the issue. </p>

          <p>To me it also gives the following error:- An error occurred while downloading
          using <code>hf_transfer</code>. Consider disabling HF_HUB_ENABLE_HF_TRANSFER
          for better error handling.</p>

          <p>So, the next steps I will try is:-</p>

          <ol>

          <li>Disable the above hf_transfer</li>

          <li>Bring down my instance size to something cheaper and smaller, till I
          am unable to solve this issue.</li>

          <li>Would try to may be download it somehow to S3 and load it from S3 instead
          of using directly the hub, which anyways fails.</li>

          </ol>

          <p>By the way I am using the Deploy script which is the same as the Hugging
          Face recommends, and not the one you mentioned. Not sure if that should
          matter.</p>

          '
        raw: "I am trying the deploy script instead,  and even for me I get the following\
          \ error on AWS Sagemaker\ncode: 28, kind: StorageFull, message: \"No space\
          \ left on device\"\n\nIt always fails for me to download \"pytorch_model-00004-of-00007.bin\"\
          \n\nI tried many things. Creating a new notebook instance, increasing the\
          \ EBS storage size to about 120 GB. But somehow the same error remains.\n\
          Not sure what is the issue. \n\nTo me it also gives the following error:-\
          \ An error occurred while downloading using `hf_transfer`. Consider disabling\
          \ HF_HUB_ENABLE_HF_TRANSFER for better error handling.\n\nSo, the next steps\
          \ I will try is:-\n1) Disable the above hf_transfer\n2) Bring down my instance\
          \ size to something cheaper and smaller, till I am unable to solve this\
          \ issue.\n3) Would try to may be download it somehow to S3 and load it from\
          \ S3 instead of using directly the hub, which anyways fails.\n\n\nBy the\
          \ way I am using the Deploy script which is the same as the Hugging Face\
          \ recommends, and not the one you mentioned. Not sure if that should matter."
        updatedAt: '2023-09-16T08:23:10.091Z'
      numEdits: 1
      reactions: []
    id: 6505658967ab943749b0732c
    type: comment
  author: praveensahu
  content: "I am trying the deploy script instead,  and even for me I get the following\
    \ error on AWS Sagemaker\ncode: 28, kind: StorageFull, message: \"No space left\
    \ on device\"\n\nIt always fails for me to download \"pytorch_model-00004-of-00007.bin\"\
    \n\nI tried many things. Creating a new notebook instance, increasing the EBS\
    \ storage size to about 120 GB. But somehow the same error remains.\nNot sure\
    \ what is the issue. \n\nTo me it also gives the following error:- An error occurred\
    \ while downloading using `hf_transfer`. Consider disabling HF_HUB_ENABLE_HF_TRANSFER\
    \ for better error handling.\n\nSo, the next steps I will try is:-\n1) Disable\
    \ the above hf_transfer\n2) Bring down my instance size to something cheaper and\
    \ smaller, till I am unable to solve this issue.\n3) Would try to may be download\
    \ it somehow to S3 and load it from S3 instead of using directly the hub, which\
    \ anyways fails.\n\n\nBy the way I am using the Deploy script which is the same\
    \ as the Hugging Face recommends, and not the one you mentioned. Not sure if that\
    \ should matter."
  created_at: 2023-09-16 07:21:29+00:00
  edited: true
  hidden: false
  id: 6505658967ab943749b0732c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: mosaicml/mpt-30b-chat
repo_type: model
status: open
target_branch: null
title: 'mosaicml/mpt-30b-chat on sagemaker ml.p3.8xlarge '
