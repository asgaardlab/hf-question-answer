!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-11-13 09:30:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-13T09:30:51.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.895605742931366
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Hi,</p>

          <p>Is it possible to release the unquantized LoRA merged weight so that
          we can apply to different quantization formats?</p>

          <p>Thanks!</p>

          '
        raw: "Hi,\r\n\r\nIs it possible to release the unquantized LoRA merged weight\
          \ so that we can apply to different quantization formats?\r\n\r\nThanks!"
        updatedAt: '2023-11-13T09:30:51.231Z'
      numEdits: 0
      reactions: []
    id: 6551eccb19c62ea90f6d660e
    type: comment
  author: Yhyu13
  content: "Hi,\r\n\r\nIs it possible to release the unquantized LoRA merged weight\
    \ so that we can apply to different quantization formats?\r\n\r\nThanks!"
  created_at: 2023-11-13 09:30:51+00:00
  edited: false
  hidden: false
  id: 6551eccb19c62ea90f6d660e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/89e57f9b1679b3c3901032140053c54c.svg
      fullname: "\u7267\u7AE5"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: zgce
      type: user
    createdAt: '2023-11-20T13:44:54.000Z'
    data:
      edited: false
      editors:
      - zgce
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9756616950035095
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/89e57f9b1679b3c3901032140053c54c.svg
          fullname: "\u7267\u7AE5"
          isHf: false
          isPro: false
          name: zgce
          type: user
        html: '<p>The original model file is too large and uploading always fails.
          If you want to use a higher quantization model, I have uploaded the 6bpw
          modular quantization model for download.</p>

          '
        raw: The original model file is too large and uploading always fails. If you
          want to use a higher quantization model, I have uploaded the 6bpw modular
          quantization model for download.
        updatedAt: '2023-11-20T13:44:54.375Z'
      numEdits: 0
      reactions: []
    id: 655b62d6f6103195fdc79970
    type: comment
  author: zgce
  content: The original model file is too large and uploading always fails. If you
    want to use a higher quantization model, I have uploaded the 6bpw modular quantization
    model for download.
  created_at: 2023-11-20 13:44:54+00:00
  edited: false
  hidden: false
  id: 655b62d6f6103195fdc79970
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: zgce/Yi-34b-200K-alpaca-rpv3-scipy-4bpw-hb6-exl2
repo_type: model
status: open
target_branch: null
title: Possible to release the unquantized LoRA merged weight?
