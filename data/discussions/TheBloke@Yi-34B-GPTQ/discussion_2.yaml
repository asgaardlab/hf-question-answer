!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yuuru
conflicting_files: null
created_at: 2023-11-06 17:58:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ecab51a4bf22f53f1dbfc1d5e99bba6c.svg
      fullname: M Veselovskiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yuuru
      type: user
    createdAt: '2023-11-06T17:58:35.000Z'
    data:
      edited: false
      editors:
      - Yuuru
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9906309247016907
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ecab51a4bf22f53f1dbfc1d5e99bba6c.svg
          fullname: M Veselovskiy
          isHf: false
          isPro: false
          name: Yuuru
          type: user
        html: '<p>Sry, which loader works currently? I''ve tried some for no success,
          but it could be me doing smth wrong.</p>

          '
        raw: Sry, which loader works currently? I've tried some for no success, but
          it could be me doing smth wrong.
        updatedAt: '2023-11-06T17:58:35.900Z'
      numEdits: 0
      reactions: []
    id: 6549294b94c5b95186788d0a
    type: comment
  author: Yuuru
  content: Sry, which loader works currently? I've tried some for no success, but
    it could be me doing smth wrong.
  created_at: 2023-11-06 17:58:35+00:00
  edited: false
  hidden: false
  id: 6549294b94c5b95186788d0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
      fullname: Choraly
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Stilgar
      type: user
    createdAt: '2023-11-06T21:25:04.000Z'
    data:
      edited: false
      editors:
      - Stilgar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.457760214805603
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
          fullname: Choraly
          isHf: false
          isPro: false
          name: Stilgar
          type: user
        html: '<p>With oobagooba and ExLlama_HF loader got the following error : KeyError:
          ''model.embed_tokens.weight''<br>Other gptq model load without issue, example
          TheBlokeSynthia-34B-v1.2-GPTQ load without error (using 21Go Vram)</p>

          '
        raw: 'With oobagooba and ExLlama_HF loader got the following error : KeyError:
          ''model.embed_tokens.weight''

          Other gptq model load without issue, example TheBlokeSynthia-34B-v1.2-GPTQ
          load without error (using 21Go Vram)'
        updatedAt: '2023-11-06T21:25:04.555Z'
      numEdits: 0
      reactions: []
    id: 654959b084ac015f9ed098f0
    type: comment
  author: Stilgar
  content: 'With oobagooba and ExLlama_HF loader got the following error : KeyError:
    ''model.embed_tokens.weight''

    Other gptq model load without issue, example TheBlokeSynthia-34B-v1.2-GPTQ load
    without error (using 21Go Vram)'
  created_at: 2023-11-06 21:25:04+00:00
  edited: false
  hidden: false
  id: 654959b084ac015f9ed098f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-06T22:25:17.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9187412858009338
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I heard that ExLlama added support for Yi recently, so you might
          just need to update ExLlama</p>

          <p>(4-bit versions only of course, not 3-bit or 8-bit)</p>

          <p>Or the Transformers loader should work.  Not AutoGPTQ yet.</p>

          '
        raw: 'I heard that ExLlama added support for Yi recently, so you might just
          need to update ExLlama


          (4-bit versions only of course, not 3-bit or 8-bit)


          Or the Transformers loader should work.  Not AutoGPTQ yet.'
        updatedAt: '2023-11-06T22:25:17.911Z'
      numEdits: 0
      reactions: []
    id: 654967cdfc80ab2d8c8e05ca
    type: comment
  author: TheBloke
  content: 'I heard that ExLlama added support for Yi recently, so you might just
    need to update ExLlama


    (4-bit versions only of course, not 3-bit or 8-bit)


    Or the Transformers loader should work.  Not AutoGPTQ yet.'
  created_at: 2023-11-06 22:25:17+00:00
  edited: false
  hidden: false
  id: 654967cdfc80ab2d8c8e05ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7ceb8416acf1236a86a7f8242ae48a8.svg
      fullname: Michael Mi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: guocuimi
      type: user
    createdAt: '2023-11-08T03:53:28.000Z'
    data:
      edited: true
      editors:
      - guocuimi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4865108132362366
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7ceb8416acf1236a86a7f8242ae48a8.svg
          fullname: Michael Mi
          isHf: false
          isPro: false
          name: guocuimi
          type: user
        html: "<p>feel free to try this new project to serve the model locally. <a\
          \ rel=\"nofollow\" href=\"https://github.com/vectorch-ai/ScaleLLM\">https://github.com/vectorch-ai/ScaleLLM</a><br>1:\
          \ start model inference server</p>\n<pre><code>docker run -it --gpus=all\
          \ --net=host --shm-size=1g \\\n  -v $HOME/.cache/huggingface/hub:/models\
          \ \\\n  -e HF_MODEL_ID=TheBloke/Yi-34B-GPTQ \\\n  -e DEVICE=auto \\\n  docker.io/vectorchai/scalellm:latest\
          \ --logtostderr\n</code></pre>\n<p>2: start REST API server</p>\n<pre><code>docker\
          \ run -it --net=host \\\n  docker.io/vectorchai/scalellm-gateway:latest\
          \ --logtostderr\n</code></pre>\n<p>you will get following running services:</p>\n\
          <ul>\n<li>ScaleLLM gRPC server on port 8888: localhost:8888</li>\n<li>ScaleLLM\
          \ HTTP server for monitoring on port 9999: localhost:9999</li>\n<li>ScaleLLM\
          \ REST API server on port 8080: localhost:8080</li>\n</ul>\n<p>then send\
          \ requests</p>\n<pre><code>curl http://localhost:8080/v1/completions   -H\
          \ \"Content-Type: application/json\"   -d '{\n    \"model\": \"TheBloke/Yi-34B-GPTQ\"\
          ,\n    \"prompt\": \"what is vue.js\",\n    \"max_tokens\": 32,\n    \"\
          temperature\": 0.7\n  }'\n</code></pre>\n"
        raw: "feel free to try this new project to serve the model locally. https://github.com/vectorch-ai/ScaleLLM\n\
          1: start model inference server\n```\ndocker run -it --gpus=all --net=host\
          \ --shm-size=1g \\\n  -v $HOME/.cache/huggingface/hub:/models \\\n  -e HF_MODEL_ID=TheBloke/Yi-34B-GPTQ\
          \ \\\n  -e DEVICE=auto \\\n  docker.io/vectorchai/scalellm:latest --logtostderr\n\
          ```\n2: start REST API server\n```\ndocker run -it --net=host \\\n  docker.io/vectorchai/scalellm-gateway:latest\
          \ --logtostderr\n```\nyou will get following running services:\n* ScaleLLM\
          \ gRPC server on port 8888: localhost:8888\n* ScaleLLM HTTP server for monitoring\
          \ on port 9999: localhost:9999\n* ScaleLLM REST API server on port 8080:\
          \ localhost:8080\n\nthen send requests\n```\ncurl http://localhost:8080/v1/completions\
          \   -H \"Content-Type: application/json\"   -d '{\n    \"model\": \"TheBloke/Yi-34B-GPTQ\"\
          ,\n    \"prompt\": \"what is vue.js\",\n    \"max_tokens\": 32,\n    \"\
          temperature\": 0.7\n  }'\n```\n"
        updatedAt: '2023-11-11T07:14:17.076Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Yhyu13
    id: 654b063883e7bfc43160ca0e
    type: comment
  author: guocuimi
  content: "feel free to try this new project to serve the model locally. https://github.com/vectorch-ai/ScaleLLM\n\
    1: start model inference server\n```\ndocker run -it --gpus=all --net=host --shm-size=1g\
    \ \\\n  -v $HOME/.cache/huggingface/hub:/models \\\n  -e HF_MODEL_ID=TheBloke/Yi-34B-GPTQ\
    \ \\\n  -e DEVICE=auto \\\n  docker.io/vectorchai/scalellm:latest --logtostderr\n\
    ```\n2: start REST API server\n```\ndocker run -it --net=host \\\n  docker.io/vectorchai/scalellm-gateway:latest\
    \ --logtostderr\n```\nyou will get following running services:\n* ScaleLLM gRPC\
    \ server on port 8888: localhost:8888\n* ScaleLLM HTTP server for monitoring on\
    \ port 9999: localhost:9999\n* ScaleLLM REST API server on port 8080: localhost:8080\n\
    \nthen send requests\n```\ncurl http://localhost:8080/v1/completions   -H \"Content-Type:\
    \ application/json\"   -d '{\n    \"model\": \"TheBloke/Yi-34B-GPTQ\",\n    \"\
    prompt\": \"what is vue.js\",\n    \"max_tokens\": 32,\n    \"temperature\": 0.7\n\
    \  }'\n```\n"
  created_at: 2023-11-08 03:53:28+00:00
  edited: true
  hidden: false
  id: 654b063883e7bfc43160ca0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-14T13:57:22.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6889170408248901
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Yuuru&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Yuuru\">@<span class=\"\
          underline\">Yuuru</span></a></span>\n\n\t</span></span> </p>\n<p>You can\
          \ also inferencing in textgen with exllamav2</p>\n<p><a href=\"https://huggingface.co/01-ai/Yi-34B/discussions/22#654fb707380ee26b49b3b180\"\
          >https://huggingface.co/01-ai/Yi-34B/discussions/22#654fb707380ee26b49b3b180</a></p>\n"
        raw: "@Yuuru \n\nYou can also inferencing in textgen with exllamav2\n\nhttps://huggingface.co/01-ai/Yi-34B/discussions/22#654fb707380ee26b49b3b180"
        updatedAt: '2023-11-14T13:57:22.320Z'
      numEdits: 0
      reactions: []
    id: 65537cc2aff7952bd5dfe667
    type: comment
  author: Yhyu13
  content: "@Yuuru \n\nYou can also inferencing in textgen with exllamav2\n\nhttps://huggingface.co/01-ai/Yi-34B/discussions/22#654fb707380ee26b49b3b180"
  created_at: 2023-11-14 13:57:22+00:00
  edited: false
  hidden: false
  id: 65537cc2aff7952bd5dfe667
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Yi-34B-GPTQ
repo_type: model
status: open
target_branch: null
title: How do i run it?
