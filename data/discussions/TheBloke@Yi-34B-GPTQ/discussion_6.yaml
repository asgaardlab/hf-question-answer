!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RonanMcGovern
conflicting_files: null
created_at: 2023-11-12 23:30:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-11-12T23:30:23.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5522579550743103
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Is anyone getting good generation quality on this model? I have\
          \ been running the recommended code and results are very poor:</p>\n<pre><code>from\
          \ transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          01-ai/Yi-34B\", device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)\n\
          tokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-34B\", trust_remote_code=True)\n\
          inputs = tokenizer(\"There's a place where time stands still. A place of\
          \ breath taking wonder, but also\", return_tensors=\"pt\")\nmax_length =\
          \ 256\n\noutputs = model.generate(\n    inputs.input_ids.cuda(),\n    max_length=max_length,\n\
          \    eos_token_id=tokenizer.eos_token_id,\n    do_sample=True,\n    repetition_penalty=1.3,\n\
          \    no_repeat_ngram_size=5,\n    temperature=0.7,\n    top_k=40,\n    top_p=0.8,\n\
          )\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</code></pre>\n"
        raw: "Is anyone getting good generation quality on this model? I have been\
          \ running the recommended code and results are very poor:\r\n```\r\nfrom\
          \ transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-34B\", device_map=\"auto\"\
          , torch_dtype=\"auto\", trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          01-ai/Yi-34B\", trust_remote_code=True)\r\ninputs = tokenizer(\"There's\
          \ a place where time stands still. A place of breath taking wonder, but\
          \ also\", return_tensors=\"pt\")\r\nmax_length = 256\r\n\r\noutputs = model.generate(\r\
          \n    inputs.input_ids.cuda(),\r\n    max_length=max_length,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
          \n    do_sample=True,\r\n    repetition_penalty=1.3,\r\n    no_repeat_ngram_size=5,\r\
          \n    temperature=0.7,\r\n    top_k=40,\r\n    top_p=0.8,\r\n)\r\nprint(tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))\r\n```"
        updatedAt: '2023-11-12T23:30:23.386Z'
      numEdits: 0
      reactions: []
    id: 6551600f43baee6b4d268109
    type: comment
  author: RonanMcGovern
  content: "Is anyone getting good generation quality on this model? I have been running\
    \ the recommended code and results are very poor:\r\n```\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    01-ai/Yi-34B\", device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)\r\
    \ntokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-34B\", trust_remote_code=True)\r\
    \ninputs = tokenizer(\"There's a place where time stands still. A place of breath\
    \ taking wonder, but also\", return_tensors=\"pt\")\r\nmax_length = 256\r\n\r\n\
    outputs = model.generate(\r\n    inputs.input_ids.cuda(),\r\n    max_length=max_length,\r\
    \n    eos_token_id=tokenizer.eos_token_id,\r\n    do_sample=True,\r\n    repetition_penalty=1.3,\r\
    \n    no_repeat_ngram_size=5,\r\n    temperature=0.7,\r\n    top_k=40,\r\n   \
    \ top_p=0.8,\r\n)\r\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\r\
    \n```"
  created_at: 2023-11-12 23:30:23+00:00
  edited: false
  hidden: false
  id: 6551600f43baee6b4d268109
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-14T13:58:39.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7552530765533447
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>You can try out textgen with exllamav2 like how I did it</p>\n\
          <p><a href=\"https://huggingface.co/01-ai/Yi-34B/discussions/22#654fb707380ee26b49b3b180\"\
          >https://huggingface.co/01-ai/Yi-34B/discussions/22#654fb707380ee26b49b3b180</a></p>\n"
        raw: "@RonanMcGovern \n\nYou can try out textgen with exllamav2 like how I\
          \ did it\n\nhttps://huggingface.co/01-ai/Yi-34B/discussions/22#654fb707380ee26b49b3b180"
        updatedAt: '2023-11-14T13:58:39.370Z'
      numEdits: 0
      reactions: []
    id: 65537d0f9ffd475c676d2b3f
    type: comment
  author: Yhyu13
  content: "@RonanMcGovern \n\nYou can try out textgen with exllamav2 like how I did\
    \ it\n\nhttps://huggingface.co/01-ai/Yi-34B/discussions/22#654fb707380ee26b49b3b180"
  created_at: 2023-11-14 13:58:39+00:00
  edited: false
  hidden: false
  id: 65537d0f9ffd475c676d2b3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-11-15T14:39:37.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.975997805595398
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Thanks, actually:</p>

          <pre><code>\nHuman: {prompt}\nAssistant:

          </code></pre>

          <p>is working ok for me. I''m also doing a chat fine-tune and that is helping.</p>

          '
        raw: 'Thanks, actually:

          ```

          \nHuman: {prompt}\nAssistant:

          ```

          is working ok for me. I''m also doing a chat fine-tune and that is helping.'
        updatedAt: '2023-11-15T14:39:37.071Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6554d8293a9b18b5c3b27607
    id: 6554d8293a9b18b5c3b27606
    type: comment
  author: RonanMcGovern
  content: 'Thanks, actually:

    ```

    \nHuman: {prompt}\nAssistant:

    ```

    is working ok for me. I''m also doing a chat fine-tune and that is helping.'
  created_at: 2023-11-15 14:39:37+00:00
  edited: false
  hidden: false
  id: 6554d8293a9b18b5c3b27606
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-11-15T14:39:37.000Z'
    data:
      status: closed
    id: 6554d8293a9b18b5c3b27607
    type: status-change
  author: RonanMcGovern
  created_at: 2023-11-15 14:39:37+00:00
  id: 6554d8293a9b18b5c3b27607
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/Yi-34B-GPTQ
repo_type: model
status: closed
target_branch: null
title: Generation quality
