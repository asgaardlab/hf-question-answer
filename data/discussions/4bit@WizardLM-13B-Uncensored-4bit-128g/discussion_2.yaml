!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Odysseum04
conflicting_files: null
created_at: 2023-05-20 14:17:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ah7cjASuOph-8uU4Ym8JW.jpeg?w=200&h=200&f=face
      fullname: "Cl\xE9ment Raes"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Odysseum04
      type: user
    createdAt: '2023-05-20T15:17:31.000Z'
    data:
      edited: false
      editors:
      - Odysseum04
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ah7cjASuOph-8uU4Ym8JW.jpeg?w=200&h=200&f=face
          fullname: "Cl\xE9ment Raes"
          isHf: false
          isPro: false
          name: Odysseum04
          type: user
        html: '<p>Hello, I was just wondering what would be the best in terms of speed
          or quality and what is for you the best compromise.</p>

          <p>I use a ryzen 7600x, 16GB ddr4 3200 ram and a 8gb 3060ti</p>

          '
        raw: "Hello, I was just wondering what would be the best in terms of speed\
          \ or quality and what is for you the best compromise.\r\n\r\nI use a ryzen\
          \ 7600x, 16GB ddr4 3200 ram and a 8gb 3060ti"
        updatedAt: '2023-05-20T15:17:31.994Z'
      numEdits: 0
      reactions: []
    id: 6468e48bdcbb937d56b6b657
    type: comment
  author: Odysseum04
  content: "Hello, I was just wondering what would be the best in terms of speed or\
    \ quality and what is for you the best compromise.\r\n\r\nI use a ryzen 7600x,\
    \ 16GB ddr4 3200 ram and a 8gb 3060ti"
  created_at: 2023-05-20 14:17:31+00:00
  edited: false
  hidden: false
  id: 6468e48bdcbb937d56b6b657
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8941e41b1d0eafaaccb4e71b793c827f.svg
      fullname: Weiss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Curia
      type: user
    createdAt: '2023-05-23T11:19:18.000Z'
    data:
      edited: false
      editors:
      - Curia
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8941e41b1d0eafaaccb4e71b793c827f.svg
          fullname: Weiss
          isHf: false
          isPro: false
          name: Curia
          type: user
        html: '<p>8GB VRAM can fit 13B 4 bit model, but you should prepare to play
          around a bit.<br>For example, maybe it will fit a 4_0 comfortably and can
          do a 400 token input, but completely screw up in a 4_1.<br>Also I recommend
          just use instruct mode, or just use cmd, it might also help if you are on
          Linux/WSL.<br>At 8GB of VRAM, fitting 13B 4 bit model means you should do
          everything to save VRAM, every little bits help.</p>

          <p>IMO 13B 4 bit would give you better result if it is stable, but if you
          just want hassle free experience, use 7B 8bit and load a LORA to have some
          fun :D</p>

          <p>Also don''t forget about GGMLs, as long as you offload most to GPU, you
          should get acceptable speed.</p>

          '
        raw: '8GB VRAM can fit 13B 4 bit model, but you should prepare to play around
          a bit.

          For example, maybe it will fit a 4_0 comfortably and can do a 400 token
          input, but completely screw up in a 4_1.

          Also I recommend just use instruct mode, or just use cmd, it might also
          help if you are on Linux/WSL.

          At 8GB of VRAM, fitting 13B 4 bit model means you should do everything to
          save VRAM, every little bits help.


          IMO 13B 4 bit would give you better result if it is stable, but if you just
          want hassle free experience, use 7B 8bit and load a LORA to have some fun
          :D


          Also don''t forget about GGMLs, as long as you offload most to GPU, you
          should get acceptable speed.'
        updatedAt: '2023-05-23T11:19:18.818Z'
      numEdits: 0
      reactions: []
    id: 646ca13610f66cc3c762958a
    type: comment
  author: Curia
  content: '8GB VRAM can fit 13B 4 bit model, but you should prepare to play around
    a bit.

    For example, maybe it will fit a 4_0 comfortably and can do a 400 token input,
    but completely screw up in a 4_1.

    Also I recommend just use instruct mode, or just use cmd, it might also help if
    you are on Linux/WSL.

    At 8GB of VRAM, fitting 13B 4 bit model means you should do everything to save
    VRAM, every little bits help.


    IMO 13B 4 bit would give you better result if it is stable, but if you just want
    hassle free experience, use 7B 8bit and load a LORA to have some fun :D


    Also don''t forget about GGMLs, as long as you offload most to GPU, you should
    get acceptable speed.'
  created_at: 2023-05-23 10:19:18+00:00
  edited: false
  hidden: false
  id: 646ca13610f66cc3c762958a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: 4bit/WizardLM-13B-Uncensored-4bit-128g
repo_type: model
status: open
target_branch: null
title: '13B 4bits vs 7B 8bits '
