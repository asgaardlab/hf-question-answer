!!python/object:huggingface_hub.community.DiscussionWithDetails
author: barghavani
conflicting_files: []
created_at: 2023-10-30 14:53:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653a76fc7721c7df1330df05/de-Pzezstackay8vIfCvV.jpeg?w=200&h=200&f=face
      fullname: Bahareh Arghavani Nobar
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: barghavani
      type: user
    createdAt: '2023-10-30T15:53:18.000Z'
    data:
      edited: false
      editors:
      - barghavani
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6761040091514587
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653a76fc7721c7df1330df05/de-Pzezstackay8vIfCvV.jpeg?w=200&h=200&f=face
          fullname: Bahareh Arghavani Nobar
          isHf: false
          isPro: false
          name: barghavani
          type: user
        html: '<p>Prerequisities:<br>Python &gt;= 3.9<br>Espeak-NG : sudo apt install
          -y espeak-ng<br>TTS (from the repo): pip install -U pip setuptools wheel
          git clone <a rel="nofollow" href="https://github.com/coqui-ai/TTS">https://github.com/coqui-ai/TTS</a>
          pip install -e TTS/<br>Setup Environment<br>init.sh--&gt; add "sudo apt
          update &amp;&amp; sudo apt upgrade -y<br>sudo apt install -y python3.10
          python3.10-dev python3.10-venv /usr/bin/python3.10 -m venv /opt/python/envs/py310
          /opt/python/envs/py310/bin/python -m pip install -U pip setuptools wheel
          /opt/python/envs/py310/bin/python -m pip install -U ipykernel ipython ipython_genutils
          jedi lets-plot aiohttp pandas</p>

          <p>sudo apt install -y espeak-ng"</p>

          <p>in attached data --&gt; on file environment.yml --&gt;change datalore-base-env:"minimal"
          to "py310"<br>background computation --&gt; Never Trun off<br>git clone
          <a rel="nofollow" href="https://github.com/coqui-ai/TTS.git">https://github.com/coqui-ai/TTS.git</a><br>navigate
          to TTS (cd TTS)<br>pip install -e.<br>Run Multi-GPU<br>CUDA_VISIBLE_DEVICES="0,1"
          accelerate launch --multi_gpu --num_processes 2 multi-speaker.py<br>For
          avoiding any intruption over your training use trainer = Trainer( TrainerArgs(use_accelerate=True),
          config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples,
          ) trainer.fit()<br>Faster train with more than 1 num_loader_workers=4 in
          advance you should do sudo mount -o remount,size=8G /dev/shm<br>Run with
          one GPU<br>!nvidia-smi(status of GPU)<br>os.environ["CUDA_VISIBLE_DEVICES"]
          = "7" which GPU you intend to run your code<br>How to fix Error over runtime<br>Error
          :"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00
          MiB. GPU 0 has a total capacty of 9.77 GiB of which 52.31 MiB is free. Including
          non-PyTorch memory, this process has 8.68 GiB memory in use. Of the allocated
          memory 8.25 GiB is allocated by PyTorch, and 155.23 MiB is reserved by PyTorch
          but unallocated. If reserved but unallocated memory is large try setting
          max_split_size_mb to avoid fragmentation. See documentation for Memory Management
          and PYTORCH_CUDA_ALLOC_CONF" Solution : reduce batch_size<br>Error: Can''t
          find some wavs although they are existed Solution : your wavs might be nested
          in files and cannot find nested files<br>if you use common-voice as your
          formatter your wavs must be store in clips<br>Error :dimension Solution
          : mix precesion =false<br>Tensorboard :<br>For tensorboard download the
          latest output and un zip it go to that file run this command on window shell
          go cd to the address you stored the file : tensorboard --logdir=. --bind_all
          --port=6007 --&gt; url open in your browser<br>Use wandb --&gt;import wandb
          Start a wandb run with sync_tensorboard=True if wandb.run is None: wandb.init(project="persian-tts-vits-grapheme-cv15-fa-male-native-multispeaker-RERUN",
          group="GPUx8 accel mixed bf16 128x32", sync_tensorboard=True)<br>For Multi-Speaker<br>use_speaker_embedding=True<br>speaker_manager
          = SpeakerManager() speaker_manager.set_ids_from_data(train_samples + eval_samples,
          parse_key="speaker_name") config.num_speakers = speaker_manager.num_speakers<br>model
          = Vits(config, ap, tokenizer, speaker_manager=speaker_manager)</p>

          '
        raw: 'Prerequisities:

          Python >= 3.9

          Espeak-NG : sudo apt install -y espeak-ng

          TTS (from the repo): pip install -U pip setuptools wheel git clone https://github.com/coqui-ai/TTS
          pip install -e TTS/

          Setup Environment

          init.sh--> add "sudo apt update && sudo apt upgrade -y

          sudo apt install -y python3.10 python3.10-dev python3.10-venv /usr/bin/python3.10
          -m venv /opt/python/envs/py310 /opt/python/envs/py310/bin/python -m pip
          install -U pip setuptools wheel /opt/python/envs/py310/bin/python -m pip
          install -U ipykernel ipython ipython_genutils jedi lets-plot aiohttp pandas


          sudo apt install -y espeak-ng"


          in attached data --> on file environment.yml -->change datalore-base-env:"minimal"
          to "py310"

          background computation --> Never Trun off

          git clone https://github.com/coqui-ai/TTS.git

          navigate to TTS (cd TTS)

          pip install -e.

          Run Multi-GPU

          CUDA_VISIBLE_DEVICES="0,1" accelerate launch --multi_gpu --num_processes
          2 multi-speaker.py

          For avoiding any intruption over your training use trainer = Trainer( TrainerArgs(use_accelerate=True),
          config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples,
          ) trainer.fit()

          Faster train with more than 1 num_loader_workers=4 in advance you should
          do sudo mount -o remount,size=8G /dev/shm

          Run with one GPU

          !nvidia-smi(status of GPU)

          os.environ["CUDA_VISIBLE_DEVICES"] = "7" which GPU you intend to run your
          code

          How to fix Error over runtime

          Error :"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate
          64.00 MiB. GPU 0 has a total capacty of 9.77 GiB of which 52.31 MiB is free.
          Including non-PyTorch memory, this process has 8.68 GiB memory in use. Of
          the allocated memory 8.25 GiB is allocated by PyTorch, and 155.23 MiB is
          reserved by PyTorch but unallocated. If reserved but unallocated memory
          is large try setting max_split_size_mb to avoid fragmentation. See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF" Solution : reduce batch_size

          Error: Can''t find some wavs although they are existed Solution : your wavs
          might be nested in files and cannot find nested files

          if you use common-voice as your formatter your wavs must be store in clips

          Error :dimension Solution : mix precesion =false

          Tensorboard :

          For tensorboard download the latest output and un zip it go to that file
          run this command on window shell go cd to the address you stored the file
          : tensorboard --logdir=. --bind_all --port=6007 --> url open in your browser

          Use wandb -->import wandb Start a wandb run with sync_tensorboard=True if
          wandb.run is None: wandb.init(project="persian-tts-vits-grapheme-cv15-fa-male-native-multispeaker-RERUN",
          group="GPUx8 accel mixed bf16 128x32", sync_tensorboard=True)

          For Multi-Speaker

          use_speaker_embedding=True

          speaker_manager = SpeakerManager() speaker_manager.set_ids_from_data(train_samples
          + eval_samples, parse_key="speaker_name") config.num_speakers = speaker_manager.num_speakers

          model = Vits(config, ap, tokenizer, speaker_manager=speaker_manager)'
        updatedAt: '2023-10-30T15:53:18.036Z'
      numEdits: 0
      reactions: []
    id: 653fd16e07faf7b0bc562f61
    type: comment
  author: barghavani
  content: 'Prerequisities:

    Python >= 3.9

    Espeak-NG : sudo apt install -y espeak-ng

    TTS (from the repo): pip install -U pip setuptools wheel git clone https://github.com/coqui-ai/TTS
    pip install -e TTS/

    Setup Environment

    init.sh--> add "sudo apt update && sudo apt upgrade -y

    sudo apt install -y python3.10 python3.10-dev python3.10-venv /usr/bin/python3.10
    -m venv /opt/python/envs/py310 /opt/python/envs/py310/bin/python -m pip install
    -U pip setuptools wheel /opt/python/envs/py310/bin/python -m pip install -U ipykernel
    ipython ipython_genutils jedi lets-plot aiohttp pandas


    sudo apt install -y espeak-ng"


    in attached data --> on file environment.yml -->change datalore-base-env:"minimal"
    to "py310"

    background computation --> Never Trun off

    git clone https://github.com/coqui-ai/TTS.git

    navigate to TTS (cd TTS)

    pip install -e.

    Run Multi-GPU

    CUDA_VISIBLE_DEVICES="0,1" accelerate launch --multi_gpu --num_processes 2 multi-speaker.py

    For avoiding any intruption over your training use trainer = Trainer( TrainerArgs(use_accelerate=True),
    config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples,
    ) trainer.fit()

    Faster train with more than 1 num_loader_workers=4 in advance you should do sudo
    mount -o remount,size=8G /dev/shm

    Run with one GPU

    !nvidia-smi(status of GPU)

    os.environ["CUDA_VISIBLE_DEVICES"] = "7" which GPU you intend to run your code

    How to fix Error over runtime

    Error :"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00
    MiB. GPU 0 has a total capacty of 9.77 GiB of which 52.31 MiB is free. Including
    non-PyTorch memory, this process has 8.68 GiB memory in use. Of the allocated
    memory 8.25 GiB is allocated by PyTorch, and 155.23 MiB is reserved by PyTorch
    but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb
    to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
    Solution : reduce batch_size

    Error: Can''t find some wavs although they are existed Solution : your wavs might
    be nested in files and cannot find nested files

    if you use common-voice as your formatter your wavs must be store in clips

    Error :dimension Solution : mix precesion =false

    Tensorboard :

    For tensorboard download the latest output and un zip it go to that file run this
    command on window shell go cd to the address you stored the file : tensorboard
    --logdir=. --bind_all --port=6007 --> url open in your browser

    Use wandb -->import wandb Start a wandb run with sync_tensorboard=True if wandb.run
    is None: wandb.init(project="persian-tts-vits-grapheme-cv15-fa-male-native-multispeaker-RERUN",
    group="GPUx8 accel mixed bf16 128x32", sync_tensorboard=True)

    For Multi-Speaker

    use_speaker_embedding=True

    speaker_manager = SpeakerManager() speaker_manager.set_ids_from_data(train_samples
    + eval_samples, parse_key="speaker_name") config.num_speakers = speaker_manager.num_speakers

    model = Vits(config, ap, tokenizer, speaker_manager=speaker_manager)'
  created_at: 2023-10-30 14:53:18+00:00
  edited: false
  hidden: false
  id: 653fd16e07faf7b0bc562f61
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653a76fc7721c7df1330df05/de-Pzezstackay8vIfCvV.jpeg?w=200&h=200&f=face
      fullname: Bahareh Arghavani Nobar
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: barghavani
      type: user
    createdAt: '2023-10-30T15:53:18.000Z'
    data:
      oid: 1040347b51cb8dd2d9f7c43804830c0589f70351
      parents:
      - 42fb11dc1385c75bb894175cb9020e0fff230004
      subject: README
    id: 653fd16e0000000000000000
    type: commit
  author: barghavani
  created_at: 2023-10-30 14:53:18+00:00
  id: 653fd16e0000000000000000
  oid: 1040347b51cb8dd2d9f7c43804830c0589f70351
  summary: README
  type: commit
is_pull_request: true
merge_commit_oid: null
num: 1
repo_id: saillab/vits_multi_cv_15_validated_dataset
repo_type: model
status: open
target_branch: refs/heads/main
title: README
