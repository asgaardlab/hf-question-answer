!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nth-fixie
conflicting_files: null
created_at: 2023-08-11 20:08:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24c3acf7b566ffedeb29fdf0370c8d44.svg
      fullname: Nick Heiner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nth-fixie
      type: user
    createdAt: '2023-08-11T21:08:55.000Z'
    data:
      edited: false
      editors:
      - nth-fixie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9667381644248962
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24c3acf7b566ffedeb29fdf0370c8d44.svg
          fullname: Nick Heiner
          isHf: false
          isPro: false
          name: nth-fixie
          type: user
        html: '<p>Hi! I''d like to experiment with this model. Is there any particular
          reason that hosted inference is disabled? Thanks.</p>

          '
        raw: Hi! I'd like to experiment with this model. Is there any particular reason
          that hosted inference is disabled? Thanks.
        updatedAt: '2023-08-11T21:08:55.304Z'
      numEdits: 0
      reactions: []
    id: 64d6a367a4839890b21083d0
    type: comment
  author: nth-fixie
  content: Hi! I'd like to experiment with this model. Is there any particular reason
    that hosted inference is disabled? Thanks.
  created_at: 2023-08-11 20:08:55+00:00
  edited: false
  hidden: false
  id: 64d6a367a4839890b21083d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-11T23:55:42.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.845004677772522
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Howdy <span data-props=\"{&quot;user&quot;:&quot;nth-fixie&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nth-fixie\"\
          >@<span class=\"underline\">nth-fixie</span></a></span>\n\n\t</span></span>\
          \ , there's no hosted inference because of something about the Llama 2 model\
          \ configuration.</p>\n<p>Note that <a href=\"https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\"\
          >Llama 2 in the Facebook repo</a> is the same.</p>\n<p>From the HuggingFace\
          \ <a href=\"https://huggingface.co/docs/hub/models-inference\">docs</a>:</p>\n\
          <pre><code>For some tasks, there might not be support in the inference API,\
          \ and, hence, there is no widget. For all libraries (except \U0001F917 Transformers),\
          \ there is a mapping of library to supported tasks in the API. When a model\
          \ repository has a task that is not supported by the repository library,\
          \ the repository has inference: false by default.\n</code></pre>\n<p>I've\
          \ posted an issue to Github on this: <a rel=\"nofollow\" href=\"https://github.com/huggingface/huggingface_hub/issues/1589\"\
          >https://github.com/huggingface/huggingface_hub/issues/1589</a></p>\n<p>In\
          \ the meantime, for those who have purchased access to this model:</p>\n\
          <ol>\n<li>There is a rough inference script <a rel=\"nofollow\" href=\"\
          https://colab.research.google.com/drive/1Ow5cQ0JNv-vXsT-apCceH6Na3b4L7JyW?usp=sharing\"\
          >here</a></li>\n<li>This much cleaner script <a rel=\"nofollow\" href=\"\
          https://colab.research.google.com/drive/1u8x41Jx8WWtI-nzHOgqTxkS3Q_lcjaSX?usp=sharing\"\
          >here</a> will work once I push a GPTQ model for 13B (which I'll make available\
          \ to those with access to this repo too).</li>\n</ol>\n"
        raw: "Howdy @nth-fixie , there's no hosted inference because of something\
          \ about the Llama 2 model configuration.\n\nNote that [Llama 2 in the Facebook\
          \ repo](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) is the same.\n\
          \nFrom the HuggingFace [docs](https://huggingface.co/docs/hub/models-inference):\n\
          ```\nFor some tasks, there might not be support in the inference API, and,\
          \ hence, there is no widget. For all libraries (except \U0001F917 Transformers),\
          \ there is a mapping of library to supported tasks in the API. When a model\
          \ repository has a task that is not supported by the repository library,\
          \ the repository has inference: false by default.\n```\n\nI've posted an\
          \ issue to Github on this: https://github.com/huggingface/huggingface_hub/issues/1589\n\
          \nIn the meantime, for those who have purchased access to this model:\n\
          1. There is a rough inference script [here](https://colab.research.google.com/drive/1Ow5cQ0JNv-vXsT-apCceH6Na3b4L7JyW?usp=sharing)\n\
          2. This much cleaner script [here](https://colab.research.google.com/drive/1u8x41Jx8WWtI-nzHOgqTxkS3Q_lcjaSX?usp=sharing)\
          \ will work once I push a GPTQ model for 13B (which I'll make available\
          \ to those with access to this repo too).\n\n"
        updatedAt: '2023-08-11T23:55:42.926Z'
      numEdits: 0
      reactions: []
    id: 64d6ca7e9fef656cfd0ea1df
    type: comment
  author: RonanMcGovern
  content: "Howdy @nth-fixie , there's no hosted inference because of something about\
    \ the Llama 2 model configuration.\n\nNote that [Llama 2 in the Facebook repo](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)\
    \ is the same.\n\nFrom the HuggingFace [docs](https://huggingface.co/docs/hub/models-inference):\n\
    ```\nFor some tasks, there might not be support in the inference API, and, hence,\
    \ there is no widget. For all libraries (except \U0001F917 Transformers), there\
    \ is a mapping of library to supported tasks in the API. When a model repository\
    \ has a task that is not supported by the repository library, the repository has\
    \ inference: false by default.\n```\n\nI've posted an issue to Github on this:\
    \ https://github.com/huggingface/huggingface_hub/issues/1589\n\nIn the meantime,\
    \ for those who have purchased access to this model:\n1. There is a rough inference\
    \ script [here](https://colab.research.google.com/drive/1Ow5cQ0JNv-vXsT-apCceH6Na3b4L7JyW?usp=sharing)\n\
    2. This much cleaner script [here](https://colab.research.google.com/drive/1u8x41Jx8WWtI-nzHOgqTxkS3Q_lcjaSX?usp=sharing)\
    \ will work once I push a GPTQ model for 13B (which I'll make available to those\
    \ with access to this repo too).\n\n"
  created_at: 2023-08-11 22:55:42+00:00
  edited: false
  hidden: false
  id: 64d6ca7e9fef656cfd0ea1df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-12T12:11:59.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8819250464439392
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Quick update here.</p>

          <p>It is possible to turn on hosted inference, but hosted inference only
          works for models that are smaller than 10 GB. This 13B model is 26B, and
          even the 7B model is 13B, so hosted inference doesn''t work.</p>

          <p>I''m just about to push <a href="https://huggingface.co/Trelis/Llama-2-13b-chat-hf-function-calling-GPTQ">13B
          in quantized (GPTQ) format</a>. That will allow for the use of the fLlama
          (function-calling Llama) <a rel="nofollow" href="https://colab.research.google.com/drive/1u8x41Jx8WWtI-nzHOgqTxkS3Q_lcjaSX?usp=sharing">Colab
          notebook</a> for inference. But GPTQ (or GGML) aren''t supported by hosted
          inference (at least they failed when I tried). </p>

          '
        raw: 'Quick update here.


          It is possible to turn on hosted inference, but hosted inference only works
          for models that are smaller than 10 GB. This 13B model is 26B, and even
          the 7B model is 13B, so hosted inference doesn''t work.


          I''m just about to push [13B in quantized (GPTQ) format](https://huggingface.co/Trelis/Llama-2-13b-chat-hf-function-calling-GPTQ).
          That will allow for the use of the fLlama (function-calling Llama) [Colab
          notebook](https://colab.research.google.com/drive/1u8x41Jx8WWtI-nzHOgqTxkS3Q_lcjaSX?usp=sharing)
          for inference. But GPTQ (or GGML) aren''t supported by hosted inference
          (at least they failed when I tried). '
        updatedAt: '2023-08-12T12:11:59.631Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64d7770f70891ac9b8990334
    id: 64d7770f70891ac9b8990333
    type: comment
  author: RonanMcGovern
  content: 'Quick update here.


    It is possible to turn on hosted inference, but hosted inference only works for
    models that are smaller than 10 GB. This 13B model is 26B, and even the 7B model
    is 13B, so hosted inference doesn''t work.


    I''m just about to push [13B in quantized (GPTQ) format](https://huggingface.co/Trelis/Llama-2-13b-chat-hf-function-calling-GPTQ).
    That will allow for the use of the fLlama (function-calling Llama) [Colab notebook](https://colab.research.google.com/drive/1u8x41Jx8WWtI-nzHOgqTxkS3Q_lcjaSX?usp=sharing)
    for inference. But GPTQ (or GGML) aren''t supported by hosted inference (at least
    they failed when I tried). '
  created_at: 2023-08-12 11:11:59+00:00
  edited: false
  hidden: false
  id: 64d7770f70891ac9b8990333
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-12T12:11:59.000Z'
    data:
      status: closed
    id: 64d7770f70891ac9b8990334
    type: status-change
  author: RonanMcGovern
  created_at: 2023-08-12 11:11:59+00:00
  id: 64d7770f70891ac9b8990334
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24c3acf7b566ffedeb29fdf0370c8d44.svg
      fullname: Nick Heiner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nth-fixie
      type: user
    createdAt: '2023-08-14T12:58:49.000Z'
    data:
      edited: false
      editors:
      - nth-fixie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7788880467414856
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24c3acf7b566ffedeb29fdf0370c8d44.svg
          fullname: Nick Heiner
          isHf: false
          isPro: false
          name: nth-fixie
          type: user
        html: '<p>Thanks!</p>

          '
        raw: Thanks!
        updatedAt: '2023-08-14T12:58:49.497Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RonanMcGovern
    id: 64da2509a8829bc784c7b1ce
    type: comment
  author: nth-fixie
  content: Thanks!
  created_at: 2023-08-14 11:58:49+00:00
  edited: false
  hidden: false
  id: 64da2509a8829bc784c7b1ce
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Trelis/Llama-2-13b-chat-hf-function-calling
repo_type: model
status: closed
target_branch: null
title: Hosted inference?
