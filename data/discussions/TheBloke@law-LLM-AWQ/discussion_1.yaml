!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Rexe
conflicting_files: null
created_at: 2023-12-26 07:33:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652972036350-62621a516c57f7447808c8ab.jpeg?w=200&h=200&f=face
      fullname: Enyinnaya Edoziem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rexe
      type: user
    createdAt: '2023-12-26T07:33:07.000Z'
    data:
      edited: false
      editors:
      - Rexe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5349395871162415
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652972036350-62621a516c57f7447808c8ab.jpeg?w=200&h=200&f=face
          fullname: Enyinnaya Edoziem
          isHf: false
          isPro: false
          name: Rexe
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> I want a guide\
          \ via AWQ quantization, I\u2019m having issues quantizing AutoAWQForCausalLM\
          \ from my code, and the issues is: </p>\n<p>ValueError: WQLinear_GEMM(in_features=14336,\
          \ out_features=4096, bias=False, w_bit=4, group_size=128) does not have\
          \ a parameter or a buffer named weight.</p>\n<p>This is the code:<br>!pip\
          \ install --upgrade transformers<br>from awq import AutoAWQForCausalLM<br>from\
          \ transformers import AutoTokenizer<br>import torch<br>import safetensors<br>import\
          \ os</p>\n<p>#Rexe/Faradaylab-aria-mistral-merge<br>model_path = 'Faradaylab/ARIA-7B-V3-mistral-french-v1'<br>quant_name\
          \ = model_path.split('/')[-1] + '-AWQ'</p>\n<p>quant_path = 'Rexe/' + quant_name<br>quant_config\
          \ = { 'zero_point': True, 'q_group_size': 128, 'w_bit': 4 }</p>\n<h1 id=\"\
          load-model\">load model</h1>\n<p>model = AutoAWQForCausalLM.from_quantized(model_path,\
          \ device_map='auto', use_safetensors=True, strict=False)<br>tokenizer =\
          \ AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=True)</p>\n\
          <h1 id=\"quantize\">Quantize</h1>\n<p>model.quantize(tokenizer, quant_config=quant_config)</p>\n\
          <h1 id=\"save-quantized-model\">save quantized model</h1>\n<p>model.save_quantized(quant_name,\
          \ safetensors=True, shard_size='10GB')<br>tokenizer.save_pretrained(quant_name)</p>\n"
        raw: "@TheBloke I want a guide via AWQ quantization, I\u2019m having issues\
          \ quantizing AutoAWQForCausalLM from my code, and the issues is: \r\n\r\n\
          ValueError: WQLinear_GEMM(in_features=14336, out_features=4096, bias=False,\
          \ w_bit=4, group_size=128) does not have a parameter or a buffer named weight.\r\
          \n\r\nThis is the code:\r\n!pip install --upgrade transformers\r\nfrom awq\
          \ import AutoAWQForCausalLM\r\nfrom transformers import AutoTokenizer\r\n\
          import torch\r\nimport safetensors\r\nimport os\r\n\r\n#Rexe/Faradaylab-aria-mistral-merge\
          \ \r\nmodel_path = 'Faradaylab/ARIA-7B-V3-mistral-french-v1'\r\nquant_name\
          \ = model_path.split('/')[-1] + '-AWQ'\r\n\r\nquant_path = 'Rexe/' + quant_name\r\
          \nquant_config = { 'zero_point': True, 'q_group_size': 128, 'w_bit': 4 }\r\
          \n# load model\r\n\r\nmodel = AutoAWQForCausalLM.from_quantized(model_path,\
          \ device_map='auto', use_safetensors=True, strict=False)\r\ntokenizer =\
          \ AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=True)\r\
          \n\r\n# Quantize\r\nmodel.quantize(tokenizer, quant_config=quant_config)\r\
          \n\r\n# save quantized model\r\nmodel.save_quantized(quant_name, safetensors=True,\
          \ shard_size='10GB')\r\ntokenizer.save_pretrained(quant_name)"
        updatedAt: '2023-12-26T07:33:07.311Z'
      numEdits: 0
      reactions: []
    id: 658a81b31a576242ef57f1ea
    type: comment
  author: Rexe
  content: "@TheBloke I want a guide via AWQ quantization, I\u2019m having issues\
    \ quantizing AutoAWQForCausalLM from my code, and the issues is: \r\n\r\nValueError:\
    \ WQLinear_GEMM(in_features=14336, out_features=4096, bias=False, w_bit=4, group_size=128)\
    \ does not have a parameter or a buffer named weight.\r\n\r\nThis is the code:\r\
    \n!pip install --upgrade transformers\r\nfrom awq import AutoAWQForCausalLM\r\n\
    from transformers import AutoTokenizer\r\nimport torch\r\nimport safetensors\r\
    \nimport os\r\n\r\n#Rexe/Faradaylab-aria-mistral-merge \r\nmodel_path = 'Faradaylab/ARIA-7B-V3-mistral-french-v1'\r\
    \nquant_name = model_path.split('/')[-1] + '-AWQ'\r\n\r\nquant_path = 'Rexe/'\
    \ + quant_name\r\nquant_config = { 'zero_point': True, 'q_group_size': 128, 'w_bit':\
    \ 4 }\r\n# load model\r\n\r\nmodel = AutoAWQForCausalLM.from_quantized(model_path,\
    \ device_map='auto', use_safetensors=True, strict=False)\r\ntokenizer = AutoTokenizer.from_pretrained(model_path,\
    \ trust_remote_code=True, use_fast=True)\r\n\r\n# Quantize\r\nmodel.quantize(tokenizer,\
    \ quant_config=quant_config)\r\n\r\n# save quantized model\r\nmodel.save_quantized(quant_name,\
    \ safetensors=True, shard_size='10GB')\r\ntokenizer.save_pretrained(quant_name)"
  created_at: 2023-12-26 07:33:07+00:00
  edited: false
  hidden: false
  id: 658a81b31a576242ef57f1ea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/law-LLM-AWQ
repo_type: model
status: open
target_branch: null
title: Error quantizing AWQ?
