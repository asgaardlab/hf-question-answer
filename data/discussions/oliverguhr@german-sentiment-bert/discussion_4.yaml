!!python/object:huggingface_hub.community.DiscussionWithDetails
author: PaulBFB
conflicting_files: null
created_at: 2023-03-22 08:00:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678276103081-noauth.jpeg?w=200&h=200&f=face
      fullname: Paul Leitner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PaulBFB
      type: user
    createdAt: '2023-03-22T09:00:06.000Z'
    data:
      edited: false
      editors:
      - PaulBFB
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678276103081-noauth.jpeg?w=200&h=200&f=face
          fullname: Paul Leitner
          isHf: false
          isPro: false
          name: PaulBFB
          type: user
        html: '<p>first of all thank you for hosting this, it''s really very helpful.
          A quick note about applying this to larger dataframes, so if I pass in a
          larger (10,000 and above) items to <code>SentimentModel.predict_sentiment()</code>
          my process (terminal) usually freezes and then gets terminated by the OS.
          Next I tried <code>pandas.DataFrame.apply(...)</code> to add it row-wise,
          which I ended up closing after 5 hours. </p>

          <p>What ultimately ended up working for me is chunking the dataframe with
          <code>numpy.array_split</code> into chunks and wrapping a generator around
          it to yield the sentiment as chunks and then concatenating the result.</p>

          <p>Is there a better/more elegant way to do this? And thank you again for
          hosting this here.</p>

          '
        raw: "first of all thank you for hosting this, it's really very helpful. A\
          \ quick note about applying this to larger dataframes, so if I pass in a\
          \ larger (10,000 and above) items to `SentimentModel.predict_sentiment()`\
          \ my process (terminal) usually freezes and then gets terminated by the\
          \ OS. Next I tried `pandas.DataFrame.apply(...)` to add it row-wise, which\
          \ I ended up closing after 5 hours. \r\n\r\nWhat ultimately ended up working\
          \ for me is chunking the dataframe with `numpy.array_split` into chunks\
          \ and wrapping a generator around it to yield the sentiment as chunks and\
          \ then concatenating the result.\r\n\r\nIs there a better/more elegant way\
          \ to do this? And thank you again for hosting this here."
        updatedAt: '2023-03-22T09:00:06.492Z'
      numEdits: 0
      reactions: []
    id: 641ac396a63c4e806231df57
    type: comment
  author: PaulBFB
  content: "first of all thank you for hosting this, it's really very helpful. A quick\
    \ note about applying this to larger dataframes, so if I pass in a larger (10,000\
    \ and above) items to `SentimentModel.predict_sentiment()` my process (terminal)\
    \ usually freezes and then gets terminated by the OS. Next I tried `pandas.DataFrame.apply(...)`\
    \ to add it row-wise, which I ended up closing after 5 hours. \r\n\r\nWhat ultimately\
    \ ended up working for me is chunking the dataframe with `numpy.array_split` into\
    \ chunks and wrapping a generator around it to yield the sentiment as chunks and\
    \ then concatenating the result.\r\n\r\nIs there a better/more elegant way to\
    \ do this? And thank you again for hosting this here."
  created_at: 2023-03-22 08:00:06+00:00
  edited: false
  hidden: false
  id: 641ac396a63c4e806231df57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ea841a70d1df220780f0433/rpEmnBmuIImkp2yo_pzfO.jpeg?w=200&h=200&f=face
      fullname: Oliver Guhr
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: oliverguhr
      type: user
    createdAt: '2023-03-22T10:20:07.000Z'
    data:
      edited: false
      editors:
      - oliverguhr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ea841a70d1df220780f0433/rpEmnBmuIImkp2yo_pzfO.jpeg?w=200&h=200&f=face
          fullname: Oliver Guhr
          isHf: false
          isPro: false
          name: oliverguhr
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;PaulBFB&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/PaulBFB\">@<span class=\"\
          underline\">PaulBFB</span></a></span>\n\n\t</span></span>,<br>thanks for\
          \ the feedback! I think its the best approach to chunk the data like you\
          \ did. It would be even better if we include this into the python package,\
          \ in this line:</p>\n<p><a rel=\"nofollow\" href=\"https://github.com/oliverguhr/german-sentiment-lib/blob/master/germansentiment/sentimentmodel.py#L26\"\
          >https://github.com/oliverguhr/german-sentiment-lib/blob/master/germansentiment/sentimentmodel.py#L26</a></p>\n\
          <p>If you like to, feel free to create a pull request with your solution.</p>\n\
          <p>Best<br>Oliver</p>\n"
        raw: 'Hi @PaulBFB,

          thanks for the feedback! I think its the best approach to chunk the data
          like you did. It would be even better if we include this into the python
          package, in this line:


          https://github.com/oliverguhr/german-sentiment-lib/blob/master/germansentiment/sentimentmodel.py#L26


          If you like to, feel free to create a pull request with your solution.


          Best

          Oliver'
        updatedAt: '2023-03-22T10:20:07.204Z'
      numEdits: 0
      reactions: []
    id: 641ad657d42926275d9f5c2e
    type: comment
  author: oliverguhr
  content: 'Hi @PaulBFB,

    thanks for the feedback! I think its the best approach to chunk the data like
    you did. It would be even better if we include this into the python package, in
    this line:


    https://github.com/oliverguhr/german-sentiment-lib/blob/master/germansentiment/sentimentmodel.py#L26


    If you like to, feel free to create a pull request with your solution.


    Best

    Oliver'
  created_at: 2023-03-22 09:20:07+00:00
  edited: false
  hidden: false
  id: 641ad657d42926275d9f5c2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7c4276edd2ee16c67da124e93cb1e1e.svg
      fullname: Leo Rettich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oberbus
      type: user
    createdAt: '2023-03-26T23:04:59.000Z'
    data:
      edited: false
      editors:
      - oberbus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7c4276edd2ee16c67da124e93cb1e1e.svg
          fullname: Leo Rettich
          isHf: false
          isPro: false
          name: oberbus
          type: user
        html: '<p>Is there some experience on the speed and the best configuration
          (for example size of used array chunks) of the model when working with large
          datasets?<br>I am planning to use the model to predict the sentiment for
          over 500''000 text elements and I am bit afraid of the expected run time.</p>

          '
        raw: 'Is there some experience on the speed and the best configuration (for
          example size of used array chunks) of the model when working with large
          datasets?

          I am planning to use the model to predict the sentiment for over 500''000
          text elements and I am bit afraid of the expected run time.'
        updatedAt: '2023-03-26T23:04:59.957Z'
      numEdits: 0
      reactions: []
    id: 6420cf9bd110590aa6951cd2
    type: comment
  author: oberbus
  content: 'Is there some experience on the speed and the best configuration (for
    example size of used array chunks) of the model when working with large datasets?

    I am planning to use the model to predict the sentiment for over 500''000 text
    elements and I am bit afraid of the expected run time.'
  created_at: 2023-03-26 22:04:59+00:00
  edited: false
  hidden: false
  id: 6420cf9bd110590aa6951cd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ea841a70d1df220780f0433/rpEmnBmuIImkp2yo_pzfO.jpeg?w=200&h=200&f=face
      fullname: Oliver Guhr
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: oliverguhr
      type: user
    createdAt: '2023-03-27T13:09:39.000Z'
    data:
      edited: true
      editors:
      - oliverguhr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ea841a70d1df220780f0433/rpEmnBmuIImkp2yo_pzfO.jpeg?w=200&h=200&f=face
          fullname: Oliver Guhr
          isHf: false
          isPro: false
          name: oliverguhr
          type: user
        html: '<p>The batch size largely depends on the CPU / GPU you have at hand.
          The other factor is the length of the documents you want to classify.</p>

          <p>I made a little colab notebook to test this:<br><a rel="nofollow" href="https://colab.research.google.com/drive/1ecUTh_TmEOjdIK6-rFqamifd9Jzk7uDA?usp=sharing">https://colab.research.google.com/drive/1ecUTh_TmEOjdIK6-rFqamifd9Jzk7uDA?usp=sharing</a></p>

          <p><strong>500000 documents in batches of 5 take about 10h on the CPU and
          20 minutes on the GPU.<br>If you increase the batch size to 50 it takes
          4 minutes to process the dataset.</strong></p>

          <p>So you can process you dataset for free with colab.</p>

          '
        raw: "The batch size largely depends on the CPU / GPU you have at hand. The\
          \ other factor is the length of the documents you want to classify.\n\n\
          I made a little colab notebook to test this:\nhttps://colab.research.google.com/drive/1ecUTh_TmEOjdIK6-rFqamifd9Jzk7uDA?usp=sharing\n\
          \n**500000 documents in batches of 5 take about 10h on the CPU and 20 minutes\
          \ on the GPU. \nIf you increase the batch size to 50 it takes 4 minutes\
          \ to process the dataset.**\n\nSo you can process you dataset for free with\
          \ colab."
        updatedAt: '2023-03-27T13:10:19.349Z'
      numEdits: 1
      reactions: []
    id: 642195935acad90e6b6dbd22
    type: comment
  author: oliverguhr
  content: "The batch size largely depends on the CPU / GPU you have at hand. The\
    \ other factor is the length of the documents you want to classify.\n\nI made\
    \ a little colab notebook to test this:\nhttps://colab.research.google.com/drive/1ecUTh_TmEOjdIK6-rFqamifd9Jzk7uDA?usp=sharing\n\
    \n**500000 documents in batches of 5 take about 10h on the CPU and 20 minutes\
    \ on the GPU. \nIf you increase the batch size to 50 it takes 4 minutes to process\
    \ the dataset.**\n\nSo you can process you dataset for free with colab."
  created_at: 2023-03-27 12:09:39+00:00
  edited: true
  hidden: false
  id: 642195935acad90e6b6dbd22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7c4276edd2ee16c67da124e93cb1e1e.svg
      fullname: Leo Rettich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oberbus
      type: user
    createdAt: '2023-03-27T13:52:12.000Z'
    data:
      edited: false
      editors:
      - oberbus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7c4276edd2ee16c67da124e93cb1e1e.svg
          fullname: Leo Rettich
          isHf: false
          isPro: false
          name: oberbus
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;oliverguhr&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/oliverguhr\"\
          >@<span class=\"underline\">oliverguhr</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<p>Thank you very much for what you are doing: Providing this model\
          \ AND being so responsive and helpful here!</p>\n<p>Your colab is golden\
          \ for testing the performance and choosing the optimal batch size. I inserted\
          \ texts that are realistic to my use case (avg. of 90 words per text) and\
          \ tested different batch sizes.<br>Interestingly, batch size 50 seems to\
          \ be a pretty good choice, even though there would be enough GPU memory\
          \ for batch size 100. 50 is around 10 minutes faster than 100. </p>\n<p>Anyway,\
          \ I can see that my dataset is no problem. Thanks again!</p>\n"
        raw: "Hi @oliverguhr ,\n\nThank you very much for what you are doing: Providing\
          \ this model AND being so responsive and helpful here!\n\nYour colab is\
          \ golden for testing the performance and choosing the optimal batch size.\
          \ I inserted texts that are realistic to my use case (avg. of 90 words per\
          \ text) and tested different batch sizes. \nInterestingly, batch size 50\
          \ seems to be a pretty good choice, even though there would be enough GPU\
          \ memory for batch size 100. 50 is around 10 minutes faster than 100. \n\
          \nAnyway, I can see that my dataset is no problem. Thanks again!"
        updatedAt: '2023-03-27T13:52:12.377Z'
      numEdits: 0
      reactions: []
    id: 64219f8c3f204ac24e172125
    type: comment
  author: oberbus
  content: "Hi @oliverguhr ,\n\nThank you very much for what you are doing: Providing\
    \ this model AND being so responsive and helpful here!\n\nYour colab is golden\
    \ for testing the performance and choosing the optimal batch size. I inserted\
    \ texts that are realistic to my use case (avg. of 90 words per text) and tested\
    \ different batch sizes. \nInterestingly, batch size 50 seems to be a pretty good\
    \ choice, even though there would be enough GPU memory for batch size 100. 50\
    \ is around 10 minutes faster than 100. \n\nAnyway, I can see that my dataset\
    \ is no problem. Thanks again!"
  created_at: 2023-03-27 12:52:12+00:00
  edited: false
  hidden: false
  id: 64219f8c3f204ac24e172125
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ea841a70d1df220780f0433/rpEmnBmuIImkp2yo_pzfO.jpeg?w=200&h=200&f=face
      fullname: Oliver Guhr
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: oliverguhr
      type: user
    createdAt: '2023-03-27T15:11:59.000Z'
    data:
      edited: false
      editors:
      - oliverguhr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ea841a70d1df220780f0433/rpEmnBmuIImkp2yo_pzfO.jpeg?w=200&h=200&f=face
          fullname: Oliver Guhr
          isHf: false
          isPro: false
          name: oliverguhr
          type: user
        html: "<p>Thank you for your feedback <span data-props=\"{&quot;user&quot;:&quot;oberbus&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/oberbus\"\
          >@<span class=\"underline\">oberbus</span></a></span>\n\n\t</span></span>\
          \ - sometimes a nice word brightens the day.</p>\n"
        raw: Thank you for your feedback @oberbus - sometimes a nice word brightens
          the day.
        updatedAt: '2023-03-27T15:11:59.375Z'
      numEdits: 0
      reactions: []
    id: 6421b23f0c65c54470e3b4ac
    type: comment
  author: oliverguhr
  content: Thank you for your feedback @oberbus - sometimes a nice word brightens
    the day.
  created_at: 2023-03-27 14:11:59+00:00
  edited: false
  hidden: false
  id: 6421b23f0c65c54470e3b4ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ea841a70d1df220780f0433/rpEmnBmuIImkp2yo_pzfO.jpeg?w=200&h=200&f=face
      fullname: Oliver Guhr
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: oliverguhr
      type: user
    createdAt: '2023-03-27T15:12:01.000Z'
    data:
      status: closed
    id: 6421b2415d5ee561d87eb680
    type: status-change
  author: oliverguhr
  created_at: 2023-03-27 14:12:01+00:00
  id: 6421b2415d5ee561d87eb680
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ea841a70d1df220780f0433/rpEmnBmuIImkp2yo_pzfO.jpeg?w=200&h=200&f=face
      fullname: Oliver Guhr
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: oliverguhr
      type: user
    createdAt: '2023-03-27T15:12:23.000Z'
    data:
      edited: false
      editors:
      - oliverguhr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ea841a70d1df220780f0433/rpEmnBmuIImkp2yo_pzfO.jpeg?w=200&h=200&f=face
          fullname: Oliver Guhr
          isHf: false
          isPro: false
          name: oliverguhr
          type: user
        html: '<p>PS: Feel free to reopen in case something is unclear.</p>

          '
        raw: 'PS: Feel free to reopen in case something is unclear.'
        updatedAt: '2023-03-27T15:12:23.260Z'
      numEdits: 0
      reactions: []
    id: 6421b25794fb039f536b29b5
    type: comment
  author: oliverguhr
  content: 'PS: Feel free to reopen in case something is unclear.'
  created_at: 2023-03-27 14:12:23+00:00
  edited: false
  hidden: false
  id: 6421b25794fb039f536b29b5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: oliverguhr/german-sentiment-bert
repo_type: model
status: closed
target_branch: null
title: Thank you for sharing! Question/Note about larger dataframes...
