!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Humeee33
conflicting_files: null
created_at: 2023-12-28 15:34:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8724a0d6ad27a956a25b3ac97325cf1.svg
      fullname: Joesph
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Humeee33
      type: user
    createdAt: '2023-12-28T15:34:35.000Z'
    data:
      edited: true
      editors:
      - Humeee33
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9610149264335632
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8724a0d6ad27a956a25b3ac97325cf1.svg
          fullname: Joesph
          isHf: false
          isPro: false
          name: Humeee33
          type: user
        html: '<p>Is it that no one reads these help request messages or no one cares,
          no one knows, or a combination?</p>

          '
        raw: Is it that no one reads these help request messages or no one cares,
          no one knows, or a combination?
        updatedAt: '2023-12-31T15:15:31.635Z'
      numEdits: 1
      reactions: []
    id: 658d958bfde57fb4e65e0c84
    type: comment
  author: Humeee33
  content: Is it that no one reads these help request messages or no one cares, no
    one knows, or a combination?
  created_at: 2023-12-28 15:34:35+00:00
  edited: true
  hidden: false
  id: 658d958bfde57fb4e65e0c84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2024-01-05T17:12:55.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8459035754203796
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;humeee33&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/humeee33\">@<span class=\"\
          underline\">humeee33</span></a></span>\n\n\t</span></span> well I wouldnt\
          \ recommend using the gptq one since you dont have enough vram for a 70b\
          \ model. Your best bet is to use llama.cpp(or llama cpp python) and download\
          \ the gguf version instead of gptq. </p>\n<p>You might be able to run it\
          \ with transformers but it will be extremely slow while llama.cpp will be\
          \ much faster</p>\n"
        raw: "@humeee33 well I wouldnt recommend using the gptq one since you dont\
          \ have enough vram for a 70b model. Your best bet is to use llama.cpp(or\
          \ llama cpp python) and download the gguf version instead of gptq. \n\n\
          You might be able to run it with transformers but it will be extremely slow\
          \ while llama.cpp will be much faster"
        updatedAt: '2024-01-05T17:12:55.991Z'
      numEdits: 0
      reactions: []
    id: 6598389736d618ee6be9df2f
    type: comment
  author: YaTharThShaRma999
  content: "@humeee33 well I wouldnt recommend using the gptq one since you dont have\
    \ enough vram for a 70b model. Your best bet is to use llama.cpp(or llama cpp\
    \ python) and download the gguf version instead of gptq. \n\nYou might be able\
    \ to run it with transformers but it will be extremely slow while llama.cpp will\
    \ be much faster"
  created_at: 2024-01-05 17:12:55+00:00
  edited: false
  hidden: false
  id: 6598389736d618ee6be9df2f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Aurora-Nights-70B-v1.0-GPTQ
repo_type: model
status: open
target_branch: null
title: Hello, can I run this model if I only have a 3090 with 24gVram  and 32gRam
  ?
