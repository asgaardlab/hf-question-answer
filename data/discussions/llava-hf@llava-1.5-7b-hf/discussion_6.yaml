!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gullal1491
conflicting_files: null
created_at: 2023-12-11 15:03:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
      fullname: Gullal Sing Cheema
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gullal1491
      type: user
    createdAt: '2023-12-11T15:03:00.000Z'
    data:
      edited: false
      editors:
      - gullal1491
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.588709831237793
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
          fullname: Gullal Sing Cheema
          isHf: false
          isPro: false
          name: gullal1491
          type: user
        html: '<p>python3.8/site-packages/transformers/models/llava/modeling_llava.py",
          line 428, in forward<br>    extended_attention_mask[batch_index, non_attended_tokens]
          = 0<br>RuntimeError: CUDA error: device-side assert triggered<br>Compile
          with <code>TORCH_USE_CUDA_DSA</code> to enable device-side assertions.</p>

          <p>Error reproduced with CUDA_LAUNCH_BLOCKING=1.</p>

          <p>Is this specific to new llava model on huggingface?</p>

          '
        raw: "python3.8/site-packages/transformers/models/llava/modeling_llava.py\"\
          , line 428, in forward\r\n    extended_attention_mask[batch_index, non_attended_tokens]\
          \ = 0\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCompile\
          \ with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\nError\
          \ reproduced with CUDA_LAUNCH_BLOCKING=1.\r\n\r\nIs this specific to new\
          \ llava model on huggingface?"
        updatedAt: '2023-12-11T15:03:00.318Z'
      numEdits: 0
      reactions: []
    id: 657724a45c5a6c93161809b6
    type: comment
  author: gullal1491
  content: "python3.8/site-packages/transformers/models/llava/modeling_llava.py\"\
    , line 428, in forward\r\n    extended_attention_mask[batch_index, non_attended_tokens]\
    \ = 0\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCompile with\
    \ `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\nError reproduced\
    \ with CUDA_LAUNCH_BLOCKING=1.\r\n\r\nIs this specific to new llava model on huggingface?"
  created_at: 2023-12-11 15:03:00+00:00
  edited: false
  hidden: false
  id: 657724a45c5a6c93161809b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-11T15:12:36.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5143142938613892
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;gullal1491&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gullal1491\"\
          >@<span class=\"underline\">gullal1491</span></a></span>\n\n\t</span></span><br>Can\
          \ you share a reproducible snippet?</p>\n"
        raw: "Hi @gullal1491 \nCan you share a reproducible snippet?"
        updatedAt: '2023-12-11T15:12:36.224Z'
      numEdits: 0
      reactions: []
    id: 657726e40370e52e3b37569c
    type: comment
  author: ybelkada
  content: "Hi @gullal1491 \nCan you share a reproducible snippet?"
  created_at: 2023-12-11 15:12:36+00:00
  edited: false
  hidden: false
  id: 657726e40370e52e3b37569c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
      fullname: Gullal Sing Cheema
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gullal1491
      type: user
    createdAt: '2023-12-11T15:19:04.000Z'
    data:
      edited: true
      editors:
      - gullal1491
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5157953500747681
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
          fullname: Gullal Sing Cheema
          isHf: false
          isPro: false
          name: gullal1491
          type: user
        html: "<p>Model is loaded as:</p>\n<pre><code>## Load model\nmodel = LlavaForConditionalGeneration.from_pretrained(\n\
          \        args.model_name, \n        torch_dtype=torch.float16, \n      \
          \  low_cpu_mem_usage=True,\n        use_flash_attention_2=True\n    ).to(device)\n\
          \nprocessor = AutoProcessor.from_pretrained(args.model_name)\n\nprompt =\
          \ \"USER: &lt;image&gt;\\n%s\\nASSISTANT:\"%(config[\"prompts\"][args.prompt])\n\
          </code></pre>\n<p>Images are processed in a batch:</p>\n<pre><code>image_descriptions\
          \ = {}\n\nfor i, batch in enumerate(loader):\n        print(i+1)\n\n   \
          \     image_ids, prompts, image_ids = batch\n\n        images = [Image.open(img_path).convert(\"\
          RGB\") for img_path in image_ids]\n\n        inputs = processor(prompts,\
          \ images, return_tensors='pt').to(device, torch.float16)\n\n        outputs\
          \ = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n\n  \
          \      generated_text = processor.batch_decode(outputs, skip_special_tokens=True)\n\
          \n        for img_id, text in zip(image_ids, generated_text):\n        \
          \    image_descriptions[img_id] = text.split(\"ASSISTANT:\")[-1]\n</code></pre>\n\
          <p>It works perfectly for one prompt over a set of 1000 images. But with\
          \ a different prompt it fails for the same batch of images.</p>\n<p>Prompts\
          \ used:<br>one: \"Write a short description for the image.\"<br>two: \"\
          Write a detailed description for the image.\"<br>three: \"Write a descriptive\
          \ caption for the image by focusing on entities and relations present in\
          \ it.\"</p>\n<p>It gives the same error for prompt two and three. Somehow\
          \ it has to do with length of text produced by the model during generation.</p>\n"
        raw: "Model is loaded as:\n```\n## Load model\nmodel = LlavaForConditionalGeneration.from_pretrained(\n\
          \        args.model_name, \n        torch_dtype=torch.float16, \n      \
          \  low_cpu_mem_usage=True,\n        use_flash_attention_2=True\n    ).to(device)\n\
          \nprocessor = AutoProcessor.from_pretrained(args.model_name)\n\nprompt =\
          \ \"USER: <image>\\n%s\\nASSISTANT:\"%(config[\"prompts\"][args.prompt])\n\
          ```\n\nImages are processed in a batch:\n```\nimage_descriptions = {}\n\n\
          for i, batch in enumerate(loader):\n        print(i+1)\n\n        image_ids,\
          \ prompts, image_ids = batch\n\n        images = [Image.open(img_path).convert(\"\
          RGB\") for img_path in image_ids]\n\n        inputs = processor(prompts,\
          \ images, return_tensors='pt').to(device, torch.float16)\n\n        outputs\
          \ = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n\n  \
          \      generated_text = processor.batch_decode(outputs, skip_special_tokens=True)\n\
          \n        for img_id, text in zip(image_ids, generated_text):\n        \
          \    image_descriptions[img_id] = text.split(\"ASSISTANT:\")[-1]\n```\n\n\
          It works perfectly for one prompt over a set of 1000 images. But with a\
          \ different prompt it fails for the same batch of images.\n\nPrompts used:\n\
          one: \"Write a short description for the image.\"\ntwo: \"Write a detailed\
          \ description for the image.\"\nthree: \"Write a descriptive caption for\
          \ the image by focusing on entities and relations present in it.\"\n\nIt\
          \ gives the same error for prompt two and three. Somehow it has to do with\
          \ length of text produced by the model during generation."
        updatedAt: '2023-12-11T15:23:27.723Z'
      numEdits: 2
      reactions: []
    id: 6577286817b474ae6b4f015c
    type: comment
  author: gullal1491
  content: "Model is loaded as:\n```\n## Load model\nmodel = LlavaForConditionalGeneration.from_pretrained(\n\
    \        args.model_name, \n        torch_dtype=torch.float16, \n        low_cpu_mem_usage=True,\n\
    \        use_flash_attention_2=True\n    ).to(device)\n\nprocessor = AutoProcessor.from_pretrained(args.model_name)\n\
    \nprompt = \"USER: <image>\\n%s\\nASSISTANT:\"%(config[\"prompts\"][args.prompt])\n\
    ```\n\nImages are processed in a batch:\n```\nimage_descriptions = {}\n\nfor i,\
    \ batch in enumerate(loader):\n        print(i+1)\n\n        image_ids, prompts,\
    \ image_ids = batch\n\n        images = [Image.open(img_path).convert(\"RGB\"\
    ) for img_path in image_ids]\n\n        inputs = processor(prompts, images, return_tensors='pt').to(device,\
    \ torch.float16)\n\n        outputs = model.generate(**inputs, max_new_tokens=100,\
    \ do_sample=False)\n\n        generated_text = processor.batch_decode(outputs,\
    \ skip_special_tokens=True)\n\n        for img_id, text in zip(image_ids, generated_text):\n\
    \            image_descriptions[img_id] = text.split(\"ASSISTANT:\")[-1]\n```\n\
    \nIt works perfectly for one prompt over a set of 1000 images. But with a different\
    \ prompt it fails for the same batch of images.\n\nPrompts used:\none: \"Write\
    \ a short description for the image.\"\ntwo: \"Write a detailed description for\
    \ the image.\"\nthree: \"Write a descriptive caption for the image by focusing\
    \ on entities and relations present in it.\"\n\nIt gives the same error for prompt\
    \ two and three. Somehow it has to do with length of text produced by the model\
    \ during generation."
  created_at: 2023-12-11 15:19:04+00:00
  edited: true
  hidden: false
  id: 6577286817b474ae6b4f015c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-11T15:50:31.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9353660941123962
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gullal1491&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gullal1491\">@<span class=\"\
          underline\">gullal1491</span></a></span>\n\n\t</span></span><br>Thanks!\
          \ Can you try to run the problematic generation on CPU instead of GPU? Then\
          \ the issue would be clearer - meanwhile I will try to reproduce based on\
          \ your script</p>\n"
        raw: "@gullal1491 \nThanks! Can you try to run the problematic generation\
          \ on CPU instead of GPU? Then the issue would be clearer - meanwhile I will\
          \ try to reproduce based on your script"
        updatedAt: '2023-12-11T15:50:31.104Z'
      numEdits: 0
      reactions: []
    id: 65772fc7c793e5b72b13b447
    type: comment
  author: ybelkada
  content: "@gullal1491 \nThanks! Can you try to run the problematic generation on\
    \ CPU instead of GPU? Then the issue would be clearer - meanwhile I will try to\
    \ reproduce based on your script"
  created_at: 2023-12-11 15:50:31+00:00
  edited: false
  hidden: false
  id: 65772fc7c793e5b72b13b447
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
      fullname: Gullal Sing Cheema
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gullal1491
      type: user
    createdAt: '2023-12-11T16:05:25.000Z'
    data:
      edited: true
      editors:
      - gullal1491
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7569949626922607
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
          fullname: Gullal Sing Cheema
          isHf: false
          isPro: false
          name: gullal1491
          type: user
        html: "<p>Interesting that there is no error when running on CPU.</p>\n<h2\
          \ id=\"load-model\">Load model</h2>\n<pre><code>model = LlavaForConditionalGeneration.from_pretrained(\n\
          \    args.model_name,\n    low_cpu_mem_usage=True\n).to(device)\n</code></pre>\n\
          <p>Could this be because of other libraries like accelerate or running the\
          \ model with half precision on GPU?<br>Any help is appreciated in getting\
          \ close to fixing this issue.</p>\n"
        raw: "Interesting that there is no error when running on CPU.\n## Load model\n\
          \    model = LlavaForConditionalGeneration.from_pretrained(\n        args.model_name,\n\
          \        low_cpu_mem_usage=True\n    ).to(device)\n\n\nCould this be because\
          \ of other libraries like accelerate or running the model with half precision\
          \ on GPU?\nAny help is appreciated in getting close to fixing this issue."
        updatedAt: '2023-12-11T16:07:36.116Z'
      numEdits: 1
      reactions: []
    id: 6577334517b474ae6b513c6a
    type: comment
  author: gullal1491
  content: "Interesting that there is no error when running on CPU.\n## Load model\n\
    \    model = LlavaForConditionalGeneration.from_pretrained(\n        args.model_name,\n\
    \        low_cpu_mem_usage=True\n    ).to(device)\n\n\nCould this be because of\
    \ other libraries like accelerate or running the model with half precision on\
    \ GPU?\nAny help is appreciated in getting close to fixing this issue."
  created_at: 2023-12-11 16:05:25+00:00
  edited: true
  hidden: false
  id: 6577334517b474ae6b513c6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
      fullname: Gullal Sing Cheema
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gullal1491
      type: user
    createdAt: '2023-12-11T20:17:23.000Z'
    data:
      edited: true
      editors:
      - gullal1491
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9607329368591309
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
          fullname: Gullal Sing Cheema
          isHf: false
          isPro: false
          name: gullal1491
          type: user
        html: '<p>Could batched inference be an issue? I did try to run with batch
          size 1 and still encountered the same error.</p>

          <p>LLava 1.5 batched inference issue: <a rel="nofollow" href="https://github.com/haotian-liu/LLaVA/issues/709">https://github.com/haotian-liu/LLaVA/issues/709</a></p>

          '
        raw: 'Could batched inference be an issue? I did try to run with batch size
          1 and still encountered the same error.


          LLava 1.5 batched inference issue: https://github.com/haotian-liu/LLaVA/issues/709'
        updatedAt: '2023-12-12T13:47:24.171Z'
      numEdits: 1
      reactions: []
    id: 65776e537685e1ce5cf64f62
    type: comment
  author: gullal1491
  content: 'Could batched inference be an issue? I did try to run with batch size
    1 and still encountered the same error.


    LLava 1.5 batched inference issue: https://github.com/haotian-liu/LLaVA/issues/709'
  created_at: 2023-12-11 20:17:23+00:00
  edited: true
  hidden: false
  id: 65776e537685e1ce5cf64f62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-12-12T13:51:39.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8012620806694031
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>We explicitly test batched generation here: <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/main/tests/models/llava/test_modeling_llava.py#L245">https://github.com/huggingface/transformers/blob/main/tests/models/llava/test_modeling_llava.py#L245</a>,
          so it should work out of the box. Could you try running the code snippet
          present there?</p>

          '
        raw: 'We explicitly test batched generation here: https://github.com/huggingface/transformers/blob/main/tests/models/llava/test_modeling_llava.py#L245,
          so it should work out of the box. Could you try running the code snippet
          present there?'
        updatedAt: '2023-12-12T13:51:39.658Z'
      numEdits: 0
      reactions: []
    id: 6578656b0766f274f3d59e4a
    type: comment
  author: nielsr
  content: 'We explicitly test batched generation here: https://github.com/huggingface/transformers/blob/main/tests/models/llava/test_modeling_llava.py#L245,
    so it should work out of the box. Could you try running the code snippet present
    there?'
  created_at: 2023-12-12 13:51:39+00:00
  edited: false
  hidden: false
  id: 6578656b0766f274f3d59e4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
      fullname: Gullal Sing Cheema
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gullal1491
      type: user
    createdAt: '2023-12-12T15:47:22.000Z'
    data:
      edited: true
      editors:
      - gullal1491
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8485392332077026
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
          fullname: Gullal Sing Cheema
          isHf: false
          isPro: false
          name: gullal1491
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;nielsr&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nielsr\"\
          >@<span class=\"underline\">nielsr</span></a></span>\n\n\t</span></span>\
          \ for the response.</p>\n<p>I did try following it line by line, but unfortunately\
          \ for a specific sample or two it fails. I am trying to debug based on the\
          \ error trace and could identify this out of bounds error in modelling_llava.py.</p>\n\
          <p>On this line where error occurs:<br><code>extended_attention_mask[batch_index,\
          \ non_attended_tokens] = 0</code></p>\n<p>The error is:</p>\n<pre><code>`/opt/conda/conda-bld/pytorch_1695392020195/work/aten/src/ATen/native/cuda/IndexKernel.cu:92:\
          \ operator(): block: [0,0,0], thread: [3,0,0] Assertion `-sizes[i] &lt;=\
          \ index &amp;&amp; index &lt; sizes[i] &amp;&amp; \"index out of bounds\"\
          ` failed.`\n</code></pre>\n<p>As it is an indexing error, there is a value\
          \ in <code>non_attended_tokens</code> which is greater than the length of\
          \ <code>extended_attention_mask</code>.<br>In this particular batch, where\
          \ it fails:<br><code>non_attended_tokens</code> looks like: tensor([ 84,\
          \ 128, 216, <strong>612</strong>, 549, 571, 571, 238, 505,  81]<br>where\
          \ as shape of <code>extended_attention_mask</code> is torch.Size([128, 575])</p>\n\
          <p>I don't know why exactly this is happening.</p>\n"
        raw: 'Thanks @nielsr for the response.


          I did try following it line by line, but unfortunately for a specific sample
          or two it fails. I am trying to debug based on the error trace and could
          identify this out of bounds error in modelling_llava.py.


          On this line where error occurs:

          `extended_attention_mask[batch_index, non_attended_tokens] = 0`


          The error is:

          ```

          `/opt/conda/conda-bld/pytorch_1695392020195/work/aten/src/ATen/native/cuda/IndexKernel.cu:92:
          operator(): block: [0,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index
          && index < sizes[i] && "index out of bounds"` failed.`

          ```


          As it is an indexing error, there is a value in `non_attended_tokens` which
          is greater than the length of `extended_attention_mask`.

          In this particular batch, where it fails:

          `non_attended_tokens` looks like: tensor([ 84, 128, 216, **612**, 549, 571,
          571, 238, 505,  81]

          where as shape of `extended_attention_mask` is torch.Size([128, 575])


          I don''t know why exactly this is happening.'
        updatedAt: '2023-12-12T15:48:48.815Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ybelkada
    id: 6578808aee40482b5c457543
    type: comment
  author: gullal1491
  content: 'Thanks @nielsr for the response.


    I did try following it line by line, but unfortunately for a specific sample or
    two it fails. I am trying to debug based on the error trace and could identify
    this out of bounds error in modelling_llava.py.


    On this line where error occurs:

    `extended_attention_mask[batch_index, non_attended_tokens] = 0`


    The error is:

    ```

    `/opt/conda/conda-bld/pytorch_1695392020195/work/aten/src/ATen/native/cuda/IndexKernel.cu:92:
    operator(): block: [0,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index
    < sizes[i] && "index out of bounds"` failed.`

    ```


    As it is an indexing error, there is a value in `non_attended_tokens` which is
    greater than the length of `extended_attention_mask`.

    In this particular batch, where it fails:

    `non_attended_tokens` looks like: tensor([ 84, 128, 216, **612**, 549, 571, 571,
    238, 505,  81]

    where as shape of `extended_attention_mask` is torch.Size([128, 575])


    I don''t know why exactly this is happening.'
  created_at: 2023-12-12 15:47:22+00:00
  edited: true
  hidden: false
  id: 6578808aee40482b5c457543
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-12T16:22:21.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8613559603691101
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>hmmm it might be a bug then, can you help us reproducing this bug
          by sending us here the problematic images and prompts?</p>

          '
        raw: hmmm it might be a bug then, can you help us reproducing this bug by
          sending us here the problematic images and prompts?
        updatedAt: '2023-12-12T16:22:21.680Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - ArthurZ
        - gullal1491
    id: 657888bd528e89e35f51449e
    type: comment
  author: ybelkada
  content: hmmm it might be a bug then, can you help us reproducing this bug by sending
    us here the problematic images and prompts?
  created_at: 2023-12-12 16:22:21+00:00
  edited: false
  hidden: false
  id: 657888bd528e89e35f51449e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
      fullname: Gullal Sing Cheema
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gullal1491
      type: user
    createdAt: '2023-12-12T17:25:17.000Z'
    data:
      edited: true
      editors:
      - gullal1491
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9570221304893494
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
          fullname: Gullal Sing Cheema
          isHf: false
          isPro: false
          name: gullal1491
          type: user
        html: "<p>Sure <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>.\
          \ Where should I send them? I cannot share the images via public links.\
          \ But I can share them privately.</p>\n<p>Another thing I noticed with token\
          \ ids is this:</p>\n<pre><code>print(model.config.text_config.pad_token_id)\
          \  -&gt; None\nprint(processor.tokenizer.pad_token_id)  -&gt; 32001\n</code></pre>\n\
          <p>Is this intended? Although making text_config pad_token_id the same as\
          \ tokenizer (as suggested in some discussions on instructBLIP batched inference\
          \ errors), does not resolve the above error.</p>\n"
        raw: 'Sure @ybelkada. Where should I send them? I cannot share the images
          via public links. But I can share them privately.


          Another thing I noticed with token ids is this:

          ```

          print(model.config.text_config.pad_token_id)  -> None

          print(processor.tokenizer.pad_token_id)  -> 32001

          ```


          Is this intended? Although making text_config pad_token_id the same as tokenizer
          (as suggested in some discussions on instructBLIP batched inference errors),
          does not resolve the above error.'
        updatedAt: '2023-12-13T12:51:03.748Z'
      numEdits: 3
      reactions: []
    id: 6578977de7880298b0da7b6f
    type: comment
  author: gullal1491
  content: 'Sure @ybelkada. Where should I send them? I cannot share the images via
    public links. But I can share them privately.


    Another thing I noticed with token ids is this:

    ```

    print(model.config.text_config.pad_token_id)  -> None

    print(processor.tokenizer.pad_token_id)  -> 32001

    ```


    Is this intended? Although making text_config pad_token_id the same as tokenizer
    (as suggested in some discussions on instructBLIP batched inference errors), does
    not resolve the above error.'
  created_at: 2023-12-12 17:25:17+00:00
  edited: true
  hidden: false
  id: 6578977de7880298b0da7b6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
      fullname: Gullal Sing Cheema
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gullal1491
      type: user
    createdAt: '2023-12-13T22:05:29.000Z'
    data:
      edited: true
      editors:
      - gullal1491
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8817843198776245
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6399bc74f066a25af5413696cbfeb54.svg
          fullname: Gullal Sing Cheema
          isHf: false
          isPro: false
          name: gullal1491
          type: user
        html: '<p>I avoided this out of bounds error for now by ensuring indices are
          within bounds. It is not changing any result in generation.<br>Would dig
          deeper later, as to why this index was in the first place in <code>non_attended_tokens</code>.</p>

          <p>For now, I am dealing with this by avoiding that index:</p>

          <pre><code># Ensuring indices are within bounds

          valid_indices = non_attended_tokens &lt; extended_attention_mask.shape[1]

          new_batch_index = batch_index[valid_indices]

          new_non_attended_tokens = non_attended_tokens[valid_indices]

          </code></pre>

          '
        raw: 'I avoided this out of bounds error for now by ensuring indices are within
          bounds. It is not changing any result in generation.

          Would dig deeper later, as to why this index was in the first place in `non_attended_tokens`.


          For now, I am dealing with this by avoiding that index:


          ```

          # Ensuring indices are within bounds

          valid_indices = non_attended_tokens < extended_attention_mask.shape[1]

          new_batch_index = batch_index[valid_indices]

          new_non_attended_tokens = non_attended_tokens[valid_indices]

          ```'
        updatedAt: '2023-12-13T22:06:13.641Z'
      numEdits: 1
      reactions: []
    id: 657a2aa96cd623f45c3c499f
    type: comment
  author: gullal1491
  content: 'I avoided this out of bounds error for now by ensuring indices are within
    bounds. It is not changing any result in generation.

    Would dig deeper later, as to why this index was in the first place in `non_attended_tokens`.


    For now, I am dealing with this by avoiding that index:


    ```

    # Ensuring indices are within bounds

    valid_indices = non_attended_tokens < extended_attention_mask.shape[1]

    new_batch_index = batch_index[valid_indices]

    new_non_attended_tokens = non_attended_tokens[valid_indices]

    ```'
  created_at: 2023-12-13 22:05:29+00:00
  edited: true
  hidden: false
  id: 657a2aa96cd623f45c3c499f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-14T10:14:13.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9746200442314148
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;gullal1491&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gullal1491\"\
          >@<span class=\"underline\">gullal1491</span></a></span>\n\n\t</span></span><br>This\
          \ seems like a valid fix and we've seen similar issues on BakLlava as well,\
          \ would you mind opening a PR to introduce this fix?</p>\n"
        raw: "Hi @gullal1491 \nThis seems like a valid fix and we've seen similar\
          \ issues on BakLlava as well, would you mind opening a PR to introduce this\
          \ fix?"
        updatedAt: '2023-12-14T10:14:13.324Z'
      numEdits: 0
      reactions: []
    id: 657ad575acd6d72f028f2784
    type: comment
  author: ybelkada
  content: "Hi @gullal1491 \nThis seems like a valid fix and we've seen similar issues\
    \ on BakLlava as well, would you mind opening a PR to introduce this fix?"
  created_at: 2023-12-14 10:14:13+00:00
  edited: false
  hidden: false
  id: 657ad575acd6d72f028f2784
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-14T10:28:11.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9499286413192749
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;gullal1491&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gullal1491\"\
          >@<span class=\"underline\">gullal1491</span></a></span>\n\n\t</span></span><br>Thanks\
          \ again for the investigation and the fix, as this might affect other usres\
          \ in the future, I quickly made <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/pull/28032\"\
          >https://github.com/huggingface/transformers/pull/28032</a> and make sure\
          \ to add you as a co-author of the fix! Thanks again for all your help</p>\n"
        raw: "Hi @gullal1491 \nThanks again for the investigation and the fix, as\
          \ this might affect other usres in the future, I quickly made https://github.com/huggingface/transformers/pull/28032\
          \ and make sure to add you as a co-author of the fix! Thanks again for all\
          \ your help"
        updatedAt: '2023-12-14T10:28:11.436Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - gullal1491
    id: 657ad8bb54021149b4527ced
    type: comment
  author: ybelkada
  content: "Hi @gullal1491 \nThanks again for the investigation and the fix, as this\
    \ might affect other usres in the future, I quickly made https://github.com/huggingface/transformers/pull/28032\
    \ and make sure to add you as a co-author of the fix! Thanks again for all your\
    \ help"
  created_at: 2023-12-14 10:28:11+00:00
  edited: false
  hidden: false
  id: 657ad8bb54021149b4527ced
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-22T17:01:19.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8688658475875854
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>This is now fixed on transformers main! Closing this issue - thanks\
          \ a lot <span data-props=\"{&quot;user&quot;:&quot;gullal1491&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gullal1491\">@<span class=\"\
          underline\">gullal1491</span></a></span>\n\n\t</span></span> for everything!</p>\n"
        raw: This is now fixed on transformers main! Closing this issue - thanks a
          lot @gullal1491 for everything!
        updatedAt: '2023-12-22T17:01:19.777Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6585c0df924e1eafa60a36e6
    id: 6585c0df924e1eafa60a36e1
    type: comment
  author: ybelkada
  content: This is now fixed on transformers main! Closing this issue - thanks a lot
    @gullal1491 for everything!
  created_at: 2023-12-22 17:01:19+00:00
  edited: false
  hidden: false
  id: 6585c0df924e1eafa60a36e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-22T17:01:19.000Z'
    data:
      status: closed
    id: 6585c0df924e1eafa60a36e6
    type: status-change
  author: ybelkada
  created_at: 2023-12-22 17:01:19+00:00
  id: 6585c0df924e1eafa60a36e6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: llava-hf/llava-1.5-7b-hf
repo_type: model
status: closed
target_branch: null
title: Cuda error under modelling_llava.py
