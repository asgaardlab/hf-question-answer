!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wk21
conflicting_files: null
created_at: 2023-12-17 12:18:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a373f870339170fcf2faf05c3fa28109.svg
      fullname: kim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wk21
      type: user
    createdAt: '2023-12-17T12:18:41.000Z'
    data:
      edited: false
      editors:
      - wk21
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9526425004005432
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a373f870339170fcf2faf05c3fa28109.svg
          fullname: kim
          isHf: false
          isPro: false
          name: wk21
          type: user
        html: '<p>Hi Thank you so much about providing this. </p>

          <p>Actually, I tested some inference from your llava version.<br>But it
          answer something which differ from the result of liuhaotian/llava-v1.5-7b.<br>It
          same as input.</p>

          <p>What is the difference? I''m confusing it.<br>There is some changes from
          your version on model? </p>

          <p>Thanks</p>

          '
        raw: "Hi Thank you so much about providing this. \r\n\r\nActually, I tested\
          \ some inference from your llava version. \r\nBut it answer something which\
          \ differ from the result of liuhaotian/llava-v1.5-7b.\r\nIt same as input.\r\
          \n\r\nWhat is the difference? I'm confusing it.\r\nThere is some changes\
          \ from your version on model? \r\n\r\nThanks"
        updatedAt: '2023-12-17T12:18:41.492Z'
      numEdits: 0
      reactions: []
    id: 657ee7213687559a6785e9ec
    type: comment
  author: wk21
  content: "Hi Thank you so much about providing this. \r\n\r\nActually, I tested\
    \ some inference from your llava version. \r\nBut it answer something which differ\
    \ from the result of liuhaotian/llava-v1.5-7b.\r\nIt same as input.\r\n\r\nWhat\
    \ is the difference? I'm confusing it.\r\nThere is some changes from your version\
    \ on model? \r\n\r\nThanks"
  created_at: 2023-12-17 12:18:41+00:00
  edited: false
  hidden: false
  id: 657ee7213687559a6785e9ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a373f870339170fcf2faf05c3fa28109.svg
      fullname: kim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wk21
      type: user
    createdAt: '2023-12-17T12:39:56.000Z'
    data:
      edited: false
      editors:
      - wk21
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6915189623832703
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a373f870339170fcf2faf05c3fa28109.svg
          fullname: kim
          isHf: false
          isPro: false
          name: wk21
          type: user
        html: "<p>Also, I have the following errors. Do you know the reason..?<br>it\
          \ always happened</p>\n<p>\"Processing on val:   0%|          | 0/500 [00:00&lt;?,\
          \ ?it/s]/databricks/python/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:\
          \ UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered\
          \ internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)<br>  return\
          \ F.conv2d(input, weight, bias, self.stride,<br>Processing on val:  40%|\u2588\
          \u2588\u2588\u2589      | 198/500 [09:38&lt;14:41,  2.92s/it]<br>../aten/src/ATen/native/cuda/IndexKernel.cu:92:\
          \ operator(): block: [0,0,0], thread: [0,0,0] Assertion <code>index &gt;=\
          \ -sizes[i] &amp;&amp; index &lt; sizes[i] &amp;&amp; \"index out of bounds\"\
          </code> failed.<br>RuntimeError: CUDA error: device-side assert triggered<br>CUDA\
          \ kernel errors might be asynchronously reported at some other API call,\
          \ so the stacktrace below might be incorrect.<br>For debugging consider\
          \ passing CUDA_LAUNCH_BLOCKING=1.<br>Compile with <code>TORCH_USE_CUDA_DSA</code>\
          \ to enable device-side assertions.<br>\"</p>\n"
        raw: "Also, I have the following errors. Do you know the reason..? \nit always\
          \ happened\n\n\"Processing on val:   0%|          | 0/500 [00:00<?, ?it/s]/databricks/python/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:\
          \ UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered\
          \ internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n  return\
          \ F.conv2d(input, weight, bias, self.stride,\nProcessing on val:  40%|\u2588\
          \u2588\u2588\u2589      | 198/500 [09:38<14:41,  2.92s/it]\n../aten/src/ATen/native/cuda/IndexKernel.cu:92:\
          \ operator(): block: [0,0,0], thread: [0,0,0] Assertion `index >= -sizes[i]\
          \ && index < sizes[i] && \"index out of bounds\"` failed.\nRuntimeError:\
          \ CUDA error: device-side assert triggered\nCUDA kernel errors might be\
          \ asynchronously reported at some other API call, so the stacktrace below\
          \ might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\
          Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\""
        updatedAt: '2023-12-17T12:39:56.210Z'
      numEdits: 0
      reactions: []
    id: 657eec1c792970912818a52d
    type: comment
  author: wk21
  content: "Also, I have the following errors. Do you know the reason..? \nit always\
    \ happened\n\n\"Processing on val:   0%|          | 0/500 [00:00<?, ?it/s]/databricks/python/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:\
    \ UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered\
    \ internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n  return F.conv2d(input,\
    \ weight, bias, self.stride,\nProcessing on val:  40%|\u2588\u2588\u2588\u2589\
    \      | 198/500 [09:38<14:41,  2.92s/it]\n../aten/src/ATen/native/cuda/IndexKernel.cu:92:\
    \ operator(): block: [0,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] &&\
    \ index < sizes[i] && \"index out of bounds\"` failed.\nRuntimeError: CUDA error:\
    \ device-side assert triggered\nCUDA kernel errors might be asynchronously reported\
    \ at some other API call, so the stacktrace below might be incorrect.\nFor debugging\
    \ consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA`\
    \ to enable device-side assertions.\n\""
  created_at: 2023-12-17 12:39:56+00:00
  edited: false
  hidden: false
  id: 657eec1c792970912818a52d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-17T13:27:29.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8466252088546753
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;wk21&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/wk21\">@<span class=\"\
          underline\">wk21</span></a></span>\n\n\t</span></span><br>Thanks for the\
          \ issue, can you share exactly how do you get: <a href=\"https://huggingface.co/llava-hf/llava-1.5-7b-hf/discussions/8#657eec1c792970912818a52d\"\
          >https://huggingface.co/llava-hf/llava-1.5-7b-hf/discussions/8#657eec1c792970912818a52d</a>\
          \ - <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/pull/28032\"\
          >https://github.com/huggingface/transformers/pull/28032</a> might fix it\
          \ but I couldn't repro at all. </p>\n"
        raw: "Hi @wk21 \nThanks for the issue, can you share exactly how do you get:\
          \ https://huggingface.co/llava-hf/llava-1.5-7b-hf/discussions/8#657eec1c792970912818a52d\
          \ - https://github.com/huggingface/transformers/pull/28032 might fix it\
          \ but I couldn't repro at all. "
        updatedAt: '2023-12-17T13:27:29.406Z'
      numEdits: 0
      reactions: []
    id: 657ef7413480ce8aaeac7ada
    type: comment
  author: ybelkada
  content: "Hi @wk21 \nThanks for the issue, can you share exactly how do you get:\
    \ https://huggingface.co/llava-hf/llava-1.5-7b-hf/discussions/8#657eec1c792970912818a52d\
    \ - https://github.com/huggingface/transformers/pull/28032 might fix it but I\
    \ couldn't repro at all. "
  created_at: 2023-12-17 13:27:29+00:00
  edited: false
  hidden: false
  id: 657ef7413480ce8aaeac7ada
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-17T13:28:00.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8447707295417786
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<blockquote>

          <p>Actually, I tested some inference from your llava version.<br>But it
          answer something which differ from the result of liuhaotian/llava-v1.5-7b.</p>

          </blockquote>

          <p>Thanks for testing ! DO you use the same prompt template for both models?</p>

          '
        raw: '> Actually, I tested some inference from your llava version.

          > But it answer something which differ from the result of liuhaotian/llava-v1.5-7b.


          Thanks for testing ! DO you use the same prompt template for both models?'
        updatedAt: '2023-12-17T13:28:00.185Z'
      numEdits: 0
      reactions: []
    id: 657ef7608888ccb894ff48dd
    type: comment
  author: ybelkada
  content: '> Actually, I tested some inference from your llava version.

    > But it answer something which differ from the result of liuhaotian/llava-v1.5-7b.


    Thanks for testing ! DO you use the same prompt template for both models?'
  created_at: 2023-12-17 13:28:00+00:00
  edited: false
  hidden: false
  id: 657ef7608888ccb894ff48dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a373f870339170fcf2faf05c3fa28109.svg
      fullname: kim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wk21
      type: user
    createdAt: '2023-12-18T08:47:19.000Z'
    data:
      edited: false
      editors:
      - wk21
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7137007713317871
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a373f870339170fcf2faf05c3fa28109.svg
          fullname: kim
          isHf: false
          isPro: false
          name: wk21
          type: user
        html: '<p>I just do image-caption. it makes some different text. </p>

          <p>Actually, I used the prompt like "USER: <img>\nPlease describe the image
          in detail \nASSISTANT:" and    output = model.generate(**inputs, max_new_tokens=300,
          do_sample=False)<br>when it comes to "", I used the prompt like "Please
          describe the image in detail \nASSISTANT:" with liuhaotian/llava-v1.5-7b.
          it uses output = model.generate(<br>                input_ids,<br>                images=images_tensor,<br>                do_sample=False,<br>                max_new_tokens=300,<br>            )</p>

          <p>but it''s different. </p>

          <p>Also, when does it apply to transformers? <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/28032">https://github.com/huggingface/transformers/pull/28032</a></p>

          '
        raw: "I just do image-caption. it makes some different text. \n\nActually,\
          \ I used the prompt like \"USER: <image>\\nPlease describe the image in\
          \ detail \\nASSISTANT:\" and    output = model.generate(**inputs, max_new_tokens=300,\
          \ do_sample=False)\nwhen it comes to \"\", I used the prompt like \"Please\
          \ describe the image in detail \\nASSISTANT:\" with liuhaotian/llava-v1.5-7b.\
          \ it uses output = model.generate(\n                input_ids,\n       \
          \         images=images_tensor,\n                do_sample=False,\n    \
          \            max_new_tokens=300,\n            )\n\n\nbut it's different.\
          \ \n\n\nAlso, when does it apply to transformers? https://github.com/huggingface/transformers/pull/28032"
        updatedAt: '2023-12-18T08:47:19.442Z'
      numEdits: 0
      reactions: []
    id: 65800717aed79486ae5f4328
    type: comment
  author: wk21
  content: "I just do image-caption. it makes some different text. \n\nActually, I\
    \ used the prompt like \"USER: <image>\\nPlease describe the image in detail \\\
    nASSISTANT:\" and    output = model.generate(**inputs, max_new_tokens=300, do_sample=False)\n\
    when it comes to \"\", I used the prompt like \"Please describe the image in detail\
    \ \\nASSISTANT:\" with liuhaotian/llava-v1.5-7b. it uses output = model.generate(\n\
    \                input_ids,\n                images=images_tensor,\n         \
    \       do_sample=False,\n                max_new_tokens=300,\n            )\n\
    \n\nbut it's different. \n\n\nAlso, when does it apply to transformers? https://github.com/huggingface/transformers/pull/28032"
  created_at: 2023-12-18 08:47:19+00:00
  edited: false
  hidden: false
  id: 65800717aed79486ae5f4328
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: llava-hf/llava-1.5-7b-hf
repo_type: model
status: open
target_branch: null
title: Why is the result differ from liuhaotian/llava-v1.5-7b
