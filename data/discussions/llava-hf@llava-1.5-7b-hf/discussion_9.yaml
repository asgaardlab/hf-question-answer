!!python/object:huggingface_hub.community.DiscussionWithDetails
author: apapiu
conflicting_files: null
created_at: 2023-12-18 08:21:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d96b919e8758f027bc60146dfaaedcd6.svg
      fullname: Alexandru Papiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: apapiu
      type: user
    createdAt: '2023-12-18T08:21:04.000Z'
    data:
      edited: false
      editors:
      - apapiu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7735723853111267
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d96b919e8758f027bc60146dfaaedcd6.svg
          fullname: Alexandru Papiu
          isHf: false
          isPro: false
          name: apapiu
          type: user
        html: '<p>Thanks for the awesome model! I am running the 4-bit model on a
          T4 in Colab and one generation is taking about 7 seconds. Even if I batch
          the images <code>outputs = pipe(imgs, prompt=prompt, generate_kwargs={"max_new_tokens":
          200})</code> I am note getting much of a speedup (i.e. for 4 images it takes
          ~28 seconds). Are there any other way to speed up text generation?</p>

          '
        raw: 'Thanks for the awesome model! I am running the 4-bit model on a T4 in
          Colab and one generation is taking about 7 seconds. Even if I batch the
          images `outputs = pipe(imgs, prompt=prompt, generate_kwargs={"max_new_tokens":
          200})` I am note getting much of a speedup (i.e. for 4 images it takes ~28
          seconds). Are there any other way to speed up text generation?'
        updatedAt: '2023-12-18T08:21:04.158Z'
      numEdits: 0
      reactions: []
    id: 658000f0156fdf0301144e82
    type: comment
  author: apapiu
  content: 'Thanks for the awesome model! I am running the 4-bit model on a T4 in
    Colab and one generation is taking about 7 seconds. Even if I batch the images
    `outputs = pipe(imgs, prompt=prompt, generate_kwargs={"max_new_tokens": 200})`
    I am note getting much of a speedup (i.e. for 4 images it takes ~28 seconds).
    Are there any other way to speed up text generation?'
  created_at: 2023-12-18 08:21:04+00:00
  edited: false
  hidden: false
  id: 658000f0156fdf0301144e82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-18T08:25:31.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.673595666885376
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;apapiu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/apapiu\">@<span class=\"\
          underline\">apapiu</span></a></span>\n\n\t</span></span><br>Thanks!<br>Make\
          \ sure to pass <code>bnb_4bit_compute_dtype=torch.float16</code> to the\
          \ quantization config:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> LlavaForConditionalGeneration,\
          \ BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=<span\
          \ class=\"hljs-literal\">True</span>, bnb_4bit_compute_dtype=torch.float16)\n\
          model = LlavaForConditionalGeneration.from_pretrained(<span class=\"hljs-string\"\
          >\"llava-hf/llava-1.5-7b-hf\"</span>, quantization_config=quantization_config)\n\
          </code></pre>\n<p>If you have access to an A100 GPU you can also use Flash-Attention-2.\
          \ I need to double check if it works out of the box but you can also pass\
          \ <code>attn_implementation=\"sdpa\"</code> to from_pretrained which should\
          \ lead to faster generation, let me get back to you on this</p>\n"
        raw: "Hi @apapiu \nThanks! \nMake sure to pass `bnb_4bit_compute_dtype=torch.float16`\
          \ to the quantization config:\n\n```python\nimport torch\nfrom transformers\
          \ import LlavaForConditionalGeneration, BitsAndBytesConfig\n\nquantization_config\
          \ = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n\
          model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\"\
          , quantization_config=quantization_config)\n```\n\nIf you have access to\
          \ an A100 GPU you can also use Flash-Attention-2. I need to double check\
          \ if it works out of the box but you can also pass `attn_implementation=\"\
          sdpa\"` to from_pretrained which should lead to faster generation, let me\
          \ get back to you on this"
        updatedAt: '2023-12-18T08:25:31.391Z'
      numEdits: 0
      reactions: []
    id: 658001fb504da7f6f30f1b00
    type: comment
  author: ybelkada
  content: "Hi @apapiu \nThanks! \nMake sure to pass `bnb_4bit_compute_dtype=torch.float16`\
    \ to the quantization config:\n\n```python\nimport torch\nfrom transformers import\
    \ LlavaForConditionalGeneration, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True,\
    \ bnb_4bit_compute_dtype=torch.float16)\nmodel = LlavaForConditionalGeneration.from_pretrained(\"\
    llava-hf/llava-1.5-7b-hf\", quantization_config=quantization_config)\n```\n\n\
    If you have access to an A100 GPU you can also use Flash-Attention-2. I need to\
    \ double check if it works out of the box but you can also pass `attn_implementation=\"\
    sdpa\"` to from_pretrained which should lead to faster generation, let me get\
    \ back to you on this"
  created_at: 2023-12-18 08:25:31+00:00
  edited: false
  hidden: false
  id: 658001fb504da7f6f30f1b00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-18T09:19:11.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5571624636650085
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;apapiu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/apapiu\">@<span class=\"\
          underline\">apapiu</span></a></span>\n\n\t</span></span><br>I just made\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/pull/28107\"\
          >https://github.com/huggingface/transformers/pull/28107</a> which should\
          \ add SDPA support in Llava, it makes the model generation faster, you will\
          \ be able to enable it through:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> pipeline,\
          \ BitsAndBytesConfig\n<span class=\"hljs-keyword\">from</span> PIL <span\
          \ class=\"hljs-keyword\">import</span> Image    \n<span class=\"hljs-keyword\"\
          >import</span> requests\n\nmodel_id = <span class=\"hljs-string\">\"llava-hf/llava-1.5-7b-hf\"\
          </span>\nquantization_config = BitsAndBytesConfig(load_in_4bit=<span class=\"\
          hljs-literal\">True</span>, bnb_4bit_compute_dtype=torch.float16)\n\npipe\
          \ = pipeline(<span class=\"hljs-string\">\"image-to-text\"</span>, model=model_id,\
          \ model_kwargs={<span class=\"hljs-string\">\"load_in_4bit\"</span>: <span\
          \ class=\"hljs-literal\">True</span>, <span class=\"hljs-string\">\"quantization_config\"\
          </span>: quantization_config, <span class=\"hljs-string\">\"attn_implementation\"\
          </span>: <span class=\"hljs-string\">\"sdpa\"</span>})\nurl = <span class=\"\
          hljs-string\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\
          </span>\n\nimage = Image.<span class=\"hljs-built_in\">open</span>(requests.get(url,\
          \ stream=<span class=\"hljs-literal\">True</span>).raw)\nprompt = <span\
          \ class=\"hljs-string\">\"USER: &lt;image&gt;\\nWhat does the label 15 represent?\
          \ (1) lava (2) core (3) tunnel (4) ash cloud\\nASSISTANT:\"</span>\n\noutputs\
          \ = pipe(image, prompt=prompt, generate_kwargs={<span class=\"hljs-string\"\
          >\"max_new_tokens\"</span>: <span class=\"hljs-number\">200</span>})\n<span\
          \ class=\"hljs-built_in\">print</span>(outputs)\n</code></pre>\n"
        raw: "Hi @apapiu \nI just made https://github.com/huggingface/transformers/pull/28107\
          \ which should add SDPA support in Llava, it makes the model generation\
          \ faster, you will be able to enable it through:\n\n```python\nimport torch\n\
          from transformers import pipeline, BitsAndBytesConfig\nfrom PIL import Image\
          \    \nimport requests\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\nquantization_config\
          \ = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n\
          \npipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"load_in_4bit\"\
          : True, \"quantization_config\": quantization_config, \"attn_implementation\"\
          : \"sdpa\"})\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\
          \n\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"\
          USER: <image>\\nWhat does the label 15 represent? (1) lava (2) core (3)\
          \ tunnel (4) ash cloud\\nASSISTANT:\"\n\noutputs = pipe(image, prompt=prompt,\
          \ generate_kwargs={\"max_new_tokens\": 200})\nprint(outputs)\n```"
        updatedAt: '2023-12-18T09:19:11.925Z'
      numEdits: 0
      reactions: []
    id: 65800e8fe1116d68e9e9e140
    type: comment
  author: ybelkada
  content: "Hi @apapiu \nI just made https://github.com/huggingface/transformers/pull/28107\
    \ which should add SDPA support in Llava, it makes the model generation faster,\
    \ you will be able to enable it through:\n\n```python\nimport torch\nfrom transformers\
    \ import pipeline, BitsAndBytesConfig\nfrom PIL import Image    \nimport requests\n\
    \nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\nquantization_config = BitsAndBytesConfig(load_in_4bit=True,\
    \ bnb_4bit_compute_dtype=torch.float16)\n\npipe = pipeline(\"image-to-text\",\
    \ model=model_id, model_kwargs={\"load_in_4bit\": True, \"quantization_config\"\
    : quantization_config, \"attn_implementation\": \"sdpa\"})\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\
    \n\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"USER: <image>\\\
    nWhat does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud\\\
    nASSISTANT:\"\n\noutputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\"\
    : 200})\nprint(outputs)\n```"
  created_at: 2023-12-18 09:19:11+00:00
  edited: false
  hidden: false
  id: 65800e8fe1116d68e9e9e140
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-18T09:23:12.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4928954839706421
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Once that PR gets merged, you can force-dispatch SDAP to use flash-attention\
          \ with the following script (works also on a T4):</p>\n<pre><code class=\"\
          language-diff\">import torch\nfrom transformers import pipeline, BitsAndBytesConfig\n\
          from PIL import Image    \nimport requests\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\
          \nquantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n\
          \npipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"load_in_4bit\"\
          : True, \"quantization_config\": quantization_config})\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\
          \n\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"\
          USER: &lt;image&gt;\\nWhat does the label 15 represent? (1) lava (2) core\
          \ (3) tunnel (4) ash cloud\\nASSISTANT:\"\n\n<span class=\"hljs-deletion\"\
          >- outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\"\
          : 200})</span>\n<span class=\"hljs-addition\">+ with torch.backends.cuda.sdp_kernel(enable_flash=True,\
          \ enable_math=False, enable_mem_efficient=False):</span>\n<span class=\"\
          hljs-addition\">+     outputs = pipe(image, prompt=prompt, generate_kwargs={\"\
          max_new_tokens\": 200})</span>\nprint(outputs)\n</code></pre>\n"
        raw: "Once that PR gets merged, you can force-dispatch SDAP to use flash-attention\
          \ with the following script (works also on a T4):\n\n```diff\nimport torch\n\
          from transformers import pipeline, BitsAndBytesConfig\nfrom PIL import Image\
          \    \nimport requests\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\nquantization_config\
          \ = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n\
          \npipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"load_in_4bit\"\
          : True, \"quantization_config\": quantization_config})\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\
          \n\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"\
          USER: <image>\\nWhat does the label 15 represent? (1) lava (2) core (3)\
          \ tunnel (4) ash cloud\\nASSISTANT:\"\n\n- outputs = pipe(image, prompt=prompt,\
          \ generate_kwargs={\"max_new_tokens\": 200})\n+ with torch.backends.cuda.sdp_kernel(enable_flash=True,\
          \ enable_math=False, enable_mem_efficient=False):\n+     outputs = pipe(image,\
          \ prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\nprint(outputs)\n\
          ```"
        updatedAt: '2023-12-18T09:32:02.846Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ArthurZ
    id: 65800f802f02b61150ce17b1
    type: comment
  author: ybelkada
  content: "Once that PR gets merged, you can force-dispatch SDAP to use flash-attention\
    \ with the following script (works also on a T4):\n\n```diff\nimport torch\nfrom\
    \ transformers import pipeline, BitsAndBytesConfig\nfrom PIL import Image    \n\
    import requests\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\nquantization_config\
    \ = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n\
    \npipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"load_in_4bit\"\
    : True, \"quantization_config\": quantization_config})\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\
    \n\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"USER: <image>\\\
    nWhat does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud\\\
    nASSISTANT:\"\n\n- outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\"\
    : 200})\n+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False,\
    \ enable_mem_efficient=False):\n+     outputs = pipe(image, prompt=prompt, generate_kwargs={\"\
    max_new_tokens\": 200})\nprint(outputs)\n```"
  created_at: 2023-12-18 09:23:12+00:00
  edited: true
  hidden: false
  id: 65800f802f02b61150ce17b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d96b919e8758f027bc60146dfaaedcd6.svg
      fullname: Alexandru Papiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: apapiu
      type: user
    createdAt: '2023-12-18T21:49:17.000Z'
    data:
      edited: false
      editors:
      - apapiu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7861142158508301
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d96b919e8758f027bc60146dfaaedcd6.svg
          fullname: Alexandru Papiu
          isHf: false
          isPro: false
          name: apapiu
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> thanks for the\
          \ reply! Unfortunately when I tried with and without the <code>torch.backends.cuda.sdp_kernel(enable_flash=True,\
          \ enable_math=False, enable_mem_efficient=False)</code> context I got the\
          \ same generation length. (using <code>4.37.0.dev0</code> for transformers)</p>\n\
          <p>Another issue I identified was that when using a pipe: <code>pipe(imgs,\
          \ prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})</code>even if\
          \ I put in a list of PIL images the processing happens sequentially (based\
          \ on GPU memory). However using the transformer directly with <code>model.generate</code>\
          \ solves the problem since for that I can pass in batches:</p>\n<pre><code\
          \ class=\"language-python\">\nmodel_id = <span class=\"hljs-string\">\"\
          llava-hf/llava-1.5-7b-hf\"</span>\n\nmodel = LlavaForConditionalGeneration.from_pretrained(\n\
          \    model_id, \n    torch_dtype=torch.float16, \n    low_cpu_mem_usage=<span\
          \ class=\"hljs-literal\">True</span>,\n    attn_implementation=<span class=\"\
          hljs-string\">\"flash_attention_2\"</span>\n).to(<span class=\"hljs-string\"\
          >'cuda'</span>)\n\n<span class=\"hljs-comment\">#get a batch of images..</span>\n\
          \nimgs = [to_pil(batch[<span class=\"hljs-number\">0</span>][i]) <span class=\"\
          hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span\
          \ class=\"hljs-built_in\">range</span>(bs)]\ninputs = processor([prompt]*bs,\
          \ imgs, return_tensors=<span class=\"hljs-string\">'pt'</span>).to(<span\
          \ class=\"hljs-number\">0</span>, torch.float16)\noutput = model.generate(**inputs,\
          \ max_new_tokens=<span class=\"hljs-number\">200</span>, do_sample=<span\
          \ class=\"hljs-literal\">False</span>)\n</code></pre>\n<p>note that I am\
          \ not using the 4bit quantization (I did have to go to an A100 for that\
          \ however). The above seems to work and is pretty fast (about 6 images per\
          \ second).</p>\n<p>My two remaining questions are:</p>\n<ol>\n<li>How can\
          \ I make the pipe accept batched inputs?</li>\n<li>Is it expected  for 4bit\
          \ models to be significantly slower than fp16 models?</li>\n</ol>\n"
        raw: "@ybelkada thanks for the reply! Unfortunately when I tried with and\
          \ without the `torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False,\
          \ enable_mem_efficient=False)` context I got the same generation length.\
          \ (using `4.37.0.dev0` for transformers)\n\nAnother issue I identified was\
          \ that when using a pipe: `pipe(imgs, prompt=prompt, generate_kwargs={\"\
          max_new_tokens\": 200})`even if I put in a list of PIL images the processing\
          \ happens sequentially (based on GPU memory). However using the transformer\
          \ directly with `model.generate` solves the problem since for that I can\
          \ pass in batches:\n\n```python\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\
          \n\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_id,\
          \ \n    torch_dtype=torch.float16, \n    low_cpu_mem_usage=True,\n    attn_implementation=\"\
          flash_attention_2\"\n).to('cuda')\n\n#get a batch of images..\n\nimgs =\
          \ [to_pil(batch[0][i]) for i in range(bs)]\ninputs = processor([prompt]*bs,\
          \ imgs, return_tensors='pt').to(0, torch.float16)\noutput = model.generate(**inputs,\
          \ max_new_tokens=200, do_sample=False)\n\n```\nnote that I am not using\
          \ the 4bit quantization (I did have to go to an A100 for that however).\
          \ The above seems to work and is pretty fast (about 6 images per second).\n\
          \nMy two remaining questions are:\n1. How can I make the pipe accept batched\
          \ inputs?\n2. Is it expected  for 4bit models to be significantly slower\
          \ than fp16 models?\n\n\n\n"
        updatedAt: '2023-12-18T21:49:17.602Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
    id: 6580be5d8c6c1cb037275707
    type: comment
  author: apapiu
  content: "@ybelkada thanks for the reply! Unfortunately when I tried with and without\
    \ the `torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False)`\
    \ context I got the same generation length. (using `4.37.0.dev0` for transformers)\n\
    \nAnother issue I identified was that when using a pipe: `pipe(imgs, prompt=prompt,\
    \ generate_kwargs={\"max_new_tokens\": 200})`even if I put in a list of PIL images\
    \ the processing happens sequentially (based on GPU memory). However using the\
    \ transformer directly with `model.generate` solves the problem since for that\
    \ I can pass in batches:\n\n```python\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\
    \n\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_id, \n \
    \   torch_dtype=torch.float16, \n    low_cpu_mem_usage=True,\n    attn_implementation=\"\
    flash_attention_2\"\n).to('cuda')\n\n#get a batch of images..\n\nimgs = [to_pil(batch[0][i])\
    \ for i in range(bs)]\ninputs = processor([prompt]*bs, imgs, return_tensors='pt').to(0,\
    \ torch.float16)\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n\
    \n```\nnote that I am not using the 4bit quantization (I did have to go to an\
    \ A100 for that however). The above seems to work and is pretty fast (about 6\
    \ images per second).\n\nMy two remaining questions are:\n1. How can I make the\
    \ pipe accept batched inputs?\n2. Is it expected  for 4bit models to be significantly\
    \ slower than fp16 models?\n\n\n\n"
  created_at: 2023-12-18 21:49:17+00:00
  edited: false
  hidden: false
  id: 6580be5d8c6c1cb037275707
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-19T14:23:04.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8142017126083374
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;apapiu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/apapiu\">@<span class=\"\
          underline\">apapiu</span></a></span>\n\n\t</span></span><br>Thanks a lot\
          \ for testing out!<br>1- I think for pipeline it indeed runs the operation\
          \ sequentially, I am not sure how to perform batched generation with pipelines.<br>2-\
          \ Yes it is expected ! check out this extensive blogpost on quantization\
          \ schemes benchmarks: <a href=\"https://huggingface.co/blog/overview-quantization-transformers\"\
          >https://huggingface.co/blog/overview-quantization-transformers</a> or this\
          \ relevant doc page: <a href=\"https://huggingface.co/docs/transformers/quantization\"\
          >https://huggingface.co/docs/transformers/quantization</a><br>I am planning\
          \ on adding AWQ for llava: <a rel=\"nofollow\" href=\"https://github.com/casper-hansen/AutoAWQ/pull/250\"\
          >https://github.com/casper-hansen/AutoAWQ/pull/250</a> once that PR gets\
          \ merged we'll be able to use llava + AWQ + fused modules which should be\
          \ faster than fp16. Read more about it here: <a href=\"https://huggingface.co/docs/transformers/quantization#make-use-of-fused-modules\"\
          >https://huggingface.co/docs/transformers/quantization#make-use-of-fused-modules</a></p>\n"
        raw: "Hi @apapiu \nThanks a lot for testing out!\n1- I think for pipeline\
          \ it indeed runs the operation sequentially, I am not sure how to perform\
          \ batched generation with pipelines. \n2- Yes it is expected ! check out\
          \ this extensive blogpost on quantization schemes benchmarks: https://huggingface.co/blog/overview-quantization-transformers\
          \ or this relevant doc page: https://huggingface.co/docs/transformers/quantization\n\
          I am planning on adding AWQ for llava: https://github.com/casper-hansen/AutoAWQ/pull/250\
          \ once that PR gets merged we'll be able to use llava + AWQ + fused modules\
          \ which should be faster than fp16. Read more about it here: https://huggingface.co/docs/transformers/quantization#make-use-of-fused-modules\n"
        updatedAt: '2023-12-19T14:23:04.956Z'
      numEdits: 0
      reactions: []
    id: 6581a74826e759e7d29b0b90
    type: comment
  author: ybelkada
  content: "Hi @apapiu \nThanks a lot for testing out!\n1- I think for pipeline it\
    \ indeed runs the operation sequentially, I am not sure how to perform batched\
    \ generation with pipelines. \n2- Yes it is expected ! check out this extensive\
    \ blogpost on quantization schemes benchmarks: https://huggingface.co/blog/overview-quantization-transformers\
    \ or this relevant doc page: https://huggingface.co/docs/transformers/quantization\n\
    I am planning on adding AWQ for llava: https://github.com/casper-hansen/AutoAWQ/pull/250\
    \ once that PR gets merged we'll be able to use llava + AWQ + fused modules which\
    \ should be faster than fp16. Read more about it here: https://huggingface.co/docs/transformers/quantization#make-use-of-fused-modules\n"
  created_at: 2023-12-19 14:23:04+00:00
  edited: false
  hidden: false
  id: 6581a74826e759e7d29b0b90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d96b919e8758f027bc60146dfaaedcd6.svg
      fullname: Alexandru Papiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: apapiu
      type: user
    createdAt: '2023-12-20T07:37:05.000Z'
    data:
      edited: false
      editors:
      - apapiu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9291507005691528
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d96b919e8758f027bc60146dfaaedcd6.svg
          fullname: Alexandru Papiu
          isHf: false
          isPro: false
          name: apapiu
          type: user
        html: '<p>Gotcha no worries the pure transformer API works just fine. And
          these are great resources - thanks for sharing (and for writing some of
          them :)</p>

          '
        raw: Gotcha no worries the pure transformer API works just fine. And these
          are great resources - thanks for sharing (and for writing some of them :)
        updatedAt: '2023-12-20T07:37:05.345Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
    id: 658299a1083a345666b16683
    type: comment
  author: apapiu
  content: Gotcha no worries the pure transformer API works just fine. And these are
    great resources - thanks for sharing (and for writing some of them :)
  created_at: 2023-12-20 07:37:05+00:00
  edited: false
  hidden: false
  id: 658299a1083a345666b16683
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-20T09:00:01.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9579920768737793
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Awesome, thanks very much <span data-props=\"{&quot;user&quot;:&quot;apapiu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/apapiu\"\
          >@<span class=\"underline\">apapiu</span></a></span>\n\n\t</span></span>\
          \ !</p>\n"
        raw: Awesome, thanks very much @apapiu !
        updatedAt: '2023-12-20T09:00:01.210Z'
      numEdits: 0
      reactions: []
    id: 6582ad115fd4b96ca6d91383
    type: comment
  author: ybelkada
  content: Awesome, thanks very much @apapiu !
  created_at: 2023-12-20 09:00:01+00:00
  edited: false
  hidden: false
  id: 6582ad115fd4b96ca6d91383
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: llava-hf/llava-1.5-7b-hf
repo_type: model
status: open
target_branch: null
title: Duration to generate a caption for 1 image
