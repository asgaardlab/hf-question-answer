!!python/object:huggingface_hub.community.DiscussionWithDetails
author: father123
conflicting_files: null
created_at: 2023-07-14 17:14:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/96b8c930f9d32a7fb561e2dbfe191223.svg
      fullname: your father
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: father123
      type: user
    createdAt: '2023-07-14T18:14:49.000Z'
    data:
      edited: false
      editors:
      - father123
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9973993897438049
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/96b8c930f9d32a7fb561e2dbfe191223.svg
          fullname: your father
          isHf: false
          isPro: false
          name: father123
          type: user
        html: "<p>\u524D\u6BB5\u65F6\u95F4\u8FD8\u80FD\u7528int4\u7248\u672C\u4E86\
          \uFF0C\u8FD9\u4E48\u53C8\u7528\u4E0D\u4E86\u4E86\uFF0C\u662F\u6539\u7248\
          \u672C\u4E86\u5417\uFF0C\u600E\u4E48\u56DE\u4E8B</p>\n"
        raw: "\u524D\u6BB5\u65F6\u95F4\u8FD8\u80FD\u7528int4\u7248\u672C\u4E86\uFF0C\
          \u8FD9\u4E48\u53C8\u7528\u4E0D\u4E86\u4E86\uFF0C\u662F\u6539\u7248\u672C\
          \u4E86\u5417\uFF0C\u600E\u4E48\u56DE\u4E8B"
        updatedAt: '2023-07-14T18:14:49.672Z'
      numEdits: 0
      reactions: []
    id: 64b19099372d434077325d7d
    type: comment
  author: father123
  content: "\u524D\u6BB5\u65F6\u95F4\u8FD8\u80FD\u7528int4\u7248\u672C\u4E86\uFF0C\
    \u8FD9\u4E48\u53C8\u7528\u4E0D\u4E86\u4E86\uFF0C\u662F\u6539\u7248\u672C\u4E86\
    \u5417\uFF0C\u600E\u4E48\u56DE\u4E8B"
  created_at: 2023-07-14 17:14:49+00:00
  edited: false
  hidden: false
  id: 64b19099372d434077325d7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3799ab37a0443c098af14108f33d257a.svg
      fullname: EthanMiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: EthanMiao
      type: user
    createdAt: '2023-07-18T06:53:36.000Z'
    data:
      edited: false
      editors:
      - EthanMiao
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9965706467628479
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3799ab37a0443c098af14108f33d257a.svg
          fullname: EthanMiao
          isHf: false
          isPro: false
          name: EthanMiao
          type: user
        html: "<p>\u62A5\u4EC0\u4E48\u9519</p>\n"
        raw: "\u62A5\u4EC0\u4E48\u9519"
        updatedAt: '2023-07-18T06:53:36.106Z'
      numEdits: 0
      reactions: []
    id: 64b636f0b3b69063d9afee37
    type: comment
  author: EthanMiao
  content: "\u62A5\u4EC0\u4E48\u9519"
  created_at: 2023-07-18 05:53:36+00:00
  edited: false
  hidden: false
  id: 64b636f0b3b69063d9afee37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/96b8c930f9d32a7fb561e2dbfe191223.svg
      fullname: your father
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: father123
      type: user
    createdAt: '2023-07-18T15:45:30.000Z'
    data:
      edited: false
      editors:
      - father123
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4037478268146515
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/96b8c930f9d32a7fb561e2dbfe191223.svg
          fullname: your father
          isHf: false
          isPro: false
          name: father123
          type: user
        html: "<blockquote>\n<p>\u62A5\u4EC0\u4E48\u9519</p>\n</blockquote>\n<p>Traceback\
          \ (most recent call last):<br>  File \"D:\\python310\\lib\\site-packages\\\
          gradio\\routes.py\", line 414, in run_predict<br>    output = await app.get_blocks().process_api(<br>\
          \  File \"D:\\python310\\lib\\site-packages\\gradio\\blocks.py\", line 1323,\
          \ in process_api<br>    result = await self.call_function(<br>  File \"\
          D:\\python310\\lib\\site-packages\\gradio\\blocks.py\", line 1067, in call_function<br>\
          \    prediction = await utils.async_iteration(iterator)<br>  File \"D:\\\
          python310\\lib\\site-packages\\gradio\\utils.py\", line 339, in async_iteration<br>\
          \    return await iterator.<strong>anext</strong>()<br>  File \"D:\\python310\\\
          lib\\site-packages\\gradio\\utils.py\", line 332, in <strong>anext</strong><br>\
          \    return await anyio.to_thread.run_sync(<br>  File \"D:\\python310\\\
          lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync<br>    return\
          \ await get_asynclib().run_sync_in_worker_thread(<br>  File \"D:\\python310\\\
          lib\\site-packages\\anyio_backends_asyncio.py\", line 877, in run_sync_in_worker_thread<br>\
          \    return await future<br>  File \"D:\\python310\\lib\\site-packages\\\
          anyio_backends_asyncio.py\", line 807, in run<br>    result = context.run(func,\
          \ *args)<br>  File \"D:\\python310\\lib\\site-packages\\gradio\\utils.py\"\
          , line 315, in run_sync_iterator_async<br>    return next(iterator)<br>\
          \  File \"D:\\ChatGLM2-6B-main\\web_demo - int4.py\", line 71, in predict<br>\
          \    for response, history, past_key_values in model.stream_chat(tokenizer,\
          \ input, history, past_key_values=past_key_values,<br>  File \"D:\\python310\\\
          lib\\site-packages\\torch\\utils_contextlib.py\", line 35, in generator_context<br>\
          \    response = gen.send(None)<br>  File \"C:\\Users\\Administrator/.cache\\\
          huggingface\\modules\\transformers_modules\\int4\\modeling_chatglm.py\"\
          , line 1057, in stream_chat<br>    for outputs in self.stream_generate(**inputs,\
          \ past_key_values=past_key_values,<br>  File \"D:\\python310\\lib\\site-packages\\\
          torch\\utils_contextlib.py\", line 35, in generator_context<br>    response\
          \ = gen.send(None)<br>  File \"C:\\Users\\Administrator/.cache\\huggingface\\\
          modules\\transformers_modules\\int4\\modeling_chatglm.py\", line 1142, in\
          \ stream_generate<br>    outputs = self(<br>  File \"D:\\python310\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl<br>\
          \    return forward_call(*args, **kwargs)<br>  File \"C:\\Users\\Administrator/.cache\\\
          huggingface\\modules\\transformers_modules\\int4\\modeling_chatglm.py\"\
          , line 931, in forward<br>    transformer_outputs = self.transformer(<br>\
          \  File \"D:\\python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\transformers_modules\\\
          int4\\modeling_chatglm.py\", line 827, in forward<br>    hidden_states,\
          \ presents, all_hidden_states, all_self_attentions = self.encoder(<br> \
          \ File \"D:\\python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\transformers_modules\\\
          int4\\modeling_chatglm.py\", line 637, in forward<br>    layer_ret = layer(<br>\
          \  File \"D:\\python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\transformers_modules\\\
          int4\\modeling_chatglm.py\", line 541, in forward<br>    attention_output,\
          \ kv_cache = self.self_attention(<br>  File \"D:\\python310\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl<br>    return\
          \ forward_call(*args, **kwargs)<br>  File \"C:\\Users\\Administrator/.cache\\\
          huggingface\\modules\\transformers_modules\\int4\\modeling_chatglm.py\"\
          , line 373, in forward<br>    mixed_x_layer = self.query_key_value(hidden_states)<br>\
          \  File \"D:\\python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\transformers_modules\\\
          int4\\quantization.py\", line 322, in forward<br>    output = W8A16Linear.apply(input,\
          \ self.weight, self.weight_scale, self.weight_bit_width)<br>  File \"D:\\\
          python310\\lib\\site-packages\\torch\\autograd\\function.py\", line 506,\
          \ in apply<br>    return super().apply(*args, **kwargs)  # type: ignore[misc]<br>\
          \  File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\transformers_modules\\\
          int4\\quantization.py\", line 54, in forward<br>    weight = extract_weight_to_half(quant_w,\
          \ scale_w, weight_bit_width)<br>  File \"C:\\Users\\Administrator/.cache\\\
          huggingface\\modules\\transformers_modules\\int4\\quantization.py\", line\
          \ 267, in extract_weight_to_half<br>    kernels.int4WeightExtractionHalf\
          \ if scale_list.dtype == torch.half else kernels.int4WeightExtractionBFloat16<br>AttributeError:\
          \ 'NoneType' object has no attribute 'int4WeightExtractionHalf'</p>\n"
        raw: "> \u62A5\u4EC0\u4E48\u9519\n\nTraceback (most recent call last):\n \
          \ File \"D:\\python310\\lib\\site-packages\\gradio\\routes.py\", line 414,\
          \ in run_predict\n    output = await app.get_blocks().process_api(\n  File\
          \ \"D:\\python310\\lib\\site-packages\\gradio\\blocks.py\", line 1323, in\
          \ process_api\n    result = await self.call_function(\n  File \"D:\\python310\\\
          lib\\site-packages\\gradio\\blocks.py\", line 1067, in call_function\n \
          \   prediction = await utils.async_iteration(iterator)\n  File \"D:\\python310\\\
          lib\\site-packages\\gradio\\utils.py\", line 339, in async_iteration\n \
          \   return await iterator.__anext__()\n  File \"D:\\python310\\lib\\site-packages\\\
          gradio\\utils.py\", line 332, in __anext__\n    return await anyio.to_thread.run_sync(\n\
          \  File \"D:\\python310\\lib\\site-packages\\anyio\\to_thread.py\", line\
          \ 33, in run_sync\n    return await get_asynclib().run_sync_in_worker_thread(\n\
          \  File \"D:\\python310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\"\
          , line 877, in run_sync_in_worker_thread\n    return await future\n  File\
          \ \"D:\\python310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\",\
          \ line 807, in run\n    result = context.run(func, *args)\n  File \"D:\\\
          python310\\lib\\site-packages\\gradio\\utils.py\", line 315, in run_sync_iterator_async\n\
          \    return next(iterator)\n  File \"D:\\ChatGLM2-6B-main\\web_demo - int4.py\"\
          , line 71, in predict\n    for response, history, past_key_values in model.stream_chat(tokenizer,\
          \ input, history, past_key_values=past_key_values,\n  File \"D:\\python310\\\
          lib\\site-packages\\torch\\utils\\_contextlib.py\", line 35, in generator_context\n\
          \    response = gen.send(None)\n  File \"C:\\Users\\Administrator/.cache\\\
          huggingface\\modules\\transformers_modules\\int4\\modeling_chatglm.py\"\
          , line 1057, in stream_chat\n    for outputs in self.stream_generate(**inputs,\
          \ past_key_values=past_key_values,\n  File \"D:\\python310\\lib\\site-packages\\\
          torch\\utils\\_contextlib.py\", line 35, in generator_context\n    response\
          \ = gen.send(None)\n  File \"C:\\Users\\Administrator/.cache\\huggingface\\\
          modules\\transformers_modules\\int4\\modeling_chatglm.py\", line 1142, in\
          \ stream_generate\n    outputs = self(\n  File \"D:\\python310\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\\
          transformers_modules\\int4\\modeling_chatglm.py\", line 931, in forward\n\
          \    transformer_outputs = self.transformer(\n  File \"D:\\python310\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n\
          \    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\Administrator/.cache\\\
          huggingface\\modules\\transformers_modules\\int4\\modeling_chatglm.py\"\
          , line 827, in forward\n    hidden_states, presents, all_hidden_states,\
          \ all_self_attentions = self.encoder(\n  File \"D:\\python310\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\\
          transformers_modules\\int4\\modeling_chatglm.py\", line 637, in forward\n\
          \    layer_ret = layer(\n  File \"D:\\python310\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\\
          transformers_modules\\int4\\modeling_chatglm.py\", line 541, in forward\n\
          \    attention_output, kv_cache = self.self_attention(\n  File \"D:\\python310\\\
          lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n\
          \    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\Administrator/.cache\\\
          huggingface\\modules\\transformers_modules\\int4\\modeling_chatglm.py\"\
          , line 373, in forward\n    mixed_x_layer = self.query_key_value(hidden_states)\n\
          \  File \"D:\\python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\transformers_modules\\\
          int4\\quantization.py\", line 322, in forward\n    output = W8A16Linear.apply(input,\
          \ self.weight, self.weight_scale, self.weight_bit_width)\n  File \"D:\\\
          python310\\lib\\site-packages\\torch\\autograd\\function.py\", line 506,\
          \ in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n\
          \  File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\transformers_modules\\\
          int4\\quantization.py\", line 54, in forward\n    weight = extract_weight_to_half(quant_w,\
          \ scale_w, weight_bit_width)\n  File \"C:\\Users\\Administrator/.cache\\\
          huggingface\\modules\\transformers_modules\\int4\\quantization.py\", line\
          \ 267, in extract_weight_to_half\n    kernels.int4WeightExtractionHalf if\
          \ scale_list.dtype == torch.half else kernels.int4WeightExtractionBFloat16\n\
          AttributeError: 'NoneType' object has no attribute 'int4WeightExtractionHalf'\n"
        updatedAt: '2023-07-18T15:45:30.511Z'
      numEdits: 0
      reactions: []
    id: 64b6b39a56f1af7b46f21c28
    type: comment
  author: father123
  content: "> \u62A5\u4EC0\u4E48\u9519\n\nTraceback (most recent call last):\n  File\
    \ \"D:\\python310\\lib\\site-packages\\gradio\\routes.py\", line 414, in run_predict\n\
    \    output = await app.get_blocks().process_api(\n  File \"D:\\python310\\lib\\\
    site-packages\\gradio\\blocks.py\", line 1323, in process_api\n    result = await\
    \ self.call_function(\n  File \"D:\\python310\\lib\\site-packages\\gradio\\blocks.py\"\
    , line 1067, in call_function\n    prediction = await utils.async_iteration(iterator)\n\
    \  File \"D:\\python310\\lib\\site-packages\\gradio\\utils.py\", line 339, in\
    \ async_iteration\n    return await iterator.__anext__()\n  File \"D:\\python310\\\
    lib\\site-packages\\gradio\\utils.py\", line 332, in __anext__\n    return await\
    \ anyio.to_thread.run_sync(\n  File \"D:\\python310\\lib\\site-packages\\anyio\\\
    to_thread.py\", line 33, in run_sync\n    return await get_asynclib().run_sync_in_worker_thread(\n\
    \  File \"D:\\python310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\",\
    \ line 877, in run_sync_in_worker_thread\n    return await future\n  File \"D:\\\
    python310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n\
    \    result = context.run(func, *args)\n  File \"D:\\python310\\lib\\site-packages\\\
    gradio\\utils.py\", line 315, in run_sync_iterator_async\n    return next(iterator)\n\
    \  File \"D:\\ChatGLM2-6B-main\\web_demo - int4.py\", line 71, in predict\n  \
    \  for response, history, past_key_values in model.stream_chat(tokenizer, input,\
    \ history, past_key_values=past_key_values,\n  File \"D:\\python310\\lib\\site-packages\\\
    torch\\utils\\_contextlib.py\", line 35, in generator_context\n    response =\
    \ gen.send(None)\n  File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\\
    transformers_modules\\int4\\modeling_chatglm.py\", line 1057, in stream_chat\n\
    \    for outputs in self.stream_generate(**inputs, past_key_values=past_key_values,\n\
    \  File \"D:\\python310\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line\
    \ 35, in generator_context\n    response = gen.send(None)\n  File \"C:\\Users\\\
    Administrator/.cache\\huggingface\\modules\\transformers_modules\\int4\\modeling_chatglm.py\"\
    , line 1142, in stream_generate\n    outputs = self(\n  File \"D:\\python310\\\
    lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n\
    \    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\Administrator/.cache\\\
    huggingface\\modules\\transformers_modules\\int4\\modeling_chatglm.py\", line\
    \ 931, in forward\n    transformer_outputs = self.transformer(\n  File \"D:\\\
    python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
    \ _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\Administrator/.cache\\\
    huggingface\\modules\\transformers_modules\\int4\\modeling_chatglm.py\", line\
    \ 827, in forward\n    hidden_states, presents, all_hidden_states, all_self_attentions\
    \ = self.encoder(\n  File \"D:\\python310\\lib\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n\
    \  File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\transformers_modules\\\
    int4\\modeling_chatglm.py\", line 637, in forward\n    layer_ret = layer(\n  File\
    \ \"D:\\python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501,\
    \ in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\\
    Administrator/.cache\\huggingface\\modules\\transformers_modules\\int4\\modeling_chatglm.py\"\
    , line 541, in forward\n    attention_output, kv_cache = self.self_attention(\n\
    \  File \"D:\\python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\",\
    \ line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    C:\\Users\\Administrator/.cache\\huggingface\\modules\\transformers_modules\\\
    int4\\modeling_chatglm.py\", line 373, in forward\n    mixed_x_layer = self.query_key_value(hidden_states)\n\
    \  File \"D:\\python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\",\
    \ line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    C:\\Users\\Administrator/.cache\\huggingface\\modules\\transformers_modules\\\
    int4\\quantization.py\", line 322, in forward\n    output = W8A16Linear.apply(input,\
    \ self.weight, self.weight_scale, self.weight_bit_width)\n  File \"D:\\python310\\\
    lib\\site-packages\\torch\\autograd\\function.py\", line 506, in apply\n    return\
    \ super().apply(*args, **kwargs)  # type: ignore[misc]\n  File \"C:\\Users\\Administrator/.cache\\\
    huggingface\\modules\\transformers_modules\\int4\\quantization.py\", line 54,\
    \ in forward\n    weight = extract_weight_to_half(quant_w, scale_w, weight_bit_width)\n\
    \  File \"C:\\Users\\Administrator/.cache\\huggingface\\modules\\transformers_modules\\\
    int4\\quantization.py\", line 267, in extract_weight_to_half\n    kernels.int4WeightExtractionHalf\
    \ if scale_list.dtype == torch.half else kernels.int4WeightExtractionBFloat16\n\
    AttributeError: 'NoneType' object has no attribute 'int4WeightExtractionHalf'\n"
  created_at: 2023-07-18 14:45:30+00:00
  edited: false
  hidden: false
  id: 64b6b39a56f1af7b46f21c28
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: THUDM/chatglm2-6b-int4
repo_type: model
status: open
target_branch: null
title: "\u524D\u6BB5\u65F6\u95F4\u8FD8\u80FD\u7528int4\u7248\u672C\u4E86\uFF0C\u73B0\
  \u5728\u53C8\u7528\u4E0D\u4E86\u4E86\uFF0C\u600E\u4E48\u56DE\u4E8B\u3002\u3002\u3002\
  \u3002\u3002\u3002\u3002"
