!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Cmansterino
conflicting_files: null
created_at: 2023-08-15 06:34:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0528ef00bf9ac8e46b059b67a60e1dfb.svg
      fullname: Cman Fire
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cmansterino
      type: user
    createdAt: '2023-08-15T07:34:13.000Z'
    data:
      edited: false
      editors:
      - Cmansterino
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8325363397598267
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0528ef00bf9ac8e46b059b67a60e1dfb.svg
          fullname: Cman Fire
          isHf: false
          isPro: false
          name: Cmansterino
          type: user
        html: '<p>Model works!<br>However I received the following error log while
          loading this in webUI:</p>

          <p>2023-08-14 22:10:47 WARNING:Exllama kernel is not installed, reset disable_exllama
          to True. This may because you installed auto_gptq using a pre-build wheel
          on Windows, in which exllama_kernels are not compiled. To use exllama_kernels
          to further speedup inference, you can re-install auto_gptq from source.<br>2023-08-14
          22:10:54 WARNING:skip module injection for FusedLlamaMLPForQuantizedModel
          not support integrate without triton yet.</p>

          '
        raw: "Model works!\r\nHowever I received the following error log while loading\
          \ this in webUI:\r\n\r\n2023-08-14 22:10:47 WARNING:Exllama kernel is not\
          \ installed, reset disable_exllama to True. This may because you installed\
          \ auto_gptq using a pre-build wheel on Windows, in which exllama_kernels\
          \ are not compiled. To use exllama_kernels to further speedup inference,\
          \ you can re-install auto_gptq from source.\r\n2023-08-14 22:10:54 WARNING:skip\
          \ module injection for FusedLlamaMLPForQuantizedModel not support integrate\
          \ without triton yet."
        updatedAt: '2023-08-15T07:34:13.733Z'
      numEdits: 0
      reactions: []
    id: 64db2a753ae9693d99f3e996
    type: comment
  author: Cmansterino
  content: "Model works!\r\nHowever I received the following error log while loading\
    \ this in webUI:\r\n\r\n2023-08-14 22:10:47 WARNING:Exllama kernel is not installed,\
    \ reset disable_exllama to True. This may because you installed auto_gptq using\
    \ a pre-build wheel on Windows, in which exllama_kernels are not compiled. To\
    \ use exllama_kernels to further speedup inference, you can re-install auto_gptq\
    \ from source.\r\n2023-08-14 22:10:54 WARNING:skip module injection for FusedLlamaMLPForQuantizedModel\
    \ not support integrate without triton yet."
  created_at: 2023-08-15 06:34:13+00:00
  edited: false
  hidden: false
  id: 64db2a753ae9693d99f3e996
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e5ae49156545f683e86734317053a7e.svg
      fullname: Sn3d9
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sn3d9
      type: user
    createdAt: '2023-09-11T16:33:16.000Z'
    data:
      edited: false
      editors:
      - sn3d9
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8988264799118042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e5ae49156545f683e86734317053a7e.svg
          fullname: Sn3d9
          isHf: false
          isPro: false
          name: sn3d9
          type: user
        html: '<p>Hi, this is not an issue of the model. This is an issue of your
          installation of webUI.</p>

          <p>Are you using AMD on Linux? If that''s the case I can help, otherwise,
          please report this to text generation webUI Github.</p>

          '
        raw: 'Hi, this is not an issue of the model. This is an issue of your installation
          of webUI.


          Are you using AMD on Linux? If that''s the case I can help, otherwise, please
          report this to text generation webUI Github.'
        updatedAt: '2023-09-11T16:33:16.897Z'
      numEdits: 0
      reactions: []
    id: 64ff414cdadb8c628f08a64d
    type: comment
  author: sn3d9
  content: 'Hi, this is not an issue of the model. This is an issue of your installation
    of webUI.


    Are you using AMD on Linux? If that''s the case I can help, otherwise, please
    report this to text generation webUI Github.'
  created_at: 2023-09-11 15:33:16+00:00
  edited: false
  hidden: false
  id: 64ff414cdadb8c628f08a64d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0528ef00bf9ac8e46b059b67a60e1dfb.svg
      fullname: Cman Fire
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cmansterino
      type: user
    createdAt: '2023-09-12T19:57:37.000Z'
    data:
      edited: false
      editors:
      - Cmansterino
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9728723764419556
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0528ef00bf9ac8e46b059b67a60e1dfb.svg
          fullname: Cman Fire
          isHf: false
          isPro: false
          name: Cmansterino
          type: user
        html: '<p>Thanks for responding!<br>I am using NVIDIA and Windows so, I will
          check on the WebUI GIT as they probably have an example of the same error
          and troubleshooting.<br>I was very satisfied with AMD in the past and might
          consider them for my next GPU.</p>

          '
        raw: 'Thanks for responding!

          I am using NVIDIA and Windows so, I will check on the WebUI GIT as they
          probably have an example of the same error and troubleshooting.

          I was very satisfied with AMD in the past and might consider them for my
          next GPU.'
        updatedAt: '2023-09-12T19:57:37.989Z'
      numEdits: 0
      reactions: []
    id: 6500c2b19d5687f82553efd2
    type: comment
  author: Cmansterino
  content: 'Thanks for responding!

    I am using NVIDIA and Windows so, I will check on the WebUI GIT as they probably
    have an example of the same error and troubleshooting.

    I was very satisfied with AMD in the past and might consider them for my next
    GPU.'
  created_at: 2023-09-12 18:57:37+00:00
  edited: false
  hidden: false
  id: 6500c2b19d5687f82553efd2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/WizardLM-1.0-Uncensored-Llama2-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Error Log
