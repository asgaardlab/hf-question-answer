!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nlp-zhikai
conflicting_files: null
created_at: 2023-11-15 11:27:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b63c31be00e4a329592dc8a93c32611e.svg
      fullname: jiazhikai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nlp-zhikai
      type: user
    createdAt: '2023-11-15T11:27:56.000Z'
    data:
      edited: false
      editors:
      - nlp-zhikai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48024889826774597
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b63c31be00e4a329592dc8a93c32611e.svg
          fullname: jiazhikai
          isHf: false
          isPro: false
          name: nlp-zhikai
          type: user
        html: '<p>I used A100 * 8 and reported insufficient graphics memory.</p>

          <p>my code:<br>from transformers import AutoTokenizer, AutoModelForCausalLM<br>math_path
          = "./res/deepseek-coder-33b-base-transform/deepseek-coder-33b-base"<br>tokenizer
          = AutoTokenizer.from_pretrained(math_path, trust_remote_code=True)<br>model
          = AutoModelForCausalLM.from_pretrained(math_path, trust_remote_code=True,
          device_map="auto").cuda()<br>input_text = "#write a quick sort algorithm"<br>inputs
          = tokenizer(input_text, return_tensors="pt").to(model.device)<br>outputs
          = model.generate(**inputs, max_length=128)<br>print(tokenizer.decode(outputs[0],
          skip_special_tokens=True))</p>

          '
        raw: "I used A100 * 8 and reported insufficient graphics memory.\r\n\r\nmy\
          \ code:\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
          \nmath_path = \"./res/deepseek-coder-33b-base-transform/deepseek-coder-33b-base\"\
          \r\ntokenizer = AutoTokenizer.from_pretrained(math_path, trust_remote_code=True)\r\
          \nmodel = AutoModelForCausalLM.from_pretrained(math_path, trust_remote_code=True,\
          \ device_map=\"auto\").cuda()\r\ninput_text = \"#write a quick sort algorithm\"\
          \  \r\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\r\
          \noutputs = model.generate(**inputs, max_length=128)\r\nprint(tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))"
        updatedAt: '2023-11-15T11:27:56.296Z'
      numEdits: 0
      reactions: []
    id: 6554ab3c308eed1ce5f3337a
    type: comment
  author: nlp-zhikai
  content: "I used A100 * 8 and reported insufficient graphics memory.\r\n\r\nmy code:\r\
    \nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\nmath_path =\
    \ \"./res/deepseek-coder-33b-base-transform/deepseek-coder-33b-base\"\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(math_path, trust_remote_code=True)\r\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(math_path, trust_remote_code=True, device_map=\"\
    auto\").cuda()\r\ninput_text = \"#write a quick sort algorithm\"  \r\ninputs =\
    \ tokenizer(input_text, return_tensors=\"pt\").to(model.device)\r\noutputs = model.generate(**inputs,\
    \ max_length=128)\r\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
  created_at: 2023-11-15 11:27:56+00:00
  edited: false
  hidden: false
  id: 6554ab3c308eed1ce5f3337a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa16e73cb80d2e51187ac4f6cc9540df.svg
      fullname: light
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zenlight777
      type: user
    createdAt: '2023-11-21T04:36:02.000Z'
    data:
      edited: true
      editors:
      - zenlight777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5432816743850708
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa16e73cb80d2e51187ac4f6cc9540df.svg
          fullname: light
          isHf: false
          isPro: false
          name: zenlight777
          type: user
        html: "<p>I had the same issue with a 24GB VRAM RTX 4090 myself when running\
          \ the code from github.<br>It took me a few good hours of searching but\
          \ eventually found out their HuggingFace app source code and now it runs\
          \ smoothly on my server and results with just 7B version are impressive,\
          \ both quality and performance-wise, just like on the website.</p>\n<p>Github\
          \ page should really be updated to reflect the optimised settings, like\
          \ the torch_dtype=torch.bfloat16 and the ones for model.generate, like temperature,\
          \ etc.<br>I've created an issue there: <a rel=\"nofollow\" href=\"https://github.com/deepseek-ai/DeepSeek-Coder/issues/39\"\
          >https://github.com/deepseek-ai/DeepSeek-Coder/issues/39</a><br>Also it'd\
          \ be good to include in the doc info about Hardware Requirements for running\
          \ the 7B and 33B models respectively.<br>Like showing needed hardware for\
          \ running un-optimised and then showing various optimisations that can make\
          \ it fit in more modest setups together with the perceived quality loss.\
          \ That'd have been really nice to have when I was searching for it. I've\
          \ also seen several people asking on various forums(reddit, etc) that the\
          \ model is very slow.</p>\n<p>It is in Files section, here:<br><a href=\"\
          https://huggingface.co/spaces/deepseek-ai/deepseek-coder-7b-instruct/tree/main\"\
          >https://huggingface.co/spaces/deepseek-ai/deepseek-coder-7b-instruct/tree/main</a><br>And\
          \ the interesting bit is in app.py, method generate:</p>\n<pre><code>def\
          \ generate(\n    message: str,\n    chat_history: list[tuple[str, str]],\n\
          \    system_prompt: str,\n    max_new_tokens: int = 1024,\n    temperature:\
          \ float = 0.6,\n    top_p: float = 0.9,\n    top_k: int = 50,\n    repetition_penalty:\
          \ float = 1,\n) -&gt; Iterator[str]:\n... \nif torch.cuda.is_available():\n\
          \    model_id = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n    model\
          \ = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16,\
          \ device_map=\"auto\")\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n\
          \    tokenizer.use_default_system_prompt = False\n\nconversation.append({\"\
          role\": \"user\", \"content\": message})\n\n    input_ids = tokenizer.apply_chat_template(conversation,\
          \ return_tensors=\"pt\")\n    if input_ids.shape[1] &gt; MAX_INPUT_TOKEN_LENGTH:\n\
          \        input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n        gr.Warning(f\"\
          Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH}\
          \ tokens.\")\n    input_ids = input_ids.to(model.device)\n\n    streamer\
          \ = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n\
          \    generate_kwargs = dict(\n        {\"input_ids\": input_ids},\n    \
          \    streamer=streamer,\n        max_new_tokens=max_new_tokens,\n      \
          \  do_sample=False,\n        top_p=top_p,\n        top_k=top_k,\n      \
          \  num_beams=1,\n        # temperature=temperature,\n        repetition_penalty=repetition_penalty,\n\
          \        eos_token_id=32021\n    )\n    t = Thread(target=model.generate,\
          \ kwargs=generate_kwargs)\n    t.start()\n\n    outputs = []\n    for text\
          \ in streamer:\n        outputs.append(text)\n        yield \"\".join(outputs).replace(\"\
          &lt;|EOT|&gt;\",\"\")\n</code></pre>\n"
        raw: "I had the same issue with a 24GB VRAM RTX 4090 myself when running the\
          \ code from github. \nIt took me a few good hours of searching but eventually\
          \ found out their HuggingFace app source code and now it runs smoothly on\
          \ my server and results with just 7B version are impressive, both quality\
          \ and performance-wise, just like on the website.\n\nGithub page should\
          \ really be updated to reflect the optimised settings, like the torch_dtype=torch.bfloat16\
          \ and the ones for model.generate, like temperature, etc. \nI've created\
          \ an issue there: https://github.com/deepseek-ai/DeepSeek-Coder/issues/39\n\
          Also it'd be good to include in the doc info about Hardware Requirements\
          \ for running the 7B and 33B models respectively. \nLike showing needed\
          \ hardware for running un-optimised and then showing various optimisations\
          \ that can make it fit in more modest setups together with the perceived\
          \ quality loss. That'd have been really nice to have when I was searching\
          \ for it. I've also seen several people asking on various forums(reddit,\
          \ etc) that the model is very slow.\n\nIt is in Files section, here:\nhttps://huggingface.co/spaces/deepseek-ai/deepseek-coder-7b-instruct/tree/main\n\
          And the interesting bit is in app.py, method generate:\n\n```\ndef generate(\n\
          \    message: str,\n    chat_history: list[tuple[str, str]],\n    system_prompt:\
          \ str,\n    max_new_tokens: int = 1024,\n    temperature: float = 0.6,\n\
          \    top_p: float = 0.9,\n    top_k: int = 50,\n    repetition_penalty:\
          \ float = 1,\n) -> Iterator[str]:\n... \nif torch.cuda.is_available():\n\
          \    model_id = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n    model\
          \ = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16,\
          \ device_map=\"auto\")\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n\
          \    tokenizer.use_default_system_prompt = False\n\nconversation.append({\"\
          role\": \"user\", \"content\": message})\n\n    input_ids = tokenizer.apply_chat_template(conversation,\
          \ return_tensors=\"pt\")\n    if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n\
          \        input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n        gr.Warning(f\"\
          Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH}\
          \ tokens.\")\n    input_ids = input_ids.to(model.device)\n\n    streamer\
          \ = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n\
          \    generate_kwargs = dict(\n        {\"input_ids\": input_ids},\n    \
          \    streamer=streamer,\n        max_new_tokens=max_new_tokens,\n      \
          \  do_sample=False,\n        top_p=top_p,\n        top_k=top_k,\n      \
          \  num_beams=1,\n        # temperature=temperature,\n        repetition_penalty=repetition_penalty,\n\
          \        eos_token_id=32021\n    )\n    t = Thread(target=model.generate,\
          \ kwargs=generate_kwargs)\n    t.start()\n\n    outputs = []\n    for text\
          \ in streamer:\n        outputs.append(text)\n        yield \"\".join(outputs).replace(\"\
          <|EOT|>\",\"\")\n```"
        updatedAt: '2023-11-21T04:53:32.219Z'
      numEdits: 2
      reactions: []
    id: 655c33b213309a611ba40964
    type: comment
  author: zenlight777
  content: "I had the same issue with a 24GB VRAM RTX 4090 myself when running the\
    \ code from github. \nIt took me a few good hours of searching but eventually\
    \ found out their HuggingFace app source code and now it runs smoothly on my server\
    \ and results with just 7B version are impressive, both quality and performance-wise,\
    \ just like on the website.\n\nGithub page should really be updated to reflect\
    \ the optimised settings, like the torch_dtype=torch.bfloat16 and the ones for\
    \ model.generate, like temperature, etc. \nI've created an issue there: https://github.com/deepseek-ai/DeepSeek-Coder/issues/39\n\
    Also it'd be good to include in the doc info about Hardware Requirements for running\
    \ the 7B and 33B models respectively. \nLike showing needed hardware for running\
    \ un-optimised and then showing various optimisations that can make it fit in\
    \ more modest setups together with the perceived quality loss. That'd have been\
    \ really nice to have when I was searching for it. I've also seen several people\
    \ asking on various forums(reddit, etc) that the model is very slow.\n\nIt is\
    \ in Files section, here:\nhttps://huggingface.co/spaces/deepseek-ai/deepseek-coder-7b-instruct/tree/main\n\
    And the interesting bit is in app.py, method generate:\n\n```\ndef generate(\n\
    \    message: str,\n    chat_history: list[tuple[str, str]],\n    system_prompt:\
    \ str,\n    max_new_tokens: int = 1024,\n    temperature: float = 0.6,\n    top_p:\
    \ float = 0.9,\n    top_k: int = 50,\n    repetition_penalty: float = 1,\n) ->\
    \ Iterator[str]:\n... \nif torch.cuda.is_available():\n    model_id = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\
    \n    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16,\
    \ device_map=\"auto\")\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n\
    \    tokenizer.use_default_system_prompt = False\n\nconversation.append({\"role\"\
    : \"user\", \"content\": message})\n\n    input_ids = tokenizer.apply_chat_template(conversation,\
    \ return_tensors=\"pt\")\n    if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n\
    \        input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n        gr.Warning(f\"\
    Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH}\
    \ tokens.\")\n    input_ids = input_ids.to(model.device)\n\n    streamer = TextIteratorStreamer(tokenizer,\
    \ timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n    generate_kwargs\
    \ = dict(\n        {\"input_ids\": input_ids},\n        streamer=streamer,\n \
    \       max_new_tokens=max_new_tokens,\n        do_sample=False,\n        top_p=top_p,\n\
    \        top_k=top_k,\n        num_beams=1,\n        # temperature=temperature,\n\
    \        repetition_penalty=repetition_penalty,\n        eos_token_id=32021\n\
    \    )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\
    \n    outputs = []\n    for text in streamer:\n        outputs.append(text)\n\
    \        yield \"\".join(outputs).replace(\"<|EOT|>\",\"\")\n```"
  created_at: 2023-11-21 04:36:02+00:00
  edited: true
  hidden: false
  id: 655c33b213309a611ba40964
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: deepseek-ai/deepseek-coder-33b-base
repo_type: model
status: open
target_branch: null
title: How much  memory is needed?
