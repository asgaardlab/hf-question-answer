!!python/object:huggingface_hub.community.DiscussionWithDetails
author: eyya
conflicting_files: null
created_at: 2024-01-01 22:48:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bdfe702487edfb9b8fdec912b21eacf2.svg
      fullname: sectix
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eyya
      type: user
    createdAt: '2024-01-01T22:48:15.000Z'
    data:
      edited: false
      editors:
      - eyya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9630921483039856
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bdfe702487edfb9b8fdec912b21eacf2.svg
          fullname: sectix
          isHf: false
          isPro: false
          name: eyya
          type: user
        html: '<p>I''m only familiar enough with MoE to know that different experts
          help in different fields, but what benefit does making 8 experts all of
          the same model do?</p>

          '
        raw: I'm only familiar enough with MoE to know that different experts help
          in different fields, but what benefit does making 8 experts all of the same
          model do?
        updatedAt: '2024-01-01T22:48:15.137Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - stutteringp0et
    id: 6593412fccbc1e2cc74ca8e3
    type: comment
  author: eyya
  content: I'm only familiar enough with MoE to know that different experts help in
    different fields, but what benefit does making 8 experts all of the same model
    do?
  created_at: 2024-01-01 22:48:15+00:00
  edited: false
  hidden: false
  id: 6593412fccbc1e2cc74ca8e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2024-01-02T09:09:32.000Z'
    data:
      edited: false
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9389263987541199
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: '<p>it is under debate whether or not combining the same model into
          a mixture of experts actually enhances its reasoning ability or not.</p>

          <p>My argument is that it increases the reasoning ability, but not the knowledge,
          because you are adding more digital neurons that fire in parallel.</p>

          <p>But theres the argument that the weights are the same and produce the
          same results, therefore its not beneficial at all.</p>

          '
        raw: 'it is under debate whether or not combining the same model into a mixture
          of experts actually enhances its reasoning ability or not.


          My argument is that it increases the reasoning ability, but not the knowledge,
          because you are adding more digital neurons that fire in parallel.


          But theres the argument that the weights are the same and produce the same
          results, therefore its not beneficial at all.'
        updatedAt: '2024-01-02T09:09:32.528Z'
      numEdits: 0
      reactions: []
    id: 6593d2ccc27d210c3eff70cc
    type: comment
  author: dillfrescott
  content: 'it is under debate whether or not combining the same model into a mixture
    of experts actually enhances its reasoning ability or not.


    My argument is that it increases the reasoning ability, but not the knowledge,
    because you are adding more digital neurons that fire in parallel.


    But theres the argument that the weights are the same and produce the same results,
    therefore its not beneficial at all.'
  created_at: 2024-01-02 09:09:32+00:00
  edited: false
  hidden: false
  id: 6593d2ccc27d210c3eff70cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2024-01-02T09:10:23.000Z'
    data:
      edited: false
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9760183691978455
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: '<p>After some testing on Eric Hartfords server it seems to actually
          do something, it does improve things. Thats all I know right now.</p>

          '
        raw: After some testing on Eric Hartfords server it seems to actually do something,
          it does improve things. Thats all I know right now.
        updatedAt: '2024-01-02T09:10:23.799Z'
      numEdits: 0
      reactions: []
    id: 6593d2ffa41c3cbad50f5eee
    type: comment
  author: dillfrescott
  content: After some testing on Eric Hartfords server it seems to actually do something,
    it does improve things. Thats all I know right now.
  created_at: 2024-01-02 09:10:23+00:00
  edited: false
  hidden: false
  id: 6593d2ffa41c3cbad50f5eee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b55879f2858e648d5460fb/ZrkS_BmET_WIXZCS_q6Bv.png?w=200&h=200&f=face
      fullname: OlO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 010O11
      type: user
    createdAt: '2024-01-02T09:31:57.000Z'
    data:
      edited: false
      editors:
      - 010O11
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9636908769607544
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b55879f2858e648d5460fb/ZrkS_BmET_WIXZCS_q6Bv.png?w=200&h=200&f=face
          fullname: OlO
          isHf: false
          isPro: false
          name: 010O11
          type: user
        html: '<p>OK, but in that case, doesn''t 2, or max 3 model MoE system with
          the same model do the job? instead of 8x, it could be just 3x and save some
          VRAM...??? Or what am I missing? </p>

          '
        raw: 'OK, but in that case, doesn''t 2, or max 3 model MoE system with the
          same model do the job? instead of 8x, it could be just 3x and save some
          VRAM...??? Or what am I missing? '
        updatedAt: '2024-01-02T09:31:57.195Z'
      numEdits: 0
      reactions: []
    id: 6593d80d43971eed459055cb
    type: comment
  author: 010O11
  content: 'OK, but in that case, doesn''t 2, or max 3 model MoE system with the same
    model do the job? instead of 8x, it could be just 3x and save some VRAM...???
    Or what am I missing? '
  created_at: 2024-01-02 09:31:57+00:00
  edited: false
  hidden: false
  id: 6593d80d43971eed459055cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2024-01-02T09:35:02.000Z'
    data:
      edited: false
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9819360971450806
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: '<p>That does make sense. The reason I did 8 was because the mixtral
          architecture seems to work best with 8 agents, or so I''ve been told.</p>

          <p>I can always make a x2 or x4 version (x3 wont work with llama.cpp) and
          do some testing to compare.</p>

          '
        raw: 'That does make sense. The reason I did 8 was because the mixtral architecture
          seems to work best with 8 agents, or so I''ve been told.


          I can always make a x2 or x4 version (x3 wont work with llama.cpp) and do
          some testing to compare.'
        updatedAt: '2024-01-02T09:35:02.660Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - 010O11
    id: 6593d8c6ccbc1e2cc76a094e
    type: comment
  author: dillfrescott
  content: 'That does make sense. The reason I did 8 was because the mixtral architecture
    seems to work best with 8 agents, or so I''ve been told.


    I can always make a x2 or x4 version (x3 wont work with llama.cpp) and do some
    testing to compare.'
  created_at: 2024-01-02 09:35:02+00:00
  edited: false
  hidden: false
  id: 6593d8c6ccbc1e2cc76a094e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b55879f2858e648d5460fb/ZrkS_BmET_WIXZCS_q6Bv.png?w=200&h=200&f=face
      fullname: OlO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 010O11
      type: user
    createdAt: '2024-01-02T09:39:15.000Z'
    data:
      edited: false
      editors:
      - 010O11
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9458736181259155
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b55879f2858e648d5460fb/ZrkS_BmET_WIXZCS_q6Bv.png?w=200&h=200&f=face
          fullname: OlO
          isHf: false
          isPro: false
          name: 010O11
          type: user
        html: '<p>would be interesting if you do, thx 4 response</p>

          '
        raw: would be interesting if you do, thx 4 response
        updatedAt: '2024-01-02T09:39:15.288Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - dillfrescott
    id: 6593d9c3b72f4ce63bc4f31d
    type: comment
  author: 010O11
  content: would be interesting if you do, thx 4 response
  created_at: 2024-01-02 09:39:15+00:00
  edited: false
  hidden: false
  id: 6593d9c3b72f4ce63bc4f31d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8be96de1d60161f4efa40e778f75712e.svg
      fullname: Johnathan Corbano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: papercanteen111
      type: user
    createdAt: '2024-01-03T04:19:12.000Z'
    data:
      edited: false
      editors:
      - papercanteen111
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.959806501865387
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8be96de1d60161f4efa40e778f75712e.svg
          fullname: Johnathan Corbano
          isHf: false
          isPro: false
          name: papercanteen111
          type: user
        html: '<p>Would like to see a 2x version just as a point of comparison.  If
          that works, could be cheap gains and a new default approach.</p>

          '
        raw: Would like to see a 2x version just as a point of comparison.  If that
          works, could be cheap gains and a new default approach.
        updatedAt: '2024-01-03T04:19:12.114Z'
      numEdits: 0
      reactions: []
    id: 6594e0403ce574ff3c31f007
    type: comment
  author: papercanteen111
  content: Would like to see a 2x version just as a point of comparison.  If that
    works, could be cheap gains and a new default approach.
  created_at: 2024-01-03 04:19:12+00:00
  edited: false
  hidden: false
  id: 6594e0403ce574ff3c31f007
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: dillfrescott/sonya-7b-x8-MoE
repo_type: model
status: open
target_branch: null
title: MoE
