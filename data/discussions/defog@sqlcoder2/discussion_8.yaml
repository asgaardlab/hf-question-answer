!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ThisIsSoMe
conflicting_files: null
created_at: 2023-12-21 11:05:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df170a64d76a665cf34517a39e79865e.svg
      fullname: WuKun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThisIsSoMe
      type: user
    createdAt: '2023-12-21T11:05:24.000Z'
    data:
      edited: false
      editors:
      - ThisIsSoMe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8857312798500061
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df170a64d76a665cf34517a39e79865e.svg
          fullname: WuKun
          isHf: false
          isPro: false
          name: ThisIsSoMe
          type: user
        html: '<p>I run the demo case in the way same as the way in Colab. My GPU
          is one 40G A100 chip.And answering the case in Colab costs 10 minutes. Is
          that normal?</p>

          '
        raw: I run the demo case in the way same as the way in Colab. My GPU is one
          40G A100 chip.And answering the case in Colab costs 10 minutes. Is that
          normal?
        updatedAt: '2023-12-21T11:05:24.212Z'
      numEdits: 0
      reactions: []
    id: 65841bf436c450e02e004d88
    type: comment
  author: ThisIsSoMe
  content: I run the demo case in the way same as the way in Colab. My GPU is one
    40G A100 chip.And answering the case in Colab costs 10 minutes. Is that normal?
  created_at: 2023-12-21 11:05:24+00:00
  edited: false
  hidden: false
  id: 65841bf436c450e02e004d88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb44d0a95cd946b17a6a45e8a97142fc.svg
      fullname: Wong
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jp-defog
      type: user
    createdAt: '2023-12-29T05:32:13.000Z'
    data:
      edited: false
      editors:
      - jp-defog
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9560267329216003
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb44d0a95cd946b17a6a45e8a97142fc.svg
          fullname: Wong
          isHf: false
          isPro: false
          name: jp-defog
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ThisIsSoMe&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ThisIsSoMe\"\
          >@<span class=\"underline\">ThisIsSoMe</span></a></span>\n\n\t</span></span>\
          \ could this be because you're downloading the model for the first time\
          \ when running the script, causing it to take 10 minutes? How long do subsequent\
          \ requests take to complete? If you could share some sample scripts that\
          \ you ran with to reproduce your latency issue we can look into it further.</p>\n"
        raw: Hi @ThisIsSoMe could this be because you're downloading the model for
          the first time when running the script, causing it to take 10 minutes? How
          long do subsequent requests take to complete? If you could share some sample
          scripts that you ran with to reproduce your latency issue we can look into
          it further.
        updatedAt: '2023-12-29T05:32:13.216Z'
      numEdits: 0
      reactions: []
    id: 658e59dde7c71d6d9ebaeb1f
    type: comment
  author: jp-defog
  content: Hi @ThisIsSoMe could this be because you're downloading the model for the
    first time when running the script, causing it to take 10 minutes? How long do
    subsequent requests take to complete? If you could share some sample scripts that
    you ran with to reproduce your latency issue we can look into it further.
  created_at: 2023-12-29 05:32:13+00:00
  edited: false
  hidden: false
  id: 658e59dde7c71d6d9ebaeb1f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df170a64d76a665cf34517a39e79865e.svg
      fullname: WuKun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThisIsSoMe
      type: user
    createdAt: '2024-01-02T12:16:45.000Z'
    data:
      edited: false
      editors:
      - ThisIsSoMe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6041521430015564
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df170a64d76a665cf34517a39e79865e.svg
          fullname: WuKun
          isHf: false
          isPro: false
          name: ThisIsSoMe
          type: user
        html: "<p>codes:</p>\n<pre><code>import torch\nimport sqlparse\nfrom tqdm\
          \ import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          res = torch.cuda.is_available()\nprint(res)\nmodel_name = \"/root/paddlejob/workspace/env_run/sqlcoder2\"\
          \n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\
          model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n\
          \    torch_dtype=torch.float16,\n    # load_in_8bit=True,\n    # load_in_4bit=True,\n\
          \    device_map=\"auto\",\n    use_cache=True\n)\n#model = model.to(\"cuda\"\
          )\neos_token_id = tokenizer.eos_token_id\nprint(\"loaded model\")\n\nfor\
          \ i in tqdm(range(1)):\n  question = \"What is our total revenue by product\
          \ in the last week?\"\n  prompt = \"\"\"### Task\n  Generate a SQL query\
          \ to answer the following question:\n  `{question}`\n\n  ### Database Schema\n\
          \  This query will run on a database whose schema is represented in this\
          \ string:\n  CREATE TABLE products (\n    product_id INTEGER PRIMARY KEY,\
          \ -- Unique ID for each product\n    name VARCHAR(50), -- Name of the product\n\
          \    price DECIMAL(10,2), -- Price of each unit of the product\n    quantity\
          \ INTEGER  -- Current quantity in stock\n  );\n\n  CREATE TABLE customers\
          \ (\n    customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n\
          \    name VARCHAR(50), -- Name of the customer\n    address VARCHAR(100)\
          \ -- Mailing address of the customer\n  );\n\n  CREATE TABLE salespeople\
          \ (\n    salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson\n\
          \    name VARCHAR(50), -- Name of the salesperson\n    region VARCHAR(50)\
          \ -- Geographic sales region\n  );\n\n  CREATE TABLE sales (\n    sale_id\
          \ INTEGER PRIMARY KEY, -- Unique ID for each sale\n    product_id INTEGER,\
          \ -- ID of product sold\n    customer_id INTEGER,  -- ID of customer who\
          \ made purchase\n    salesperson_id INTEGER, -- ID of salesperson who made\
          \ the sale\n    sale_date DATE, -- Date the sale occurred\n    quantity\
          \ INTEGER -- Quantity of product sold\n  );\n\n  CREATE TABLE product_suppliers\
          \ (\n    supplier_id INTEGER PRIMARY KEY, -- Unique ID for each supplier\n\
          \    product_id INTEGER, -- Product ID supplied\n    supply_price DECIMAL(10,2)\
          \ -- Unit price charged by supplier\n  );\n\n  -- sales.product_id can be\
          \ joined with products.product_id\n  -- sales.customer_id can be joined\
          \ with customers.customer_id\n  -- sales.salesperson_id can be joined with\
          \ salespeople.salesperson_id\n  -- product_suppliers.product_id can be joined\
          \ with products.product_id\n\n  ### SQL\n  Given the database schema, here\
          \ is the SQL query that answers `{question}`:\n  \"\"\".format(question=question)\n\
          \n  inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n  #print(inputs)\n\
          \  generated_ids = model.generate(\n      **inputs,\n      num_return_sequences=1,\n\
          \      eos_token_id=eos_token_id,\n      pad_token_id=eos_token_id,\n  \
          \    max_new_tokens=400,\n      do_sample=False,\n      num_beams=1,\n\n\
          \  )\n  outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n\
          \  torch.cuda.empty_cache()\n  #torch.cuda.synchronize()\n  # empty cache\
          \ so that you do generate more results w/o memory crashing\n  # particularly\
          \ important on Colab \u2013 memory management is much more straightforward\n\
          \  # when running on an inference service\n  print(outputs[0])\n  print(sqlparse.format(outputs[0].split(\"\
          ```sql\")[-1], reindent=True))\n</code></pre>\n<p><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/63b7cc06e60862785afad981/5RWnqryaHZSgoo1VTSpxj.png\"\
          ><img alt=\"b16044ab29cc5a5360b96554e.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63b7cc06e60862785afad981/5RWnqryaHZSgoo1VTSpxj.png\"\
          ></a><br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63b7cc06e60862785afad981/_-13lHNYJvtcOjSJZ5hzm.png\"\
          ><img alt=\"369ef15c196a0c60e48b37352.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63b7cc06e60862785afad981/_-13lHNYJvtcOjSJZ5hzm.png\"\
          ></a></p>\n<p>maybe it takes 10+ minutes to complete.</p>\n"
        raw: "codes:\n```\nimport torch\nimport sqlparse\nfrom tqdm import tqdm\n\
          from transformers import AutoTokenizer, AutoModelForCausalLM\nres = torch.cuda.is_available()\n\
          print(res)\nmodel_name = \"/root/paddlejob/workspace/env_run/sqlcoder2\"\
          \n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\
          model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n\
          \    torch_dtype=torch.float16,\n    # load_in_8bit=True,\n    # load_in_4bit=True,\n\
          \    device_map=\"auto\",\n    use_cache=True\n)\n#model = model.to(\"cuda\"\
          )\neos_token_id = tokenizer.eos_token_id\nprint(\"loaded model\")\n\nfor\
          \ i in tqdm(range(1)):\n  question = \"What is our total revenue by product\
          \ in the last week?\"\n  prompt = \"\"\"### Task\n  Generate a SQL query\
          \ to answer the following question:\n  `{question}`\n\n  ### Database Schema\n\
          \  This query will run on a database whose schema is represented in this\
          \ string:\n  CREATE TABLE products (\n    product_id INTEGER PRIMARY KEY,\
          \ -- Unique ID for each product\n    name VARCHAR(50), -- Name of the product\n\
          \    price DECIMAL(10,2), -- Price of each unit of the product\n    quantity\
          \ INTEGER  -- Current quantity in stock\n  );\n\n  CREATE TABLE customers\
          \ (\n    customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n\
          \    name VARCHAR(50), -- Name of the customer\n    address VARCHAR(100)\
          \ -- Mailing address of the customer\n  );\n\n  CREATE TABLE salespeople\
          \ (\n    salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson\n\
          \    name VARCHAR(50), -- Name of the salesperson\n    region VARCHAR(50)\
          \ -- Geographic sales region\n  );\n\n  CREATE TABLE sales (\n    sale_id\
          \ INTEGER PRIMARY KEY, -- Unique ID for each sale\n    product_id INTEGER,\
          \ -- ID of product sold\n    customer_id INTEGER,  -- ID of customer who\
          \ made purchase\n    salesperson_id INTEGER, -- ID of salesperson who made\
          \ the sale\n    sale_date DATE, -- Date the sale occurred\n    quantity\
          \ INTEGER -- Quantity of product sold\n  );\n\n  CREATE TABLE product_suppliers\
          \ (\n    supplier_id INTEGER PRIMARY KEY, -- Unique ID for each supplier\n\
          \    product_id INTEGER, -- Product ID supplied\n    supply_price DECIMAL(10,2)\
          \ -- Unit price charged by supplier\n  );\n\n  -- sales.product_id can be\
          \ joined with products.product_id\n  -- sales.customer_id can be joined\
          \ with customers.customer_id\n  -- sales.salesperson_id can be joined with\
          \ salespeople.salesperson_id\n  -- product_suppliers.product_id can be joined\
          \ with products.product_id\n\n  ### SQL\n  Given the database schema, here\
          \ is the SQL query that answers `{question}`:\n  \"\"\".format(question=question)\n\
          \n  inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n  #print(inputs)\n\
          \  generated_ids = model.generate(\n      **inputs,\n      num_return_sequences=1,\n\
          \      eos_token_id=eos_token_id,\n      pad_token_id=eos_token_id,\n  \
          \    max_new_tokens=400,\n      do_sample=False,\n      num_beams=1,\n\n\
          \  )\n  outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n\
          \  torch.cuda.empty_cache()\n  #torch.cuda.synchronize()\n  # empty cache\
          \ so that you do generate more results w/o memory crashing\n  # particularly\
          \ important on Colab \u2013 memory management is much more straightforward\n\
          \  # when running on an inference service\n  print(outputs[0])\n  print(sqlparse.format(outputs[0].split(\"\
          ```sql\")[-1], reindent=True))\n```\n\n![b16044ab29cc5a5360b96554e.png](https://cdn-uploads.huggingface.co/production/uploads/63b7cc06e60862785afad981/5RWnqryaHZSgoo1VTSpxj.png)\n\
          ![369ef15c196a0c60e48b37352.png](https://cdn-uploads.huggingface.co/production/uploads/63b7cc06e60862785afad981/_-13lHNYJvtcOjSJZ5hzm.png)\n\
          \nmaybe it takes 10+ minutes to complete."
        updatedAt: '2024-01-02T12:16:45.471Z'
      numEdits: 0
      reactions: []
    id: 6593fead674349122cb83c59
    type: comment
  author: ThisIsSoMe
  content: "codes:\n```\nimport torch\nimport sqlparse\nfrom tqdm import tqdm\nfrom\
    \ transformers import AutoTokenizer, AutoModelForCausalLM\nres = torch.cuda.is_available()\n\
    print(res)\nmodel_name = \"/root/paddlejob/workspace/env_run/sqlcoder2\"\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel =\
    \ AutoModelForCausalLM.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n\
    \    torch_dtype=torch.float16,\n    # load_in_8bit=True,\n    # load_in_4bit=True,\n\
    \    device_map=\"auto\",\n    use_cache=True\n)\n#model = model.to(\"cuda\")\n\
    eos_token_id = tokenizer.eos_token_id\nprint(\"loaded model\")\n\nfor i in tqdm(range(1)):\n\
    \  question = \"What is our total revenue by product in the last week?\"\n  prompt\
    \ = \"\"\"### Task\n  Generate a SQL query to answer the following question:\n\
    \  `{question}`\n\n  ### Database Schema\n  This query will run on a database\
    \ whose schema is represented in this string:\n  CREATE TABLE products (\n   \
    \ product_id INTEGER PRIMARY KEY, -- Unique ID for each product\n    name VARCHAR(50),\
    \ -- Name of the product\n    price DECIMAL(10,2), -- Price of each unit of the\
    \ product\n    quantity INTEGER  -- Current quantity in stock\n  );\n\n  CREATE\
    \ TABLE customers (\n    customer_id INTEGER PRIMARY KEY, -- Unique ID for each\
    \ customer\n    name VARCHAR(50), -- Name of the customer\n    address VARCHAR(100)\
    \ -- Mailing address of the customer\n  );\n\n  CREATE TABLE salespeople (\n \
    \   salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson\n  \
    \  name VARCHAR(50), -- Name of the salesperson\n    region VARCHAR(50) -- Geographic\
    \ sales region\n  );\n\n  CREATE TABLE sales (\n    sale_id INTEGER PRIMARY KEY,\
    \ -- Unique ID for each sale\n    product_id INTEGER, -- ID of product sold\n\
    \    customer_id INTEGER,  -- ID of customer who made purchase\n    salesperson_id\
    \ INTEGER, -- ID of salesperson who made the sale\n    sale_date DATE, -- Date\
    \ the sale occurred\n    quantity INTEGER -- Quantity of product sold\n  );\n\n\
    \  CREATE TABLE product_suppliers (\n    supplier_id INTEGER PRIMARY KEY, -- Unique\
    \ ID for each supplier\n    product_id INTEGER, -- Product ID supplied\n    supply_price\
    \ DECIMAL(10,2) -- Unit price charged by supplier\n  );\n\n  -- sales.product_id\
    \ can be joined with products.product_id\n  -- sales.customer_id can be joined\
    \ with customers.customer_id\n  -- sales.salesperson_id can be joined with salespeople.salesperson_id\n\
    \  -- product_suppliers.product_id can be joined with products.product_id\n\n\
    \  ### SQL\n  Given the database schema, here is the SQL query that answers `{question}`:\n\
    \  \"\"\".format(question=question)\n\n  inputs = tokenizer(prompt, return_tensors=\"\
    pt\").to(\"cuda\")\n  #print(inputs)\n  generated_ids = model.generate(\n    \
    \  **inputs,\n      num_return_sequences=1,\n      eos_token_id=eos_token_id,\n\
    \      pad_token_id=eos_token_id,\n      max_new_tokens=400,\n      do_sample=False,\n\
    \      num_beams=1,\n\n  )\n  outputs = tokenizer.batch_decode(generated_ids,\
    \ skip_special_tokens=True)\n  torch.cuda.empty_cache()\n  #torch.cuda.synchronize()\n\
    \  # empty cache so that you do generate more results w/o memory crashing\n  #\
    \ particularly important on Colab \u2013 memory management is much more straightforward\n\
    \  # when running on an inference service\n  print(outputs[0])\n  print(sqlparse.format(outputs[0].split(\"\
    ```sql\")[-1], reindent=True))\n```\n\n![b16044ab29cc5a5360b96554e.png](https://cdn-uploads.huggingface.co/production/uploads/63b7cc06e60862785afad981/5RWnqryaHZSgoo1VTSpxj.png)\n\
    ![369ef15c196a0c60e48b37352.png](https://cdn-uploads.huggingface.co/production/uploads/63b7cc06e60862785afad981/_-13lHNYJvtcOjSJZ5hzm.png)\n\
    \nmaybe it takes 10+ minutes to complete."
  created_at: 2024-01-02 12:16:45+00:00
  edited: false
  hidden: false
  id: 6593fead674349122cb83c59
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb44d0a95cd946b17a6a45e8a97142fc.svg
      fullname: Wong
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jp-defog
      type: user
    createdAt: '2024-01-06T15:15:18.000Z'
    data:
      edited: true
      editors:
      - jp-defog
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9789264798164368
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb44d0a95cd946b17a6a45e8a97142fc.svg
          fullname: Wong
          isHf: false
          isPro: false
          name: jp-defog
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ThisIsSoMe&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ThisIsSoMe\"\
          >@<span class=\"underline\">ThisIsSoMe</span></a></span>\n\n\t</span></span>\
          \ , thanks for the code and screenshot. From the code and screenshot, it\
          \ seems to me that the part that is taking very long is the inference code\
          \ within tqdm. I'm guessing this might be due to your environment not using\
          \ the GPU. I saw the nvidia-smi print with 0MB memory used and 0% utilization\
          \ but wasn't sure if that was before or after the model was loaded. To rule\
          \ that out, could you print out the model device after loading it via <code>model.device</code>,\
          \ and check nvidia-smi while running inference? 10 mins sound like the approximate\
          \ time it would have taken on a cpu.</p>\n"
        raw: Hi @ThisIsSoMe , thanks for the code and screenshot. From the code and
          screenshot, it seems to me that the part that is taking very long is the
          inference code within tqdm. I'm guessing this might be due to your environment
          not using the GPU. I saw the nvidia-smi print with 0MB memory used and 0%
          utilization but wasn't sure if that was before or after the model was loaded.
          To rule that out, could you print out the model device after loading it
          via `model.device`, and check nvidia-smi while running inference? 10 mins
          sound like the approximate time it would have taken on a cpu.
        updatedAt: '2024-01-06T15:18:26.252Z'
      numEdits: 1
      reactions: []
    id: 65996e865f7a6d40f718edd0
    type: comment
  author: jp-defog
  content: Hi @ThisIsSoMe , thanks for the code and screenshot. From the code and
    screenshot, it seems to me that the part that is taking very long is the inference
    code within tqdm. I'm guessing this might be due to your environment not using
    the GPU. I saw the nvidia-smi print with 0MB memory used and 0% utilization but
    wasn't sure if that was before or after the model was loaded. To rule that out,
    could you print out the model device after loading it via `model.device`, and
    check nvidia-smi while running inference? 10 mins sound like the approximate time
    it would have taken on a cpu.
  created_at: 2024-01-06 15:15:18+00:00
  edited: true
  hidden: false
  id: 65996e865f7a6d40f718edd0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: defog/sqlcoder2
repo_type: model
status: open
target_branch: null
title: Inference Speed
