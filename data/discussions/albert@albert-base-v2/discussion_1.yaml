!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tianming
conflicting_files: null
created_at: 2022-07-13 05:52:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d54001bfef0fad9f21b7121555ee7a95.svg
      fullname: Du
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tianming
      type: user
    createdAt: '2022-07-13T06:52:48.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/d54001bfef0fad9f21b7121555ee7a95.svg
          fullname: Du
          isHf: false
          isPro: false
          name: Tianming
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-07-13T07:07:43.027Z'
      numEdits: 1
      reactions: []
    id: 62ce6bc0b5811ba53c0b9b0e
    type: comment
  author: Tianming
  content: This comment has been hidden
  created_at: 2022-07-13 05:52:48+00:00
  edited: true
  hidden: true
  id: 62ce6bc0b5811ba53c0b9b0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d54001bfef0fad9f21b7121555ee7a95.svg
      fullname: Du
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tianming
      type: user
    createdAt: '2022-07-13T07:07:16.000Z'
    data:
      status: closed
    id: 62ce6f246f925221afe12a77
    type: status-change
  author: Tianming
  created_at: 2022-07-13 06:07:16+00:00
  id: 62ce6f246f925221afe12a77
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/d54001bfef0fad9f21b7121555ee7a95.svg
      fullname: Du
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tianming
      type: user
    createdAt: '2022-07-13T07:07:52.000Z'
    data:
      from: The Pytorch source code of Albert seems not to implement "cross-layer
        parameter sharing", Could someone show me how the code implement it?
      to: Test
    id: 62ce6f486f925221afe12bd1
    type: title-change
  author: Tianming
  created_at: 2022-07-13 06:07:52+00:00
  id: 62ce6f486f925221afe12bd1
  new_title: Test
  old_title: The Pytorch source code of Albert seems not to implement "cross-layer
    parameter sharing", Could someone show me how the code implement it?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2022-07-13T07:11:11.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Tianming&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Tianming\"\
          >@<span class=\"underline\">Tianming</span></a></span>\n\n\t</span></span>,\
          \ the ALBERT implementation does implement cross-layer parameter sharing\
          \ exactly as the initial implementation did. The initial implementation\
          \ works in terms of layer groups: you can have 12 layers but 1 layer group,\
          \ which will mean one layer repeated 12 times, or you could have 12 layers\
          \ with 3 layer groups (so 3 different layers, repeated 4 times each).</p>\n\
          <p>If you take a look at the configuration file (<a href=\"https://huggingface.co/albert-base-v2/blob/main/config.json#L23\"\
          >https://huggingface.co/albert-base-v2/blob/main/config.json#L23</a>), you'll\
          \ see that there's a single layer group for this model (\"num_hidden_groups\"\
          : 1).</p>\n<p>Now if we take a look at the implementation, you'll see <a\
          \ rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/d4ebd4e112034b4a429ab7f813d7e168e7bb63c3/src/transformers/models/albert/modeling_albert.py#L477-L497\"\
          >the following</a>:</p>\n<p>The <code>layers_per_group</code> variable is\
          \ defined as follows:</p>\n<pre><code>layers_per_group = int(self.config.num_hidden_layers\
          \ / self.config.num_hidden_groups)\n</code></pre>\n<p>it will therefore\
          \ be equal to the number of hidden layers, so 12.</p>\n<p>The layer group\
          \ ID that will be used afterwards is defined here:</p>\n<pre><code>group_idx\
          \ = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))\n\
          </code></pre>\n<p>This will always be <code>0</code>: the <code>Transformer</code>\
          \ will only iterate through the first layer group, which is a single layer\
          \ repeated 12 times.</p>\n<p>Hope that helps!</p>\n"
        raw: 'Hey @Tianming, the ALBERT implementation does implement cross-layer
          parameter sharing exactly as the initial implementation did. The initial
          implementation works in terms of layer groups: you can have 12 layers but
          1 layer group, which will mean one layer repeated 12 times, or you could
          have 12 layers with 3 layer groups (so 3 different layers, repeated 4 times
          each).


          If you take a look at the configuration file (https://huggingface.co/albert-base-v2/blob/main/config.json#L23),
          you''ll see that there''s a single layer group for this model ("num_hidden_groups":
          1).


          Now if we take a look at the implementation, you''ll see [the following](https://github.com/huggingface/transformers/blob/d4ebd4e112034b4a429ab7f813d7e168e7bb63c3/src/transformers/models/albert/modeling_albert.py#L477-L497):


          The `layers_per_group` variable is defined as follows:

          ```

          layers_per_group = int(self.config.num_hidden_layers / self.config.num_hidden_groups)

          ```

          it will therefore be equal to the number of hidden layers, so 12.


          The layer group ID that will be used afterwards is defined here:

          ```

          group_idx = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))

          ```


          This will always be `0`: the `Transformer` will only iterate through the
          first layer group, which is a single layer repeated 12 times.


          Hope that helps!'
        updatedAt: '2022-07-13T07:11:11.298Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Tianming
    id: 62ce700fc770178a774581c1
    type: comment
  author: lysandre
  content: 'Hey @Tianming, the ALBERT implementation does implement cross-layer parameter
    sharing exactly as the initial implementation did. The initial implementation
    works in terms of layer groups: you can have 12 layers but 1 layer group, which
    will mean one layer repeated 12 times, or you could have 12 layers with 3 layer
    groups (so 3 different layers, repeated 4 times each).


    If you take a look at the configuration file (https://huggingface.co/albert-base-v2/blob/main/config.json#L23),
    you''ll see that there''s a single layer group for this model ("num_hidden_groups":
    1).


    Now if we take a look at the implementation, you''ll see [the following](https://github.com/huggingface/transformers/blob/d4ebd4e112034b4a429ab7f813d7e168e7bb63c3/src/transformers/models/albert/modeling_albert.py#L477-L497):


    The `layers_per_group` variable is defined as follows:

    ```

    layers_per_group = int(self.config.num_hidden_layers / self.config.num_hidden_groups)

    ```

    it will therefore be equal to the number of hidden layers, so 12.


    The layer group ID that will be used afterwards is defined here:

    ```

    group_idx = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))

    ```


    This will always be `0`: the `Transformer` will only iterate through the first
    layer group, which is a single layer repeated 12 times.


    Hope that helps!'
  created_at: 2022-07-13 06:11:11+00:00
  edited: false
  hidden: false
  id: 62ce700fc770178a774581c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/44f7b12187955aeb76ea01f44621061e.svg
      fullname: Tao Gong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gong
      type: user
    createdAt: '2022-07-19T03:31:37.000Z'
    data:
      edited: false
      editors:
      - Gong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/44f7b12187955aeb76ea01f44621061e.svg
          fullname: Tao Gong
          isHf: false
          isPro: false
          name: Gong
          type: user
        html: '<p>I use AutoModel and AutoConfig to load the model, use hungarface''s
          built-in trainer for training, after the training end of the saved model
          each time loading print parameters, the parameter values are different,
          and if you load the saved model again for training, the model will become
          invalid</p>

          '
        raw: I use AutoModel and AutoConfig to load the model, use hungarface's built-in
          trainer for training, after the training end of the saved model each time
          loading print parameters, the parameter values are different, and if you
          load the saved model again for training, the model will become invalid
        updatedAt: '2022-07-19T03:31:37.375Z'
      numEdits: 0
      reactions: []
    id: 62d62599701eebfa08b4469b
    type: comment
  author: Gong
  content: I use AutoModel and AutoConfig to load the model, use hungarface's built-in
    trainer for training, after the training end of the saved model each time loading
    print parameters, the parameter values are different, and if you load the saved
    model again for training, the model will become invalid
  created_at: 2022-07-19 02:31:37+00:00
  edited: false
  hidden: false
  id: 62d62599701eebfa08b4469b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2022-07-19T06:58:36.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Gong&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Gong\">@<span class=\"\
          underline\">Gong</span></a></span>\n\n\t</span></span>! Do you mind filling-in\
          \ the bug template <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/new?assignees=&amp;labels=bug&amp;template=bug-report.yml\"\
          >here</a> with some reproducible code? This will help us get to the root\
          \ of the problem with you. See you there!</p>\n"
        raw: Hey @Gong! Do you mind filling-in the bug template [here](https://github.com/huggingface/transformers/issues/new?assignees=&labels=bug&template=bug-report.yml)
          with some reproducible code? This will help us get to the root of the problem
          with you. See you there!
        updatedAt: '2022-07-19T06:58:36.197Z'
      numEdits: 0
      reactions: []
    id: 62d6561c6279202d95f69908
    type: comment
  author: lysandre
  content: Hey @Gong! Do you mind filling-in the bug template [here](https://github.com/huggingface/transformers/issues/new?assignees=&labels=bug&template=bug-report.yml)
    with some reproducible code? This will help us get to the root of the problem
    with you. See you there!
  created_at: 2022-07-19 05:58:36+00:00
  edited: false
  hidden: false
  id: 62d6561c6279202d95f69908
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2022-07-20T12:53:00.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: '<p>(you can also post your issue URL here for future reference)</p>

          '
        raw: (you can also post your issue URL here for future reference)
        updatedAt: '2022-07-20T12:53:00.506Z'
      numEdits: 0
      reactions: []
    id: 62d7faacf830483a731513b0
    type: comment
  author: julien-c
  content: (you can also post your issue URL here for future reference)
  created_at: 2022-07-20 11:53:00+00:00
  edited: false
  hidden: false
  id: 62d7faacf830483a731513b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d54001bfef0fad9f21b7121555ee7a95.svg
      fullname: Du
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tianming
      type: user
    createdAt: '2023-01-03T03:55:36.000Z'
    data:
      edited: true
      editors:
      - Tianming
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d54001bfef0fad9f21b7121555ee7a95.svg
          fullname: Du
          isHf: false
          isPro: false
          name: Tianming
          type: user
        html: "<blockquote>\n<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Tianming&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Tianming\"\
          >@<span class=\"underline\">Tianming</span></a></span>\n\n\t</span></span>,\
          \ the ALBERT implementation does implement cross-layer parameter sharing\
          \ exactly as the initial implementation did. The initial implementation\
          \ works in terms of layer groups: you can have 12 layers but 1 layer group,\
          \ which will mean one layer repeated 12 times, or you could have 12 layers\
          \ with 3 layer groups (so 3 different layers, repeated 4 times each).</p>\n\
          <p>If you take a look at the configuration file (<a href=\"https://huggingface.co/albert-base-v2/blob/main/config.json#L23\"\
          >https://huggingface.co/albert-base-v2/blob/main/config.json#L23</a>), you'll\
          \ see that there's a single layer group for this model (\"num_hidden_groups\"\
          : 1).</p>\n<p>Now if we take a look at the implementation, you'll see <a\
          \ rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/d4ebd4e112034b4a429ab7f813d7e168e7bb63c3/src/transformers/models/albert/modeling_albert.py#L477-L497\"\
          >the following</a>:</p>\n<p>The <code>layers_per_group</code> variable is\
          \ defined as follows:</p>\n<pre><code>layers_per_group = int(self.config.num_hidden_layers\
          \ / self.config.num_hidden_groups)\n</code></pre>\n<p>it will therefore\
          \ be equal to the number of hidden layers, so 12.</p>\n<p>The layer group\
          \ ID that will be used afterwards is defined here:</p>\n<pre><code>group_idx\
          \ = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))\n\
          </code></pre>\n<p>This will always be <code>0</code>: the <code>Transformer</code>\
          \ will only iterate through the first layer group, which is a single layer\
          \ repeated 12 times.</p>\n<p>Hope that helps!</p>\n</blockquote>\n<p>Thanks.</p>\n"
        raw: "> Hey @Tianming, the ALBERT implementation does implement cross-layer\
          \ parameter sharing exactly as the initial implementation did. The initial\
          \ implementation works in terms of layer groups: you can have 12 layers\
          \ but 1 layer group, which will mean one layer repeated 12 times, or you\
          \ could have 12 layers with 3 layer groups (so 3 different layers, repeated\
          \ 4 times each).\n> \n> If you take a look at the configuration file (https://huggingface.co/albert-base-v2/blob/main/config.json#L23),\
          \ you'll see that there's a single layer group for this model (\"num_hidden_groups\"\
          : 1).\n> \n> Now if we take a look at the implementation, you'll see [the\
          \ following](https://github.com/huggingface/transformers/blob/d4ebd4e112034b4a429ab7f813d7e168e7bb63c3/src/transformers/models/albert/modeling_albert.py#L477-L497):\n\
          > \n> The `layers_per_group` variable is defined as follows:\n> ```\n> layers_per_group\
          \ = int(self.config.num_hidden_layers / self.config.num_hidden_groups)\n\
          > ```\n> it will therefore be equal to the number of hidden layers, so 12.\n\
          > \n> The layer group ID that will be used afterwards is defined here:\n\
          > ```\n> group_idx = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))\n\
          > ```\n> \n> This will always be `0`: the `Transformer` will only iterate\
          \ through the first layer group, which is a single layer repeated 12 times.\n\
          > \n> Hope that helps!\n\n\nThanks."
        updatedAt: '2023-01-03T03:55:55.440Z'
      numEdits: 1
      reactions: []
    id: 63b3a73854211fcd063396f9
    type: comment
  author: Tianming
  content: "> Hey @Tianming, the ALBERT implementation does implement cross-layer\
    \ parameter sharing exactly as the initial implementation did. The initial implementation\
    \ works in terms of layer groups: you can have 12 layers but 1 layer group, which\
    \ will mean one layer repeated 12 times, or you could have 12 layers with 3 layer\
    \ groups (so 3 different layers, repeated 4 times each).\n> \n> If you take a\
    \ look at the configuration file (https://huggingface.co/albert-base-v2/blob/main/config.json#L23),\
    \ you'll see that there's a single layer group for this model (\"num_hidden_groups\"\
    : 1).\n> \n> Now if we take a look at the implementation, you'll see [the following](https://github.com/huggingface/transformers/blob/d4ebd4e112034b4a429ab7f813d7e168e7bb63c3/src/transformers/models/albert/modeling_albert.py#L477-L497):\n\
    > \n> The `layers_per_group` variable is defined as follows:\n> ```\n> layers_per_group\
    \ = int(self.config.num_hidden_layers / self.config.num_hidden_groups)\n> ```\n\
    > it will therefore be equal to the number of hidden layers, so 12.\n> \n> The\
    \ layer group ID that will be used afterwards is defined here:\n> ```\n> group_idx\
    \ = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))\n\
    > ```\n> \n> This will always be `0`: the `Transformer` will only iterate through\
    \ the first layer group, which is a single layer repeated 12 times.\n> \n> Hope\
    \ that helps!\n\n\nThanks."
  created_at: 2023-01-03 03:55:36+00:00
  edited: true
  hidden: false
  id: 63b3a73854211fcd063396f9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: albert/albert-base-v2
repo_type: model
status: closed
target_branch: null
title: Test
