!!python/object:huggingface_hub.community.DiscussionWithDetails
author: OrangeApples
conflicting_files: null
created_at: 2023-11-30 17:14:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6rJmMwNSqswKhvckak5vn.jpeg?w=200&h=200&f=face
      fullname: Orange Apples
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OrangeApples
      type: user
    createdAt: '2023-11-30T17:14:05.000Z'
    data:
      edited: false
      editors:
      - OrangeApples
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9272799491882324
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6rJmMwNSqswKhvckak5vn.jpeg?w=200&h=200&f=face
          fullname: Orange Apples
          isHf: false
          isPro: false
          name: OrangeApples
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jebcarter&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jebcarter\">@<span class=\"\
          underline\">jebcarter</span></a></span>\n\n\t</span></span> have you tried\
          \ royallab/PsyOrca2-13b-DARE? Was wondering if you had any thoughts on it\
          \ since it merges the same two models together albeit in a different way\
          \ (and being 13B instead of 20B like yours).</p>\n"
        raw: '@jebcarter have you tried royallab/PsyOrca2-13b-DARE? Was wondering
          if you had any thoughts on it since it merges the same two models together
          albeit in a different way (and being 13B instead of 20B like yours).'
        updatedAt: '2023-11-30T17:14:05.006Z'
      numEdits: 0
      reactions: []
    id: 6568c2dd4527e9d1ff8d127f
    type: comment
  author: OrangeApples
  content: '@jebcarter have you tried royallab/PsyOrca2-13b-DARE? Was wondering if
    you had any thoughts on it since it merges the same two models together albeit
    in a different way (and being 13B instead of 20B like yours).'
  created_at: 2023-11-30 17:14:05+00:00
  edited: false
  hidden: false
  id: 6568c2dd4527e9d1ff8d127f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3ad668e2748e02dddb61b8980c87da4.svg
      fullname: Jeb Carter
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jebcarter
      type: user
    createdAt: '2023-11-30T18:28:07.000Z'
    data:
      edited: false
      editors:
      - jebcarter
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.948529839515686
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3ad668e2748e02dddb61b8980c87da4.svg
          fullname: Jeb Carter
          isHf: false
          isPro: false
          name: jebcarter
          type: user
        html: "<p>Howdy there <span data-props=\"{&quot;user&quot;:&quot;OrangeApples&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/OrangeApples\"\
          >@<span class=\"underline\">OrangeApples</span></a></span>\n\n\t</span></span>\
          \ -</p>\n<p>I\u2019ve tried PsyOrca2-13b\u2019s DARE merge - I think it\
          \ goes in the right direction but my collaborator and I are working on a\
          \ different recipe that brings in Orca2 a little heavier. The logic boost\
          \ that comes with a 20B stack won\u2019t be present, of course, but the\
          \ attentiveness and writing style should be able to be brought in.</p>\n\
          <p>I\u2019d rather have a 13B than a 20B since they are much more accessible\
          \ to run, of course. :) </p>\n<p>Overall, I\u2019m excited to see Orca2\
          \ propagate out into the merging spaces - it\u2019s good fuel, even if the\
          \ base writing is as (expectedly) dry as it is. </p>\n"
        raw: "Howdy there @OrangeApples -\n\nI\u2019ve tried PsyOrca2-13b\u2019s DARE\
          \ merge - I think it goes in the right direction but my collaborator and\
          \ I are working on a different recipe that brings in Orca2 a little heavier.\
          \ The logic boost that comes with a 20B stack won\u2019t be present, of\
          \ course, but the attentiveness and writing style should be able to be brought\
          \ in.\n\nI\u2019d rather have a 13B than a 20B since they are much more\
          \ accessible to run, of course. :) \n\nOverall, I\u2019m excited to see\
          \ Orca2 propagate out into the merging spaces - it\u2019s good fuel, even\
          \ if the base writing is as (expectedly) dry as it is. "
        updatedAt: '2023-11-30T18:28:07.178Z'
      numEdits: 0
      reactions: []
    id: 6568d4374e2f694cf57ae0c4
    type: comment
  author: jebcarter
  content: "Howdy there @OrangeApples -\n\nI\u2019ve tried PsyOrca2-13b\u2019s DARE\
    \ merge - I think it goes in the right direction but my collaborator and I are\
    \ working on a different recipe that brings in Orca2 a little heavier. The logic\
    \ boost that comes with a 20B stack won\u2019t be present, of course, but the\
    \ attentiveness and writing style should be able to be brought in.\n\nI\u2019\
    d rather have a 13B than a 20B since they are much more accessible to run, of\
    \ course. :) \n\nOverall, I\u2019m excited to see Orca2 propagate out into the\
    \ merging spaces - it\u2019s good fuel, even if the base writing is as (expectedly)\
    \ dry as it is. "
  created_at: 2023-11-30 18:28:07+00:00
  edited: false
  hidden: false
  id: 6568d4374e2f694cf57ae0c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6rJmMwNSqswKhvckak5vn.jpeg?w=200&h=200&f=face
      fullname: Orange Apples
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OrangeApples
      type: user
    createdAt: '2023-12-01T16:26:38.000Z'
    data:
      edited: true
      editors:
      - OrangeApples
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9404832720756531
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6rJmMwNSqswKhvckak5vn.jpeg?w=200&h=200&f=face
          fullname: Orange Apples
          isHf: false
          isPro: false
          name: OrangeApples
          type: user
        html: "<p>Thanks for the complete answer, <span data-props=\"{&quot;user&quot;:&quot;jebcarter&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jebcarter\"\
          >@<span class=\"underline\">jebcarter</span></a></span>\n\n\t</span></span>!\
          \ 13Bs are definitely more accessible! I noticed that 20Bs tend to use an\
          \ ungodly amount of kvcache in vram compared to others. Looking forward\
          \ to trying out your new recipe for a heavier Orca2 merge once it's out.\
          \ :)</p>\n<p>Edit: Some have noticed that some DARE ties models require\
          \ exactly 4096 context size to use. My above observation may not be a 20B\
          \ issue but a DARE one.</p>\n"
        raw: 'Thanks for the complete answer, @jebcarter! 13Bs are definitely more
          accessible! I noticed that 20Bs tend to use an ungodly amount of kvcache
          in vram compared to others. Looking forward to trying out your new recipe
          for a heavier Orca2 merge once it''s out. :)


          Edit: Some have noticed that some DARE ties models require exactly 4096
          context size to use. My above observation may not be a 20B issue but a DARE
          one.'
        updatedAt: '2023-12-02T05:03:45.515Z'
      numEdits: 1
      reactions: []
    id: 656a093eedd446c42becd7f0
    type: comment
  author: OrangeApples
  content: 'Thanks for the complete answer, @jebcarter! 13Bs are definitely more accessible!
    I noticed that 20Bs tend to use an ungodly amount of kvcache in vram compared
    to others. Looking forward to trying out your new recipe for a heavier Orca2 merge
    once it''s out. :)


    Edit: Some have noticed that some DARE ties models require exactly 4096 context
    size to use. My above observation may not be a 20B issue but a DARE one.'
  created_at: 2023-12-01 16:26:38+00:00
  edited: true
  hidden: false
  id: 656a093eedd446c42becd7f0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jebcarter/psyonic-cetacean-20B
repo_type: model
status: open
target_branch: null
title: This model vs PsyOrca2
