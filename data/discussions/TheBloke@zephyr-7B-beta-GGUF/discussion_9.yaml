!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GidiGumDrop
conflicting_files: null
created_at: 2024-01-03 12:38:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d70a3e35e1dffe00803530422938fd91.svg
      fullname: Gideon Weiss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GidiGumDrop
      type: user
    createdAt: '2024-01-03T12:38:13.000Z'
    data:
      edited: false
      editors:
      - GidiGumDrop
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9291943907737732
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d70a3e35e1dffe00803530422938fd91.svg
          fullname: Gideon Weiss
          isHf: false
          isPro: false
          name: GidiGumDrop
          type: user
        html: '<p>Hi, </p>

          <p>I''ve got a ChromaDB filled with data, everytime I prompt the model passing
          some of that data as context, the first time generating a response is slow
          and thereafter it is fast (15 seconds vs 1 second). Even when changing the
          question being asked, it still remains fast, however inference time slows
          back to 15s when the context from ChromaDB changes. However, going back
          to the first prompt context after switching context slows inference back
          down again. I assume this some sort of caching that is taking place on the
          data from ChromaDB, but can someone explain the inner details in more detail
          and provide a way to keep inference time low even when changing context
          (some sort of preprocessing maybe?).</p>

          <p>Thanks</p>

          '
        raw: "Hi, \r\n\r\nI've got a ChromaDB filled with data, everytime I prompt\
          \ the model passing some of that data as context, the first time generating\
          \ a response is slow and thereafter it is fast (15 seconds vs 1 second).\
          \ Even when changing the question being asked, it still remains fast, however\
          \ inference time slows back to 15s when the context from ChromaDB changes.\
          \ However, going back to the first prompt context after switching context\
          \ slows inference back down again. I assume this some sort of caching that\
          \ is taking place on the data from ChromaDB, but can someone explain the\
          \ inner details in more detail and provide a way to keep inference time\
          \ low even when changing context (some sort of preprocessing maybe?).\r\n\
          \r\nThanks"
        updatedAt: '2024-01-03T12:38:13.818Z'
      numEdits: 0
      reactions: []
    id: 65955535a73d50be78eefd70
    type: comment
  author: GidiGumDrop
  content: "Hi, \r\n\r\nI've got a ChromaDB filled with data, everytime I prompt the\
    \ model passing some of that data as context, the first time generating a response\
    \ is slow and thereafter it is fast (15 seconds vs 1 second). Even when changing\
    \ the question being asked, it still remains fast, however inference time slows\
    \ back to 15s when the context from ChromaDB changes. However, going back to the\
    \ first prompt context after switching context slows inference back down again.\
    \ I assume this some sort of caching that is taking place on the data from ChromaDB,\
    \ but can someone explain the inner details in more detail and provide a way to\
    \ keep inference time low even when changing context (some sort of preprocessing\
    \ maybe?).\r\n\r\nThanks"
  created_at: 2024-01-03 12:38:13+00:00
  edited: false
  hidden: false
  id: 65955535a73d50be78eefd70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2024-01-03T16:16:15.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8455443978309631
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;GidiGumDrop&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/GidiGumDrop\"\
          >@<span class=\"underline\">GidiGumDrop</span></a></span>\n\n\t</span></span>\
          \ langchain uses llama.cpp to load the model. Llama.cpp uses blas which\
          \ does have a slower prompt processing time then other libraries.</p>\n\
          <p>putting all of the layers on gpu and installing llama.cpp with cublas\
          \ should make the prompt processing time much faster(still slightly slower\
          \ then some other libraries)</p>\n<p>cpu will take a much longer time and\
          \ theres really no way to avoid it</p>\n<p>If you are using cpu, langchain\
          \ with llama.cpp is your best bet<br>If you can fully load the model on\
          \ gpu then using exllamav2 will be fastest(llama.cpp is close) but i dont\
          \ think langchain supports exllamav2</p>\n"
        raw: '@GidiGumDrop langchain uses llama.cpp to load the model. Llama.cpp uses
          blas which does have a slower prompt processing time then other libraries.


          putting all of the layers on gpu and installing llama.cpp with cublas should
          make the prompt processing time much faster(still slightly slower then some
          other libraries)


          cpu will take a much longer time and theres really no way to avoid it


          If you are using cpu, langchain with llama.cpp is your best bet

          If you can fully load the model on gpu then using exllamav2 will be fastest(llama.cpp
          is close) but i dont think langchain supports exllamav2'
        updatedAt: '2024-01-03T16:16:15.636Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - GidiGumDrop
    id: 6595884f573369a3e6bcc9f3
    type: comment
  author: YaTharThShaRma999
  content: '@GidiGumDrop langchain uses llama.cpp to load the model. Llama.cpp uses
    blas which does have a slower prompt processing time then other libraries.


    putting all of the layers on gpu and installing llama.cpp with cublas should make
    the prompt processing time much faster(still slightly slower then some other libraries)


    cpu will take a much longer time and theres really no way to avoid it


    If you are using cpu, langchain with llama.cpp is your best bet

    If you can fully load the model on gpu then using exllamav2 will be fastest(llama.cpp
    is close) but i dont think langchain supports exllamav2'
  created_at: 2024-01-03 16:16:15+00:00
  edited: false
  hidden: false
  id: 6595884f573369a3e6bcc9f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d70a3e35e1dffe00803530422938fd91.svg
      fullname: Gideon Weiss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GidiGumDrop
      type: user
    createdAt: '2024-01-05T10:50:18.000Z'
    data:
      status: closed
    id: 6597deea9608f3f8764ed40b
    type: status-change
  author: GidiGumDrop
  created_at: 2024-01-05 10:50:18+00:00
  id: 6597deea9608f3f8764ed40b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: TheBloke/zephyr-7B-beta-GGUF
repo_type: model
status: closed
target_branch: null
title: Slow inference on first prompt and fast thereafter using ChromaDB and Langchain
