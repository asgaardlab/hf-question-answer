!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alexcardo
conflicting_files: null
created_at: 2023-12-11 15:43:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58604ab5a083bb6e5d401d8d578b9c01.svg
      fullname: Alex Cardo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexcardo
      type: user
    createdAt: '2023-12-11T15:43:44.000Z'
    data:
      edited: true
      editors:
      - alexcardo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9711450338363647
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58604ab5a083bb6e5d401d8d578b9c01.svg
          fullname: Alex Cardo
          isHf: false
          isPro: false
          name: alexcardo
          type: user
        html: '<p>I''m shocked a bit. I''m working with this model every day. Surprisingly,
          I discovered the better quantization is the worse answer I get. </p>

          <p>My general purpose is the assistance in writing articles. The Q4_K_M
          copes with every task while any other quantization fails on the same tasks
          with the same parameters. Can anyone explain it to me? </p>

          <p>I don''t claim there is an error in quantization process, however, it
          looks completely impossible that higher quality models perform much worser.
          </p>

          <p>I can''t run the models above Q4_K_M on my 8GB MacBook M1, that''s why
          I''m using this model. However, I was able to run the superior  quantized
          versions of this model on CPU on my dedicated server, and all the experiments
          since the moment of model''s release concluded without results. </p>

          <p>I''m confused as there should be some explanation. I can''t provide here
          any example as I need thereby to publish big articles containing 1500-1800
          words for comparison. But you can check this out by yourself.</p>

          '
        raw: "I'm shocked a bit. I'm working with this model every day. Surprisingly,\
          \ I discovered the better quantization is the worse answer I get. \n\nMy\
          \ general purpose is the assistance in writing articles. The Q4_K_M copes\
          \ with every task while any other quantization fails on the same tasks with\
          \ the same parameters. Can anyone explain it to me? \n\nI don't claim there\
          \ is an error in quantization process, however, it looks completely impossible\
          \ that higher quality models perform much worser. \n\nI can't run the models\
          \ above Q4_K_M on my 8GB MacBook M1, that's why I'm using this model. However,\
          \ I was able to run the superior  quantized versions of this model on CPU\
          \ on my dedicated server, and all the experiments since the moment of model's\
          \ release concluded without results. \n\nI'm confused as there should be\
          \ some explanation. I can't provide here any example as I need thereby to\
          \ publish big articles containing 1500-1800 words for comparison. But you\
          \ can check this out by yourself."
        updatedAt: '2023-12-11T15:44:16.115Z'
      numEdits: 1
      reactions: []
    id: 65772e30f1a24205977d8fe4
    type: comment
  author: alexcardo
  content: "I'm shocked a bit. I'm working with this model every day. Surprisingly,\
    \ I discovered the better quantization is the worse answer I get. \n\nMy general\
    \ purpose is the assistance in writing articles. The Q4_K_M copes with every task\
    \ while any other quantization fails on the same tasks with the same parameters.\
    \ Can anyone explain it to me? \n\nI don't claim there is an error in quantization\
    \ process, however, it looks completely impossible that higher quality models\
    \ perform much worser. \n\nI can't run the models above Q4_K_M on my 8GB MacBook\
    \ M1, that's why I'm using this model. However, I was able to run the superior\
    \  quantized versions of this model on CPU on my dedicated server, and all the\
    \ experiments since the moment of model's release concluded without results. \n\
    \nI'm confused as there should be some explanation. I can't provide here any example\
    \ as I need thereby to publish big articles containing 1500-1800 words for comparison.\
    \ But you can check this out by yourself."
  created_at: 2023-12-11 15:43:44+00:00
  edited: true
  hidden: false
  id: 65772e30f1a24205977d8fe4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-12-11T15:50:08.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9262150526046753
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>Well q4 will not be worse in everything and might be slightly better
          then higher quantizations. However, higher quantization will be generally
          better then q4 and almost for everything higher quantization will be better</p>

          '
        raw: Well q4 will not be worse in everything and might be slightly better
          then higher quantizations. However, higher quantization will be generally
          better then q4 and almost for everything higher quantization will be better
        updatedAt: '2023-12-11T15:50:08.582Z'
      numEdits: 0
      reactions: []
    id: 65772fb0f9cd531693da45ec
    type: comment
  author: YaTharThShaRma999
  content: Well q4 will not be worse in everything and might be slightly better then
    higher quantizations. However, higher quantization will be generally better then
    q4 and almost for everything higher quantization will be better
  created_at: 2023-12-11 15:50:08+00:00
  edited: false
  hidden: false
  id: 65772fb0f9cd531693da45ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/58604ab5a083bb6e5d401d8d578b9c01.svg
      fullname: Alex Cardo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexcardo
      type: user
    createdAt: '2023-12-11T16:13:15.000Z'
    data:
      from: How it's possible that Q4_K_M performs better than any Q5, Q6 and even
        Q8?
      to: How is it possible that Q4_K_M performs better than any Q5, Q6 and even
        Q8?
    id: 6577351b8e449026ae3db13b
    type: title-change
  author: alexcardo
  created_at: 2023-12-11 16:13:15+00:00
  id: 6577351b8e449026ae3db13b
  new_title: How is it possible that Q4_K_M performs better than any Q5, Q6 and even
    Q8?
  old_title: How it's possible that Q4_K_M performs better than any Q5, Q6 and even
    Q8?
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/zephyr-7B-beta-GGUF
repo_type: model
status: open
target_branch: null
title: How is it possible that Q4_K_M performs better than any Q5, Q6 and even Q8?
