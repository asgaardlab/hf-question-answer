!!python/object:huggingface_hub.community.DiscussionWithDetails
author: shivammehta
conflicting_files: null
created_at: 2023-11-17 12:05:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e441d2ae599b51824fb9db8d2a89fff.svg
      fullname: mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shivammehta
      type: user
    createdAt: '2023-11-17T12:05:46.000Z'
    data:
      edited: false
      editors:
      - shivammehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8023830652236938
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e441d2ae599b51824fb9db8d2a89fff.svg
          fullname: mehta
          isHf: false
          isPro: false
          name: shivammehta
          type: user
        html: "<p>When experimenting with this model, I've observed occasional discrepancies\
          \ in its output. Sometimes it provides the correct response, and sometimes\
          \ times it doesn't, even when presented with the same or similar questions.\
          \ I have two inquiries: Why does this occur, and how can we address this\
          \ issue?<br>The output of Agent goes into an infinite loop with the LLM\
          \ not making changes to its reasoning as can<br>be seen in the highlighted\
          \ block\u2026 this block keeps repeating till the agent runs out of iterations\
          \ and<br>hence does not arrive at the final answer.</p>\n<p>Code -<br>from\
          \ huggingface_hub import hf_hub_download<br>from langchain.llms import LlamaCpp<br>from\
          \ langchain.agents import create_csv_agent</p>\n<p>MODEL_ID = \"TheBloke/zephyr-7B-beta-GGUF\"\
          <br>MODEL_BASENAME = \"zephyr-7b-beta.Q4_K_M.gguf\"</p>\n<p>CONTEXT_WINDOW_SIZE\
          \ = 4096<br>MAX_NEW_TOKENS = 1024</p>\n<p>model_path = hf_hub_download(<br>\
          \            repo_id=MODEL_ID,<br>            filename=MODEL_BASENAME,<br>\
          \            resume_download=True,<br>            cache_dir=\"./models\"\
          ,<br>        )<br>kwargs = {<br>            \"model_path\": model_path,<br>\
          \            \"n_ctx\": CONTEXT_WINDOW_SIZE,<br>            \"max_tokens\"\
          : MAX_NEW_TOKENS,<br>            \"n_gpu_layers\":4<br>        }<br>llm\
          \ = LlamaCpp(<br>    model_path=model_path,<br>    temperature=0.1,<br>\
          \    n_ctx=4096,<br>    max_tokens=1024,<br>    n_batch=100,<br>    top_p=1,<br>\
          \    verbose=True,<br>    n_gpu_layers=100)</p>\n<p>agent = create_csv_agent(llm,\
          \ ['./Data/Employees.csv','./Data/Verticals.csv'], verbose=True)<br>response\
          \ = agent.run(\"Which vertical name has the most number of resignations\"\
          )<br>print(response)</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6453a499fc2b5f69e8fb0fde/kiDZJYFug-PJY19xFiGQX.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6453a499fc2b5f69e8fb0fde/kiDZJYFug-PJY19xFiGQX.png\"\
          ></a></p>\n<p>QUERY:</p>\n<ol>\n<li>How to correct such reasoning of the\
          \ LLM such that the LLM reasons out that its actions are<br>being repeated\
          \ but not helping at arriving at the right answer? Does the AgentExecutor\
          \ need<br>to be corrected in this case ? If yes, how and what needs to be\
          \ done?</li>\n</ol>\n"
        raw: "When experimenting with this model, I've observed occasional discrepancies\
          \ in its output. Sometimes it provides the correct response, and sometimes\
          \ times it doesn't, even when presented with the same or similar questions.\
          \ I have two inquiries: Why does this occur, and how can we address this\
          \ issue?\r\nThe output of Agent goes into an infinite loop with the LLM\
          \ not making changes to its reasoning as can\r\nbe seen in the highlighted\
          \ block\u2026 this block keeps repeating till the agent runs out of iterations\
          \ and\r\nhence does not arrive at the final answer.\r\n\r\nCode - \r\nfrom\
          \ huggingface_hub import hf_hub_download\r\nfrom langchain.llms import LlamaCpp\r\
          \nfrom langchain.agents import create_csv_agent\r\n\r\nMODEL_ID = \"TheBloke/zephyr-7B-beta-GGUF\"\
          \r\nMODEL_BASENAME = \"zephyr-7b-beta.Q4_K_M.gguf\"\r\n\r\nCONTEXT_WINDOW_SIZE\
          \ = 4096\r\nMAX_NEW_TOKENS = 1024\r\n\r\nmodel_path = hf_hub_download(\r\
          \n            repo_id=MODEL_ID,\r\n            filename=MODEL_BASENAME,\r\
          \n            resume_download=True,\r\n            cache_dir=\"./models\"\
          ,\r\n        )\r\nkwargs = {\r\n            \"model_path\": model_path,\r\
          \n            \"n_ctx\": CONTEXT_WINDOW_SIZE,\r\n            \"max_tokens\"\
          : MAX_NEW_TOKENS,\r\n            \"n_gpu_layers\":4\r\n        }\r\nllm\
          \ = LlamaCpp(\r\n    model_path=model_path,\r\n    temperature=0.1,\r\n\
          \    n_ctx=4096,\r\n    max_tokens=1024,\r\n    n_batch=100,\r\n    top_p=1,\r\
          \n    verbose=True,\r\n    n_gpu_layers=100)\r\n\r\n\r\nagent = create_csv_agent(llm,\
          \ ['./Data/Employees.csv','./Data/Verticals.csv'], verbose=True)\r\nresponse\
          \ = agent.run(\"Which vertical name has the most number of resignations\"\
          )\r\nprint(response)\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6453a499fc2b5f69e8fb0fde/kiDZJYFug-PJY19xFiGQX.png)\r\
          \n\r\nQUERY:\r\n1. How to correct such reasoning of the LLM such that the\
          \ LLM reasons out that its actions are\r\nbeing repeated but not helping\
          \ at arriving at the right answer? Does the AgentExecutor need\r\nto be\
          \ corrected in this case ? If yes, how and what needs to be done?\r\n"
        updatedAt: '2023-11-17T12:05:46.472Z'
      numEdits: 0
      reactions: []
    id: 6557571a35f26c82c0c24cf0
    type: comment
  author: shivammehta
  content: "When experimenting with this model, I've observed occasional discrepancies\
    \ in its output. Sometimes it provides the correct response, and sometimes times\
    \ it doesn't, even when presented with the same or similar questions. I have two\
    \ inquiries: Why does this occur, and how can we address this issue?\r\nThe output\
    \ of Agent goes into an infinite loop with the LLM not making changes to its reasoning\
    \ as can\r\nbe seen in the highlighted block\u2026 this block keeps repeating\
    \ till the agent runs out of iterations and\r\nhence does not arrive at the final\
    \ answer.\r\n\r\nCode - \r\nfrom huggingface_hub import hf_hub_download\r\nfrom\
    \ langchain.llms import LlamaCpp\r\nfrom langchain.agents import create_csv_agent\r\
    \n\r\nMODEL_ID = \"TheBloke/zephyr-7B-beta-GGUF\"\r\nMODEL_BASENAME = \"zephyr-7b-beta.Q4_K_M.gguf\"\
    \r\n\r\nCONTEXT_WINDOW_SIZE = 4096\r\nMAX_NEW_TOKENS = 1024\r\n\r\nmodel_path\
    \ = hf_hub_download(\r\n            repo_id=MODEL_ID,\r\n            filename=MODEL_BASENAME,\r\
    \n            resume_download=True,\r\n            cache_dir=\"./models\",\r\n\
    \        )\r\nkwargs = {\r\n            \"model_path\": model_path,\r\n      \
    \      \"n_ctx\": CONTEXT_WINDOW_SIZE,\r\n            \"max_tokens\": MAX_NEW_TOKENS,\r\
    \n            \"n_gpu_layers\":4\r\n        }\r\nllm = LlamaCpp(\r\n    model_path=model_path,\r\
    \n    temperature=0.1,\r\n    n_ctx=4096,\r\n    max_tokens=1024,\r\n    n_batch=100,\r\
    \n    top_p=1,\r\n    verbose=True,\r\n    n_gpu_layers=100)\r\n\r\n\r\nagent\
    \ = create_csv_agent(llm, ['./Data/Employees.csv','./Data/Verticals.csv'], verbose=True)\r\
    \nresponse = agent.run(\"Which vertical name has the most number of resignations\"\
    )\r\nprint(response)\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6453a499fc2b5f69e8fb0fde/kiDZJYFug-PJY19xFiGQX.png)\r\
    \n\r\nQUERY:\r\n1. How to correct such reasoning of the LLM such that the LLM\
    \ reasons out that its actions are\r\nbeing repeated but not helping at arriving\
    \ at the right answer? Does the AgentExecutor need\r\nto be corrected in this\
    \ case ? If yes, how and what needs to be done?\r\n"
  created_at: 2023-11-17 12:05:46+00:00
  edited: false
  hidden: false
  id: 6557571a35f26c82c0c24cf0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/zephyr-7B-beta-GGUF
repo_type: model
status: open
target_branch: null
title: 'Addressing Inconsistencies in Model Outputs: Understanding and Solutions'
