!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yachty66
conflicting_files: null
created_at: 2023-06-13 08:00:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb51217d31f0a1b8a7e0e4a03b4c500b.svg
      fullname: Max Hager
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yachty66
      type: user
    createdAt: '2023-06-13T09:00:38.000Z'
    data:
      edited: false
      editors:
      - yachty66
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7418413758277893
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb51217d31f0a1b8a7e0e4a03b4c500b.svg
          fullname: Max Hager
          isHf: false
          isPro: false
          name: yachty66
          type: user
        html: "<p>I am currently facing a challenge while trying to load a model stored\
          \ in the safetensors format using the Transformers library.</p>\n<p>Below\
          \ is the code I am using:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> LlamaForCausalLM, LlamaTokenizer\n\ntokenizer = LlamaTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"path/to/model\"</span>)\nmodel = LlamaForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"path/to/model\"</span>, use_safetensors=<span\
          \ class=\"hljs-literal\">True</span>)\n</code></pre>\n<p>Unfortunately,\
          \ this results in the following error:</p>\n<pre><code class=\"language-python\"\
          >Traceback (most recent call last):\n  File <span class=\"hljs-string\"\
          >\"/Users/maxhager/Projects2023/nsfw/model_run.py\"</span>, line <span class=\"\
          hljs-number\">4</span>, <span class=\"hljs-keyword\">in</span> &lt;module&gt;\n\
          \    model = LlamaForCausalLM.from_pretrained(<span class=\"hljs-string\"\
          >\"path/to/model\"</span>, use_safetensors=<span class=\"hljs-literal\"\
          >True</span>)\n  File <span class=\"hljs-string\">\"/Users/maxhager/.virtualenvs/nsfw/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          </span>, line <span class=\"hljs-number\">2449</span>, <span class=\"hljs-keyword\"\
          >in</span> from_pretrained\n    <span class=\"hljs-keyword\">raise</span>\
          \ EnvironmentError(\nOSError: Error no file named pytorch_model.<span class=\"\
          hljs-built_in\">bin</span>, tf_model.h5, model.ckpt.index <span class=\"\
          hljs-keyword\">or</span> flax_model.msgpack found <span class=\"hljs-keyword\"\
          >in</span> directory path/to/model.\n</code></pre>\n<p>In my model directory\
          \ (path/to/model), I have the following files:</p>\n<p>4bit-128g.safetensors<br>config.json<br>generation_config.json<br>pytorch_model.bin.index.json<br>special_tokens_map.json<br>tokenizer.json<br>tokenizer.model<br>tokenizer_config.json<br>I\
          \ was under the impression that setting use_safetensors=True should instruct\
          \ the from_pretrained() method to load the model from the safetensors format.\
          \ However, it appears the method is searching for the usual model file formats\
          \ (pytorch_model.bin, tf_model.h5, etc) instead of recognizing the safetensors\
          \ format.</p>\n<p>Has anyone faced this issue before or does anyone have\
          \ insight into what might be happening here? I would greatly appreciate\
          \ any assistance or guidance on how to successfully load a model stored\
          \ in the safetensors format using the Transformers library.</p>\n<p>Thank\
          \ you!</p>\n"
        raw: "I am currently facing a challenge while trying to load a model stored\
          \ in the safetensors format using the Transformers library.\r\n\r\nBelow\
          \ is the code I am using:\r\n\r\n```python\r\nfrom transformers import LlamaForCausalLM,\
          \ LlamaTokenizer\r\n\r\ntokenizer = LlamaTokenizer.from_pretrained(\"path/to/model\"\
          )\r\nmodel = LlamaForCausalLM.from_pretrained(\"path/to/model\", use_safetensors=True)\r\
          \n```\r\n\r\nUnfortunately, this results in the following error:\r\n\r\n\
          ```python\r\nTraceback (most recent call last):\r\n  File \"/Users/maxhager/Projects2023/nsfw/model_run.py\"\
          , line 4, in <module>\r\n    model = LlamaForCausalLM.from_pretrained(\"\
          path/to/model\", use_safetensors=True)\r\n  File \"/Users/maxhager/.virtualenvs/nsfw/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 2449, in from_pretrained\r\n    raise EnvironmentError(\r\nOSError:\
          \ Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or\
          \ flax_model.msgpack found in directory path/to/model.\r\n```\r\n\r\nIn\
          \ my model directory (path/to/model), I have the following files:\r\n\r\n\
          4bit-128g.safetensors\r\nconfig.json\r\ngeneration_config.json\r\npytorch_model.bin.index.json\r\
          \nspecial_tokens_map.json\r\ntokenizer.json\r\ntokenizer.model\r\ntokenizer_config.json\r\
          \nI was under the impression that setting use_safetensors=True should instruct\
          \ the from_pretrained() method to load the model from the safetensors format.\
          \ However, it appears the method is searching for the usual model file formats\
          \ (pytorch_model.bin, tf_model.h5, etc) instead of recognizing the safetensors\
          \ format.\r\n\r\nHas anyone faced this issue before or does anyone have\
          \ insight into what might be happening here? I would greatly appreciate\
          \ any assistance or guidance on how to successfully load a model stored\
          \ in the safetensors format using the Transformers library.\r\n\r\nThank\
          \ you!"
        updatedAt: '2023-06-13T09:00:38.322Z'
      numEdits: 0
      reactions: []
    id: 6488303689c9bf5527436636
    type: comment
  author: yachty66
  content: "I am currently facing a challenge while trying to load a model stored\
    \ in the safetensors format using the Transformers library.\r\n\r\nBelow is the\
    \ code I am using:\r\n\r\n```python\r\nfrom transformers import LlamaForCausalLM,\
    \ LlamaTokenizer\r\n\r\ntokenizer = LlamaTokenizer.from_pretrained(\"path/to/model\"\
    )\r\nmodel = LlamaForCausalLM.from_pretrained(\"path/to/model\", use_safetensors=True)\r\
    \n```\r\n\r\nUnfortunately, this results in the following error:\r\n\r\n```python\r\
    \nTraceback (most recent call last):\r\n  File \"/Users/maxhager/Projects2023/nsfw/model_run.py\"\
    , line 4, in <module>\r\n    model = LlamaForCausalLM.from_pretrained(\"path/to/model\"\
    , use_safetensors=True)\r\n  File \"/Users/maxhager/.virtualenvs/nsfw/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 2449, in from_pretrained\r\n    raise EnvironmentError(\r\nOSError: Error\
    \ no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
    \ found in directory path/to/model.\r\n```\r\n\r\nIn my model directory (path/to/model),\
    \ I have the following files:\r\n\r\n4bit-128g.safetensors\r\nconfig.json\r\n\
    generation_config.json\r\npytorch_model.bin.index.json\r\nspecial_tokens_map.json\r\
    \ntokenizer.json\r\ntokenizer.model\r\ntokenizer_config.json\r\nI was under the\
    \ impression that setting use_safetensors=True should instruct the from_pretrained()\
    \ method to load the model from the safetensors format. However, it appears the\
    \ method is searching for the usual model file formats (pytorch_model.bin, tf_model.h5,\
    \ etc) instead of recognizing the safetensors format.\r\n\r\nHas anyone faced\
    \ this issue before or does anyone have insight into what might be happening here?\
    \ I would greatly appreciate any assistance or guidance on how to successfully\
    \ load a model stored in the safetensors format using the Transformers library.\r\
    \n\r\nThank you!"
  created_at: 2023-06-13 08:00:38+00:00
  edited: false
  hidden: false
  id: 6488303689c9bf5527436636
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: notstoic/pygmalion-13b-4bit-128g
repo_type: model
status: open
target_branch: null
title: Loading Model in HF Transformers
