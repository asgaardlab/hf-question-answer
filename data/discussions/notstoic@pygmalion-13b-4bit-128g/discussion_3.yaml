!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Afster
conflicting_files: null
created_at: 2023-05-20 03:38:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4baadc20bce9fcea1ed115d0bf5fccb1.svg
      fullname: Afster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Afster
      type: user
    createdAt: '2023-05-20T04:38:17.000Z'
    data:
      edited: false
      editors:
      - Afster
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4baadc20bce9fcea1ed115d0bf5fccb1.svg
          fullname: Afster
          isHf: false
          isPro: false
          name: Afster
          type: user
        html: '<p>.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB
          (GPU 0; 8.00 GiB total capacity; 7.08 GiB already allocated; 0 bytes free;
          7.31 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated
          memory try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>Getting this issue. Tried to change the pre-layer within Oooba-booga
          but no success. </p>

          '
        raw: ".OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU\
          \ 0; 8.00 GiB total capacity; 7.08 GiB already allocated; 0 bytes free;\
          \ 7.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated\
          \ memory try setting max_split_size_mb to avoid fragmentation.  See documentation\
          \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\nGetting this\
          \ issue. Tried to change the pre-layer within Oooba-booga but no success. "
        updatedAt: '2023-05-20T04:38:17.855Z'
      numEdits: 0
      reactions: []
    id: 64684eb9374fe5728d43c611
    type: comment
  author: Afster
  content: ".OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU\
    \ 0; 8.00 GiB total capacity; 7.08 GiB already allocated; 0 bytes free; 7.31 GiB\
    \ reserved in total by PyTorch) If reserved memory is >> allocated memory try\
    \ setting max_split_size_mb to avoid fragmentation.  See documentation for Memory\
    \ Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\nGetting this issue. Tried to change\
    \ the pre-layer within Oooba-booga but no success. "
  created_at: 2023-05-20 03:38:17+00:00
  edited: false
  hidden: false
  id: 64684eb9374fe5728d43c611
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d70ae605c07e368f8a50543ca42e0eba.svg
      fullname: Pablito
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: boricuapab
      type: user
    createdAt: '2023-05-20T08:51:55.000Z'
    data:
      edited: true
      editors:
      - boricuapab
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d70ae605c07e368f8a50543ca42e0eba.svg
          fullname: Pablito
          isHf: false
          isPro: false
          name: boricuapab
          type: user
        html: '<p>I''ve been able to get responses on an rtx 2060 super 8gb card with
          the following flags in ooba</p>

          <p>call python server.py --auto-devices --extensions api --model notstoic_pygmalion-13b-4bit-128g
          --model_type LLaMA --wbits 4 --groupsize 128 --no-cache --pre_layer 30</p>

          '
        raw: 'I''ve been able to get responses on an rtx 2060 super 8gb card with
          the following flags in ooba


          call python server.py --auto-devices --extensions api --model notstoic_pygmalion-13b-4bit-128g
          --model_type LLaMA --wbits 4 --groupsize 128 --no-cache --pre_layer 30'
        updatedAt: '2023-05-20T08:55:49.273Z'
      numEdits: 1
      reactions: []
    id: 64688a2b97ffc33d43c28bea
    type: comment
  author: boricuapab
  content: 'I''ve been able to get responses on an rtx 2060 super 8gb card with the
    following flags in ooba


    call python server.py --auto-devices --extensions api --model notstoic_pygmalion-13b-4bit-128g
    --model_type LLaMA --wbits 4 --groupsize 128 --no-cache --pre_layer 30'
  created_at: 2023-05-20 07:51:55+00:00
  edited: true
  hidden: false
  id: 64688a2b97ffc33d43c28bea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a6cbbda20d5742034341860d4f6c24e.svg
      fullname: Zensu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zensu
      type: user
    createdAt: '2023-05-22T15:40:44.000Z'
    data:
      edited: false
      editors:
      - Zensu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a6cbbda20d5742034341860d4f6c24e.svg
          fullname: Zensu
          isHf: false
          isPro: false
          name: Zensu
          type: user
        html: '<blockquote>

          <p>I''ve been able to get responses on an rtx 2060 super 8gb card with the
          following flags in ooba</p>

          <p>call python server.py --auto-devices --extensions api --model notstoic_pygmalion-13b-4bit-128g
          --model_type LLaMA --wbits 4 --groupsize 128 --no-cache --pre_layer 30</p>

          </blockquote>

          <p>Not working for me</p>

          '
        raw: "> I've been able to get responses on an rtx 2060 super 8gb card with\
          \ the following flags in ooba\n> \n> call python server.py --auto-devices\
          \ --extensions api --model notstoic_pygmalion-13b-4bit-128g --model_type\
          \ LLaMA --wbits 4 --groupsize 128 --no-cache --pre_layer 30\n\nNot working\
          \ for me"
        updatedAt: '2023-05-22T15:40:44.354Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - DESP117
    id: 646b8cfcf85ebf65c532b804
    type: comment
  author: Zensu
  content: "> I've been able to get responses on an rtx 2060 super 8gb card with the\
    \ following flags in ooba\n> \n> call python server.py --auto-devices --extensions\
    \ api --model notstoic_pygmalion-13b-4bit-128g --model_type LLaMA --wbits 4 --groupsize\
    \ 128 --no-cache --pre_layer 30\n\nNot working for me"
  created_at: 2023-05-22 14:40:44+00:00
  edited: false
  hidden: false
  id: 646b8cfcf85ebf65c532b804
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/4baadc20bce9fcea1ed115d0bf5fccb1.svg
      fullname: Afster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Afster
      type: user
    createdAt: '2023-05-23T00:00:26.000Z'
    data:
      status: closed
    id: 646c021adb697c798a50c47c
    type: status-change
  author: Afster
  created_at: 2023-05-22 23:00:26+00:00
  id: 646c021adb697c798a50c47c
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/4baadc20bce9fcea1ed115d0bf5fccb1.svg
      fullname: Afster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Afster
      type: user
    createdAt: '2023-05-23T00:00:33.000Z'
    data:
      status: open
    id: 646c02215d68f5c15a371512
    type: status-change
  author: Afster
  created_at: 2023-05-22 23:00:33+00:00
  id: 646c02215d68f5c15a371512
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: notstoic/pygmalion-13b-4bit-128g
repo_type: model
status: open
target_branch: null
title: CUDA Out of memory
