!!python/object:huggingface_hub.community.DiscussionWithDetails
author: anobu88
conflicting_files: null
created_at: 2023-05-19 13:17:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37a0ac7e65f8617af5a3adaf27f82d07.svg
      fullname: Timothy Ross
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anobu88
      type: user
    createdAt: '2023-05-19T14:17:11.000Z'
    data:
      edited: false
      editors:
      - anobu88
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37a0ac7e65f8617af5a3adaf27f82d07.svg
          fullname: Timothy Ross
          isHf: false
          isPro: false
          name: anobu88
          type: user
        html: '<p>I get this error when loading the model:<br>ERROR:The model could
          not be loaded because its type could not be inferred from its name.<br>ERROR:Please
          specify the type manually using the --model_type argument.</p>

          <p>How do I load the model?</p>

          '
        raw: "I get this error when loading the model: \r\nERROR:The model could not\
          \ be loaded because its type could not be inferred from its name.\r\nERROR:Please\
          \ specify the type manually using the --model_type argument.\r\n\r\nHow\
          \ do I load the model?"
        updatedAt: '2023-05-19T14:17:11.120Z'
      numEdits: 0
      reactions: []
    id: 646784e7946476c5d216c52d
    type: comment
  author: anobu88
  content: "I get this error when loading the model: \r\nERROR:The model could not\
    \ be loaded because its type could not be inferred from its name.\r\nERROR:Please\
    \ specify the type manually using the --model_type argument.\r\n\r\nHow do I load\
    \ the model?"
  created_at: 2023-05-19 13:17:11+00:00
  edited: false
  hidden: false
  id: 646784e7946476c5d216c52d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac61b8d43a3c5a51be696a8d199ca816.svg
      fullname: Entropic Embrace
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Entity
      type: user
    createdAt: '2023-05-19T14:22:12.000Z'
    data:
      edited: false
      editors:
      - Entity
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac61b8d43a3c5a51be696a8d199ca816.svg
          fullname: Entropic Embrace
          isHf: false
          isPro: false
          name: Entity
          type: user
        html: '<p>put llama as the type</p>

          '
        raw: put llama as the type
        updatedAt: '2023-05-19T14:22:12.132Z'
      numEdits: 0
      reactions: []
    id: 64678614a48c2b6f0d619867
    type: comment
  author: Entity
  content: put llama as the type
  created_at: 2023-05-19 13:22:12+00:00
  edited: false
  hidden: false
  id: 64678614a48c2b6f0d619867
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37a0ac7e65f8617af5a3adaf27f82d07.svg
      fullname: Timothy Ross
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anobu88
      type: user
    createdAt: '2023-05-19T14:39:31.000Z'
    data:
      edited: false
      editors:
      - anobu88
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37a0ac7e65f8617af5a3adaf27f82d07.svg
          fullname: Timothy Ross
          isHf: false
          isPro: false
          name: anobu88
          type: user
        html: '<p>I did, I did it in the UI and it says the same thing. I did in with
          --model_type llama as well and it jsut say ''done! Complete" and then It''ll
          shut down after pressing enter</p>

          '
        raw: I did, I did it in the UI and it says the same thing. I did in with --model_type
          llama as well and it jsut say 'done! Complete" and then It'll shut down
          after pressing enter
        updatedAt: '2023-05-19T14:39:31.387Z'
      numEdits: 0
      reactions: []
    id: 64678a23ab75d9cb3c483711
    type: comment
  author: anobu88
  content: I did, I did it in the UI and it says the same thing. I did in with --model_type
    llama as well and it jsut say 'done! Complete" and then It'll shut down after
    pressing enter
  created_at: 2023-05-19 13:39:31+00:00
  edited: false
  hidden: false
  id: 64678a23ab75d9cb3c483711
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e5ccbc0dac5c1e16bdddd489802d363.svg
      fullname: minipasila
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mpasila
      type: user
    createdAt: '2023-05-19T21:42:37.000Z'
    data:
      edited: false
      editors:
      - mpasila
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e5ccbc0dac5c1e16bdddd489802d363.svg
          fullname: minipasila
          isHf: false
          isPro: false
          name: mpasila
          type: user
        html: '<p>This was quantized using <a rel="nofollow" href="https://github.com/0cc4m/GPTQ-for-LLaMa">https://github.com/0cc4m/GPTQ-for-LLaMa</a>
          meaning if you have some other version of GPTQ it might not work.</p>

          '
        raw: This was quantized using https://github.com/0cc4m/GPTQ-for-LLaMa meaning
          if you have some other version of GPTQ it might not work.
        updatedAt: '2023-05-19T21:42:37.606Z'
      numEdits: 0
      reactions: []
    id: 6467ed4de92e2372d5d5e9d1
    type: comment
  author: mpasila
  content: This was quantized using https://github.com/0cc4m/GPTQ-for-LLaMa meaning
    if you have some other version of GPTQ it might not work.
  created_at: 2023-05-19 20:42:37+00:00
  edited: false
  hidden: false
  id: 6467ed4de92e2372d5d5e9d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3929876923b39e97b0715b58edc8f7fc.svg
      fullname: Schiller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AliOsman123
      type: user
    createdAt: '2023-05-19T23:28:31.000Z'
    data:
      edited: false
      editors:
      - AliOsman123
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3929876923b39e97b0715b58edc8f7fc.svg
          fullname: Schiller
          isHf: false
          isPro: false
          name: AliOsman123
          type: user
        html: '<p>how can i get to GPTQ version needed?</p>

          '
        raw: how can i get to GPTQ version needed?
        updatedAt: '2023-05-19T23:28:31.134Z'
      numEdits: 0
      reactions: []
    id: 6468061fa48c2b6f0d695208
    type: comment
  author: AliOsman123
  content: how can i get to GPTQ version needed?
  created_at: 2023-05-19 22:28:31+00:00
  edited: false
  hidden: false
  id: 6468061fa48c2b6f0d695208
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/afbc48df2e8c47c35be48168113d83c0.svg
      fullname: s
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tom-Neverwinter
      type: user
    createdAt: '2023-05-20T16:34:29.000Z'
    data:
      edited: false
      editors:
      - Tom-Neverwinter
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/afbc48df2e8c47c35be48168113d83c0.svg
          fullname: s
          isHf: false
          isPro: false
          name: Tom-Neverwinter
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/commit/2d5db48371052087a83974abda3767d1aedec598">https://github.com/ggerganov/llama.cpp/commit/2d5db48371052087a83974abda3767d1aedec598</a>
          llama version was bumped. model will need to be changed</p>

          '
        raw: https://github.com/ggerganov/llama.cpp/commit/2d5db48371052087a83974abda3767d1aedec598
          llama version was bumped. model will need to be changed
        updatedAt: '2023-05-20T16:34:29.948Z'
      numEdits: 0
      reactions: []
    id: 6468f6957407ab1cff35c9d3
    type: comment
  author: Tom-Neverwinter
  content: https://github.com/ggerganov/llama.cpp/commit/2d5db48371052087a83974abda3767d1aedec598
    llama version was bumped. model will need to be changed
  created_at: 2023-05-20 15:34:29+00:00
  edited: false
  hidden: false
  id: 6468f6957407ab1cff35c9d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c40f0658b4486af803d44744a8af649.svg
      fullname: stoic
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: notstoic
      type: user
    createdAt: '2023-05-20T17:15:01.000Z'
    data:
      edited: false
      editors:
      - notstoic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c40f0658b4486af803d44744a8af649.svg
          fullname: stoic
          isHf: false
          isPro: false
          name: notstoic
          type: user
        html: '<blockquote>

          <p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/commit/2d5db48371052087a83974abda3767d1aedec598">https://github.com/ggerganov/llama.cpp/commit/2d5db48371052087a83974abda3767d1aedec598</a>
          llama version was bumped. model will need to be changed</p>

          </blockquote>

          <p>This is the gptq repo, ggml repo here:<br><a href="https://huggingface.co/notstoic/pygmalion-13b-ggml/">https://huggingface.co/notstoic/pygmalion-13b-ggml/</a></p>

          <p>Although it''s true that they''re not bumped to the latest version of
          llama.cpp, yet.</p>

          '
        raw: '> https://github.com/ggerganov/llama.cpp/commit/2d5db48371052087a83974abda3767d1aedec598
          llama version was bumped. model will need to be changed


          This is the gptq repo, ggml repo here:

          https://huggingface.co/notstoic/pygmalion-13b-ggml/


          Although it''s true that they''re not bumped to the latest version of llama.cpp,
          yet.'
        updatedAt: '2023-05-20T17:15:01.060Z'
      numEdits: 0
      reactions: []
    id: 6469001597ffc33d43c98467
    type: comment
  author: notstoic
  content: '> https://github.com/ggerganov/llama.cpp/commit/2d5db48371052087a83974abda3767d1aedec598
    llama version was bumped. model will need to be changed


    This is the gptq repo, ggml repo here:

    https://huggingface.co/notstoic/pygmalion-13b-ggml/


    Although it''s true that they''re not bumped to the latest version of llama.cpp,
    yet.'
  created_at: 2023-05-20 16:15:01+00:00
  edited: false
  hidden: false
  id: 6469001597ffc33d43c98467
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37a0ac7e65f8617af5a3adaf27f82d07.svg
      fullname: Timothy Ross
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anobu88
      type: user
    createdAt: '2023-05-20T19:49:21.000Z'
    data:
      edited: false
      editors:
      - anobu88
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37a0ac7e65f8617af5a3adaf27f82d07.svg
          fullname: Timothy Ross
          isHf: false
          isPro: false
          name: anobu88
          type: user
        html: '<blockquote>

          <blockquote>

          <p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/commit/2d5db48371052087a83974abda3767d1aedec598">https://github.com/ggerganov/llama.cpp/commit/2d5db48371052087a83974abda3767d1aedec598</a>
          llama version was bumped. model will need to be changed</p>

          </blockquote>

          <p>This is the gptq repo, ggml repo here:<br><a href="https://huggingface.co/notstoic/pygmalion-13b-ggml/">https://huggingface.co/notstoic/pygmalion-13b-ggml/</a></p>

          <p>Although it''s true that they''re not bumped to the latest version of
          llama.cpp, yet.</p>

          </blockquote>

          <p>How do I run this? Where''s the other files?</p>

          '
        raw: "> > https://github.com/ggerganov/llama.cpp/commit/2d5db48371052087a83974abda3767d1aedec598\
          \ llama version was bumped. model will need to be changed\n> \n> This is\
          \ the gptq repo, ggml repo here:\n> https://huggingface.co/notstoic/pygmalion-13b-ggml/\n\
          > \n> Although it's true that they're not bumped to the latest version of\
          \ llama.cpp, yet.\n\nHow do I run this? Where's the other files?"
        updatedAt: '2023-05-20T19:49:21.207Z'
      numEdits: 0
      reactions: []
    id: 646924417407ab1cff39b516
    type: comment
  author: anobu88
  content: "> > https://github.com/ggerganov/llama.cpp/commit/2d5db48371052087a83974abda3767d1aedec598\
    \ llama version was bumped. model will need to be changed\n> \n> This is the gptq\
    \ repo, ggml repo here:\n> https://huggingface.co/notstoic/pygmalion-13b-ggml/\n\
    > \n> Although it's true that they're not bumped to the latest version of llama.cpp,\
    \ yet.\n\nHow do I run this? Where's the other files?"
  created_at: 2023-05-20 18:49:21+00:00
  edited: false
  hidden: false
  id: 646924417407ab1cff39b516
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0aee01c204eb1c70d1c674a2abedc8d5.svg
      fullname: Yuan Branch
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: psychether
      type: user
    createdAt: '2023-05-21T00:38:14.000Z'
    data:
      edited: false
      editors:
      - psychether
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0aee01c204eb1c70d1c674a2abedc8d5.svg
          fullname: Yuan Branch
          isHf: false
          isPro: false
          name: psychether
          type: user
        html: '<blockquote>

          <p>I get this error when loading the model:<br>ERROR:The model could not
          be loaded because its type could not be inferred from its name.<br>ERROR:Please
          specify the type manually using the --model_type argument.</p>

          <p>How do I load the model?</p>

          </blockquote>

          <p>same thing happening to me</p>

          '
        raw: "> I get this error when loading the model: \n> ERROR:The model could\
          \ not be loaded because its type could not be inferred from its name.\n\
          > ERROR:Please specify the type manually using the --model_type argument.\n\
          > \n> How do I load the model?\n\nsame thing happening to me"
        updatedAt: '2023-05-21T00:38:14.225Z'
      numEdits: 0
      reactions: []
    id: 646967f6511e5cdeb0f0cce6
    type: comment
  author: psychether
  content: "> I get this error when loading the model: \n> ERROR:The model could not\
    \ be loaded because its type could not be inferred from its name.\n> ERROR:Please\
    \ specify the type manually using the --model_type argument.\n> \n> How do I load\
    \ the model?\n\nsame thing happening to me"
  created_at: 2023-05-20 23:38:14+00:00
  edited: false
  hidden: false
  id: 646967f6511e5cdeb0f0cce6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8724a0d6ad27a956a25b3ac97325cf1.svg
      fullname: Joesph
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Humeee33
      type: user
    createdAt: '2023-05-23T17:44:47.000Z'
    data:
      edited: false
      editors:
      - Humeee33
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8724a0d6ad27a956a25b3ac97325cf1.svg
          fullname: Joesph
          isHf: false
          isPro: false
          name: Humeee33
          type: user
        html: '<p>notstoic_PygmalionCoT-7b  and  notstoic_pygmalion-13b-4bit-128g  are
          the only two models oogabooga will load. The other two just crap out with
          vomit text.  Thank you for making them all but can you make it so the other
          ones work in oogabooga?<br>Thanks</p>

          '
        raw: 'notstoic_PygmalionCoT-7b  and  notstoic_pygmalion-13b-4bit-128g  are
          the only two models oogabooga will load. The other two just crap out with
          vomit text.  Thank you for making them all but can you make it so the other
          ones work in oogabooga?

          Thanks'
        updatedAt: '2023-05-23T17:44:47.979Z'
      numEdits: 0
      reactions: []
    id: 646cfb8fe0c5e395734f5ee0
    type: comment
  author: Humeee33
  content: 'notstoic_PygmalionCoT-7b  and  notstoic_pygmalion-13b-4bit-128g  are the
    only two models oogabooga will load. The other two just crap out with vomit text.  Thank
    you for making them all but can you make it so the other ones work in oogabooga?

    Thanks'
  created_at: 2023-05-23 16:44:47+00:00
  edited: false
  hidden: false
  id: 646cfb8fe0c5e395734f5ee0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b90ceabf371b20494ee449cd6e3d62e8.svg
      fullname: Kyodan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kyodan
      type: user
    createdAt: '2023-05-23T21:31:26.000Z'
    data:
      edited: true
      editors:
      - Kyodan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b90ceabf371b20494ee449cd6e3d62e8.svg
          fullname: Kyodan
          isHf: false
          isPro: false
          name: Kyodan
          type: user
        html: '<blockquote>

          <p>I get this error when loading the model:<br>ERROR:The model could not
          be loaded because its type could not be inferred from its name.<br>ERROR:Please
          specify the type manually using the --model_type argument.</p>

          <p>How do I load the model?</p>

          </blockquote>

          <p>For oobabooga:</p>

          <p>Open ''<strong>webui.py</strong>'' in a text editor, then in <em>line
          15</em> (should be the <code>CMD_FLAGS</code> line), remove <code>--model_menu</code>,
          then add the following within the <code>'' ''</code>:</p>

          <p><code>--model notstoic_pygmalion-13b-4bit-128g --model_type Llama</code></p>

          <p>so it should look like (this is an example, yours may have other lines
          for extensions):</p>

          <p><code>CMD_FLAGS = ''--chat --groupsize 128 --wbits 4 --model notstoic_pygmalion-13b-4bit-128g
          --model_type Llama''</code></p>

          '
        raw: "> I get this error when loading the model: \n> ERROR:The model could\
          \ not be loaded because its type could not be inferred from its name.\n\
          > ERROR:Please specify the type manually using the --model_type argument.\n\
          > \n> How do I load the model?\n\nFor oobabooga:\n\nOpen '**webui.py**'\
          \ in a text editor, then in *line 15* (should be the `CMD_FLAGS` line),\
          \ remove `--model_menu`, then add the following within the `' '`:\n\n`--model\
          \ notstoic_pygmalion-13b-4bit-128g --model_type Llama`\n\nso it should look\
          \ like (this is an example, yours may have other lines for extensions):\n\
          \n`CMD_FLAGS = '--chat --groupsize 128 --wbits 4 --model notstoic_pygmalion-13b-4bit-128g\
          \ --model_type Llama'`"
        updatedAt: '2023-05-23T21:35:36.581Z'
      numEdits: 12
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jarryidle
    id: 646d30ae2abe5323fe1cc014
    type: comment
  author: Kyodan
  content: "> I get this error when loading the model: \n> ERROR:The model could not\
    \ be loaded because its type could not be inferred from its name.\n> ERROR:Please\
    \ specify the type manually using the --model_type argument.\n> \n> How do I load\
    \ the model?\n\nFor oobabooga:\n\nOpen '**webui.py**' in a text editor, then in\
    \ *line 15* (should be the `CMD_FLAGS` line), remove `--model_menu`, then add\
    \ the following within the `' '`:\n\n`--model notstoic_pygmalion-13b-4bit-128g\
    \ --model_type Llama`\n\nso it should look like (this is an example, yours may\
    \ have other lines for extensions):\n\n`CMD_FLAGS = '--chat --groupsize 128 --wbits\
    \ 4 --model notstoic_pygmalion-13b-4bit-128g --model_type Llama'`"
  created_at: 2023-05-23 20:31:26+00:00
  edited: true
  hidden: false
  id: 646d30ae2abe5323fe1cc014
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666865130562-635a56b56365824bdf0b349a.png?w=200&h=200&f=face
      fullname: Tht
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Elrenuir
      type: user
    createdAt: '2023-06-04T07:32:45.000Z'
    data:
      edited: false
      editors:
      - Elrenuir
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8865277767181396
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666865130562-635a56b56365824bdf0b349a.png?w=200&h=200&f=face
          fullname: Tht
          isHf: false
          isPro: false
          name: Elrenuir
          type: user
        html: '<p>same error, however in my webui.py no such line(( and if I copy
          it there it is no effect</p>

          '
        raw: same error, however in my webui.py no such line(( and if I copy it there
          it is no effect
        updatedAt: '2023-06-04T07:32:45.669Z'
      numEdits: 0
      reactions: []
    id: 647c3e1d60dfe0f35d48c4aa
    type: comment
  author: Elrenuir
  content: same error, however in my webui.py no such line(( and if I copy it there
    it is no effect
  created_at: 2023-06-04 06:32:45+00:00
  edited: false
  hidden: false
  id: 647c3e1d60dfe0f35d48c4aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666865130562-635a56b56365824bdf0b349a.png?w=200&h=200&f=face
      fullname: Tht
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Elrenuir
      type: user
    createdAt: '2023-06-06T17:13:09.000Z'
    data:
      edited: false
      editors:
      - Elrenuir
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6041367053985596
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666865130562-635a56b56365824bdf0b349a.png?w=200&h=200&f=face
          fullname: Tht
          isHf: false
          isPro: false
          name: Elrenuir
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I get this error when loading the model:<br>ERROR:The model could not
          be loaded because its type could not be inferred from its name.<br>ERROR:Please
          specify the type manually using the --model_type argument.</p>

          <p>How do I load the model?</p>

          </blockquote>

          <p>For oobabooga:</p>

          <p>Open ''<strong>webui.py</strong>'' in a text editor, then in <em>line
          15</em> (should be the <code>CMD_FLAGS</code> line), remove <code>--model_menu</code>,
          then add the following within the <code>'' ''</code>:</p>

          <p><code>--model notstoic_pygmalion-13b-4bit-128g --model_type Llama</code></p>

          <p>so it should look like (this is an example, yours may have other lines
          for extensions):</p>

          <p><code>CMD_FLAGS = ''--chat --groupsize 128 --wbits 4 --model notstoic_pygmalion-13b-4bit-128g
          --model_type Llama''</code></p>

          </blockquote>

          <p>same error, however in my webui.py no such line(( and if I copy it there
          it is no effect</p>

          '
        raw: "> > I get this error when loading the model: \n> > ERROR:The model could\
          \ not be loaded because its type could not be inferred from its name.\n\
          > > ERROR:Please specify the type manually using the --model_type argument.\n\
          > > \n> > How do I load the model?\n> \n> For oobabooga:\n> \n> Open '**webui.py**'\
          \ in a text editor, then in *line 15* (should be the `CMD_FLAGS` line),\
          \ remove `--model_menu`, then add the following within the `' '`:\n> \n\
          > `--model notstoic_pygmalion-13b-4bit-128g --model_type Llama`\n> \n> so\
          \ it should look like (this is an example, yours may have other lines for\
          \ extensions):\n> \n> `CMD_FLAGS = '--chat --groupsize 128 --wbits 4 --model\
          \ notstoic_pygmalion-13b-4bit-128g --model_type Llama'`\n\nsame error, however\
          \ in my webui.py no such line(( and if I copy it there it is no effect"
        updatedAt: '2023-06-06T17:13:09.932Z'
      numEdits: 0
      reactions: []
    id: 647f69252a7bcaa307a9ba0c
    type: comment
  author: Elrenuir
  content: "> > I get this error when loading the model: \n> > ERROR:The model could\
    \ not be loaded because its type could not be inferred from its name.\n> > ERROR:Please\
    \ specify the type manually using the --model_type argument.\n> > \n> > How do\
    \ I load the model?\n> \n> For oobabooga:\n> \n> Open '**webui.py**' in a text\
    \ editor, then in *line 15* (should be the `CMD_FLAGS` line), remove `--model_menu`,\
    \ then add the following within the `' '`:\n> \n> `--model notstoic_pygmalion-13b-4bit-128g\
    \ --model_type Llama`\n> \n> so it should look like (this is an example, yours\
    \ may have other lines for extensions):\n> \n> `CMD_FLAGS = '--chat --groupsize\
    \ 128 --wbits 4 --model notstoic_pygmalion-13b-4bit-128g --model_type Llama'`\n\
    \nsame error, however in my webui.py no such line(( and if I copy it there it\
    \ is no effect"
  created_at: 2023-06-06 16:13:09+00:00
  edited: false
  hidden: false
  id: 647f69252a7bcaa307a9ba0c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: notstoic/pygmalion-13b-4bit-128g
repo_type: model
status: open
target_branch: null
title: ERROR:The model could not be loaded because its type could not be inferred
  from its name.
