!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mullerse
conflicting_files: null
created_at: 2023-08-25 04:33:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9918b3a7bad2615f9190320b51d1109.svg
      fullname: "M\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mullerse
      type: user
    createdAt: '2023-08-25T05:33:54.000Z'
    data:
      edited: true
      editors:
      - mullerse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8438151478767395
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9918b3a7bad2615f9190320b51d1109.svg
          fullname: "M\xFCller"
          isHf: false
          isPro: false
          name: mullerse
          type: user
        html: '<p>Hey there :) </p>

          <p>Big thankyou for all the good documentations and the really helpfull
          models! But there is a Problem we cant solve :( </p>

          <p>We downloaded and installed the oobabooga text generation webui and it
          runs as expected</p>

          <p>What we want to do next is: Run e.g. this Model with python (<a href="https://huggingface.co/TheBloke/wizardLM-7B-GPTQ">https://huggingface.co/TheBloke/wizardLM-7B-GPTQ</a>).<br>As
          in the instruduction written, we installed autogpt etc. </p>

          <p>But if we run the "example code" from your Introduction "How to use this
          GPTQ model from Python code" we always geht this error:<br>OSError: TheBloke_vicuna-13b-v1.3.0-GPTQ
          is not a local folder and is not a valid model identifier listed on ''<a
          href="https://huggingface.co/models''">https://huggingface.co/models''</a><br>If
          this is a private repository, make sure to pass a token having permission
          to this repo with <code>use_auth_token</code> or log in with <code>huggingface-cli
          login</code> and pass <code>use_a uth_token=True</code>.</p>

          <p>But the local folder TheBloke_vicuna-13b-v1.3.0-GPTQ exists at /text-generation-webui/models/
          :(</p>

          <p>Where do we have to place the example_code.py? Actually it is in the
          Main oobabooga-Folder. But the same error appears when the .py is at /text-generation-webui/
          and as well when it is at /text-generation-webui/models/</p>

          <p>if i use the complete path at model_name_or_path = "C:/Users/user/oobabooga/text-generation-webui/models/TheBloke_vicuna-13b-v1.3.0-GPTQ"
          i''ll get: "FileNotFoundError: Could not find model in ..."</p>

          <p>As well i tried with use_auto_token = True in the tokenizer and added
          my huggingface token to the model. But there returns another failure then
          :/</p>

          <p>Do you have any idea?</p>

          <p>Thank you very much :)</p>

          '
        raw: "Hey there :) \n\nBig thankyou for all the good documentations and the\
          \ really helpfull models! But there is a Problem we cant solve :( \n\nWe\
          \ downloaded and installed the oobabooga text generation webui and it runs\
          \ as expected\n\nWhat we want to do next is: Run e.g. this Model with python\
          \ (https://huggingface.co/TheBloke/wizardLM-7B-GPTQ).\nAs in the instruduction\
          \ written, we installed autogpt etc. \n\nBut if we run the \"example code\"\
          \ from your Introduction \"How to use this GPTQ model from Python code\"\
          \ we always geht this error:\nOSError: TheBloke_vicuna-13b-v1.3.0-GPTQ is\
          \ not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n\
          If this is a private repository, make sure to pass a token having permission\
          \ to this repo with `use_auth_token` or log in with `huggingface-cli login`\
          \ and pass `use_a\nuth_token=True`.\n\nBut the local folder TheBloke_vicuna-13b-v1.3.0-GPTQ\
          \ exists at /text-generation-webui/models/ :(\n\nWhere do we have to place\
          \ the example_code.py? Actually it is in the Main oobabooga-Folder. But\
          \ the same error appears when the .py is at /text-generation-webui/ and\
          \ as well when it is at /text-generation-webui/models/\n\nif i use the complete\
          \ path at model_name_or_path = \"C:/Users/user/oobabooga/text-generation-webui/models/TheBloke_vicuna-13b-v1.3.0-GPTQ\"\
          \ i'll get: \"FileNotFoundError: Could not find model in ...\"\n\nAs well\
          \ i tried with use_auto_token = True in the tokenizer and added my huggingface\
          \ token to the model. But there returns another failure then :/\n\nDo you\
          \ have any idea?\n\nThank you very much :)\n"
        updatedAt: '2023-08-25T05:43:22.920Z'
      numEdits: 1
      reactions: []
    id: 64e83d42123d27f6f067a64b
    type: comment
  author: mullerse
  content: "Hey there :) \n\nBig thankyou for all the good documentations and the\
    \ really helpfull models! But there is a Problem we cant solve :( \n\nWe downloaded\
    \ and installed the oobabooga text generation webui and it runs as expected\n\n\
    What we want to do next is: Run e.g. this Model with python (https://huggingface.co/TheBloke/wizardLM-7B-GPTQ).\n\
    As in the instruduction written, we installed autogpt etc. \n\nBut if we run the\
    \ \"example code\" from your Introduction \"How to use this GPTQ model from Python\
    \ code\" we always geht this error:\nOSError: TheBloke_vicuna-13b-v1.3.0-GPTQ\
    \ is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n\
    If this is a private repository, make sure to pass a token having permission to\
    \ this repo with `use_auth_token` or log in with `huggingface-cli login` and pass\
    \ `use_a\nuth_token=True`.\n\nBut the local folder TheBloke_vicuna-13b-v1.3.0-GPTQ\
    \ exists at /text-generation-webui/models/ :(\n\nWhere do we have to place the\
    \ example_code.py? Actually it is in the Main oobabooga-Folder. But the same error\
    \ appears when the .py is at /text-generation-webui/ and as well when it is at\
    \ /text-generation-webui/models/\n\nif i use the complete path at model_name_or_path\
    \ = \"C:/Users/user/oobabooga/text-generation-webui/models/TheBloke_vicuna-13b-v1.3.0-GPTQ\"\
    \ i'll get: \"FileNotFoundError: Could not find model in ...\"\n\nAs well i tried\
    \ with use_auto_token = True in the tokenizer and added my huggingface token to\
    \ the model. But there returns another failure then :/\n\nDo you have any idea?\n\
    \nThank you very much :)\n"
  created_at: 2023-08-25 04:33:54+00:00
  edited: true
  hidden: false
  id: 64e83d42123d27f6f067a64b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-25T07:32:14.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9140453338623047
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please show your full code.  If you''re passing <code>model_basename
          = "gptq-model...."</code> then remove that, as that''s the problem.  I recently
          renamed the models so they''re now called <code>model.safetensors</code>
          and <code>model_basename</code> is no longer required.</p>

          <p>It might be easier if you start again and do GPTQ directly from Transformers,
          which is now supported since Transformers 4.32.0.  There''s a blog post
          with explanations and links to Google Colab code, here: <a href="https://huggingface.co/blog/gptq-integration">https://huggingface.co/blog/gptq-integration</a></p>

          <p>I will be updating my GPTQ documentation to reflect that new method shortly,
          this weekend</p>

          '
        raw: 'Please show your full code.  If you''re passing `model_basename = "gptq-model...."`
          then remove that, as that''s the problem.  I recently renamed the models
          so they''re now called `model.safetensors` and `model_basename` is no longer
          required.


          It might be easier if you start again and do GPTQ directly from Transformers,
          which is now supported since Transformers 4.32.0.  There''s a blog post
          with explanations and links to Google Colab code, here: https://huggingface.co/blog/gptq-integration


          I will be updating my GPTQ documentation to reflect that new method shortly,
          this weekend'
        updatedAt: '2023-08-25T07:32:14.824Z'
      numEdits: 0
      reactions: []
    id: 64e858fe8937a6b056805430
    type: comment
  author: TheBloke
  content: 'Please show your full code.  If you''re passing `model_basename = "gptq-model...."`
    then remove that, as that''s the problem.  I recently renamed the models so they''re
    now called `model.safetensors` and `model_basename` is no longer required.


    It might be easier if you start again and do GPTQ directly from Transformers,
    which is now supported since Transformers 4.32.0.  There''s a blog post with explanations
    and links to Google Colab code, here: https://huggingface.co/blog/gptq-integration


    I will be updating my GPTQ documentation to reflect that new method shortly, this
    weekend'
  created_at: 2023-08-25 06:32:14+00:00
  edited: false
  hidden: false
  id: 64e858fe8937a6b056805430
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9918b3a7bad2615f9190320b51d1109.svg
      fullname: "M\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mullerse
      type: user
    createdAt: '2023-08-28T16:30:10.000Z'
    data:
      edited: true
      editors:
      - mullerse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5229868292808533
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9918b3a7bad2615f9190320b51d1109.svg
          fullname: "M\xFCller"
          isHf: false
          isPro: false
          name: mullerse
          type: user
        html: '<p>Hey TheBloke,</p>

          <p>thank you very much for your time and advices :)<br>Yes, i used <code>model_basename</code><br>So
          then i tried your Wizard-Vicuna-7B-Uncensored GPTQ and it runs perfectly
          :)  </p>

          '
        raw: "Hey TheBloke,\n\nthank you very much for your time and advices :)\n\
          Yes, i used <code>model_basename</code>\nSo then i tried your Wizard-Vicuna-7B-Uncensored\
          \ GPTQ and it runs perfectly :)  \n"
        updatedAt: '2023-08-29T07:16:13.420Z'
      numEdits: 1
      reactions: []
    id: 64eccb926d765d1e7d5305e5
    type: comment
  author: mullerse
  content: "Hey TheBloke,\n\nthank you very much for your time and advices :)\nYes,\
    \ i used <code>model_basename</code>\nSo then i tried your Wizard-Vicuna-7B-Uncensored\
    \ GPTQ and it runs perfectly :)  \n"
  created_at: 2023-08-28 15:30:10+00:00
  edited: true
  hidden: false
  id: 64eccb926d765d1e7d5305e5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/vicuna-13b-v1.3.0-GPTQ
repo_type: model
status: open
target_branch: null
title: OSError:<ModelName> is not a local folder
