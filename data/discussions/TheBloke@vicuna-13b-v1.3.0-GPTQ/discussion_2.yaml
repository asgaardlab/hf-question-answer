!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Klimkou
conflicting_files: null
created_at: 2023-07-26 11:51:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a25152eabc09f94f436457e7f381d317.svg
      fullname: Gold Klim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Klimkou
      type: user
    createdAt: '2023-07-26T12:51:15.000Z'
    data:
      edited: true
      editors:
      - Klimkou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7191391587257385
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a25152eabc09f94f436457e7f381d317.svg
          fullname: Gold Klim
          isHf: false
          isPro: false
          name: Klimkou
          type: user
        html: '<p>Hi, I was trying to load the 8bit-128g-actorder_False version of
          the model by using such code:<br>"""<br>from transformers import AutoTokenizer,
          pipeline, logging<br>from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig</p>

          <p>model_name_or_path = "TheBloke/vicuna-13b-v1.3.0-GPTQ"<br>model_basename
          = "gptq-8bit-128g-actorder_False" #also tried ''gptq_model-8bit-128g'',
          got the same result</p>

          <p>use_triton = False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>        revision="gptq-8bit-128g-actorder_False",<br>        model_basename=model_basename,<br>        use_safetensors=True,<br>        trust_remote_code=True,<br>        device="cuda:0",<br>        use_triton=use_triton,<br>        quantize_config=None)<br>"""</p>

          <p>but I''ve got an error FileNotFoundError: Could not find model in TheBloke/vicuna-13b-v1.3.0-GPTQ.
          It should be said that I don''t get an error when I launch the default version
          from the main branch as it is written in "How to use this GPTQ model from
          Python code example". What am I doing wrong?<br>Thanks for your efforts
          a lot!</p>

          '
        raw: "Hi, I was trying to load the 8bit-128g-actorder_False version of the\
          \ model by using such code: \n\"\"\"\nfrom transformers import AutoTokenizer,\
          \ pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          \nmodel_name_or_path = \"TheBloke/vicuna-13b-v1.3.0-GPTQ\"\nmodel_basename\
          \ = \"gptq-8bit-128g-actorder_False\" #also tried 'gptq_model-8bit-128g',\
          \ got the same result\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        revision=\"gptq-8bit-128g-actorder_False\",\n        model_basename=model_basename,\n\
          \        use_safetensors=True,\n        trust_remote_code=True,\n      \
          \  device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          \"\"\"\n\nbut I've got an error FileNotFoundError: Could not find model\
          \ in TheBloke/vicuna-13b-v1.3.0-GPTQ. It should be said that I don't get\
          \ an error when I launch the default version from the main branch as it\
          \ is written in \"How to use this GPTQ model from Python code example\"\
          . What am I doing wrong?\nThanks for your efforts a lot!"
        updatedAt: '2023-07-26T12:54:34.831Z'
      numEdits: 1
      reactions: []
    id: 64c116c3f661ace7c40244da
    type: comment
  author: Klimkou
  content: "Hi, I was trying to load the 8bit-128g-actorder_False version of the model\
    \ by using such code: \n\"\"\"\nfrom transformers import AutoTokenizer, pipeline,\
    \ logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path\
    \ = \"TheBloke/vicuna-13b-v1.3.0-GPTQ\"\nmodel_basename = \"gptq-8bit-128g-actorder_False\"\
    \ #also tried 'gptq_model-8bit-128g', got the same result\n\nuse_triton = False\n\
    \ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
    \nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n        revision=\"\
    gptq-8bit-128g-actorder_False\",\n        model_basename=model_basename,\n   \
    \     use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
    cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\"\"\
    \"\n\nbut I've got an error FileNotFoundError: Could not find model in TheBloke/vicuna-13b-v1.3.0-GPTQ.\
    \ It should be said that I don't get an error when I launch the default version\
    \ from the main branch as it is written in \"How to use this GPTQ model from Python\
    \ code example\". What am I doing wrong?\nThanks for your efforts a lot!"
  created_at: 2023-07-26 11:51:15+00:00
  edited: true
  hidden: false
  id: 64c116c3f661ace7c40244da
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/vicuna-13b-v1.3.0-GPTQ
repo_type: model
status: open
target_branch: null
title: load model from different branches
