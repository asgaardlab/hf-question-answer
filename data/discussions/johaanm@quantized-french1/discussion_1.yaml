!!python/object:huggingface_hub.community.DiscussionWithDetails
author: samuelmat19
conflicting_files: null
created_at: 2023-08-09 10:31:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654976815644-6163edf1c412845bc9fc0b94.jpeg?w=200&h=200&f=face
      fullname: Sam Koesnadi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: samuelmat19
      type: user
    createdAt: '2023-08-09T11:31:15.000Z'
    data:
      edited: false
      editors:
      - samuelmat19
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5641054511070251
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654976815644-6163edf1c412845bc9fc0b94.jpeg?w=200&h=200&f=face
          fullname: Sam Koesnadi
          isHf: false
          isPro: false
          name: samuelmat19
          type: user
        html: "<p>I had this error <code>OSError: johaanm/llama2-quantized1 does not\
          \ appear to have a file named config.json. Checkout 'https://huggingface.co/johaanm/llama2-quantized1/main'\
          \ for available files.</code> when trying to load the model. As I now understand,\
          \ the model is not yet converted to huggingface format.</p>\n<p>My code\
          \ is as follows:</p>\n<pre><code>from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\nimport torch\n\n\nmodel_id = \"johaanm/llama2-quantized1\"\
          \n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    load_in_8bit=\
          \ False,\n    load_in_4bit=True,\n    lm_int8_threshold= 6.0,\n    llm_int8_skip_modules=\
          \ None,\n    llm_int8_enable_fp32_cpu_offload= False,\n    llm_int8_has_fp16_weight=\
          \ False,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"\
          nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\
          \ntext = \"What is one plus one? \"\ndevice = \"cuda:0\"\n\ninputs = tokenizer(text,\
          \ return_tensors=\"pt\").to(device)\noutputs = model.generate(**inputs,\
          \ max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\
          </code></pre>\n"
        raw: "I had this error `OSError: johaanm/llama2-quantized1 does not appear\
          \ to have a file named config.json. Checkout 'https://huggingface.co/johaanm/llama2-quantized1/main'\
          \ for available files.` when trying to load the model. As I now understand,\
          \ the model is not yet converted to huggingface format.\r\n\r\nMy code is\
          \ as follows:\r\n\r\n```\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\r\nimport torch\r\n\r\n\r\nmodel_id = \"johaanm/llama2-quantized1\"\
          \r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_id,\r\
          \n    load_in_8bit= False,\r\n    load_in_4bit=True,\r\n    lm_int8_threshold=\
          \ 6.0,\r\n    llm_int8_skip_modules= None,\r\n    llm_int8_enable_fp32_cpu_offload=\
          \ False,\r\n    llm_int8_has_fp16_weight= False,\r\n    bnb_4bit_use_double_quant=True,\r\
          \n    bnb_4bit_quant_type=\"nf4\",\r\n    bnb_4bit_compute_dtype=torch.float16,\r\
          \n)\r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\n\r\ntext =\
          \ \"What is one plus one? \"\r\ndevice = \"cuda:0\"\r\n\r\ninputs = tokenizer(text,\
          \ return_tensors=\"pt\").to(device)\r\noutputs = model.generate(**inputs,\
          \ max_new_tokens=20)\r\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\r\
          \n```"
        updatedAt: '2023-08-09T11:31:15.123Z'
      numEdits: 0
      reactions: []
    id: 64d379036345a50bade9a21d
    type: comment
  author: samuelmat19
  content: "I had this error `OSError: johaanm/llama2-quantized1 does not appear to\
    \ have a file named config.json. Checkout 'https://huggingface.co/johaanm/llama2-quantized1/main'\
    \ for available files.` when trying to load the model. As I now understand, the\
    \ model is not yet converted to huggingface format.\r\n\r\nMy code is as follows:\r\
    \n\r\n```\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\
    import torch\r\n\r\n\r\nmodel_id = \"johaanm/llama2-quantized1\"\r\n\r\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(\r\n    model_id,\r\n    load_in_8bit=\
    \ False,\r\n    load_in_4bit=True,\r\n    lm_int8_threshold= 6.0,\r\n    llm_int8_skip_modules=\
    \ None,\r\n    llm_int8_enable_fp32_cpu_offload= False,\r\n    llm_int8_has_fp16_weight=\
    \ False,\r\n    bnb_4bit_use_double_quant=True,\r\n    bnb_4bit_quant_type=\"\
    nf4\",\r\n    bnb_4bit_compute_dtype=torch.float16,\r\n)\r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\
    \n\r\ntext = \"What is one plus one? \"\r\ndevice = \"cuda:0\"\r\n\r\ninputs =\
    \ tokenizer(text, return_tensors=\"pt\").to(device)\r\noutputs = model.generate(**inputs,\
    \ max_new_tokens=20)\r\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\r\
    \n```"
  created_at: 2023-08-09 10:31:15+00:00
  edited: false
  hidden: false
  id: 64d379036345a50bade9a21d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85dadfdb92dad51b18f475ccb49602ca.svg
      fullname: Johaan Mannanal
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: johaanm
      type: user
    createdAt: '2023-08-09T18:19:59.000Z'
    data:
      edited: false
      editors:
      - johaanm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.979447066783905
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85dadfdb92dad51b18f475ccb49602ca.svg
          fullname: Johaan Mannanal
          isHf: false
          isPro: false
          name: johaanm
          type: user
        html: '<p>I will fix that today or tomorrow, that is my mistake, thanks for
          letting me know.</p>

          '
        raw: 'I will fix that today or tomorrow, that is my mistake, thanks for letting
          me know.

          '
        updatedAt: '2023-08-09T18:19:59.368Z'
      numEdits: 0
      reactions: []
    id: 64d3d8cf6345a50badf6b10b
    type: comment
  author: johaanm
  content: 'I will fix that today or tomorrow, that is my mistake, thanks for letting
    me know.

    '
  created_at: 2023-08-09 17:19:59+00:00
  edited: false
  hidden: false
  id: 64d3d8cf6345a50badf6b10b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654976815644-6163edf1c412845bc9fc0b94.jpeg?w=200&h=200&f=face
      fullname: Sam Koesnadi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: samuelmat19
      type: user
    createdAt: '2023-08-09T18:25:42.000Z'
    data:
      edited: false
      editors:
      - samuelmat19
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8679133653640747
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654976815644-6163edf1c412845bc9fc0b94.jpeg?w=200&h=200&f=face
          fullname: Sam Koesnadi
          isHf: false
          isPro: false
          name: samuelmat19
          type: user
        html: '<p>Thank you, you are the man! :)) Feel free to let me know after...</p>

          '
        raw: Thank you, you are the man! :)) Feel free to let me know after...
        updatedAt: '2023-08-09T18:25:42.936Z'
      numEdits: 0
      reactions: []
    id: 64d3da263ce34445f5ee431e
    type: comment
  author: samuelmat19
  content: Thank you, you are the man! :)) Feel free to let me know after...
  created_at: 2023-08-09 17:25:42+00:00
  edited: false
  hidden: false
  id: 64d3da263ce34445f5ee431e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85dadfdb92dad51b18f475ccb49602ca.svg
      fullname: Johaan Mannanal
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: johaanm
      type: user
    createdAt: '2023-08-11T02:27:28.000Z'
    data:
      edited: false
      editors:
      - johaanm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9752165675163269
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85dadfdb92dad51b18f475ccb49602ca.svg
          fullname: Johaan Mannanal
          isHf: false
          isPro: false
          name: johaanm
          type: user
        html: '<p>Can you check again, if you are having the same issue?</p>

          '
        raw: 'Can you check again, if you are having the same issue?

          '
        updatedAt: '2023-08-11T02:27:28.549Z'
      numEdits: 0
      reactions: []
    id: 64d59c909fef656cfde7f24d
    type: comment
  author: johaanm
  content: 'Can you check again, if you are having the same issue?

    '
  created_at: 2023-08-11 01:27:28+00:00
  edited: false
  hidden: false
  id: 64d59c909fef656cfde7f24d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: johaanm/quantized-french1
repo_type: model
status: open
target_branch: null
title: 'ISSUE: No file named config.json'
