!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Notel
conflicting_files: null
created_at: 2023-11-15 08:54:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/764d37bf2c9bf95929e14ff4e9224a09.svg
      fullname: Joey van Griethuijsen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Notel
      type: user
    createdAt: '2023-11-15T08:54:48.000Z'
    data:
      edited: true
      editors:
      - Notel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8512647151947021
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/764d37bf2c9bf95929e14ff4e9224a09.svg
          fullname: Joey van Griethuijsen
          isHf: false
          isPro: false
          name: Notel
          type: user
        html: '<p>Other GGUF models work fine, but this one gives an empty output.
          I''ve tried both Q4_0 and Q4_K_M. Any idea what I''m doing wrong? I''ve
          tried it with several context lengths, here''s my code:</p>

          <blockquote>

          <p>llm2 = Llama(model_path=model_path, n_threads=conf.cg[''threads''], verbose=True)<br>for
          output in llm2("USER: The world is \nASSISTANT:\n", max_tokens=500, stop=["&lt;/s&gt;"],
          temperature=0.7, echo=True, stream=True):</p>

          </blockquote>

          <blockquote>

          <p>Question:<br>USER: The world is \nASSISTANT:\n</p>

          </blockquote>

          <blockquote>

          <p>Answer:<br>&lt;/s&gt;</p>

          </blockquote>

          <p>(sometimes it also answers with a single dot ".")</p>

          <blockquote>

          <p>llm_load_vocab: mismatch in special tokens definition ( 498/64000 vs
          267/64000 ).<br>llm_load_print_meta: format           = GGUF V3 (latest)<br>llm_load_print_meta:
          arch             = llama<br>llm_load_print_meta: vocab type       = SPM<br>llm_load_print_meta:
          n_vocab          = 64000<br>llm_load_print_meta: n_merges         = 0<br>llm_load_print_meta:
          n_ctx_train      = 200000<br>llm_load_print_meta: n_embd           = 7168<br>llm_load_print_meta:
          n_head           = 56<br>llm_load_print_meta: n_head_kv        = 8<br>llm_load_print_meta:
          n_layer          = 60<br>llm_load_print_meta: n_rot            = 128<br>llm_load_print_meta:
          n_gqa            = 7<br>llm_load_print_meta: f_norm_eps       = 0.0e+00<br>llm_load_print_meta:
          f_norm_rms_eps   = 1.0e-05<br>llm_load_print_meta: f_clamp_kqv      = 0.0e+00<br>llm_load_print_meta:
          f_max_alibi_bias = 0.0e+00<br>llm_load_print_meta: n_ff             = 20480<br>llm_load_print_meta:
          rope scaling     = linear<br>llm_load_print_meta: freq_base_train  = 5000000.0<br>llm_load_print_meta:
          freq_scale_train = 1<br>llm_load_print_meta: n_yarn_orig_ctx  = 200000<br>llm_load_print_meta:
          rope_finetuned   = unknown<br>llm_load_print_meta: model type       = 30B<br>llm_load_print_meta:
          model ftype      = mostly Q4_0<br>llm_load_print_meta: model params     =
          34.39 B<br>llm_load_print_meta: model size       = 18.13 GiB (4.53 BPW)<br>llm_load_print_meta:
          general.name   = nousresearch_nous-capybara-34b<br>llm_load_print_meta:
          BOS token = 144 ''<br>''<br>llm_load_print_meta: EOS token = 2 ''&lt;|endoftext|&gt;''<br>llm_load_print_meta:
          UNK token = 0 ''''<br>llm_load_print_meta: PAD token = 0 ''''<br>llm_load_print_meta:
          LF token  = 315 ''&lt;0x0A&gt;''<br>llm_load_tensors: ggml ctx size =    0.20
          MB<br>llm_load_tensors: mem required  = 18563.49 MB<br>...................................................................................................<br>llama_new_context_with_model:
          n_ctx      = 512<br>llama_new_context_with_model: freq_base  = 5000000.0<br>llama_new_context_with_model:
          freq_scale = 1<br>llama_new_context_with_model: kv self size  =  120.00
          MB<br>llama_build_graph: non-view tensors processed: 1384/1384<br>llama_new_context_with_model:
          compute buffer total size = 140.56 MB<br>AVX = 1 | AVX2 = 1 | AVX512 = 0
          | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 |
          F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0
          | VSX = 0 |</p>

          </blockquote>

          '
        raw: 'Other GGUF models work fine, but this one gives an empty output. I''ve
          tried both Q4_0 and Q4_K_M. Any idea what I''m doing wrong? I''ve tried
          it with several context lengths, here''s my code:


          > llm2 = Llama(model_path=model_path, n_threads=conf.cg[''threads''], verbose=True)

          for output in llm2("USER: The world is \nASSISTANT:\n", max_tokens=500,
          stop=["<\/s>"], temperature=0.7, echo=True, stream=True):


          > Question:

          USER: The world is \nASSISTANT:\n


          > Answer:

          <\/s>


          (sometimes it also answers with a single dot ".</s>")


          > llm_load_vocab: mismatch in special tokens definition ( 498/64000 vs 267/64000
          ).

          llm_load_print_meta: format           = GGUF V3 (latest)

          llm_load_print_meta: arch             = llama

          llm_load_print_meta: vocab type       = SPM

          llm_load_print_meta: n_vocab          = 64000

          llm_load_print_meta: n_merges         = 0

          llm_load_print_meta: n_ctx_train      = 200000

          llm_load_print_meta: n_embd           = 7168

          llm_load_print_meta: n_head           = 56

          llm_load_print_meta: n_head_kv        = 8

          llm_load_print_meta: n_layer          = 60

          llm_load_print_meta: n_rot            = 128

          llm_load_print_meta: n_gqa            = 7

          llm_load_print_meta: f_norm_eps       = 0.0e+00

          llm_load_print_meta: f_norm_rms_eps   = 1.0e-05

          llm_load_print_meta: f_clamp_kqv      = 0.0e+00

          llm_load_print_meta: f_max_alibi_bias = 0.0e+00

          llm_load_print_meta: n_ff             = 20480

          llm_load_print_meta: rope scaling     = linear

          llm_load_print_meta: freq_base_train  = 5000000.0

          llm_load_print_meta: freq_scale_train = 1

          llm_load_print_meta: n_yarn_orig_ctx  = 200000

          llm_load_print_meta: rope_finetuned   = unknown

          llm_load_print_meta: model type       = 30B

          llm_load_print_meta: model ftype      = mostly Q4_0

          llm_load_print_meta: model params     = 34.39 B

          llm_load_print_meta: model size       = 18.13 GiB (4.53 BPW)

          llm_load_print_meta: general.name   = nousresearch_nous-capybara-34b

          llm_load_print_meta: BOS token = 144 ''

          ''

          llm_load_print_meta: EOS token = 2 ''<|endoftext|>''

          llm_load_print_meta: UNK token = 0 ''<unk>''

          llm_load_print_meta: PAD token = 0 ''<unk>''

          llm_load_print_meta: LF token  = 315 ''<0x0A>''

          llm_load_tensors: ggml ctx size =    0.20 MB

          llm_load_tensors: mem required  = 18563.49 MB

          ...................................................................................................

          llama_new_context_with_model: n_ctx      = 512

          llama_new_context_with_model: freq_base  = 5000000.0

          llama_new_context_with_model: freq_scale = 1

          llama_new_context_with_model: kv self size  =  120.00 MB

          llama_build_graph: non-view tensors processed: 1384/1384

          llama_new_context_with_model: compute buffer total size = 140.56 MB

          AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA
          = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 |
          BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |'
        updatedAt: '2023-11-15T08:56:30.322Z'
      numEdits: 2
      reactions: []
    id: 655487586af25dab350b158f
    type: comment
  author: Notel
  content: 'Other GGUF models work fine, but this one gives an empty output. I''ve
    tried both Q4_0 and Q4_K_M. Any idea what I''m doing wrong? I''ve tried it with
    several context lengths, here''s my code:


    > llm2 = Llama(model_path=model_path, n_threads=conf.cg[''threads''], verbose=True)

    for output in llm2("USER: The world is \nASSISTANT:\n", max_tokens=500, stop=["<\/s>"],
    temperature=0.7, echo=True, stream=True):


    > Question:

    USER: The world is \nASSISTANT:\n


    > Answer:

    <\/s>


    (sometimes it also answers with a single dot ".</s>")


    > llm_load_vocab: mismatch in special tokens definition ( 498/64000 vs 267/64000
    ).

    llm_load_print_meta: format           = GGUF V3 (latest)

    llm_load_print_meta: arch             = llama

    llm_load_print_meta: vocab type       = SPM

    llm_load_print_meta: n_vocab          = 64000

    llm_load_print_meta: n_merges         = 0

    llm_load_print_meta: n_ctx_train      = 200000

    llm_load_print_meta: n_embd           = 7168

    llm_load_print_meta: n_head           = 56

    llm_load_print_meta: n_head_kv        = 8

    llm_load_print_meta: n_layer          = 60

    llm_load_print_meta: n_rot            = 128

    llm_load_print_meta: n_gqa            = 7

    llm_load_print_meta: f_norm_eps       = 0.0e+00

    llm_load_print_meta: f_norm_rms_eps   = 1.0e-05

    llm_load_print_meta: f_clamp_kqv      = 0.0e+00

    llm_load_print_meta: f_max_alibi_bias = 0.0e+00

    llm_load_print_meta: n_ff             = 20480

    llm_load_print_meta: rope scaling     = linear

    llm_load_print_meta: freq_base_train  = 5000000.0

    llm_load_print_meta: freq_scale_train = 1

    llm_load_print_meta: n_yarn_orig_ctx  = 200000

    llm_load_print_meta: rope_finetuned   = unknown

    llm_load_print_meta: model type       = 30B

    llm_load_print_meta: model ftype      = mostly Q4_0

    llm_load_print_meta: model params     = 34.39 B

    llm_load_print_meta: model size       = 18.13 GiB (4.53 BPW)

    llm_load_print_meta: general.name   = nousresearch_nous-capybara-34b

    llm_load_print_meta: BOS token = 144 ''

    ''

    llm_load_print_meta: EOS token = 2 ''<|endoftext|>''

    llm_load_print_meta: UNK token = 0 ''<unk>''

    llm_load_print_meta: PAD token = 0 ''<unk>''

    llm_load_print_meta: LF token  = 315 ''<0x0A>''

    llm_load_tensors: ggml ctx size =    0.20 MB

    llm_load_tensors: mem required  = 18563.49 MB

    ...................................................................................................

    llama_new_context_with_model: n_ctx      = 512

    llama_new_context_with_model: freq_base  = 5000000.0

    llama_new_context_with_model: freq_scale = 1

    llama_new_context_with_model: kv self size  =  120.00 MB

    llama_build_graph: non-view tensors processed: 1384/1384

    llama_new_context_with_model: compute buffer total size = 140.56 MB

    AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1
    | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 |
    SSE3 = 1 | SSSE3 = 0 | VSX = 0 |'
  created_at: 2023-11-15 08:54:48+00:00
  edited: true
  hidden: false
  id: 655487586af25dab350b158f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-15T11:45:08.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8860259056091309
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Yeah I see the same - it seems this model is very sensitive to the\
          \ prompt template. The key seems to be not to add a space and/or newline\
          \ after <code>ASSISTANT:</code></p>\n<p>Also, it looks like there shouldn't\
          \ be a newline between USER and ASSISTANT. I have therefore corrected my\
          \ prompt template to be:</p>\n<p><code>USER: {prompt} ASSISTANT:</code></p>\n\
          <p>Test 1:</p>\n<pre><code># space after ASSISTANT:\n-p \"USER:\\nThe world\
          \ is\\nASSISTANT: \"\n\nOutput = bad:\nUSER:\\nThe world is\\nASSISTANT:\
          \ .&lt;/s&gt; [end of text]\n</code></pre>\n<p>Test 2:</p>\n<pre><code>#\
          \ No space after ASSISTANT:\n-p \"USER:\\nThe world is\\nASSISTANT:\"\n\n\
          Output = OK:\nUSER:\\nThe world is\\nASSISTANT: The world is a vast and\
          \ diverse place, full of different cultures, languages, and landscapes.\
          \ It has a rich history that has shaped the lives of ... \n</code></pre>\n\
          <p>Test 3:</p>\n<pre><code># Same again\n-p \"USER:\\nThe world is\\nASSISTANT:\"\
          \n\nOutput = bad:\nUSER:\\nThe world is\\nASSISTANT: full of possibilities.&lt;/s&gt;\
          \ [end of text]\n</code></pre>\n<p>Test 4:</p>\n<pre><code># No newlines,\
          \ no space on end\n-p \"USER: The world is ASSISTANT:\"\n\nOutput = OK:\n\
          The world is a vast and complex place, filled with an infinite number of\
          \ experiences, perspectives, cultures, and ideas. It is a place of wonder,\
          \ mystery, and awe-inspiring beauty, as well as great challenges and hardships.\
          \ The world is ...\n</code></pre>\n<p>I did multiple more tests of Test\
          \ 4 and all were fine, so that's what I've updated the prompt template to\
          \ be.</p>\n"
        raw: "Yeah I see the same - it seems this model is very sensitive to the prompt\
          \ template. The key seems to be not to add a space and/or newline after\
          \ `ASSISTANT:`\n\nAlso, it looks like there shouldn't be a newline between\
          \ USER and ASSISTANT. I have therefore corrected my prompt template to be:\n\
          \n```USER: {prompt} ASSISTANT:```\n\nTest 1:\n```\n# space after ASSISTANT:\n\
          -p \"USER:\\nThe world is\\nASSISTANT: \"\n\nOutput = bad:\nUSER:\\nThe\
          \ world is\\nASSISTANT: .</s> [end of text]\n```\nTest 2:\n```\n# No space\
          \ after ASSISTANT:\n-p \"USER:\\nThe world is\\nASSISTANT:\"\n\nOutput =\
          \ OK:\nUSER:\\nThe world is\\nASSISTANT: The world is a vast and diverse\
          \ place, full of different cultures, languages, and landscapes. It has a\
          \ rich history that has shaped the lives of ... \n```\n\nTest 3:\n```\n\
          # Same again\n-p \"USER:\\nThe world is\\nASSISTANT:\"\n\nOutput = bad:\n\
          USER:\\nThe world is\\nASSISTANT: full of possibilities.</s> [end of text]\n\
          ```\n\nTest 4:\n```\n# No newlines, no space on end\n-p \"USER: The world\
          \ is ASSISTANT:\"\n\nOutput = OK:\nThe world is a vast and complex place,\
          \ filled with an infinite number of experiences, perspectives, cultures,\
          \ and ideas. It is a place of wonder, mystery, and awe-inspiring beauty,\
          \ as well as great challenges and hardships. The world is ...\n```\n\nI\
          \ did multiple more tests of Test 4 and all were fine, so that's what I've\
          \ updated the prompt template to be."
        updatedAt: '2023-11-15T11:45:21.413Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - YearZero
        - tofufishes
        - newsletter
    id: 6554af44d7b239fd39cdb573
    type: comment
  author: TheBloke
  content: "Yeah I see the same - it seems this model is very sensitive to the prompt\
    \ template. The key seems to be not to add a space and/or newline after `ASSISTANT:`\n\
    \nAlso, it looks like there shouldn't be a newline between USER and ASSISTANT.\
    \ I have therefore corrected my prompt template to be:\n\n```USER: {prompt} ASSISTANT:```\n\
    \nTest 1:\n```\n# space after ASSISTANT:\n-p \"USER:\\nThe world is\\nASSISTANT:\
    \ \"\n\nOutput = bad:\nUSER:\\nThe world is\\nASSISTANT: .</s> [end of text]\n\
    ```\nTest 2:\n```\n# No space after ASSISTANT:\n-p \"USER:\\nThe world is\\nASSISTANT:\"\
    \n\nOutput = OK:\nUSER:\\nThe world is\\nASSISTANT: The world is a vast and diverse\
    \ place, full of different cultures, languages, and landscapes. It has a rich\
    \ history that has shaped the lives of ... \n```\n\nTest 3:\n```\n# Same again\n\
    -p \"USER:\\nThe world is\\nASSISTANT:\"\n\nOutput = bad:\nUSER:\\nThe world is\\\
    nASSISTANT: full of possibilities.</s> [end of text]\n```\n\nTest 4:\n```\n# No\
    \ newlines, no space on end\n-p \"USER: The world is ASSISTANT:\"\n\nOutput =\
    \ OK:\nThe world is a vast and complex place, filled with an infinite number of\
    \ experiences, perspectives, cultures, and ideas. It is a place of wonder, mystery,\
    \ and awe-inspiring beauty, as well as great challenges and hardships. The world\
    \ is ...\n```\n\nI did multiple more tests of Test 4 and all were fine, so that's\
    \ what I've updated the prompt template to be."
  created_at: 2023-11-15 11:45:08+00:00
  edited: true
  hidden: false
  id: 6554af44d7b239fd39cdb573
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/764d37bf2c9bf95929e14ff4e9224a09.svg
      fullname: Joey van Griethuijsen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Notel
      type: user
    createdAt: '2023-11-15T14:45:09.000Z'
    data:
      edited: false
      editors:
      - Notel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9793576002120972
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/764d37bf2c9bf95929e14ff4e9224a09.svg
          fullname: Joey van Griethuijsen
          isHf: false
          isPro: false
          name: Notel
          type: user
        html: '<p>Wow it works! Didn''t figure to look at the template, appearently
          it''s indeed very sensitive. Thanks so much for the quick response Tom!!
          Going to put this model to the test now.</p>

          '
        raw: Wow it works! Didn't figure to look at the template, appearently it's
          indeed very sensitive. Thanks so much for the quick response Tom!! Going
          to put this model to the test now.
        updatedAt: '2023-11-15T14:45:09.724Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - YearZero
        - tculler91
    id: 6554d9756f40f4d62478828b
    type: comment
  author: Notel
  content: Wow it works! Didn't figure to look at the template, appearently it's indeed
    very sensitive. Thanks so much for the quick response Tom!! Going to put this
    model to the test now.
  created_at: 2023-11-15 14:45:09+00:00
  edited: false
  hidden: false
  id: 6554d9756f40f4d62478828b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/wVUbcSysTa9KRoheKNWR4.jpeg?w=200&h=200&f=face
      fullname: Thilo E.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThiloteE
      type: user
    createdAt: '2024-01-10T11:31:19.000Z'
    data:
      edited: true
      editors:
      - ThiloteE
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9536508917808533
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/wVUbcSysTa9KRoheKNWR4.jpeg?w=200&h=200&f=face
          fullname: Thilo E.
          isHf: false
          isPro: false
          name: ThiloteE
          type: user
        html: '<p>For some reason, I only get ok outputs with</p>

          <pre><code># newlines in front, no space on end

          "\nUSER: The world is\nASSISTANT:"


          Output = OK:

          The world is a complex and interconnected place, made up of many different
          cultures, languages, and people. It is characterized by its natural beauty,
          diverse landscapes, and rich history. The world is also ...

          </code></pre>

          '
        raw: 'For some reason, I only get ok outputs with


          ```

          # newlines in front, no space on end

          "\nUSER: The world is\nASSISTANT:"


          Output = OK:

          The world is a complex and interconnected place, made up of many different
          cultures, languages, and people. It is characterized by its natural beauty,
          diverse landscapes, and rich history. The world is also ...

          ```'
        updatedAt: '2024-01-10T11:33:03.552Z'
      numEdits: 1
      reactions: []
    id: 659e8007640a62fe9ed61df4
    type: comment
  author: ThiloteE
  content: 'For some reason, I only get ok outputs with


    ```

    # newlines in front, no space on end

    "\nUSER: The world is\nASSISTANT:"


    Output = OK:

    The world is a complex and interconnected place, made up of many different cultures,
    languages, and people. It is characterized by its natural beauty, diverse landscapes,
    and rich history. The world is also ...

    ```'
  created_at: 2024-01-10 11:31:19+00:00
  edited: true
  hidden: false
  id: 659e8007640a62fe9ed61df4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Nous-Capybara-34B-GGUF
repo_type: model
status: open
target_branch: null
title: Getting empty output with latest llama-cpp-python (0.2.18)
