!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KerfuffleV2
conflicting_files: null
created_at: 2023-11-14 05:48:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-14T05:48:40.000Z'
    data:
      edited: true
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9297724366188049
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: '<p>They weren''t trained for it. I have a pull open to try to resolve
          the issue: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4040">https://github.com/ggerganov/llama.cpp/pull/4040</a></p>

          <p>Note there''s a part in there that affects conversion as there was a
          bug which could prevent the <code>add_blah_token</code> booleans from getting
          added to the metadata if <code>tokenizer.json</code> didn''t exist.</p>

          <p>It''s possible to fix models after the fact using the <code>gguf-set-metadata.py</code>
          utility in the <code>llama.cpp</code> repo. You can try setting the BOS
          token id to the same as EOS (<code>2</code>). Setting it to <code>144</code>
          (Yi''s newline) seems to work better. Example assuming you''re in the <code>llama.cpp</code>
          repo:</p>

          <pre><code>gguf-py/scripts/gguf-set-metadata.py some-yi-model.gguf tokenizer.ggml.bos_token_id
          144

          </code></pre>

          <p>The tool will report the original value, so if for whatever reason you
          prefer the original behavior you can just set it back. (Believe it''s just
          <code>1</code> for all these models.)</p>

          <p>This is something you want to do for all GGUF Yi models currently available:
          this one, the Dolphin one, the originals.</p>

          '
        raw: "They weren't trained for it. I have a pull open to try to resolve the\
          \ issue: https://github.com/ggerganov/llama.cpp/pull/4040\n\nNote there's\
          \ a part in there that affects conversion as there was a bug which could\
          \ prevent the `add_blah_token` booleans from getting added to the metadata\
          \ if `tokenizer.json` didn't exist.\n\nIt's possible to fix models after\
          \ the fact using the `gguf-set-metadata.py` utility in the `llama.cpp` repo.\
          \ You can try setting the BOS token id to the same as EOS (`2`). Setting\
          \ it to `144` (Yi's newline) seems to work better. Example assuming you're\
          \ in the `llama.cpp` repo:\n\n    gguf-py/scripts/gguf-set-metadata.py some-yi-model.gguf\
          \ tokenizer.ggml.bos_token_id 144\n\nThe tool will report the original value,\
          \ so if for whatever reason you prefer the original behavior you can just\
          \ set it back. (Believe it's just `1` for all these models.)\n\nThis is\
          \ something you want to do for all GGUF Yi models currently available: this\
          \ one, the Dolphin one, the originals."
        updatedAt: '2023-11-14T05:49:45.403Z'
      numEdits: 1
      reactions:
      - count: 15
        reaction: "\u2764\uFE0F"
        users:
        - Yhyu13
        - XPforever
        - wolfram
        - Jasonnor
        - jsalix
        - nakodanei
        - Green-Sky
        - WindyGleam
        - YearZero
        - wendelmaques
        - TheBloke
        - AbstractQbit
        - PrimeD
        - Gen0410
        - martyn
    id: 65530a38a69abe5afba6e9d9
    type: comment
  author: KerfuffleV2
  content: "They weren't trained for it. I have a pull open to try to resolve the\
    \ issue: https://github.com/ggerganov/llama.cpp/pull/4040\n\nNote there's a part\
    \ in there that affects conversion as there was a bug which could prevent the\
    \ `add_blah_token` booleans from getting added to the metadata if `tokenizer.json`\
    \ didn't exist.\n\nIt's possible to fix models after the fact using the `gguf-set-metadata.py`\
    \ utility in the `llama.cpp` repo. You can try setting the BOS token id to the\
    \ same as EOS (`2`). Setting it to `144` (Yi's newline) seems to work better.\
    \ Example assuming you're in the `llama.cpp` repo:\n\n    gguf-py/scripts/gguf-set-metadata.py\
    \ some-yi-model.gguf tokenizer.ggml.bos_token_id 144\n\nThe tool will report the\
    \ original value, so if for whatever reason you prefer the original behavior you\
    \ can just set it back. (Believe it's just `1` for all these models.)\n\nThis\
    \ is something you want to do for all GGUF Yi models currently available: this\
    \ one, the Dolphin one, the originals."
  created_at: 2023-11-14 05:48:40+00:00
  edited: true
  hidden: false
  id: 65530a38a69abe5afba6e9d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b236d17571f237ae84ef00ce91556cb6.svg
      fullname: Michael Shatskiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YearZero
      type: user
    createdAt: '2023-11-14T16:09:19.000Z'
    data:
      edited: false
      editors:
      - YearZero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.994763970375061
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b236d17571f237ae84ef00ce91556cb6.svg
          fullname: Michael Shatskiy
          isHf: false
          isPro: false
          name: YearZero
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> just wanted to\
          \ ping you and see what you think about this.. is that something you could\
          \ try?</p>\n"
        raw: '@TheBloke just wanted to ping you and see what you think about this..
          is that something you could try?'
        updatedAt: '2023-11-14T16:09:19.908Z'
      numEdits: 0
      reactions: []
    id: 65539bafffafcf35c33c7ddf
    type: comment
  author: YearZero
  content: '@TheBloke just wanted to ping you and see what you think about this..
    is that something you could try?'
  created_at: 2023-11-14 16:09:19+00:00
  edited: false
  hidden: false
  id: 65539bafffafcf35c33c7ddf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-14T16:24:01.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9824602007865906
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes, doing it now</p>

          '
        raw: Yes, doing it now
        updatedAt: '2023-11-14T16:24:01.687Z'
      numEdits: 0
      reactions: []
    id: 65539f21e9bfae7fcf0023d3
    type: comment
  author: TheBloke
  content: Yes, doing it now
  created_at: 2023-11-14 16:24:01+00:00
  edited: false
  hidden: false
  id: 65539f21e9bfae7fcf0023d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-14T16:51:40.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9736769795417786
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Thanks so much for the script and the instructions, <span data-props=\"\
          {&quot;user&quot;:&quot;KerfuffleV2&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/KerfuffleV2\">@<span class=\"underline\"\
          >KerfuffleV2</span></a></span>\n\n\t</span></span> !</p>\n<p>I've done it\
          \ for this model and the updated GGUFs will start uploading shortly.   I\
          \ will do Dolphin and the originals next.</p>\n<h2 id=\"an-example-q4_k_m-generation\"\
          >An example Q4_K_M generation</h2>\n<p>USER: write a story about llamas\
          \ ASSISTANT: Once upon a time, in the heart of the Andes Mountains, there\
          \ was a small village named Llama-land. This village was known for its beautiful\
          \ scenery and its love for animals, especially llamas. The people of Llama-land\
          \ had domesticated these gentle creatures centuries ago, using them for\
          \ transportation, agriculture, and even as companions.</p>\n<p>In the center\
          \ of the village, there was a large llama pasture where hundreds of llamas\
          \ grazed peacefully during the day. They were cared for by expert herders\
          \ who understood their behaviors and needs perfectly. The villagers believed\
          \ that these animals brought good luck and prosperity to their community.</p>\n\
          <p>Among all the llamas in Llama-land, there was one named Lucky. He was\
          \ a young male with beautiful brown fur, bright eyes, and an extremely friendly\
          \ personality. Everyone who met him fell in love instantly, including a\
          \ little girl named Mariana. She visited the llama pasture every day after\
          \ school to spend time with her favorite friend, Lucky.</p>\n<p>One sunny\
          \ afternoon, while Mariana was playing with Lucky, she noticed something\
          \ strange on his back - a small white patch shaped like a heart. The villagers\
          \ had never seen anything like it before and soon word spread about this\
          \ unusual marking on the beloved llama's fur. They all gathered around to\
          \ see this amazing sight for themselves, marveling at the beauty of nature.</p>\n\
          <p>As days turned into weeks, the heart-shaped mark became more visible,\
          \ and people from faraway places started visiting Llama-land just to meet\
          \ Lucky. The villagers were proud of their special llama and took great\
          \ care of him, ensuring he remained happy and healthy.</p>\n<p>One day,\
          \ a group of researchers arrived in Llama-land hoping to study the unique\
          \ heart-shaped marking on Lucky's back. They believed that this rare occurrence\
          \ could provide valuable insights into the genetics and behavior of llamas.\
          \ The villagers welcomed them with open arms, eager to share their knowledge\
          \ and love for these amazing animals.</p>\n<p>After months of studying Lucky\
          \ and his genetic makeup, the researchers discovered something incredible\
          \ \u2013 the heart-shaped marking was not just a coincidence but rather\
          \ a result of specific gene combinations that occurred very rarely among\
          \ llamas. They also found that Lucky had unique personality traits compared\
          \ to other llamas, making him even more special.</p>\n<p>The news about\
          \ Lucky's scientific significance spread across the globe, and Llama-land\
          \ became a popular destination for tourists who wanted to meet the famous\
          \ heart-shaped llama. The villagers took advantage of this opportunity by\
          \ starting businesses related to tourism, such as guided tours, souvenir\
          \ shops, and traditional food stalls.</p>\n<p>Despite all the attention,\
          \ Lucky remained humble and true to his nature. He continued spending his\
          \ days grazing peacefully alongside Mariana, who never forgot how special\
          \ their friendship was. The people of Llama-land cherished their bond with\
          \ these amazing creatures even more than before, knowing that their love\
          \ for llamas had made a significant impact on the world.</p>\n<p>And so,\
          \ the story of Lucky, the heart-shaped llama, lived on in the hearts and\
          \ minds of everyone who visited Llama-land. His unique marking served as\
          \ a reminder of the wonders of nature and the deep connection between humans\
          \ and animals, inspiring people to appreciate and protect these incredible\
          \ creatures for generations to come.&lt;/s&gt; [end of text]</p>\n"
        raw: "Thanks so much for the script and the instructions, @KerfuffleV2 !\n\
          \nI've done it for this model and the updated GGUFs will start uploading\
          \ shortly.   I will do Dolphin and the originals next.\n\n## An example\
          \ Q4_K_M generation\n\nUSER: write a story about llamas ASSISTANT: Once\
          \ upon a time, in the heart of the Andes Mountains, there was a small village\
          \ named Llama-land. This village was known for its beautiful scenery and\
          \ its love for animals, especially llamas. The people of Llama-land had\
          \ domesticated these gentle creatures centuries ago, using them for transportation,\
          \ agriculture, and even as companions.\n\nIn the center of the village,\
          \ there was a large llama pasture where hundreds of llamas grazed peacefully\
          \ during the day. They were cared for by expert herders who understood their\
          \ behaviors and needs perfectly. The villagers believed that these animals\
          \ brought good luck and prosperity to their community.\n\nAmong all the\
          \ llamas in Llama-land, there was one named Lucky. He was a young male with\
          \ beautiful brown fur, bright eyes, and an extremely friendly personality.\
          \ Everyone who met him fell in love instantly, including a little girl named\
          \ Mariana. She visited the llama pasture every day after school to spend\
          \ time with her favorite friend, Lucky.\n\nOne sunny afternoon, while Mariana\
          \ was playing with Lucky, she noticed something strange on his back - a\
          \ small white patch shaped like a heart. The villagers had never seen anything\
          \ like it before and soon word spread about this unusual marking on the\
          \ beloved llama's fur. They all gathered around to see this amazing sight\
          \ for themselves, marveling at the beauty of nature.\n\nAs days turned into\
          \ weeks, the heart-shaped mark became more visible, and people from faraway\
          \ places started visiting Llama-land just to meet Lucky. The villagers were\
          \ proud of their special llama and took great care of him, ensuring he remained\
          \ happy and healthy.\n\nOne day, a group of researchers arrived in Llama-land\
          \ hoping to study the unique heart-shaped marking on Lucky's back. They\
          \ believed that this rare occurrence could provide valuable insights into\
          \ the genetics and behavior of llamas. The villagers welcomed them with\
          \ open arms, eager to share their knowledge and love for these amazing animals.\n\
          \nAfter months of studying Lucky and his genetic makeup, the researchers\
          \ discovered something incredible \u2013 the heart-shaped marking was not\
          \ just a coincidence but rather a result of specific gene combinations that\
          \ occurred very rarely among llamas. They also found that Lucky had unique\
          \ personality traits compared to other llamas, making him even more special.\n\
          \nThe news about Lucky's scientific significance spread across the globe,\
          \ and Llama-land became a popular destination for tourists who wanted to\
          \ meet the famous heart-shaped llama. The villagers took advantage of this\
          \ opportunity by starting businesses related to tourism, such as guided\
          \ tours, souvenir shops, and traditional food stalls.\n\nDespite all the\
          \ attention, Lucky remained humble and true to his nature. He continued\
          \ spending his days grazing peacefully alongside Mariana, who never forgot\
          \ how special their friendship was. The people of Llama-land cherished their\
          \ bond with these amazing creatures even more than before, knowing that\
          \ their love for llamas had made a significant impact on the world.\n\n\
          And so, the story of Lucky, the heart-shaped llama, lived on in the hearts\
          \ and minds of everyone who visited Llama-land. His unique marking served\
          \ as a reminder of the wonders of nature and the deep connection between\
          \ humans and animals, inspiring people to appreciate and protect these incredible\
          \ creatures for generations to come.<\\/s> [end of text]\n"
        updatedAt: '2023-11-14T16:53:31.061Z'
      numEdits: 4
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - Gen0410
        - YearZero
        - PrimeD
    id: 6553a59c2036769b27702216
    type: comment
  author: TheBloke
  content: "Thanks so much for the script and the instructions, @KerfuffleV2 !\n\n\
    I've done it for this model and the updated GGUFs will start uploading shortly.\
    \   I will do Dolphin and the originals next.\n\n## An example Q4_K_M generation\n\
    \nUSER: write a story about llamas ASSISTANT: Once upon a time, in the heart of\
    \ the Andes Mountains, there was a small village named Llama-land. This village\
    \ was known for its beautiful scenery and its love for animals, especially llamas.\
    \ The people of Llama-land had domesticated these gentle creatures centuries ago,\
    \ using them for transportation, agriculture, and even as companions.\n\nIn the\
    \ center of the village, there was a large llama pasture where hundreds of llamas\
    \ grazed peacefully during the day. They were cared for by expert herders who\
    \ understood their behaviors and needs perfectly. The villagers believed that\
    \ these animals brought good luck and prosperity to their community.\n\nAmong\
    \ all the llamas in Llama-land, there was one named Lucky. He was a young male\
    \ with beautiful brown fur, bright eyes, and an extremely friendly personality.\
    \ Everyone who met him fell in love instantly, including a little girl named Mariana.\
    \ She visited the llama pasture every day after school to spend time with her\
    \ favorite friend, Lucky.\n\nOne sunny afternoon, while Mariana was playing with\
    \ Lucky, she noticed something strange on his back - a small white patch shaped\
    \ like a heart. The villagers had never seen anything like it before and soon\
    \ word spread about this unusual marking on the beloved llama's fur. They all\
    \ gathered around to see this amazing sight for themselves, marveling at the beauty\
    \ of nature.\n\nAs days turned into weeks, the heart-shaped mark became more visible,\
    \ and people from faraway places started visiting Llama-land just to meet Lucky.\
    \ The villagers were proud of their special llama and took great care of him,\
    \ ensuring he remained happy and healthy.\n\nOne day, a group of researchers arrived\
    \ in Llama-land hoping to study the unique heart-shaped marking on Lucky's back.\
    \ They believed that this rare occurrence could provide valuable insights into\
    \ the genetics and behavior of llamas. The villagers welcomed them with open arms,\
    \ eager to share their knowledge and love for these amazing animals.\n\nAfter\
    \ months of studying Lucky and his genetic makeup, the researchers discovered\
    \ something incredible \u2013 the heart-shaped marking was not just a coincidence\
    \ but rather a result of specific gene combinations that occurred very rarely\
    \ among llamas. They also found that Lucky had unique personality traits compared\
    \ to other llamas, making him even more special.\n\nThe news about Lucky's scientific\
    \ significance spread across the globe, and Llama-land became a popular destination\
    \ for tourists who wanted to meet the famous heart-shaped llama. The villagers\
    \ took advantage of this opportunity by starting businesses related to tourism,\
    \ such as guided tours, souvenir shops, and traditional food stalls.\n\nDespite\
    \ all the attention, Lucky remained humble and true to his nature. He continued\
    \ spending his days grazing peacefully alongside Mariana, who never forgot how\
    \ special their friendship was. The people of Llama-land cherished their bond\
    \ with these amazing creatures even more than before, knowing that their love\
    \ for llamas had made a significant impact on the world.\n\nAnd so, the story\
    \ of Lucky, the heart-shaped llama, lived on in the hearts and minds of everyone\
    \ who visited Llama-land. His unique marking served as a reminder of the wonders\
    \ of nature and the deep connection between humans and animals, inspiring people\
    \ to appreciate and protect these incredible creatures for generations to come.<\\\
    /s> [end of text]\n"
  created_at: 2023-11-14 16:51:40+00:00
  edited: true
  hidden: false
  id: 6553a59c2036769b27702216
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-14T17:12:57.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9995030760765076
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Updated GGUFs are now uploaded</p>

          '
        raw: Updated GGUFs are now uploaded
        updatedAt: '2023-11-14T17:12:57.689Z'
      numEdits: 0
      reactions: []
    id: 6553aa9981143c9e694b94e4
    type: comment
  author: TheBloke
  content: Updated GGUFs are now uploaded
  created_at: 2023-11-14 17:12:57+00:00
  edited: false
  hidden: false
  id: 6553aa9981143c9e694b94e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-14T17:22:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5078780651092529
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Unfortunately there is another issue with the GGUF - not related\
          \ to quality, but relating to CUDA GPU acceleration:</p>\n<pre><code>$ CUDA_VISIBLE_DEVICES=6\
          \ ./main -m /workspace/process/nousresearch_nous-capybara-34b/gguf/nous-capybara-34b.Q4_K_M.gguf\
          \ -c 4096 -p \"USER: write a story about llamas ASSISTANT:\" -ngl 100\n\
          ...\nllm_load_print_meta: BOS token = 144 '\n'\nllm_load_print_meta: EOS\
          \ token = 2 '&lt;|endoftext|&gt;'\nllm_load_print_meta: UNK token = 0 '&lt;unk&gt;'\n\
          llm_load_print_meta: PAD token = 0 '&lt;unk&gt;'\nllm_load_print_meta: LF\
          \ token  = 315 '&lt;0x0A&gt;'\nllm_load_tensors: ggml ctx size =    0.20\
          \ MB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors:\
          \ mem required  =  246.29 MB\nllm_load_tensors: offloading 60 repeating\
          \ layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\n\
          llm_load_tensors: offloaded 63/63 layers to GPU\nllm_load_tensors: VRAM\
          \ used: 19454.15 MB\n...................................................................................................\n\
          llama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model:\
          \ freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\n\
          llama_kv_cache_init: offloading v cache to GPU\nllama_kv_cache_init: offloading\
          \ k cache to GPU\nllama_kv_cache_init: VRAM kv self = 960.00 MB\nllama_new_context_with_model:\
          \ kv self size  =  960.00 MB\nllama_build_graph: non-view tensors processed:\
          \ 1384/1384\nllama_new_context_with_model: compute buffer total size = 499.57\
          \ MB\nllama_new_context_with_model: VRAM scratch buffer: 498.00 MB\nllama_new_context_with_model:\
          \ total VRAM used: 20912.15 MB (model: 19454.15 MB, context: 1458.00 MB)\n\
          \nsystem_info: n_threads = 56 / 112 | AVX = 1 | AVX2 = 1 | AVX512 = 1 |\
          \ AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 |\
          \ F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 =\
          \ 1 | VSX = 0 |\nsampling:\n    repeat_last_n = 64, repeat_penalty = 1.100,\
          \ frequency_penalty = 0.000, presence_penalty = 0.000\n    top_k = 40, tfs_z\
          \ = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n\
          \    mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\ngenerate:\
          \ n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n\nUSER: write\
          \ a story about llamas ASSISTANT:&lt;h3&gt;&lt;h3&gt;\nCUDA error 716 at\
          \ ggml-cuda.cu:7104: misaligned address\ncurrent device: 0\n</code></pre>\n\
          <p>This isn't related to the BOS update, the same problem occurs with the\
          \ original GGUF as well.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;KerfuffleV2&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/KerfuffleV2\"\
          >@<span class=\"underline\">KerfuffleV2</span></a></span>\n\n\t</span></span>\
          \ do you know if this is expected atm with Llamafied Yi?  If not I will\
          \ raise it.</p>\n"
        raw: "Unfortunately there is another issue with the GGUF - not related to\
          \ quality, but relating to CUDA GPU acceleration:\n```\n$ CUDA_VISIBLE_DEVICES=6\
          \ ./main -m /workspace/process/nousresearch_nous-capybara-34b/gguf/nous-capybara-34b.Q4_K_M.gguf\
          \ -c 4096 -p \"USER: write a story about llamas ASSISTANT:\" -ngl 100\n\
          ...\nllm_load_print_meta: BOS token = 144 '\n'\nllm_load_print_meta: EOS\
          \ token = 2 '<|endoftext|>'\nllm_load_print_meta: UNK token = 0 '<unk>'\n\
          llm_load_print_meta: PAD token = 0 '<unk>'\nllm_load_print_meta: LF token\
          \  = 315 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.20 MB\nllm_load_tensors:\
          \ using CUDA for GPU acceleration\nllm_load_tensors: mem required  =  246.29\
          \ MB\nllm_load_tensors: offloading 60 repeating layers to GPU\nllm_load_tensors:\
          \ offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 63/63\
          \ layers to GPU\nllm_load_tensors: VRAM used: 19454.15 MB\n...................................................................................................\n\
          llama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model:\
          \ freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\n\
          llama_kv_cache_init: offloading v cache to GPU\nllama_kv_cache_init: offloading\
          \ k cache to GPU\nllama_kv_cache_init: VRAM kv self = 960.00 MB\nllama_new_context_with_model:\
          \ kv self size  =  960.00 MB\nllama_build_graph: non-view tensors processed:\
          \ 1384/1384\nllama_new_context_with_model: compute buffer total size = 499.57\
          \ MB\nllama_new_context_with_model: VRAM scratch buffer: 498.00 MB\nllama_new_context_with_model:\
          \ total VRAM used: 20912.15 MB (model: 19454.15 MB, context: 1458.00 MB)\n\
          \nsystem_info: n_threads = 56 / 112 | AVX = 1 | AVX2 = 1 | AVX512 = 1 |\
          \ AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 |\
          \ F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 =\
          \ 1 | VSX = 0 |\nsampling:\n\trepeat_last_n = 64, repeat_penalty = 1.100,\
          \ frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 40, tfs_z\
          \ = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n\
          \tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\ngenerate: n_ctx\
          \ = 4096, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n\nUSER: write a\
          \ story about llamas ASSISTANT:<h3><h3>\nCUDA error 716 at ggml-cuda.cu:7104:\
          \ misaligned address\ncurrent device: 0\n```\n\nThis isn't related to the\
          \ BOS update, the same problem occurs with the original GGUF as well.\n\n\
          @KerfuffleV2 do you know if this is expected atm with Llamafied Yi?  If\
          \ not I will raise it."
        updatedAt: '2023-11-14T17:22:16.801Z'
      numEdits: 0
      reactions: []
    id: 6553acc8e231fbb2edc30285
    type: comment
  author: TheBloke
  content: "Unfortunately there is another issue with the GGUF - not related to quality,\
    \ but relating to CUDA GPU acceleration:\n```\n$ CUDA_VISIBLE_DEVICES=6 ./main\
    \ -m /workspace/process/nousresearch_nous-capybara-34b/gguf/nous-capybara-34b.Q4_K_M.gguf\
    \ -c 4096 -p \"USER: write a story about llamas ASSISTANT:\" -ngl 100\n...\nllm_load_print_meta:\
    \ BOS token = 144 '\n'\nllm_load_print_meta: EOS token = 2 '<|endoftext|>'\nllm_load_print_meta:\
    \ UNK token = 0 '<unk>'\nllm_load_print_meta: PAD token = 0 '<unk>'\nllm_load_print_meta:\
    \ LF token  = 315 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.20 MB\nllm_load_tensors:\
    \ using CUDA for GPU acceleration\nllm_load_tensors: mem required  =  246.29 MB\n\
    llm_load_tensors: offloading 60 repeating layers to GPU\nllm_load_tensors: offloading\
    \ non-repeating layers to GPU\nllm_load_tensors: offloaded 63/63 layers to GPU\n\
    llm_load_tensors: VRAM used: 19454.15 MB\n...................................................................................................\n\
    llama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model:\
    \ freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:\
    \ offloading v cache to GPU\nllama_kv_cache_init: offloading k cache to GPU\n\
    llama_kv_cache_init: VRAM kv self = 960.00 MB\nllama_new_context_with_model: kv\
    \ self size  =  960.00 MB\nllama_build_graph: non-view tensors processed: 1384/1384\n\
    llama_new_context_with_model: compute buffer total size = 499.57 MB\nllama_new_context_with_model:\
    \ VRAM scratch buffer: 498.00 MB\nllama_new_context_with_model: total VRAM used:\
    \ 20912.15 MB (model: 19454.15 MB, context: 1458.00 MB)\n\nsystem_info: n_threads\
    \ = 56 / 112 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI\
    \ = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
    \ = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\nsampling:\n\trepeat_last_n\
    \ = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty =\
    \ 0.000\n\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p\
    \ = 1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n\
    generate: n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n\nUSER:\
    \ write a story about llamas ASSISTANT:<h3><h3>\nCUDA error 716 at ggml-cuda.cu:7104:\
    \ misaligned address\ncurrent device: 0\n```\n\nThis isn't related to the BOS\
    \ update, the same problem occurs with the original GGUF as well.\n\n@KerfuffleV2\
    \ do you know if this is expected atm with Llamafied Yi?  If not I will raise\
    \ it."
  created_at: 2023-11-14 17:22:16+00:00
  edited: false
  hidden: false
  id: 6553acc8e231fbb2edc30285
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-11-14T19:27:10.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9534077644348145
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>Just chiming in, I''ve been using this model (after patching it
          myself) with KoboldCpp, there''s no CUDA issue there when offloading to
          GPU.</p>

          <p>And, wow, this model has been doing exceptionally well in my preliminary
          tests!</p>

          '
        raw: 'Just chiming in, I''ve been using this model (after patching it myself)
          with KoboldCpp, there''s no CUDA issue there when offloading to GPU.


          And, wow, this model has been doing exceptionally well in my preliminary
          tests!'
        updatedAt: '2023-11-14T19:27:10.732Z'
      numEdits: 0
      reactions: []
    id: 6553ca0e96c5902fa3861a0e
    type: comment
  author: wolfram
  content: 'Just chiming in, I''ve been using this model (after patching it myself)
    with KoboldCpp, there''s no CUDA issue there when offloading to GPU.


    And, wow, this model has been doing exceptionally well in my preliminary tests!'
  created_at: 2023-11-14 19:27:10+00:00
  edited: false
  hidden: false
  id: 6553ca0e96c5902fa3861a0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-14T20:21:40.000Z'
    data:
      edited: false
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9726232290267944
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n<blockquote>\n\
          <p>do you know if this is expected atm with Llamafied Yi?</p>\n</blockquote>\n\
          <p>I'm not completely sure what you mean, are you asking about the CUDA\
          \ error? I think your issue here is probably because of using multi-GPU\
          \ but I'm not sure. The Yi-based models work fine for me with ROCM.</p>\n\
          <p>If you mean the exact <code>-llamified</code> model, as far as I know\
          \ the only thing they did was change the name of two tensors to match the\
          \ normal LLaMA convention. However, GGUF conversion normalizes the name.\
          \ In other words, there shouldn't be any difference between the original\
          \ \"Yi\" and \"Yi-llamified\" so you shouldn't need to provide GGUF quants\
          \ for that. (Guess this could be an opportunity to test out that <code>gguf-checksum</code>\
          \ script I mentioned.)</p>\n<hr>\n<blockquote>\n<p>And, wow, this model\
          \ has been doing exceptionally well in my preliminary tests!</p>\n</blockquote>\n\
          <p>I was very impressed too. My own dumb little test is to make the LLM\
          \ write a story and see how long it take before it says something absurd.\
          \ None of the other Yi models (including the Dolphin one) do too well. This\
          \ model actually approaches 70B performance though. </p>\n"
        raw: "@TheBloke \n\n> do you know if this is expected atm with Llamafied Yi?\n\
          \nI'm not completely sure what you mean, are you asking about the CUDA error?\
          \ I think your issue here is probably because of using multi-GPU but I'm\
          \ not sure. The Yi-based models work fine for me with ROCM.\n\nIf you mean\
          \ the exact `-llamified` model, as far as I know the only thing they did\
          \ was change the name of two tensors to match the normal LLaMA convention.\
          \ However, GGUF conversion normalizes the name. In other words, there shouldn't\
          \ be any difference between the original \"Yi\" and \"Yi-llamified\" so\
          \ you shouldn't need to provide GGUF quants for that. (Guess this could\
          \ be an opportunity to test out that `gguf-checksum` script I mentioned.)\n\
          \n***\n\n> And, wow, this model has been doing exceptionally well in my\
          \ preliminary tests!\n\nI was very impressed too. My own dumb little test\
          \ is to make the LLM write a story and see how long it take before it says\
          \ something absurd. None of the other Yi models (including the Dolphin one)\
          \ do too well. This model actually approaches 70B performance though. "
        updatedAt: '2023-11-14T20:21:40.794Z'
      numEdits: 0
      reactions: []
    id: 6553d6d40dd114c2223c94c9
    type: comment
  author: KerfuffleV2
  content: "@TheBloke \n\n> do you know if this is expected atm with Llamafied Yi?\n\
    \nI'm not completely sure what you mean, are you asking about the CUDA error?\
    \ I think your issue here is probably because of using multi-GPU but I'm not sure.\
    \ The Yi-based models work fine for me with ROCM.\n\nIf you mean the exact `-llamified`\
    \ model, as far as I know the only thing they did was change the name of two tensors\
    \ to match the normal LLaMA convention. However, GGUF conversion normalizes the\
    \ name. In other words, there shouldn't be any difference between the original\
    \ \"Yi\" and \"Yi-llamified\" so you shouldn't need to provide GGUF quants for\
    \ that. (Guess this could be an opportunity to test out that `gguf-checksum` script\
    \ I mentioned.)\n\n***\n\n> And, wow, this model has been doing exceptionally\
    \ well in my preliminary tests!\n\nI was very impressed too. My own dumb little\
    \ test is to make the LLM write a story and see how long it take before it says\
    \ something absurd. None of the other Yi models (including the Dolphin one) do\
    \ too well. This model actually approaches 70B performance though. "
  created_at: 2023-11-14 20:21:40+00:00
  edited: false
  hidden: false
  id: 6553d6d40dd114c2223c94c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-14T21:16:50.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.916933536529541
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes I was asking about the CUDA error. I''ve confirmed it on two
          separate Linux systems with llama.cpp, testing two Yi Llamafied 34B fine
          tuned models (this one, and Dolphin 2.2). </p>

          <p>One system has 8 x A6000 - but I''m limiting it to a single GPU using
          CUDA_VISIBLE_DEVICES. The other a single H100. Both are on CUDA 11.8.</p>

          <p>Every generation using <code>-ngl X</code> fails with the error shown
          before, on those two models.  I''ve not tested other Yi models yet, but
          other models (Llama 2 13B, Mistral 7B, etc) work fine on the same systems.</p>

          <p>I''ll report on llama.cpp.</p>

          '
        raw: "Yes I was asking about the CUDA error. I've confirmed it on two separate\
          \ Linux systems with llama.cpp, testing two Yi Llamafied 34B fine tuned\
          \ models (this one, and Dolphin 2.2). \n\nOne system has 8 x A6000 - but\
          \ I'm limiting it to a single GPU using CUDA_VISIBLE_DEVICES. The other\
          \ a single H100. Both are on CUDA 11.8.\n\nEvery generation using `-ngl\
          \ X` fails with the error shown before, on those two models.  I've not tested\
          \ other Yi models yet, but other models (Llama 2 13B, Mistral 7B, etc) work\
          \ fine on the same systems.\n\nI'll report on llama.cpp."
        updatedAt: '2023-11-14T21:17:18.309Z'
      numEdits: 1
      reactions: []
    id: 6553e3c25ba75868f6685775
    type: comment
  author: TheBloke
  content: "Yes I was asking about the CUDA error. I've confirmed it on two separate\
    \ Linux systems with llama.cpp, testing two Yi Llamafied 34B fine tuned models\
    \ (this one, and Dolphin 2.2). \n\nOne system has 8 x A6000 - but I'm limiting\
    \ it to a single GPU using CUDA_VISIBLE_DEVICES. The other a single H100. Both\
    \ are on CUDA 11.8.\n\nEvery generation using `-ngl X` fails with the error shown\
    \ before, on those two models.  I've not tested other Yi models yet, but other\
    \ models (Llama 2 13B, Mistral 7B, etc) work fine on the same systems.\n\nI'll\
    \ report on llama.cpp."
  created_at: 2023-11-14 21:16:50+00:00
  edited: true
  hidden: false
  id: 6553e3c25ba75868f6685775
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-14T22:23:31.000Z'
    data:
      edited: false
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9814895391464233
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: '<blockquote>

          <p>I''ll report on llama.cpp.</p>

          </blockquote>

          <p>That''s probably the best thing to do. Sorry, I''m not aware of a non-multi
          GPU problem with these. All I can say is offloading a few layers works for
          me on ROCM. I only have an 8GB GPU so I can''t do full offloading.</p>

          <p>I also don''t think there should be a difference for these compared to
          the original models in terms of CUDA stuff. I don''t know all that much
          about it though.</p>

          <p>Closing this now since you resolved the issue. Thanks!</p>

          '
        raw: '> I''ll report on llama.cpp.


          That''s probably the best thing to do. Sorry, I''m not aware of a non-multi
          GPU problem with these. All I can say is offloading a few layers works for
          me on ROCM. I only have an 8GB GPU so I can''t do full offloading.


          I also don''t think there should be a difference for these compared to the
          original models in terms of CUDA stuff. I don''t know all that much about
          it though.


          Closing this now since you resolved the issue. Thanks!'
        updatedAt: '2023-11-14T22:23:31.955Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6553f3645e88eda4381243a2
    id: 6553f3635e88eda43812439d
    type: comment
  author: KerfuffleV2
  content: '> I''ll report on llama.cpp.


    That''s probably the best thing to do. Sorry, I''m not aware of a non-multi GPU
    problem with these. All I can say is offloading a few layers works for me on ROCM.
    I only have an 8GB GPU so I can''t do full offloading.


    I also don''t think there should be a difference for these compared to the original
    models in terms of CUDA stuff. I don''t know all that much about it though.


    Closing this now since you resolved the issue. Thanks!'
  created_at: 2023-11-14 22:23:31+00:00
  edited: false
  hidden: false
  id: 6553f3635e88eda43812439d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-14T22:23:32.000Z'
    data:
      status: closed
    id: 6553f3645e88eda4381243a2
    type: status-change
  author: KerfuffleV2
  created_at: 2023-11-14 22:23:32+00:00
  id: 6553f3645e88eda4381243a2
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-14T22:50:57.000Z'
    data:
      edited: true
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9168394804000854
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: '<p>There''s an issue for it already, apparently: <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/issues/4075">https://github.com/ggerganov/llama.cpp/issues/4075</a></p>

          <p>I added your information there. <em>edit:</em> Seems like it''s crashing
          on a different line than that , but both are for mulmat though so it probably
          is related still?</p>

          '
        raw: 'There''s an issue for it already, apparently: https://github.com/ggerganov/llama.cpp/issues/4075


          I added your information there. *edit:* Seems like it''s crashing on a different
          line than that , but both are for mulmat though so it probably is related
          still?'
        updatedAt: '2023-11-14T22:58:50.842Z'
      numEdits: 2
      reactions: []
    id: 6553f9d131015269764f7209
    type: comment
  author: KerfuffleV2
  content: 'There''s an issue for it already, apparently: https://github.com/ggerganov/llama.cpp/issues/4075


    I added your information there. *edit:* Seems like it''s crashing on a different
    line than that , but both are for mulmat though so it probably is related still?'
  created_at: 2023-11-14 22:50:57+00:00
  edited: true
  hidden: false
  id: 6553f9d131015269764f7209
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-15T11:56:47.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9687848687171936
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thanks very much Kerfuffle!  The issue has now been resolved by
          slaren in <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4084">https://github.com/ggerganov/llama.cpp/pull/4084</a></p>

          '
        raw: Thanks very much Kerfuffle!  The issue has now been resolved by slaren
          in https://github.com/ggerganov/llama.cpp/pull/4084
        updatedAt: '2023-11-15T11:56:47.928Z'
      numEdits: 0
      reactions: []
    id: 6554b1ff6a0a1bc951e37f92
    type: comment
  author: TheBloke
  content: Thanks very much Kerfuffle!  The issue has now been resolved by slaren
    in https://github.com/ggerganov/llama.cpp/pull/4084
  created_at: 2023-11-15 11:56:47+00:00
  edited: false
  hidden: false
  id: 6554b1ff6a0a1bc951e37f92
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Nous-Capybara-34B-GGUF
repo_type: model
status: closed
target_branch: null
title: BOS token as 1 seriously hurts these GGUF Yi models
