!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Henk717
conflicting_files: null
created_at: 2023-11-14 18:35:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2023-11-14T18:35:01.000Z'
    data:
      edited: true
      editors:
      - Henk717
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7883539199829102
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
          fullname: Henky!!
          isHf: false
          isPro: false
          name: Henk717
          type: user
        html: '<p>Using Koboldcpp the model reports &lt;|endoftext|&gt; as the Endoftext
          token, however the finetune generates<code> &lt;/s&gt;</code> as the end
          of text token.<br>This makes our usual techniques to produce nice outputs
          fail on this model, might be an upstream issue but the GGUF is definately
          not doing it right.</p>

          '
        raw: 'Using Koboldcpp the model reports <|endoftext|> as the Endoftext token,
          however the finetune generates`` </s>`` as the end of text token.

          This makes our usual techniques to produce nice outputs fail on this model,
          might be an upstream issue but the GGUF is definately not doing it right.'
        updatedAt: '2023-11-14T18:37:31.183Z'
      numEdits: 1
      reactions: []
    id: 6553bdd58b3b7b9656eef874
    type: comment
  author: Henk717
  content: 'Using Koboldcpp the model reports <|endoftext|> as the Endoftext token,
    however the finetune generates`` </s>`` as the end of text token.

    This makes our usual techniques to produce nice outputs fail on this model, might
    be an upstream issue but the GGUF is definately not doing it right.'
  created_at: 2023-11-14 18:35:01+00:00
  edited: true
  hidden: false
  id: 6553bdd58b3b7b9656eef874
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e3b34f0605e6e2c9b5c5beb1a9c192f.svg
      fullname: Xiao Jin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mljxy
      type: user
    createdAt: '2023-11-14T18:41:04.000Z'
    data:
      edited: false
      editors:
      - mljxy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8886794447898865
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e3b34f0605e6e2c9b5c5beb1a9c192f.svg
          fullname: Xiao Jin
          isHf: false
          isPro: false
          name: mljxy
          type: user
        html: '<p>upstream issue. The generated <code>&lt;/s&gt;</code> is not from
          a single token.</p>

          '
        raw: upstream issue. The generated `</s>` is not from a single token.
        updatedAt: '2023-11-14T18:41:04.349Z'
      numEdits: 0
      reactions: []
    id: 6553bf404b8517fd01929418
    type: comment
  author: mljxy
  content: upstream issue. The generated `</s>` is not from a single token.
  created_at: 2023-11-14 18:41:04+00:00
  edited: false
  hidden: false
  id: 6553bf404b8517fd01929418
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2023-11-14T19:47:55.000Z'
    data:
      edited: false
      editors:
      - Henk717
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9722908139228821
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
          fullname: Henky!!
          isHf: false
          isPro: false
          name: Henk717
          type: user
        html: '<p>Reported it to them to, but if its not from a single token thats
          probably going to require a full retune.</p>

          '
        raw: Reported it to them to, but if its not from a single token thats probably
          going to require a full retune.
        updatedAt: '2023-11-14T19:47:55.804Z'
      numEdits: 0
      reactions: []
    id: 6553ceeb3f708f209ccefdfe
    type: comment
  author: Henk717
  content: Reported it to them to, but if its not from a single token thats probably
    going to require a full retune.
  created_at: 2023-11-14 19:47:55+00:00
  edited: false
  hidden: false
  id: 6553ceeb3f708f209ccefdfe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-11-14T19:51:10.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9869598746299744
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>Yes, noticed that as well. SillyTavern is filtering it out, but
          I still am seeing it pop up before it gets deleted.</p>

          '
        raw: Yes, noticed that as well. SillyTavern is filtering it out, but I still
          am seeing it pop up before it gets deleted.
        updatedAt: '2023-11-14T19:51:10.077Z'
      numEdits: 0
      reactions: []
    id: 6553cfae5ba75868f6644f5a
    type: comment
  author: wolfram
  content: Yes, noticed that as well. SillyTavern is filtering it out, but I still
    am seeing it pop up before it gets deleted.
  created_at: 2023-11-14 19:51:10+00:00
  edited: false
  hidden: false
  id: 6553cfae5ba75868f6644f5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-14T21:10:07.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9755483865737915
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I noticed that rogue <code>&lt;/s&gt;</code> at the end of
          llama.cpp generation as well. It stops at the right point, but that extra
          token is there at the end.</p>

          <p>I was going to suggest that we could use Kerfuffle''s new script to set
          the GGUF EOS token to <code>&lt;/s&gt;</code> but if you''re saying it''s
          not a single token ID, then I guess we can''t?</p>

          '
        raw: 'Yeah I noticed that rogue `</s>` at the end of llama.cpp generation
          as well. It stops at the right point, but that extra token is there at the
          end.


          I was going to suggest that we could use Kerfuffle''s new script to set
          the GGUF EOS token to `</s>` but if you''re saying it''s not a single token
          ID, then I guess we can''t?'
        updatedAt: '2023-11-14T21:10:07.259Z'
      numEdits: 0
      reactions: []
    id: 6553e22feeb42b373ff26b26
    type: comment
  author: TheBloke
  content: 'Yeah I noticed that rogue `</s>` at the end of llama.cpp generation as
    well. It stops at the right point, but that extra token is there at the end.


    I was going to suggest that we could use Kerfuffle''s new script to set the GGUF
    EOS token to `</s>` but if you''re saying it''s not a single token ID, then I
    guess we can''t?'
  created_at: 2023-11-14 21:10:07+00:00
  edited: false
  hidden: false
  id: 6553e22feeb42b373ff26b26
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-14T22:26:53.000Z'
    data:
      edited: false
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8960682153701782
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: '<p>If you don''t actually want the model to general HTML/code then
          you could possibly try setting logit biases that ban tokens that start with
          <code>&lt;</code>. If it can''t produce the weird <code>&lt;/s&gt;</code>
          thing it might generate an EOS. If it just isn''t generating EOS correctly,
          that won''t help (but it will get rid of stray <code>&lt;/s&gt;</code> in
          the output possibly. The base Yi model token id for <code>&lt;/</code> is
          1359 so with <code>llama.cpp</code> at least you can specify something like
          <code>-l 1359-inf</code> to ban the token.</p>

          '
        raw: If you don't actually want the model to general HTML/code then you could
          possibly try setting logit biases that ban tokens that start with `<`. If
          it can't produce the weird `</s>` thing it might generate an EOS. If it
          just isn't generating EOS correctly, that won't help (but it will get rid
          of stray `</s>` in the output possibly. The base Yi model token id for `</`
          is 1359 so with `llama.cpp` at least you can specify something like `-l
          1359-inf` to ban the token.
        updatedAt: '2023-11-14T22:26:53.146Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Yhyu13
    id: 6553f42d854adee4f52fd8c9
    type: comment
  author: KerfuffleV2
  content: If you don't actually want the model to general HTML/code then you could
    possibly try setting logit biases that ban tokens that start with `<`. If it can't
    produce the weird `</s>` thing it might generate an EOS. If it just isn't generating
    EOS correctly, that won't help (but it will get rid of stray `</s>` in the output
    possibly. The base Yi model token id for `</` is 1359 so with `llama.cpp` at least
    you can specify something like `-l 1359-inf` to ban the token.
  created_at: 2023-11-14 22:26:53+00:00
  edited: false
  hidden: false
  id: 6553f42d854adee4f52fd8c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2023-11-16T00:09:04.000Z'
    data:
      edited: true
      editors:
      - Henk717
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7735169529914856
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
          fullname: Henky!!
          isHf: false
          isPro: false
          name: Henk717
          type: user
        html: '<p>Workaround for Koboldcpp users is to use <code>&lt;/s&gt;</code>
          as your stop sequence, problem is that you can''t tell the model to keep
          generating that way so its short responses only.</p>

          '
        raw: Workaround for Koboldcpp users is to use ``</s>`` as your stop sequence,
          problem is that you can't tell the model to keep generating that way so
          its short responses only.
        updatedAt: '2023-11-16T00:09:13.481Z'
      numEdits: 1
      reactions: []
    id: 65555da0995d14130cefc842
    type: comment
  author: Henk717
  content: Workaround for Koboldcpp users is to use ``</s>`` as your stop sequence,
    problem is that you can't tell the model to keep generating that way so its short
    responses only.
  created_at: 2023-11-16 00:09:04+00:00
  edited: true
  hidden: false
  id: 65555da0995d14130cefc842
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aad204488134c26ea0b26d70e02f573d.svg
      fullname: Riki M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phooney
      type: user
    createdAt: '2024-01-02T04:17:29.000Z'
    data:
      edited: false
      editors:
      - phooney
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9325414299964905
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aad204488134c26ea0b26d70e02f573d.svg
          fullname: Riki M
          isHf: false
          isPro: false
          name: phooney
          type: user
        html: '<p>Is there any update on this one?  I''m using the GPTQ model and
          EXLLama2 on Kobold United and it has the same issue, but I think I can only
          use stop sequences when using it via the API?  Any workaround for using  as
          a stop sequence within the UI itself?</p>

          '
        raw: Is there any update on this one?  I'm using the GPTQ model and EXLLama2
          on Kobold United and it has the same issue, but I think I can only use stop
          sequences when using it via the API?  Any workaround for using </s> as a
          stop sequence within the UI itself?
        updatedAt: '2024-01-02T04:17:29.073Z'
      numEdits: 0
      reactions: []
    id: 65938e591adf6d577e12ce00
    type: comment
  author: phooney
  content: Is there any update on this one?  I'm using the GPTQ model and EXLLama2
    on Kobold United and it has the same issue, but I think I can only use stop sequences
    when using it via the API?  Any workaround for using </s> as a stop sequence within
    the UI itself?
  created_at: 2024-01-02 04:17:29+00:00
  edited: false
  hidden: false
  id: 65938e591adf6d577e12ce00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2024-01-03T03:17:41.000Z'
    data:
      edited: false
      editors:
      - Henk717
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9791343808174133
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
          fullname: Henky!!
          isHf: false
          isPro: false
          name: Henk717
          type: user
        html: '<p>Lite has it, and in the normal UI you might be having more luck
          with phrase biasing. At this point its safe to say the model won''t be fixed
          and people should move on to their hermes or a different tune instead.</p>

          '
        raw: Lite has it, and in the normal UI you might be having more luck with
          phrase biasing. At this point its safe to say the model won't be fixed and
          people should move on to their hermes or a different tune instead.
        updatedAt: '2024-01-03T03:17:41.841Z'
      numEdits: 0
      reactions: []
    id: 6594d1d5a78a27780377ed00
    type: comment
  author: Henk717
  content: Lite has it, and in the normal UI you might be having more luck with phrase
    biasing. At this point its safe to say the model won't be fixed and people should
    move on to their hermes or a different tune instead.
  created_at: 2024-01-03 03:17:41+00:00
  edited: false
  hidden: false
  id: 6594d1d5a78a27780377ed00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aad204488134c26ea0b26d70e02f573d.svg
      fullname: Riki M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phooney
      type: user
    createdAt: '2024-01-03T12:52:45.000Z'
    data:
      edited: false
      editors:
      - phooney
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9785431027412415
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aad204488134c26ea0b26d70e02f573d.svg
          fullname: Riki M
          isHf: false
          isPro: false
          name: phooney
          type: user
        html: '<p>Ah, that''s a shame, this seems like such a smart model at the size
          otherwise.</p>

          '
        raw: Ah, that's a shame, this seems like such a smart model at the size otherwise.
        updatedAt: '2024-01-03T12:52:45.015Z'
      numEdits: 0
      reactions: []
    id: 6595589d24c58cdc92109edf
    type: comment
  author: phooney
  content: Ah, that's a shame, this seems like such a smart model at the size otherwise.
  created_at: 2024-01-03 12:52:45+00:00
  edited: false
  hidden: false
  id: 6595589d24c58cdc92109edf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Nous-Capybara-34B-GGUF
repo_type: model
status: open
target_branch: null
title: Incorrect EOS token
