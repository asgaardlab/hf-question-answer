!!python/object:huggingface_hub.community.DiscussionWithDetails
author: monkmartinez
conflicting_files: null
created_at: 2023-04-08 18:47:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632e2ab7183321090a5ff116/NFRJKDs_PVKwqGZblzDEk.jpeg?w=200&h=200&f=face
      fullname: Michael Martinez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: monkmartinez
      type: user
    createdAt: '2023-04-08T19:47:18.000Z'
    data:
      edited: false
      editors:
      - monkmartinez
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632e2ab7183321090a5ff116/NFRJKDs_PVKwqGZblzDEk.jpeg?w=200&h=200&f=face
          fullname: Michael Martinez
          isHf: false
          isPro: false
          name: monkmartinez
          type: user
        html: '<p>Thank you for posting this as I have been struggling to convert
          myself.</p>

          <p>My only problem is not related to the model itself, but with inference
          performance. The 13GB unquantized models are just sooooo slow with text-generation-webui
          and my system. For reference, I have a Dell Precision with 32GB of RAM and
          Quadro P6000 with 24GB of VRAM.</p>

          <p>13B: 1.13 token/s to 1.58token/s with this model.<br>7B: 6.85 token/s</p>

          <p>So you can see it is painful to use the 13B models with that kind of
          speed... have any ideas to speed it up?</p>

          '
        raw: "Thank you for posting this as I have been struggling to convert myself.\r\
          \n\r\nMy only problem is not related to the model itself, but with inference\
          \ performance. The 13GB unquantized models are just sooooo slow with text-generation-webui\
          \ and my system. For reference, I have a Dell Precision with 32GB of RAM\
          \ and Quadro P6000 with 24GB of VRAM.\r\n\r\n13B: 1.13 token/s to 1.58token/s\
          \ with this model. \r\n7B: 6.85 token/s\r\n\r\nSo you can see it is painful\
          \ to use the 13B models with that kind of speed... have any ideas to speed\
          \ it up?\r\n"
        updatedAt: '2023-04-08T19:47:18.372Z'
      numEdits: 0
      reactions: []
    id: 6431c4c61e22a07ceb8c59c4
    type: comment
  author: monkmartinez
  content: "Thank you for posting this as I have been struggling to convert myself.\r\
    \n\r\nMy only problem is not related to the model itself, but with inference performance.\
    \ The 13GB unquantized models are just sooooo slow with text-generation-webui\
    \ and my system. For reference, I have a Dell Precision with 32GB of RAM and Quadro\
    \ P6000 with 24GB of VRAM.\r\n\r\n13B: 1.13 token/s to 1.58token/s with this model.\
    \ \r\n7B: 6.85 token/s\r\n\r\nSo you can see it is painful to use the 13B models\
    \ with that kind of speed... have any ideas to speed it up?\r\n"
  created_at: 2023-04-08 18:47:18+00:00
  edited: false
  hidden: false
  id: 6431c4c61e22a07ceb8c59c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-08T19:49:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah, quantisation should speed it up.  You commented in my other
          repo, where I was describing all the problems I was having loading Koala
          myself.  That repo is the GPTQ quantised versions of the 7B repo.</p>

          <p>They take up a lot less space than the full model, requiring less RAM
          or VRAM, and should infer faster.</p>

          <p>I''ve not run quantisation for 13B yet but will do it shortly and will
          upload it soon.</p>

          '
        raw: 'Yeah, quantisation should speed it up.  You commented in my other repo,
          where I was describing all the problems I was having loading Koala myself.  That
          repo is the GPTQ quantised versions of the 7B repo.


          They take up a lot less space than the full model, requiring less RAM or
          VRAM, and should infer faster.


          I''ve not run quantisation for 13B yet but will do it shortly and will upload
          it soon.'
        updatedAt: '2023-04-08T19:49:45.362Z'
      numEdits: 0
      reactions: []
    id: 6431c5597d795d8e7032d690
    type: comment
  author: TheBloke
  content: 'Yeah, quantisation should speed it up.  You commented in my other repo,
    where I was describing all the problems I was having loading Koala myself.  That
    repo is the GPTQ quantised versions of the 7B repo.


    They take up a lot less space than the full model, requiring less RAM or VRAM,
    and should infer faster.


    I''ve not run quantisation for 13B yet but will do it shortly and will upload
    it soon.'
  created_at: 2023-04-08 18:49:45+00:00
  edited: false
  hidden: false
  id: 6431c5597d795d8e7032d690
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/koala-13B-HF
repo_type: model
status: open
target_branch: null
title: Awesome... but slow (at least on my system)
