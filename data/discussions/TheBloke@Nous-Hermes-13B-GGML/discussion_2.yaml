!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mancub
conflicting_files: null
created_at: 2023-06-07 00:14:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-07T01:14:54.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9558998346328735
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Could you please give us the lowdown on all of these q''s, I''m
          totally lost? What kinds of benefits are in using the new k-quant methods
          (if any) over what we are used to so far?</p>

          <p>I read the model card and, but I don''t understand now what should I
          be using. Maybe offer suggestions for those who do GPU+CPU vs CPU with GGML...</p>

          <p>TIA!</p>

          '
        raw: "Could you please give us the lowdown on all of these q's, I'm totally\
          \ lost? What kinds of benefits are in using the new k-quant methods (if\
          \ any) over what we are used to so far?\r\n\r\nI read the model card and,\
          \ but I don't understand now what should I be using. Maybe offer suggestions\
          \ for those who do GPU+CPU vs CPU with GGML...\r\n\r\nTIA!"
        updatedAt: '2023-06-07T01:14:54.605Z'
      numEdits: 0
      reactions: []
    id: 647fda0e80fd1d698c6de4ea
    type: comment
  author: mancub
  content: "Could you please give us the lowdown on all of these q's, I'm totally\
    \ lost? What kinds of benefits are in using the new k-quant methods (if any) over\
    \ what we are used to so far?\r\n\r\nI read the model card and, but I don't understand\
    \ now what should I be using. Maybe offer suggestions for those who do GPU+CPU\
    \ vs CPU with GGML...\r\n\r\nTIA!"
  created_at: 2023-06-07 00:14:54+00:00
  edited: false
  hidden: false
  id: 647fda0e80fd1d698c6de4ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-06-07T01:21:56.000Z'
    data:
      edited: true
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9809914827346802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>I was trying to figure out the same. I was lurking around llamacpp
          github page to find the answer, but what I found only led me to more confusion
          lol, so I second this. I would like to know the definitive answer what are
          the benefits if any, because according to what I''ve read there, the benefit
          might be speed, but that depends on the version of k-quantizer and hardware,
          there were some tests which showed some performance improvement and then
          there were other tests which showed performance decrease, so yeah. I would
          wait for a good explanation and avoid these new versions until them. Also,
          if you''re using KoboldCpp like I do, avoid it from the simple principle
          that KoboldCpp hasn''t been updated to use the latest version of llamacpp
          which introduced support for these new k-quantizers. Hopefully a new version
          will be out soon enough in couple of next days.</p>

          '
        raw: I was trying to figure out the same. I was lurking around llamacpp github
          page to find the answer, but what I found only led me to more confusion
          lol, so I second this. I would like to know the definitive answer what are
          the benefits if any, because according to what I've read there, the benefit
          might be speed, but that depends on the version of k-quantizer and hardware,
          there were some tests which showed some performance improvement and then
          there were other tests which showed performance decrease, so yeah. I would
          wait for a good explanation and avoid these new versions until them. Also,
          if you're using KoboldCpp like I do, avoid it from the simple principle
          that KoboldCpp hasn't been updated to use the latest version of llamacpp
          which introduced support for these new k-quantizers. Hopefully a new version
          will be out soon enough in couple of next days.
        updatedAt: '2023-06-07T01:23:44.091Z'
      numEdits: 1
      reactions: []
    id: 647fdbb4cbb8294ed80e6523
    type: comment
  author: MrDevolver
  content: I was trying to figure out the same. I was lurking around llamacpp github
    page to find the answer, but what I found only led me to more confusion lol, so
    I second this. I would like to know the definitive answer what are the benefits
    if any, because according to what I've read there, the benefit might be speed,
    but that depends on the version of k-quantizer and hardware, there were some tests
    which showed some performance improvement and then there were other tests which
    showed performance decrease, so yeah. I would wait for a good explanation and
    avoid these new versions until them. Also, if you're using KoboldCpp like I do,
    avoid it from the simple principle that KoboldCpp hasn't been updated to use the
    latest version of llamacpp which introduced support for these new k-quantizers.
    Hopefully a new version will be out soon enough in couple of next days.
  created_at: 2023-06-07 00:21:56+00:00
  edited: true
  hidden: false
  id: 647fdbb4cbb8294ed80e6523
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-07T01:23:10.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.917932391166687
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Here''s a table:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/_uHJrmRlYuw4BCscqZtcN.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/_uHJrmRlYuw4BCscqZtcN.png"></a></p>

          <p>Here''s a simplified view of the table:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/J9kk29zuKpmbMuxrOUfEd.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/J9kk29zuKpmbMuxrOUfEd.png"></a></p>

          <p>Comparing the figures in this table to the same data for the original
          quant types in the llama.cpp README:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/cyd1u3ArTJKwi-UeZFEhO.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/cyd1u3ArTJKwi-UeZFEhO.png"></a></p>

          <p>Gives some idea of where we are.</p>

          <p>Beyond that I don''t know specifics, which is one reason why I''m including
          everything, so that people can experiment and then maybe in a week or two
          there will be some consensus as to what formats are worth using longer term.  </p>

          <p>I wouldn''t be surprised if the original methods were dropped at some
          point, leaving ''just'' the newer methods to choose between.</p>

          <p>Here''s a brief snippet from <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/1684">the
          llama.cpp PR that introduced the new quants</a> that may be illuminating
          as to how one might decide what method to use, and how this decision will
          be personal to a given piece of hardware, and a user''s own requirements:</p>

          <p><em>"As we can see from this graph, generation performance as measured
          by perplexity is basically a fairly smooth function of quantized model size,
          and the quantization types added by the PR allow the user to pick the best
          performing quantized model, given the limits of their compute resources
          (in terms of being able to fully load the model into memory, but also in
          terms of inference speed, which tends to depend on the model size). As a
          specific example, the 2-bit quantization of the 30B model fits on the 16
          GB RTX 4080 GPU that I have available, while the others do not, resulting
          in a large difference in inference performance."</em></p>

          '
        raw: "Here's a table:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/_uHJrmRlYuw4BCscqZtcN.png)\n\
          \nHere's a simplified view of the table:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/J9kk29zuKpmbMuxrOUfEd.png)\n\
          \nComparing the figures in this table to the same data for the original\
          \ quant types in the llama.cpp README:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/cyd1u3ArTJKwi-UeZFEhO.png)\n\
          \nGives some idea of where we are.\n\nBeyond that I don't know specifics,\
          \ which is one reason why I'm including everything, so that people can experiment\
          \ and then maybe in a week or two there will be some consensus as to what\
          \ formats are worth using longer term.  \n\nI wouldn't be surprised if the\
          \ original methods were dropped at some point, leaving 'just' the newer\
          \ methods to choose between.\n\nHere's a brief snippet from [the llama.cpp\
          \ PR that introduced the new quants](https://github.com/ggerganov/llama.cpp/pull/1684)\
          \ that may be illuminating as to how one might decide what method to use,\
          \ and how this decision will be personal to a given piece of hardware, and\
          \ a user's own requirements:\n\n*\"As we can see from this graph, generation\
          \ performance as measured by perplexity is basically a fairly smooth function\
          \ of quantized model size, and the quantization types added by the PR allow\
          \ the user to pick the best performing quantized model, given the limits\
          \ of their compute resources (in terms of being able to fully load the model\
          \ into memory, but also in terms of inference speed, which tends to depend\
          \ on the model size). As a specific example, the 2-bit quantization of the\
          \ 30B model fits on the 16 GB RTX 4080 GPU that I have available, while\
          \ the others do not, resulting in a large difference in inference performance.\"\
          *"
        updatedAt: '2023-06-07T01:23:10.444Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - MrDevolver
        - mechanicmuthu
        - cjrd
    id: 647fdbfecbb8294ed80e6dad
    type: comment
  author: TheBloke
  content: "Here's a table:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/_uHJrmRlYuw4BCscqZtcN.png)\n\
    \nHere's a simplified view of the table:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/J9kk29zuKpmbMuxrOUfEd.png)\n\
    \nComparing the figures in this table to the same data for the original quant\
    \ types in the llama.cpp README:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/cyd1u3ArTJKwi-UeZFEhO.png)\n\
    \nGives some idea of where we are.\n\nBeyond that I don't know specifics, which\
    \ is one reason why I'm including everything, so that people can experiment and\
    \ then maybe in a week or two there will be some consensus as to what formats\
    \ are worth using longer term.  \n\nI wouldn't be surprised if the original methods\
    \ were dropped at some point, leaving 'just' the newer methods to choose between.\n\
    \nHere's a brief snippet from [the llama.cpp PR that introduced the new quants](https://github.com/ggerganov/llama.cpp/pull/1684)\
    \ that may be illuminating as to how one might decide what method to use, and\
    \ how this decision will be personal to a given piece of hardware, and a user's\
    \ own requirements:\n\n*\"As we can see from this graph, generation performance\
    \ as measured by perplexity is basically a fairly smooth function of quantized\
    \ model size, and the quantization types added by the PR allow the user to pick\
    \ the best performing quantized model, given the limits of their compute resources\
    \ (in terms of being able to fully load the model into memory, but also in terms\
    \ of inference speed, which tends to depend on the model size). As a specific\
    \ example, the 2-bit quantization of the 30B model fits on the 16 GB RTX 4080\
    \ GPU that I have available, while the others do not, resulting in a large difference\
    \ in inference performance.\"*"
  created_at: 2023-06-07 00:23:10+00:00
  edited: false
  hidden: false
  id: 647fdbfecbb8294ed80e6dad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-07T01:25:11.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9743213653564453
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>From my POV, performance isn''t everything, and I would not trade
          performance over accuracy. I''m definitely interested in knowing are we
          trading in more hallucination for couple of extra tokens per second.</p>

          '
        raw: From my POV, performance isn't everything, and I would not trade performance
          over accuracy. I'm definitely interested in knowing are we trading in more
          hallucination for couple of extra tokens per second.
        updatedAt: '2023-06-07T01:25:11.805Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - faisalhr1997
    id: 647fdc77db6dca2bd2515bfd
    type: comment
  author: mancub
  content: From my POV, performance isn't everything, and I would not trade performance
    over accuracy. I'm definitely interested in knowing are we trading in more hallucination
    for couple of extra tokens per second.
  created_at: 2023-06-07 00:25:11+00:00
  edited: false
  hidden: false
  id: 647fdc77db6dca2bd2515bfd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-07T01:29:55.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9411384463310242
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Perhaps a simplification could be:</p>

          <ol>

          <li>If you don''t care about speed - eg because accuracy is of paramount
          importance to you, and/or because you run queries in the background without
          waiting for responses - then pick the largest model your hardware can physically
          run.</li>

          <li>If you do care about speed, pick the largest model you can completely
          fit into your GPU</li>

          </ol>

          <p>This doesn''t answer every case, eg it doesn''t necessarily help us choose
          between models of different types. Eg should you use WizardLM 30B or Hermes
          13B?  That''s a much more complex question, because the models are trained
          on different data and may have very different performance for a given use
          case.</p>

          <p>But in the ''easy'' case of for example "what should I use out of Guanaco
          7B, 13B, 33B, or 65B", you should theoretically be able to apply the above
          logic and find the optimum model for your HW and use case.</p>

          '
        raw: 'Perhaps a simplification could be:


          1. If you don''t care about speed - eg because accuracy is of paramount
          importance to you, and/or because you run queries in the background without
          waiting for responses - then pick the largest model your hardware can physically
          run.

          2. If you do care about speed, pick the largest model you can completely
          fit into your GPU


          This doesn''t answer every case, eg it doesn''t necessarily help us choose
          between models of different types. Eg should you use WizardLM 30B or Hermes
          13B?  That''s a much more complex question, because the models are trained
          on different data and may have very different performance for a given use
          case.


          But in the ''easy'' case of for example "what should I use out of Guanaco
          7B, 13B, 33B, or 65B", you should theoretically be able to apply the above
          logic and find the optimum model for your HW and use case.'
        updatedAt: '2023-06-07T01:29:55.070Z'
      numEdits: 0
      reactions: []
    id: 647fdd93cbb8294ed80e9082
    type: comment
  author: TheBloke
  content: 'Perhaps a simplification could be:


    1. If you don''t care about speed - eg because accuracy is of paramount importance
    to you, and/or because you run queries in the background without waiting for responses
    - then pick the largest model your hardware can physically run.

    2. If you do care about speed, pick the largest model you can completely fit into
    your GPU


    This doesn''t answer every case, eg it doesn''t necessarily help us choose between
    models of different types. Eg should you use WizardLM 30B or Hermes 13B?  That''s
    a much more complex question, because the models are trained on different data
    and may have very different performance for a given use case.


    But in the ''easy'' case of for example "what should I use out of Guanaco 7B,
    13B, 33B, or 65B", you should theoretically be able to apply the above logic and
    find the optimum model for your HW and use case.'
  created_at: 2023-06-07 00:29:55+00:00
  edited: false
  hidden: false
  id: 647fdd93cbb8294ed80e9082
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-07T01:32:01.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9633413553237915
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>From my POV, performance isn''t everything, and I would not trade performance
          over accuracy. I''m definitely interested in knowing are we trading in more
          hallucination for couple of extra tokens per second.</p>

          </blockquote>

          <p>That''s harder to answer. All we know is the synthetic perplexity benchmark
          scores. We don''t know how that translates into real world performance -
          and how it translates may well vary by use case.</p>

          <p>A difference of 0.1 on perplexity might have no noticeable difference
          for use case X, but might have quite a difference for use case Y.  Or it
          might have no difference for either.</p>

          <p>I don''t think we''ll know that unless and until more comprehensive benchmarking
          is done, over the  coming days and weeks.   </p>

          <p>For now you just to rely on your own and others'' anecdotal experiences.</p>

          '
        raw: "> From my POV, performance isn't everything, and I would not trade performance\
          \ over accuracy. I'm definitely interested in knowing are we trading in\
          \ more hallucination for couple of extra tokens per second.\n\nThat's harder\
          \ to answer. All we know is the synthetic perplexity benchmark scores. We\
          \ don't know how that translates into real world performance - and how it\
          \ translates may well vary by use case.\n\nA difference of 0.1 on perplexity\
          \ might have no noticeable difference for use case X, but might have quite\
          \ a difference for use case Y.  Or it might have no difference for either.\n\
          \nI don't think we'll know that unless and until more comprehensive benchmarking\
          \ is done, over the  coming days and weeks.   \n\nFor now you just to rely\
          \ on your own and others' anecdotal experiences."
        updatedAt: '2023-06-07T01:32:01.180Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mancub
    id: 647fde111637c1c0e6f3586f
    type: comment
  author: TheBloke
  content: "> From my POV, performance isn't everything, and I would not trade performance\
    \ over accuracy. I'm definitely interested in knowing are we trading in more hallucination\
    \ for couple of extra tokens per second.\n\nThat's harder to answer. All we know\
    \ is the synthetic perplexity benchmark scores. We don't know how that translates\
    \ into real world performance - and how it translates may well vary by use case.\n\
    \nA difference of 0.1 on perplexity might have no noticeable difference for use\
    \ case X, but might have quite a difference for use case Y.  Or it might have\
    \ no difference for either.\n\nI don't think we'll know that unless and until\
    \ more comprehensive benchmarking is done, over the  coming days and weeks.  \
    \ \n\nFor now you just to rely on your own and others' anecdotal experiences."
  created_at: 2023-06-07 00:32:01+00:00
  edited: false
  hidden: false
  id: 647fde111637c1c0e6f3586f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-07T01:38:29.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9606053233146667
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>So looking at the tables with my layman''s eyes, on a 13B model
          a Q6_K seems to be the best bet if you have 24GB VRAM since it has about
          the same perplexity score as Q8_0, but 3-4x faster.</p>

          <p>Going to give it a try and report!</p>

          '
        raw: 'So looking at the tables with my layman''s eyes, on a 13B model a Q6_K
          seems to be the best bet if you have 24GB VRAM since it has about the same
          perplexity score as Q8_0, but 3-4x faster.


          Going to give it a try and report!'
        updatedAt: '2023-06-07T01:38:29.105Z'
      numEdits: 0
      reactions: []
    id: 647fdf95cbb8294ed80ec5ff
    type: comment
  author: mancub
  content: 'So looking at the tables with my layman''s eyes, on a 13B model a Q6_K
    seems to be the best bet if you have 24GB VRAM since it has about the same perplexity
    score as Q8_0, but 3-4x faster.


    Going to give it a try and report!'
  created_at: 2023-06-07 00:38:29+00:00
  edited: false
  hidden: false
  id: 647fdf95cbb8294ed80ec5ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-06-07T02:08:52.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9806207418441772
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>So looking at the table just comparing performance of ms/t in 7B''s,
          Q6_K against Q8_0, it looks like Q8_0 performs slightly better in 4th scenario,
          but slightly worse in 8th scenario. I mean lower is better, right? And Q6_K
          has a file size of 5.15GB, whereas Q8_0 has a file size of 6.7GB. I''d say
          that''s expectable, but when it comes to VRAM offloading in times when most
          users would have at least 8GB GPU nowadays, I don''t see much of a benefit
          that would convince me to use Q6_K over Q8_0. I would always think of Q8_0
          being probably more precise, because higher is better, right? And if the
          performance is comparable, there''s no reason to go lower to Q6_K. Still
          I would probably give it a try, because those 8th scenarios caught my eyes,
          because doing my own tests on my hardware, I did notice some models actually
          perform better with 4 threads only instead of 8 threads, which I found pretty
          weird, but if these new k-quantizers performed better on more threads than
          the old versions, that would be something I would like to look at and test.</p>

          '
        raw: So looking at the table just comparing performance of ms/t in 7B's, Q6_K
          against Q8_0, it looks like Q8_0 performs slightly better in 4th scenario,
          but slightly worse in 8th scenario. I mean lower is better, right? And Q6_K
          has a file size of 5.15GB, whereas Q8_0 has a file size of 6.7GB. I'd say
          that's expectable, but when it comes to VRAM offloading in times when most
          users would have at least 8GB GPU nowadays, I don't see much of a benefit
          that would convince me to use Q6_K over Q8_0. I would always think of Q8_0
          being probably more precise, because higher is better, right? And if the
          performance is comparable, there's no reason to go lower to Q6_K. Still
          I would probably give it a try, because those 8th scenarios caught my eyes,
          because doing my own tests on my hardware, I did notice some models actually
          perform better with 4 threads only instead of 8 threads, which I found pretty
          weird, but if these new k-quantizers performed better on more threads than
          the old versions, that would be something I would like to look at and test.
        updatedAt: '2023-06-07T02:08:52.193Z'
      numEdits: 0
      reactions: []
    id: 647fe6b486888bbffbe4582f
    type: comment
  author: MrDevolver
  content: So looking at the table just comparing performance of ms/t in 7B's, Q6_K
    against Q8_0, it looks like Q8_0 performs slightly better in 4th scenario, but
    slightly worse in 8th scenario. I mean lower is better, right? And Q6_K has a
    file size of 5.15GB, whereas Q8_0 has a file size of 6.7GB. I'd say that's expectable,
    but when it comes to VRAM offloading in times when most users would have at least
    8GB GPU nowadays, I don't see much of a benefit that would convince me to use
    Q6_K over Q8_0. I would always think of Q8_0 being probably more precise, because
    higher is better, right? And if the performance is comparable, there's no reason
    to go lower to Q6_K. Still I would probably give it a try, because those 8th scenarios
    caught my eyes, because doing my own tests on my hardware, I did notice some models
    actually perform better with 4 threads only instead of 8 threads, which I found
    pretty weird, but if these new k-quantizers performed better on more threads than
    the old versions, that would be something I would like to look at and test.
  created_at: 2023-06-07 01:08:52+00:00
  edited: false
  hidden: false
  id: 647fe6b486888bbffbe4582f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-06-07T02:18:50.000Z'
    data:
      edited: true
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9343329668045044
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: "<blockquote>\n<p>But in the 'easy' case of for example \"what should\
          \ I use out of Guanaco 7B, 13B, 33B, or 65B\", you should theoretically\
          \ be able to apply the above logic and find the optimum model for your HW\
          \ and use case.</p>\n</blockquote>\n<p>Well, it's easier than that with\
          \ Guanaco, there are no k-quantizer versions for it yet, so the choice is\
          \ pretty much straightforward. \U0001F601</p>\n"
        raw: "> But in the 'easy' case of for example \"what should I use out of Guanaco\
          \ 7B, 13B, 33B, or 65B\", you should theoretically be able to apply the\
          \ above logic and find the optimum model for your HW and use case.\n\nWell,\
          \ it's easier than that with Guanaco, there are no k-quantizer versions\
          \ for it yet, so the choice is pretty much straightforward. \U0001F601"
        updatedAt: '2023-06-07T02:19:22.409Z'
      numEdits: 1
      reactions: []
    id: 647fe90adb6dca2bd252b490
    type: comment
  author: MrDevolver
  content: "> But in the 'easy' case of for example \"what should I use out of Guanaco\
    \ 7B, 13B, 33B, or 65B\", you should theoretically be able to apply the above\
    \ logic and find the optimum model for your HW and use case.\n\nWell, it's easier\
    \ than that with Guanaco, there are no k-quantizer versions for it yet, so the\
    \ choice is pretty much straightforward. \U0001F601"
  created_at: 2023-06-07 01:18:50+00:00
  edited: true
  hidden: false
  id: 647fe90adb6dca2bd252b490
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-07T02:25:14.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5022591352462769
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>:P</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/L2FXjJdPTzc7IDx2WctRN.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/L2FXjJdPTzc7IDx2WctRN.png"></a></p>

          '
        raw: ':P


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/L2FXjJdPTzc7IDx2WctRN.png)'
        updatedAt: '2023-06-07T02:25:14.641Z'
      numEdits: 0
      reactions: []
    id: 647fea8a86888bbffbe4cb07
    type: comment
  author: TheBloke
  content: ':P


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/L2FXjJdPTzc7IDx2WctRN.png)'
  created_at: 2023-06-07 01:25:14+00:00
  edited: false
  hidden: false
  id: 647fea8a86888bbffbe4cb07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-06-07T02:33:42.000Z'
    data:
      edited: true
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9531295895576477
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: "<p>Cool! What's the smallest file size of 65B with these new k-quantizers?\
          \ Still not fitting in 16GB of RAM? \U0001F914 Just kidding (although that\
          \ would be cool lol), but I guess 33B could fit with these new k-quantizers,\
          \ no? \U0001F600</p>\n"
        raw: "Cool! What's the smallest file size of 65B with these new k-quantizers?\
          \ Still not fitting in 16GB of RAM? \U0001F914 Just kidding (although that\
          \ would be cool lol), but I guess 33B could fit with these new k-quantizers,\
          \ no? \U0001F600"
        updatedAt: '2023-06-07T02:34:02.908Z'
      numEdits: 1
      reactions: []
    id: 647fec8686888bbffbe5027a
    type: comment
  author: MrDevolver
  content: "Cool! What's the smallest file size of 65B with these new k-quantizers?\
    \ Still not fitting in 16GB of RAM? \U0001F914 Just kidding (although that would\
    \ be cool lol), but I guess 33B could fit with these new k-quantizers, no? \U0001F600"
  created_at: 2023-06-07 01:33:42+00:00
  edited: true
  hidden: false
  id: 647fec8686888bbffbe5027a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-07T02:35:07.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9381535053253174
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>LOL, it was inevitable!</p>

          <p>I''m getting about 115ms/t with q6_K on my 3090 in WSL2. Loading all
          40 layers into the GPU, with 8 threads (2x XeonE5v3)</p>

          <p>With 10 threads I get ~128ms/t, with 6 threads I get  ~105ms/t and with
          4 threads ~130ms/t.</p>

          <p>More is not better, and it depends on the model, too.</p>

          '
        raw: 'LOL, it was inevitable!


          I''m getting about 115ms/t with q6_K on my 3090 in WSL2. Loading all 40
          layers into the GPU, with 8 threads (2x XeonE5v3)


          With 10 threads I get ~128ms/t, with 6 threads I get  ~105ms/t and with
          4 threads ~130ms/t.


          More is not better, and it depends on the model, too.'
        updatedAt: '2023-06-07T02:35:07.506Z'
      numEdits: 0
      reactions: []
    id: 647fecdb86888bbffbe50a18
    type: comment
  author: mancub
  content: 'LOL, it was inevitable!


    I''m getting about 115ms/t with q6_K on my 3090 in WSL2. Loading all 40 layers
    into the GPU, with 8 threads (2x XeonE5v3)


    With 10 threads I get ~128ms/t, with 6 threads I get  ~105ms/t and with 4 threads
    ~130ms/t.


    More is not better, and it depends on the model, too.'
  created_at: 2023-06-07 01:35:07+00:00
  edited: false
  hidden: false
  id: 647fecdb86888bbffbe50a18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-07T02:37:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9425959587097168
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>26GB for q2_K, meaning around 29GB RAM usage assuming no GPU offload.</p>

          <p>With GPU offload you can reduce that, but you''d need a a 16GB GPU to
          be able to offload enough to get under 16GB RAM usage.  And I''m assuming
          that with 16GB RAM, it''s unlikely you have 16GB VRAM :)  </p>

          <p>But yeah, 33B should definitely be do-able.</p>

          '
        raw: "26GB for q2_K, meaning around 29GB RAM usage assuming no GPU offload.\n\
          \nWith GPU offload you can reduce that, but you'd need a a 16GB GPU to be\
          \ able to offload enough to get under 16GB RAM usage.  And I'm assuming\
          \ that with 16GB RAM, it's unlikely you have 16GB VRAM :)  \n\nBut yeah,\
          \ 33B should definitely be do-able."
        updatedAt: '2023-06-07T02:37:27.326Z'
      numEdits: 0
      reactions: []
    id: 647fed671637c1c0e6f507e4
    type: comment
  author: TheBloke
  content: "26GB for q2_K, meaning around 29GB RAM usage assuming no GPU offload.\n\
    \nWith GPU offload you can reduce that, but you'd need a a 16GB GPU to be able\
    \ to offload enough to get under 16GB RAM usage.  And I'm assuming that with 16GB\
    \ RAM, it's unlikely you have 16GB VRAM :)  \n\nBut yeah, 33B should definitely\
    \ be do-able."
  created_at: 2023-06-07 01:37:27+00:00
  edited: false
  hidden: false
  id: 647fed671637c1c0e6f507e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-07T02:46:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9666852951049805
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>LOL, it was inevitable!</p>

          <p>I''m getting about 115ms/t with q6_K on my 3090 in WSL2. Loading all
          40 layers into the GPU, with 8 threads (2x XeonE5v3)</p>

          <p>With 10 threads I get ~128ms/t, with 6 threads I get  ~105ms/t and with
          4 threads ~130ms/t.</p>

          <p>More is not better, and it depends on the model, too.</p>

          </blockquote>

          <p>Interesting. On my 18-core Intel i9-10980XE, without GPU offload, I found
          it scaled acceptably up to 12 cores. 14 cores was a little faster but not
          by enough to be worth the extra electricity. And 16 was such a tiny increase
          from 14 to be even less worth it. Then at 18 threads it was slower than
          16.</p>

          <p>So I''m wondering if you get diminishing returns earlier with GPU offload.  Although
          it could also just vary by CPU.  What CPU is that?</p>

          '
        raw: "> LOL, it was inevitable!\n> \n> I'm getting about 115ms/t with q6_K\
          \ on my 3090 in WSL2. Loading all 40 layers into the GPU, with 8 threads\
          \ (2x XeonE5v3)\n> \n> With 10 threads I get ~128ms/t, with 6 threads I\
          \ get  ~105ms/t and with 4 threads ~130ms/t.\n> \n> More is not better,\
          \ and it depends on the model, too.\n\nInteresting. On my 18-core Intel\
          \ i9-10980XE, without GPU offload, I found it scaled acceptably up to 12\
          \ cores. 14 cores was a little faster but not by enough to be worth the\
          \ extra electricity. And 16 was such a tiny increase from 14 to be even\
          \ less worth it. Then at 18 threads it was slower than 16.\n\nSo I'm wondering\
          \ if you get diminishing returns earlier with GPU offload.  Although it\
          \ could also just vary by CPU.  What CPU is that?"
        updatedAt: '2023-06-07T02:46:46.463Z'
      numEdits: 0
      reactions: []
    id: 647fef96db6dca2bd2536b25
    type: comment
  author: TheBloke
  content: "> LOL, it was inevitable!\n> \n> I'm getting about 115ms/t with q6_K on\
    \ my 3090 in WSL2. Loading all 40 layers into the GPU, with 8 threads (2x XeonE5v3)\n\
    > \n> With 10 threads I get ~128ms/t, with 6 threads I get  ~105ms/t and with\
    \ 4 threads ~130ms/t.\n> \n> More is not better, and it depends on the model,\
    \ too.\n\nInteresting. On my 18-core Intel i9-10980XE, without GPU offload, I\
    \ found it scaled acceptably up to 12 cores. 14 cores was a little faster but\
    \ not by enough to be worth the extra electricity. And 16 was such a tiny increase\
    \ from 14 to be even less worth it. Then at 18 threads it was slower than 16.\n\
    \nSo I'm wondering if you get diminishing returns earlier with GPU offload.  Although\
    \ it could also just vary by CPU.  What CPU is that?"
  created_at: 2023-06-07 01:46:46+00:00
  edited: false
  hidden: false
  id: 647fef96db6dca2bd2536b25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-06-07T16:25:28.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7903996706008911
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<p>Any reco''s on 65B models on a 4090 24VRAM + 64GIG system?</p>

          <p>Also, what wondering what would work best on an M1 Mac with Metal ( 16
          core GPU ) 32GIG Ram.</p>

          '
        raw: 'Any reco''s on 65B models on a 4090 24VRAM + 64GIG system?


          Also, what wondering what would work best on an M1 Mac with Metal ( 16 core
          GPU ) 32GIG Ram.'
        updatedAt: '2023-06-07T16:25:28.785Z'
      numEdits: 0
      reactions: []
    id: 6480af78bb25a636c9de4599
    type: comment
  author: vdruts
  content: 'Any reco''s on 65B models on a 4090 24VRAM + 64GIG system?


    Also, what wondering what would work best on an M1 Mac with Metal ( 16 core GPU
    ) 32GIG Ram.'
  created_at: 2023-06-07 15:25:28+00:00
  edited: false
  hidden: false
  id: 6480af78bb25a636c9de4599
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-08T00:32:31.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9533095955848694
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n<p>These\
          \ are dual Xeon E5-2690 v3 in Supermicro X10DAi board. RTX 3090 is definitely\
          \ sitting in a PCIe x16 slot but all I ever see is x8 connection. I don't\
          \ know if there's a penalty for x8 connection, but according to some charts\
          \ on the web, my CPUs are bottlenecking the RTX 3090.</p>\n<p>And as you\
          \ mentioned before, single threaded higher clock on more modern CPU works\
          \ better for the current code and models. I'm waiting for some SSDs to arrive\
          \ then I can install native Linux and see just how much WSL2 slows things\
          \ down. :)</p>\n"
        raw: "@TheBloke \n\nThese are dual Xeon E5-2690 v3 in Supermicro X10DAi board.\
          \ RTX 3090 is definitely sitting in a PCIe x16 slot but all I ever see is\
          \ x8 connection. I don't know if there's a penalty for x8 connection, but\
          \ according to some charts on the web, my CPUs are bottlenecking the RTX\
          \ 3090.\n\nAnd as you mentioned before, single threaded higher clock on\
          \ more modern CPU works better for the current code and models. I'm waiting\
          \ for some SSDs to arrive then I can install native Linux and see just how\
          \ much WSL2 slows things down. :)"
        updatedAt: '2023-06-08T00:32:31.174Z'
      numEdits: 0
      reactions: []
    id: 6481219f40facadc55776f1a
    type: comment
  author: mancub
  content: "@TheBloke \n\nThese are dual Xeon E5-2690 v3 in Supermicro X10DAi board.\
    \ RTX 3090 is definitely sitting in a PCIe x16 slot but all I ever see is x8 connection.\
    \ I don't know if there's a penalty for x8 connection, but according to some charts\
    \ on the web, my CPUs are bottlenecking the RTX 3090.\n\nAnd as you mentioned\
    \ before, single threaded higher clock on more modern CPU works better for the\
    \ current code and models. I'm waiting for some SSDs to arrive then I can install\
    \ native Linux and see just how much WSL2 slows things down. :)"
  created_at: 2023-06-07 23:32:31+00:00
  edited: false
  hidden: false
  id: 6481219f40facadc55776f1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-08T08:38:34.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9607295989990234
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Some motherboards have slots that are PCIe x16 size, but only electrically
          connected at x8.  Or, the connection depends on other cards connected.  CPUs
          have a finite number of PCIe lanes. So it can be the case that a motherboard
          can support installing multiple PCIe x16 cards, but one or more of them
          will run at x8 if there are multiple installed.</p>

          <p>It can also be the case that the CPUs supported by the motherboard have
          a varying number of PCIe lanes. Some CPUs support X lanes, and others only
          support Y, which could be 4 or 8 fewer perhaps.  So the motherboard may
          support 4 x PCIe x16 with one CPU, but will run one or more slots at x8
          with another CPU.</p>

          <p>Or, slots can share resources with other things on the motherboard. For
          example, I have an old Westmere-generation Tian motherboard where one of
          the PCIe slots shares resources with the SATA ports.  So if I plug in a
          bunch of SATA drives as well as a PCIe card in that specific slot, the PCIe
          card runs at x2 or something like that.</p>

          <p>Or it can be the case that certain slots are always x8.  So it could
          be that swapping the card to another slot might get x16.  Or it can even
          be the slot is damaged and not detecting some pins, causing it to drop to
          x8.</p>

          <p>In the manual for your motherboard you should find an explanation for
          what slot will run at what speed, under what circumstance.</p>

          <p>All that said, I don''t believe running at x8 is going to be making a
          huge difference in GPTQ testing, especially as you''re already CPU bottlenecked  </p>

          <p>That''s a pretty old CPU, and it''s a server CPU.  So it''s going to
          have pretty poor single-core performance compared to a modern gaming CPU,
          which are the kings of pytorch/GPTQ at the moment.  And yes, WSL may be
          contributing to some slowdown as well.</p>

          '
        raw: "Some motherboards have slots that are PCIe x16 size, but only electrically\
          \ connected at x8.  Or, the connection depends on other cards connected.\
          \  CPUs have a finite number of PCIe lanes. So it can be the case that a\
          \ motherboard can support installing multiple PCIe x16 cards, but one or\
          \ more of them will run at x8 if there are multiple installed.\n\nIt can\
          \ also be the case that the CPUs supported by the motherboard have a varying\
          \ number of PCIe lanes. Some CPUs support X lanes, and others only support\
          \ Y, which could be 4 or 8 fewer perhaps.  So the motherboard may support\
          \ 4 x PCIe x16 with one CPU, but will run one or more slots at x8 with another\
          \ CPU.\n\nOr, slots can share resources with other things on the motherboard.\
          \ For example, I have an old Westmere-generation Tian motherboard where\
          \ one of the PCIe slots shares resources with the SATA ports.  So if I plug\
          \ in a bunch of SATA drives as well as a PCIe card in that specific slot,\
          \ the PCIe card runs at x2 or something like that.\n\nOr it can be the case\
          \ that certain slots are always x8.  So it could be that swapping the card\
          \ to another slot might get x16.  Or it can even be the slot is damaged\
          \ and not detecting some pins, causing it to drop to x8.\n\nIn the manual\
          \ for your motherboard you should find an explanation for what slot will\
          \ run at what speed, under what circumstance.\n\nAll that said, I don't\
          \ believe running at x8 is going to be making a huge difference in GPTQ\
          \ testing, especially as you're already CPU bottlenecked  \n\nThat's a pretty\
          \ old CPU, and it's a server CPU.  So it's going to have pretty poor single-core\
          \ performance compared to a modern gaming CPU, which are the kings of pytorch/GPTQ\
          \ at the moment.  And yes, WSL may be contributing to some slowdown as well."
        updatedAt: '2023-06-08T08:38:34.272Z'
      numEdits: 0
      reactions: []
    id: 6481938a17f2fba0008824dc
    type: comment
  author: TheBloke
  content: "Some motherboards have slots that are PCIe x16 size, but only electrically\
    \ connected at x8.  Or, the connection depends on other cards connected.  CPUs\
    \ have a finite number of PCIe lanes. So it can be the case that a motherboard\
    \ can support installing multiple PCIe x16 cards, but one or more of them will\
    \ run at x8 if there are multiple installed.\n\nIt can also be the case that the\
    \ CPUs supported by the motherboard have a varying number of PCIe lanes. Some\
    \ CPUs support X lanes, and others only support Y, which could be 4 or 8 fewer\
    \ perhaps.  So the motherboard may support 4 x PCIe x16 with one CPU, but will\
    \ run one or more slots at x8 with another CPU.\n\nOr, slots can share resources\
    \ with other things on the motherboard. For example, I have an old Westmere-generation\
    \ Tian motherboard where one of the PCIe slots shares resources with the SATA\
    \ ports.  So if I plug in a bunch of SATA drives as well as a PCIe card in that\
    \ specific slot, the PCIe card runs at x2 or something like that.\n\nOr it can\
    \ be the case that certain slots are always x8.  So it could be that swapping\
    \ the card to another slot might get x16.  Or it can even be the slot is damaged\
    \ and not detecting some pins, causing it to drop to x8.\n\nIn the manual for\
    \ your motherboard you should find an explanation for what slot will run at what\
    \ speed, under what circumstance.\n\nAll that said, I don't believe running at\
    \ x8 is going to be making a huge difference in GPTQ testing, especially as you're\
    \ already CPU bottlenecked  \n\nThat's a pretty old CPU, and it's a server CPU.\
    \  So it's going to have pretty poor single-core performance compared to a modern\
    \ gaming CPU, which are the kings of pytorch/GPTQ at the moment.  And yes, WSL\
    \ may be contributing to some slowdown as well."
  created_at: 2023-06-08 07:38:34+00:00
  edited: false
  hidden: false
  id: 6481938a17f2fba0008824dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-09T00:11:16.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9204347729682922
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I got 2 Xeon CPUs and I''ve ensured that the 3090 is in a x16 slot.</p>

          <p>According to this diagram PCH doesn''t even drive x16 slots: <a rel="nofollow"
          href="https://static.tweaktown.com/content/6/7/6750_03_supermicro_x10dai_intel_c612_workstation_motherboard_review.jpg">https://static.tweaktown.com/content/6/7/6750_03_supermicro_x10dai_intel_c612_workstation_motherboard_review.jpg</a></p>

          <p>But yeah, I guess should''ve upgraded to a gaming system. I stuck with
          2011-3 because everything is watercooled (incl. 3090) and 2011 CPU blocks
          are compatible with 2011-3 CPUs. :)</p>

          '
        raw: 'I got 2 Xeon CPUs and I''ve ensured that the 3090 is in a x16 slot.


          According to this diagram PCH doesn''t even drive x16 slots: https://static.tweaktown.com/content/6/7/6750_03_supermicro_x10dai_intel_c612_workstation_motherboard_review.jpg


          But yeah, I guess should''ve upgraded to a gaming system. I stuck with 2011-3
          because everything is watercooled (incl. 3090) and 2011 CPU blocks are compatible
          with 2011-3 CPUs. :)'
        updatedAt: '2023-06-09T00:11:16.848Z'
      numEdits: 0
      reactions: []
    id: 64826e2485ce4d2973c77c25
    type: comment
  author: mancub
  content: 'I got 2 Xeon CPUs and I''ve ensured that the 3090 is in a x16 slot.


    According to this diagram PCH doesn''t even drive x16 slots: https://static.tweaktown.com/content/6/7/6750_03_supermicro_x10dai_intel_c612_workstation_motherboard_review.jpg


    But yeah, I guess should''ve upgraded to a gaming system. I stuck with 2011-3
    because everything is watercooled (incl. 3090) and 2011 CPU blocks are compatible
    with 2011-3 CPUs. :)'
  created_at: 2023-06-08 23:11:16+00:00
  edited: false
  hidden: false
  id: 64826e2485ce4d2973c77c25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-11T03:29:05.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9373084306716919
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n<p>So in\
          \ your opinion what would be a good (local/home) setup for ML from presently\
          \ available hardware that would cover most bases (both on GPU inference\
          \ and on CPU/llama.cpp)?</p>\n<p>Would CPUs like Intel i7-13700KF or AMD\
          \ 7900X do, or older gen like Intel i7-12700 and AMD 5900X?</p>\n<p>Problem\
          \ I see is that most consumer motherboards only have 4 DIMM slots and RAM\
          \ limit of 128GB which might not be enough.</p>\n<p> Is it worth getting\
          \ some of those older AMD Ryzen Threadripper combos (CPU/mobo/RAM) that\
          \ are appearing on eBay?</p>\n"
        raw: "@TheBloke \n\nSo in your opinion what would be a good (local/home) setup\
          \ for ML from presently available hardware that would cover most bases (both\
          \ on GPU inference and on CPU/llama.cpp)?\n\nWould CPUs like Intel i7-13700KF\
          \ or AMD 7900X do, or older gen like Intel i7-12700 and AMD 5900X?\n\nProblem\
          \ I see is that most consumer motherboards only have 4 DIMM slots and RAM\
          \ limit of 128GB which might not be enough.\n\n Is it worth getting some\
          \ of those older AMD Ryzen Threadripper combos (CPU/mobo/RAM) that are appearing\
          \ on eBay?"
        updatedAt: '2023-06-11T03:29:05.649Z'
      numEdits: 0
      reactions: []
    id: 64853f81c63f2e20793f0e3d
    type: comment
  author: mancub
  content: "@TheBloke \n\nSo in your opinion what would be a good (local/home) setup\
    \ for ML from presently available hardware that would cover most bases (both on\
    \ GPU inference and on CPU/llama.cpp)?\n\nWould CPUs like Intel i7-13700KF or\
    \ AMD 7900X do, or older gen like Intel i7-12700 and AMD 5900X?\n\nProblem I see\
    \ is that most consumer motherboards only have 4 DIMM slots and RAM limit of 128GB\
    \ which might not be enough.\n\n Is it worth getting some of those older AMD Ryzen\
    \ Threadripper combos (CPU/mobo/RAM) that are appearing on eBay?"
  created_at: 2023-06-11 02:29:05+00:00
  edited: false
  hidden: false
  id: 64853f81c63f2e20793f0e3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-11T21:16:16.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9677274227142334
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Also, I finally installed Ubuntu 22.04 LTS onto a seprate SSD and
          I am not seeing much of an improvement over running in WSL2, maybe +1 t/s
          or less.</p>

          <p>But, what surfaced up was that there''s now less VRAM available for the
          models because X is running, and so I can''t load the Falcon-40B anymore.</p>

          <p>I can load the 40B model and run it just fine in WSL2, so unless native
          Linux is running headless, I see no benefit (for me at least) running it
          over WSL2.</p>

          '
        raw: 'Also, I finally installed Ubuntu 22.04 LTS onto a seprate SSD and I
          am not seeing much of an improvement over running in WSL2, maybe +1 t/s
          or less.


          But, what surfaced up was that there''s now less VRAM available for the
          models because X is running, and so I can''t load the Falcon-40B anymore.


          I can load the 40B model and run it just fine in WSL2, so unless native
          Linux is running headless, I see no benefit (for me at least) running it
          over WSL2.'
        updatedAt: '2023-06-11T21:16:16.379Z'
      numEdits: 0
      reactions: []
    id: 648639a046cac8edc3fd0da5
    type: comment
  author: mancub
  content: 'Also, I finally installed Ubuntu 22.04 LTS onto a seprate SSD and I am
    not seeing much of an improvement over running in WSL2, maybe +1 t/s or less.


    But, what surfaced up was that there''s now less VRAM available for the models
    because X is running, and so I can''t load the Falcon-40B anymore.


    I can load the 40B model and run it just fine in WSL2, so unless native Linux
    is running headless, I see no benefit (for me at least) running it over WSL2.'
  created_at: 2023-06-11 20:16:16+00:00
  edited: false
  hidden: false
  id: 648639a046cac8edc3fd0da5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-06-11T21:56:03.000Z'
    data:
      edited: false
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8999478816986084
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: '<p>change your run level to not start X. you get vram back.</p>

          '
        raw: change your run level to not start X. you get vram back.
        updatedAt: '2023-06-11T21:56:03.838Z'
      numEdits: 0
      reactions: []
    id: 648642f39299c3a4e58e7865
    type: comment
  author: Nurb432
  content: change your run level to not start X. you get vram back.
  created_at: 2023-06-11 20:56:03+00:00
  edited: false
  hidden: false
  id: 648642f39299c3a4e58e7865
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-13T01:19:49.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9548614621162415
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Nurb432&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Nurb432\">@<span class=\"\
          underline\">Nurb432</span></a></span>\n\n\t</span></span> </p>\n<p>Yes,\
          \ but then I don't have a browser to connect to the text-gen-webui so I'm\
          \ stuck in the CLI.</p>\n<p>I think my whole setup is wrong...I upgraded\
          \ from dual Xeon E5v2 to a dual Xeon E5v3 thinking a single generation step-up\
          \ adding AVX2 and DDR4 would do it. And because I wanted to reuse my existing\
          \ CPU waterblocks and the watercooling setup.</p>\n<p>Then I found out that\
          \ I need better single thread performance, and that multiple cores aren't\
          \ where it's at for PyTorch. In hindsight, I should've spent more money,\
          \ and go for a single Threadripper 39xx or 59xx.</p>\n<p>20/20</p>\n"
        raw: "@Nurb432 \n\nYes, but then I don't have a browser to connect to the\
          \ text-gen-webui so I'm stuck in the CLI.\n\nI think my whole setup is wrong...I\
          \ upgraded from dual Xeon E5v2 to a dual Xeon E5v3 thinking a single generation\
          \ step-up adding AVX2 and DDR4 would do it. And because I wanted to reuse\
          \ my existing CPU waterblocks and the watercooling setup.\n\nThen I found\
          \ out that I need better single thread performance, and that multiple cores\
          \ aren't where it's at for PyTorch. In hindsight, I should've spent more\
          \ money, and go for a single Threadripper 39xx or 59xx.\n\n20/20"
        updatedAt: '2023-06-13T01:19:49.710Z'
      numEdits: 0
      reactions: []
    id: 6487c4359b6275005bd8691a
    type: comment
  author: mancub
  content: "@Nurb432 \n\nYes, but then I don't have a browser to connect to the text-gen-webui\
    \ so I'm stuck in the CLI.\n\nI think my whole setup is wrong...I upgraded from\
    \ dual Xeon E5v2 to a dual Xeon E5v3 thinking a single generation step-up adding\
    \ AVX2 and DDR4 would do it. And because I wanted to reuse my existing CPU waterblocks\
    \ and the watercooling setup.\n\nThen I found out that I need better single thread\
    \ performance, and that multiple cores aren't where it's at for PyTorch. In hindsight,\
    \ I should've spent more money, and go for a single Threadripper 39xx or 59xx.\n\
    \n20/20"
  created_at: 2023-06-13 00:19:49+00:00
  edited: false
  hidden: false
  id: 6487c4359b6275005bd8691a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e42bdaa0cbfc34d7daf5eb318dfc7f8.svg
      fullname: Pierre-Louis Braun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alkeryn
      type: user
    createdAt: '2023-06-26T09:40:20.000Z'
    data:
      edited: true
      editors:
      - alkeryn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8928502798080444
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e42bdaa0cbfc34d7daf5eb318dfc7f8.svg
          fullname: Pierre-Louis Braun
          isHf: false
          isPro: false
          name: alkeryn
          type: user
        html: '<p>i think the most important thing is memory bandwidth, so if you
          take a Threadripper PRO 5955WX for example, and use the 8 memory channel,
          you may have better t/s than using the ryzen 9 7950x but i wish there was
          a benchmark, though it is ddr4 instead of ddr5 so i''m really curious which
          would beat the other.</p>

          '
        raw: i think the most important thing is memory bandwidth, so if you take
          a Threadripper PRO 5955WX for example, and use the 8 memory channel, you
          may have better t/s than using the ryzen 9 7950x but i wish there was a
          benchmark, though it is ddr4 instead of ddr5 so i'm really curious which
          would beat the other.
        updatedAt: '2023-06-26T09:41:27.956Z'
      numEdits: 2
      reactions: []
    id: 64995d04344c725e2a2aeef9
    type: comment
  author: alkeryn
  content: i think the most important thing is memory bandwidth, so if you take a
    Threadripper PRO 5955WX for example, and use the 8 memory channel, you may have
    better t/s than using the ryzen 9 7950x but i wish there was a benchmark, though
    it is ddr4 instead of ddr5 so i'm really curious which would beat the other.
  created_at: 2023-06-26 08:40:20+00:00
  edited: true
  hidden: false
  id: 64995d04344c725e2a2aeef9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-07-06T18:22:21.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9626093506813049
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> can you help\
          \ me, I'm looking for LLM models which can easily run on CPU. I'm not able\
          \ to find any examples, kindly help me </p>\n"
        raw: '@TheBloke can you help me, I''m looking for LLM models which can easily
          run on CPU. I''m not able to find any examples, kindly help me '
        updatedAt: '2023-07-06T18:22:21.316Z'
      numEdits: 0
      reactions: []
    id: 64a7065d28813fbc837ddfec
    type: comment
  author: deepakkaura26
  content: '@TheBloke can you help me, I''m looking for LLM models which can easily
    run on CPU. I''m not able to find any examples, kindly help me '
  created_at: 2023-07-06 17:22:21+00:00
  edited: false
  hidden: false
  id: 64a7065d28813fbc837ddfec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T09:12:40.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9289860725402832
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ can you help me, I'm looking for LLM models which can easily run on CPU.\
          \ I'm not able to find any examples, kindly help me</p>\n</blockquote>\n\
          <p>GGML models work best on CPU, I recommend you use a GGML model.</p>\n"
        raw: '> @TheBloke can you help me, I''m looking for LLM models which can easily
          run on CPU. I''m not able to find any examples, kindly help me


          GGML models work best on CPU, I recommend you use a GGML model.'
        updatedAt: '2023-07-07T09:12:40.169Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nurb432
    id: 64a7d708e2b22b80f8703648
    type: comment
  author: TheBloke
  content: '> @TheBloke can you help me, I''m looking for LLM models which can easily
    run on CPU. I''m not able to find any examples, kindly help me


    GGML models work best on CPU, I recommend you use a GGML model.'
  created_at: 2023-07-07 08:12:40+00:00
  edited: false
  hidden: false
  id: 64a7d708e2b22b80f8703648
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-07-08T18:37:35.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9353874921798706
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> thanks big much\
          \ \U0001F601\U0001F601</p>\n"
        raw: "@TheBloke thanks big much \U0001F601\U0001F601"
        updatedAt: '2023-07-08T18:37:35.206Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
    id: 64a9acefe3fa00fb2c6fa02b
    type: comment
  author: deepakkaura26
  content: "@TheBloke thanks big much \U0001F601\U0001F601"
  created_at: 2023-07-08 17:37:35+00:00
  edited: false
  hidden: false
  id: 64a9acefe3fa00fb2c6fa02b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-07-08T19:50:16.000Z'
    data:
      edited: true
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9078395962715149
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: '<p>For me, oddly, using the GGML version for just CPU is faster  than
          using GPU for the same thing with a regular bin/safetensor file. ( even
          if the model is smaller than availble Vram ).  Unless i using SD, then CPU
          is dreadful slow, but GPU flies. </p>

          '
        raw: "For me, oddly, using the GGML version for just CPU is faster  than using\
          \ GPU for the same thing with a regular bin/safetensor file. ( even if the\
          \ model is smaller than availble Vram ).  Unless i using SD, then CPU is\
          \ dreadful slow, but GPU flies. \n"
        updatedAt: '2023-07-08T19:51:12.774Z'
      numEdits: 1
      reactions: []
    id: 64a9bdf85fc663fee2362eab
    type: comment
  author: Nurb432
  content: "For me, oddly, using the GGML version for just CPU is faster  than using\
    \ GPU for the same thing with a regular bin/safetensor file. ( even if the model\
    \ is smaller than availble Vram ).  Unless i using SD, then CPU is dreadful slow,\
    \ but GPU flies. \n"
  created_at: 2023-07-08 18:50:16+00:00
  edited: true
  hidden: false
  id: 64a9bdf85fc663fee2362eab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d38fcaa72d9f9cb04ba8e7f72211e34.svg
      fullname: Bohan Du
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: acrastt
      type: user
    createdAt: '2023-07-15T03:44:51.000Z'
    data:
      edited: false
      editors:
      - acrastt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9073007106781006
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d38fcaa72d9f9cb04ba8e7f72211e34.svg
          fullname: Bohan Du
          isHf: false
          isPro: false
          name: acrastt
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> so does a 13b\
          \ q2_k(e.g. this model, nous hermes, in q2_k) fit into 8gb ram with cpu?\
          \ would it be performant? what would be an estimation of the speed?</p>\n"
        raw: '@TheBloke so does a 13b q2_k(e.g. this model, nous hermes, in q2_k)
          fit into 8gb ram with cpu? would it be performant? what would be an estimation
          of the speed?

          '
        updatedAt: '2023-07-15T03:44:51.331Z'
      numEdits: 0
      reactions: []
    id: 64b21633102ed6e7aecf3a27
    type: comment
  author: acrastt
  content: '@TheBloke so does a 13b q2_k(e.g. this model, nous hermes, in q2_k) fit
    into 8gb ram with cpu? would it be performant? what would be an estimation of
    the speed?

    '
  created_at: 2023-07-15 02:44:51+00:00
  edited: false
  hidden: false
  id: 64b21633102ed6e7aecf3a27
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Nous-Hermes-13B-GGML
repo_type: model
status: open
target_branch: null
title: Uh oh, the "q's"...
