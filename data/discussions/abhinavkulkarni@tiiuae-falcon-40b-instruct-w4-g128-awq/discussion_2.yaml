!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mike-ravkine
conflicting_files: null
created_at: 2023-07-13 19:20:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-07-13T20:20:43.000Z'
    data:
      edited: false
      editors:
      - mike-ravkine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7665017247200012
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: "<p>This model is a little bit too large for inference to work on A10G,\
          \ for anyone in a similar situation try this:</p>\n<pre><code>        max_memory\
          \ = {0:\"18GiB\", \"cpu\":\"99GiB\"}\n        device_map = infer_auto_device_map(model,\n\
          \                                           no_split_module_classes=[\"\
          DecoderLayer\"],\n                                           max_memory=max_memory)\n\
          \n        if device_map['lm_head'] == 'cpu': device_map['lm_head'] = 0\n\
          \n        model = load_checkpoint_and_dispatch(model, load_quant, device_map=device_map)\n\
          </code></pre>\n<p>This configuration leaves the last 12/60 layers on the\
          \ CPU to bring down GPU memory usage and works around an issue where the\
          \ model crashes if lm_head is not on the GPU.</p>\n"
        raw: "This model is a little bit too large for inference to work on A10G,\
          \ for anyone in a similar situation try this:\r\n\r\n            max_memory\
          \ = {0:\"18GiB\", \"cpu\":\"99GiB\"}\r\n            device_map = infer_auto_device_map(model,\r\
          \n                                               no_split_module_classes=[\"\
          DecoderLayer\"],\r\n                                               max_memory=max_memory)\r\
          \n\r\n            if device_map['lm_head'] == 'cpu': device_map['lm_head']\
          \ = 0\r\n\r\n            model = load_checkpoint_and_dispatch(model, load_quant,\
          \ device_map=device_map)\r\n\r\nThis configuration leaves the last 12/60\
          \ layers on the CPU to bring down GPU memory usage and works around an issue\
          \ where the model crashes if lm_head is not on the GPU."
        updatedAt: '2023-07-13T20:20:43.481Z'
      numEdits: 0
      reactions: []
    id: 64b05c9b08f7eb472eed8e75
    type: comment
  author: mike-ravkine
  content: "This model is a little bit too large for inference to work on A10G, for\
    \ anyone in a similar situation try this:\r\n\r\n            max_memory = {0:\"\
    18GiB\", \"cpu\":\"99GiB\"}\r\n            device_map = infer_auto_device_map(model,\r\
    \n                                               no_split_module_classes=[\"DecoderLayer\"\
    ],\r\n                                               max_memory=max_memory)\r\n\
    \r\n            if device_map['lm_head'] == 'cpu': device_map['lm_head'] = 0\r\
    \n\r\n            model = load_checkpoint_and_dispatch(model, load_quant, device_map=device_map)\r\
    \n\r\nThis configuration leaves the last 12/60 layers on the CPU to bring down\
    \ GPU memory usage and works around an issue where the model crashes if lm_head\
    \ is not on the GPU."
  created_at: 2023-07-13 19:20:43+00:00
  edited: false
  hidden: false
  id: 64b05c9b08f7eb472eed8e75
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: abhinavkulkarni/tiiuae-falcon-40b-instruct-w4-g128-awq
repo_type: model
status: open
target_branch: null
title: Successful inference using a 24GB GPU
