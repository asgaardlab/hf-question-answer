!!python/object:huggingface_hub.community.DiscussionWithDetails
author: krao
conflicting_files: null
created_at: 2023-11-24 05:00:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64b87359e05edf50df4bf22fac0805e7.svg
      fullname: K Rao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krao
      type: user
    createdAt: '2023-11-24T05:00:13.000Z'
    data:
      edited: true
      editors:
      - krao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.335101455450058
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64b87359e05edf50df4bf22fac0805e7.svg
          fullname: K Rao
          isHf: false
          isPro: false
          name: krao
          type: user
        html: "<p>I am confused about the correct EOS/BOS. </p>\n<p>In <code>generation_config.json</code>\
          \ and <code>config.json</code> the settings differ:</p>\n<p>latter:</p>\n\
          <pre><code>{\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n \
          \ \"attention_bias\": false,\n  \"bos_token_id\": 1,\n  \"eos_token_id\"\
          : 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 7168,\n  \"initializer_range\"\
          : 0.02,\n  \"intermediate_size\": 20480,\n  \"max_position_embeddings\"\
          : 4096,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 56,\n\
          \  \"num_hidden_layers\": 60,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\"\
          : 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\"\
          : 5000000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\"\
          ,\n  \"transformers_version\": \"4.35.0\",\n  \"use_cache\": true,\n  \"\
          vocab_size\": 64000\n}\n</code></pre>\n<p>former:</p>\n<pre><code>{\n  \"\
          bos_token_id\": 6,\n  \"do_sample\": true,\n  \"eos_token_id\": 7,\n  \"\
          pad_token_id\": 0,\n  \"temperature\": 0.6,\n  \"max_length\": 4096,\n \
          \ \"top_p\": 0.8,\n  \"transformers_version\": \"4.35.0\"\n}\n</code></pre>\n\
          <p>If I understand <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/25395#issuecomment-1677796723\"\
          >https://github.com/huggingface/transformers/issues/25395#issuecomment-1677796723</a>\
          \ correctly, it's a fallback mechanism.<br>That would give</p>\n<pre><code>\
          \  \"bos_token_id\": 6,\n  \"eos_token_id\": 7,\n</code></pre>\n<p>However,\
          \ maybe they are not added at all, as in <code>tokenizer_config.json</code>\
          \ we have:</p>\n<pre><code>{\n  \"add_bos_token\": false,\n  \"add_eos_token\"\
          : false,\n  \"added_tokens_decoder\": {\n    \"0\": {\n      \"content\"\
          : \"&lt;unk&gt;\",\n      \"lstrip\": false,\n      \"normalized\": true,\n\
          \      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\"\
          : true\n    },\n    \"1\": {\n      \"content\": \"&lt;|startoftext|&gt;\"\
          ,\n      \"lstrip\": false,\n      \"normalized\": true,\n      \"rstrip\"\
          : false,\n      \"single_word\": false,\n      \"special\": true\n    },\n\
          \    \"2\": {\n      \"content\": \"&lt;|endoftext|&gt;\",\n      \"lstrip\"\
          : false,\n      \"normalized\": true,\n      \"rstrip\": false,\n      \"\
          single_word\": false,\n      \"special\": true\n    },\n    \"6\": {\n \
          \     \"content\": \"&lt;|im_start|&gt;\",\n      \"lstrip\": false,\n \
          \     \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\"\
          : false,\n      \"special\": true\n    },\n    \"7\": {\n      \"content\"\
          : \"&lt;|im_end|&gt;\",\n      \"lstrip\": false,\n      \"normalized\"\
          : false,\n      \"rstrip\": false,\n      \"single_word\": false,\n    \
          \  \"special\": true\n    },\n    \"8\": {\n      \"content\": \"&lt;|im_sep|&gt;\"\
          ,\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\"\
          : false,\n      \"single_word\": false,\n      \"special\": true\n    }\n\
          \  },\n  \"additional_special_tokens\": [\n    \"&lt;|im_start|&gt;\",\n\
          \    \"&lt;|im_end|&gt;\",\n    \"&lt;|im_sep|&gt;\"\n  ],\n  \"bos_token\"\
          : \"&lt;|startoftext|&gt;\",\n  \"chat_template\": \"{% if not add_generation_prompt\
          \ is defined %}{% set add_generation_prompt = false %}{% endif %}{% for\
          \ message in messages %}{{'&lt;|im_start|&gt;' + message['role'] + '\\n'\
          \ + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}{% endfor %}{% if add_generation_prompt\
          \ %}{{ '&lt;|im_start|&gt;assistant\\n' }}{% endif %}\",\n  \"clean_up_tokenization_spaces\"\
          : false,\n  \"eos_token\": \"&lt;|endoftext|&gt;\",\n  \"legacy\": true,\n\
          \  \"model_max_length\": 4096,\n  \"pad_token\": \"&lt;unk&gt;\",\n  \"\
          padding_side\": \"right\",\n  \"sp_model_kwargs\": {},\n  \"spaces_between_special_tokens\"\
          : false,\n  \"tokenizer_class\": \"LlamaTokenizer\",\n  \"unk_token\": \"\
          &lt;unk&gt;\",\n  \"use_default_system_prompt\": true\n}\n</code></pre>\n\
          <p>note the:</p>\n<pre><code>  \"add_bos_token\": false,\n  \"add_eos_token\"\
          : false,\n</code></pre>\n<p>And thus, why are bos_token and eos_token still\
          \ specified here?</p>\n"
        raw: "I am confused about the correct EOS/BOS. \n\nIn `generation_config.json`\
          \ and `config.json` the settings differ:\n\nlatter:\n```\n{\n  \"architectures\"\
          : [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"\
          bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n\
          \  \"hidden_size\": 7168,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\"\
          : 20480,\n  \"max_position_embeddings\": 4096,\n  \"model_type\": \"llama\"\
          ,\n  \"num_attention_heads\": 56,\n  \"num_hidden_layers\": 60,\n  \"num_key_value_heads\"\
          : 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\"\
          : null,\n  \"rope_theta\": 5000000.0,\n  \"tie_word_embeddings\": false,\n\
          \  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.35.0\"\
          ,\n  \"use_cache\": true,\n  \"vocab_size\": 64000\n}\n```\n\nformer:\n\
          ```\n{\n  \"bos_token_id\": 6,\n  \"do_sample\": true,\n  \"eos_token_id\"\
          : 7,\n  \"pad_token_id\": 0,\n  \"temperature\": 0.6,\n  \"max_length\"\
          : 4096,\n  \"top_p\": 0.8,\n  \"transformers_version\": \"4.35.0\"\n}\n\
          ```\n\nIf I understand https://github.com/huggingface/transformers/issues/25395#issuecomment-1677796723\
          \ correctly, it's a fallback mechanism.\nThat would give\n```\n  \"bos_token_id\"\
          : 6,\n  \"eos_token_id\": 7,\n```\n\nHowever, maybe they are not added at\
          \ all, as in `tokenizer_config.json` we have:\n```\n{\n  \"add_bos_token\"\
          : false,\n  \"add_eos_token\": false,\n  \"added_tokens_decoder\": {\n \
          \   \"0\": {\n      \"content\": \"<unk>\",\n      \"lstrip\": false,\n\
          \      \"normalized\": true,\n      \"rstrip\": false,\n      \"single_word\"\
          : false,\n      \"special\": true\n    },\n    \"1\": {\n      \"content\"\
          : \"<|startoftext|>\",\n      \"lstrip\": false,\n      \"normalized\":\
          \ true,\n      \"rstrip\": false,\n      \"single_word\": false,\n     \
          \ \"special\": true\n    },\n    \"2\": {\n      \"content\": \"<|endoftext|>\"\
          ,\n      \"lstrip\": false,\n      \"normalized\": true,\n      \"rstrip\"\
          : false,\n      \"single_word\": false,\n      \"special\": true\n    },\n\
          \    \"6\": {\n      \"content\": \"<|im_start|>\",\n      \"lstrip\": false,\n\
          \      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\"\
          : false,\n      \"special\": true\n    },\n    \"7\": {\n      \"content\"\
          : \"<|im_end|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n\
          \      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\"\
          : true\n    },\n    \"8\": {\n      \"content\": \"<|im_sep|>\",\n     \
          \ \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n\
          \      \"single_word\": false,\n      \"special\": true\n    }\n  },\n \
          \ \"additional_special_tokens\": [\n    \"<|im_start|>\",\n    \"<|im_end|>\"\
          ,\n    \"<|im_sep|>\"\n  ],\n  \"bos_token\": \"<|startoftext|>\",\n  \"\
          chat_template\": \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt\
          \ = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role']\
          \ + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if\
          \ add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\
          ,\n  \"clean_up_tokenization_spaces\": false,\n  \"eos_token\": \"<|endoftext|>\"\
          ,\n  \"legacy\": true,\n  \"model_max_length\": 4096,\n  \"pad_token\":\
          \ \"<unk>\",\n  \"padding_side\": \"right\",\n  \"sp_model_kwargs\": {},\n\
          \  \"spaces_between_special_tokens\": false,\n  \"tokenizer_class\": \"\
          LlamaTokenizer\",\n  \"unk_token\": \"<unk>\",\n  \"use_default_system_prompt\"\
          : true\n}\n```\nnote the:\n```\n  \"add_bos_token\": false,\n  \"add_eos_token\"\
          : false,\n```\n\nAnd thus, why are bos_token and eos_token still specified\
          \ here?"
        updatedAt: '2023-11-24T05:00:59.732Z'
      numEdits: 1
      reactions: []
    id: 65602ddd842e0ce604d1dadb
    type: comment
  author: krao
  content: "I am confused about the correct EOS/BOS. \n\nIn `generation_config.json`\
    \ and `config.json` the settings differ:\n\nlatter:\n```\n{\n  \"architectures\"\
    : [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"bos_token_id\"\
    : 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\":\
    \ 7168,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 20480,\n  \"\
    max_position_embeddings\": 4096,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\"\
    : 56,\n  \"num_hidden_layers\": 60,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\"\
    : 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\"\
    : 5000000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\"\
    ,\n  \"transformers_version\": \"4.35.0\",\n  \"use_cache\": true,\n  \"vocab_size\"\
    : 64000\n}\n```\n\nformer:\n```\n{\n  \"bos_token_id\": 6,\n  \"do_sample\": true,\n\
    \  \"eos_token_id\": 7,\n  \"pad_token_id\": 0,\n  \"temperature\": 0.6,\n  \"\
    max_length\": 4096,\n  \"top_p\": 0.8,\n  \"transformers_version\": \"4.35.0\"\
    \n}\n```\n\nIf I understand https://github.com/huggingface/transformers/issues/25395#issuecomment-1677796723\
    \ correctly, it's a fallback mechanism.\nThat would give\n```\n  \"bos_token_id\"\
    : 6,\n  \"eos_token_id\": 7,\n```\n\nHowever, maybe they are not added at all,\
    \ as in `tokenizer_config.json` we have:\n```\n{\n  \"add_bos_token\": false,\n\
    \  \"add_eos_token\": false,\n  \"added_tokens_decoder\": {\n    \"0\": {\n  \
    \    \"content\": \"<unk>\",\n      \"lstrip\": false,\n      \"normalized\":\
    \ true,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\"\
    : true\n    },\n    \"1\": {\n      \"content\": \"<|startoftext|>\",\n      \"\
    lstrip\": false,\n      \"normalized\": true,\n      \"rstrip\": false,\n    \
    \  \"single_word\": false,\n      \"special\": true\n    },\n    \"2\": {\n  \
    \    \"content\": \"<|endoftext|>\",\n      \"lstrip\": false,\n      \"normalized\"\
    : true,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\"\
    : true\n    },\n    \"6\": {\n      \"content\": \"<|im_start|>\",\n      \"lstrip\"\
    : false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\"\
    : false,\n      \"special\": true\n    },\n    \"7\": {\n      \"content\": \"\
    <|im_end|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"\
    rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n   \
    \ },\n    \"8\": {\n      \"content\": \"<|im_sep|>\",\n      \"lstrip\": false,\n\
    \      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\"\
    : false,\n      \"special\": true\n    }\n  },\n  \"additional_special_tokens\"\
    : [\n    \"<|im_start|>\",\n    \"<|im_end|>\",\n    \"<|im_sep|>\"\n  ],\n  \"\
    bos_token\": \"<|startoftext|>\",\n  \"chat_template\": \"{% if not add_generation_prompt\
    \ is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message\
    \ in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content']\
    \ + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\\
    n' }}{% endif %}\",\n  \"clean_up_tokenization_spaces\": false,\n  \"eos_token\"\
    : \"<|endoftext|>\",\n  \"legacy\": true,\n  \"model_max_length\": 4096,\n  \"\
    pad_token\": \"<unk>\",\n  \"padding_side\": \"right\",\n  \"sp_model_kwargs\"\
    : {},\n  \"spaces_between_special_tokens\": false,\n  \"tokenizer_class\": \"\
    LlamaTokenizer\",\n  \"unk_token\": \"<unk>\",\n  \"use_default_system_prompt\"\
    : true\n}\n```\nnote the:\n```\n  \"add_bos_token\": false,\n  \"add_eos_token\"\
    : false,\n```\n\nAnd thus, why are bos_token and eos_token still specified here?"
  created_at: 2023-11-24 05:00:13+00:00
  edited: true
  hidden: false
  id: 65602ddd842e0ce604d1dadb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6556d7287eaa0731c915f42e/qqgaoVkK3tj7qWQyPLWAz.png?w=200&h=200&f=face
      fullname: Kai
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Kai01ai
      type: user
    createdAt: '2023-11-24T05:13:45.000Z'
    data:
      edited: false
      editors:
      - Kai01ai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7861028909683228
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6556d7287eaa0731c915f42e/qqgaoVkK3tj7qWQyPLWAz.png?w=200&h=200&f=face
          fullname: Kai
          isHf: false
          isPro: false
          name: Kai01ai
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;krao&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/krao\">@<span class=\"\
          underline\">krao</span></a></span>\n\n\t</span></span><br>The <code>generate</code>\
          \ function will use eos token as stop token. In our chat template format(which\
          \ is ChatML), we use the \"&lt;|im_end|&gt;\" as the end token of the response.\
          \ So we change the <code>eos_token_id</code> in <code>generation_config</code>\
          \ to <code>7</code> which map to \"&lt;|im_end|&gt;\".<br>And yes, the <code>bos_token_id</code>\
          \ should have no effect here.</p>\n"
        raw: "Hi @krao\nThe `generate` function will use eos token as stop token.\
          \ In our chat template format(which is ChatML), we use the \"<|im_end|>\"\
          \ as the end token of the response. So we change the `eos_token_id` in `generation_config`\
          \ to `7` which map to \"<|im_end|>\". \nAnd yes, the `bos_token_id` should\
          \ have no effect here."
        updatedAt: '2023-11-24T05:13:45.120Z'
      numEdits: 0
      reactions: []
    id: 65603109842e0ce604d25ee2
    type: comment
  author: Kai01ai
  content: "Hi @krao\nThe `generate` function will use eos token as stop token. In\
    \ our chat template format(which is ChatML), we use the \"<|im_end|>\" as the\
    \ end token of the response. So we change the `eos_token_id` in `generation_config`\
    \ to `7` which map to \"<|im_end|>\". \nAnd yes, the `bos_token_id` should have\
    \ no effect here."
  created_at: 2023-11-24 05:13:45+00:00
  edited: false
  hidden: false
  id: 65603109842e0ce604d25ee2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64b87359e05edf50df4bf22fac0805e7.svg
      fullname: K Rao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krao
      type: user
    createdAt: '2023-11-25T01:23:15.000Z'
    data:
      edited: false
      editors:
      - krao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8109761476516724
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64b87359e05edf50df4bf22fac0805e7.svg
          fullname: K Rao
          isHf: false
          isPro: false
          name: krao
          type: user
        html: '<blockquote>

          <p>And yes, the bos_token_id should have no effect here.</p>

          </blockquote>

          <p>Do you mean <code>bos_token</code> and <code>eos_token</code>?</p>

          '
        raw: '> And yes, the bos_token_id should have no effect here.


          Do you mean `bos_token` and `eos_token`?'
        updatedAt: '2023-11-25T01:23:15.695Z'
      numEdits: 0
      reactions: []
    id: 65614c832ac8d0ceacfb68cc
    type: comment
  author: krao
  content: '> And yes, the bos_token_id should have no effect here.


    Do you mean `bos_token` and `eos_token`?'
  created_at: 2023-11-25 01:23:15+00:00
  edited: false
  hidden: false
  id: 65614c832ac8d0ceacfb68cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6556d7287eaa0731c915f42e/qqgaoVkK3tj7qWQyPLWAz.png?w=200&h=200&f=face
      fullname: Kai
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Kai01ai
      type: user
    createdAt: '2023-11-25T01:47:07.000Z'
    data:
      edited: true
      editors:
      - reedcli
      - Kai01ai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8150169253349304
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64194e784939d72681882ead/JpvtmR0T1MYCNb-ugY_rM.jpeg?w=200&h=200&f=face
          fullname: Chao Li
          isHf: false
          isPro: false
          name: reedcli
          type: user
        html: '<p>The chat model is developed upon the base model, which utilizes
          distinct training templates:</p>

          <ul>

          <li><strong>base model</strong>: Typically trained with a template such
          as "{document}&lt;|endoftext|&gt;", To format this appropriately, one can
          employ <code>tokenizer.encode(document, add_bos_token=add_bos_token, add_eos_token=add_eos_token)</code>,
          and designate "&lt;|endoftext|&gt;" as the stop token during generation.</li>

          <li><strong>chat model</strong>: Often trained using a template represented
          by  "&lt;|im_start|&gt;...&lt;|im_end|&gt;", For proper formatting, the
          method <code>tokenizer.apply_chat_template(messages)</code> is used, and
          designate "&lt;|im_end|&gt;" as stop token during generation.</li>

          </ul>

          <p>It''s important to note that the bos and eos settings found in <code>config.json</code>
          and <code>tokenizer_config.json</code> are inherited from the base model.
          However, the settings in <code>generation_config.json</code> are specifically
          defined by the chat model.</p>

          <p>If you have any further question, feel free to ask!</p>

          '
        raw: 'The chat model is developed upon the base model, which utilizes distinct
          training templates:

          - **base model**: Typically trained with a template such as "{document}<|endoftext|>",
          To format this appropriately, one can employ `tokenizer.encode(document,
          add_bos_token=add_bos_token, add_eos_token=add_eos_token)`, and designate
          "<|endoftext|>" as the stop token during generation.

          - **chat model**: Often trained using a template represented by  "<|im_start|>...<|im_end|>",
          For proper formatting, the method `tokenizer.apply_chat_template(messages)`
          is used, and designate "<|im_end|>" as stop token during generation.


          It''s important to note that the bos and eos settings found in `config.json`
          and `tokenizer_config.json` are inherited from the base model. However,
          the settings in `generation_config.json` are specifically defined by the
          chat model.


          If you have any further question, feel free to ask!'
        updatedAt: '2023-11-25T03:11:07.964Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - richardllin
        - MB7977
        - LeoZha
    id: 6561521b6ef2a1d0f931c85e
    type: comment
  author: Kai01ai
  content: 'The chat model is developed upon the base model, which utilizes distinct
    training templates:

    - **base model**: Typically trained with a template such as "{document}<|endoftext|>",
    To format this appropriately, one can employ `tokenizer.encode(document, add_bos_token=add_bos_token,
    add_eos_token=add_eos_token)`, and designate "<|endoftext|>" as the stop token
    during generation.

    - **chat model**: Often trained using a template represented by  "<|im_start|>...<|im_end|>",
    For proper formatting, the method `tokenizer.apply_chat_template(messages)` is
    used, and designate "<|im_end|>" as stop token during generation.


    It''s important to note that the bos and eos settings found in `config.json` and
    `tokenizer_config.json` are inherited from the base model. However, the settings
    in `generation_config.json` are specifically defined by the chat model.


    If you have any further question, feel free to ask!'
  created_at: 2023-11-25 01:47:07+00:00
  edited: true
  hidden: false
  id: 6561521b6ef2a1d0f931c85e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64b87359e05edf50df4bf22fac0805e7.svg
      fullname: K Rao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krao
      type: user
    createdAt: '2023-11-26T01:38:57.000Z'
    data:
      edited: false
      editors:
      - krao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.999129056930542
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64b87359e05edf50df4bf22fac0805e7.svg
          fullname: K Rao
          isHf: false
          isPro: false
          name: krao
          type: user
        html: '<p>Thank you, that was really helpful!</p>

          '
        raw: Thank you, that was really helpful!
        updatedAt: '2023-11-26T01:38:57.691Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6562a1b1f5532ac1bd0f9614
    id: 6562a1b1f5532ac1bd0f960d
    type: comment
  author: krao
  content: Thank you, that was really helpful!
  created_at: 2023-11-26 01:38:57+00:00
  edited: false
  hidden: false
  id: 6562a1b1f5532ac1bd0f960d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/64b87359e05edf50df4bf22fac0805e7.svg
      fullname: K Rao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krao
      type: user
    createdAt: '2023-11-26T01:38:57.000Z'
    data:
      status: closed
    id: 6562a1b1f5532ac1bd0f9614
    type: status-change
  author: krao
  created_at: 2023-11-26 01:38:57+00:00
  id: 6562a1b1f5532ac1bd0f9614
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14822a1d02d5edf0107f002a0a406658.svg
      fullname: Anna Hung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Annorita
      type: user
    createdAt: '2023-12-19T02:42:27.000Z'
    data:
      edited: false
      editors:
      - Annorita
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6591567993164062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14822a1d02d5edf0107f002a0a406658.svg
          fullname: Anna Hung
          isHf: false
          isPro: false
          name: Annorita
          type: user
        html: '<p>Hi, can I just modify the bos and eos setting in <code>config.json</code>
          and <code>tokenizer_config.json</code> and make it align with <code>generation_config.json</code>?
          If yes, I can make a PR.</p>

          '
        raw: Hi, can I just modify the bos and eos setting in `config.json` and `tokenizer_config.json`
          and make it align with `generation_config.json`? If yes, I can make a PR.
        updatedAt: '2023-12-19T02:42:27.537Z'
      numEdits: 0
      reactions: []
    id: 65810313208369a94027d25d
    type: comment
  author: Annorita
  content: Hi, can I just modify the bos and eos setting in `config.json` and `tokenizer_config.json`
    and make it align with `generation_config.json`? If yes, I can make a PR.
  created_at: 2023-12-19 02:42:27+00:00
  edited: false
  hidden: false
  id: 65810313208369a94027d25d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: 01-ai/Yi-34B-Chat
repo_type: model
status: closed
target_branch: null
title: Configuration confusion
