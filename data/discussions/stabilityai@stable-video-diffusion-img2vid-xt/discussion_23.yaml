!!python/object:huggingface_hub.community.DiscussionWithDetails
author: macadeliccc
conflicting_files: null
created_at: 2023-11-28 20:28:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
      fullname: Tim Dolan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: macadeliccc
      type: user
    createdAt: '2023-11-28T20:28:14.000Z'
    data:
      edited: true
      editors:
      - macadeliccc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8417140245437622
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
          fullname: Tim Dolan
          isHf: false
          isPro: false
          name: macadeliccc
          type: user
        html: "<p>I have made a docker SDK of the SVD_XT demo from the <a rel=\"nofollow\"\
          \ href=\"https://github.com/Stability-AI/generative-models\">generative-models</a>\
          \ repository.</p>\n<p>You must have docker installed with nvidia GPU capabilities.</p>\n\
          <p>The base image is nvidia/cuda:12.2.0-devel-ubuntu20.04 and  it also includes\
          \ the svd_xt.safetensors and the svd_xt_image_decoder.safetensors</p>\n\
          <p>To run the image you just need  to run</p>\n<pre><code class=\"language-bash\"\
          >docker pull miniagi/stable_video_diffusion_xt \ndocker run --gpus all -p\
          \ 8501:8501 miniagi/stable_video_diffusion_xt\n</code></pre>\n<p>This will\
          \ run the video_ sampling.py demo from the generative-models repo.</p>\n\
          <p>I have a 4090 and I have tested the <code>Decode t frames at a time (set\
          \ small if you are low on VRAM)</code> parameter as high as 5 with VRAM\
          \ usage as high as 22GB</p>\n<p>For 16GB cards you can run this with the\
          \ <code>Decode t frames at a time (set small if you are low on VRAM)</code>\
          \ parameter set to either 1 or 2.</p>\n<p>Double loading models will cause\
          \ unpredictable behavior including crashes if you exceed VRAM. This issue\
          \ is present in the base application from stabilityai.</p>\n"
        raw: "I have made a docker SDK of the SVD_XT demo from the [generative-models](https://github.com/Stability-AI/generative-models)\
          \ repository.\n\nYou must have docker installed with nvidia GPU capabilities.\n\
          \nThe base image is nvidia/cuda:12.2.0-devel-ubuntu20.04 and  it also includes\
          \ the svd_xt.safetensors and the svd_xt_image_decoder.safetensors\n\nTo\
          \ run the image you just need  to run\n\n```bash\ndocker pull miniagi/stable_video_diffusion_xt\
          \ \ndocker run --gpus all -p 8501:8501 miniagi/stable_video_diffusion_xt\n\
          ```\nThis will run the video_ sampling.py demo from the generative-models\
          \ repo.\n\nI have a 4090 and I have tested the ```Decode t frames at a time\
          \ (set small if you are low on VRAM)``` parameter as high as 5 with VRAM\
          \ usage as high as 22GB\n\nFor 16GB cards you can run this with the ```Decode\
          \ t frames at a time (set small if you are low on VRAM)``` parameter set\
          \ to either 1 or 2.\n\nDouble loading models will cause unpredictable behavior\
          \ including crashes if you exceed VRAM. This issue is present in the base\
          \ application from stabilityai.\n"
        updatedAt: '2023-11-28T20:46:51.252Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - velaia
        - LuciferYagami
        - tripathiarpan20
    id: 65664d5e50386bbde719a9aa
    type: comment
  author: macadeliccc
  content: "I have made a docker SDK of the SVD_XT demo from the [generative-models](https://github.com/Stability-AI/generative-models)\
    \ repository.\n\nYou must have docker installed with nvidia GPU capabilities.\n\
    \nThe base image is nvidia/cuda:12.2.0-devel-ubuntu20.04 and  it also includes\
    \ the svd_xt.safetensors and the svd_xt_image_decoder.safetensors\n\nTo run the\
    \ image you just need  to run\n\n```bash\ndocker pull miniagi/stable_video_diffusion_xt\
    \ \ndocker run --gpus all -p 8501:8501 miniagi/stable_video_diffusion_xt\n```\n\
    This will run the video_ sampling.py demo from the generative-models repo.\n\n\
    I have a 4090 and I have tested the ```Decode t frames at a time (set small if\
    \ you are low on VRAM)``` parameter as high as 5 with VRAM usage as high as 22GB\n\
    \nFor 16GB cards you can run this with the ```Decode t frames at a time (set small\
    \ if you are low on VRAM)``` parameter set to either 1 or 2.\n\nDouble loading\
    \ models will cause unpredictable behavior including crashes if you exceed VRAM.\
    \ This issue is present in the base application from stabilityai.\n"
  created_at: 2023-11-28 20:28:14+00:00
  edited: true
  hidden: false
  id: 65664d5e50386bbde719a9aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7e1c6a14d79eb7fe7b64eed7127a0ca.svg
      fullname: Lucifer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LuciferYagami
      type: user
    createdAt: '2023-11-29T14:16:26.000Z'
    data:
      edited: false
      editors:
      - LuciferYagami
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7425666451454163
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7e1c6a14d79eb7fe7b64eed7127a0ca.svg
          fullname: Lucifer
          isHf: false
          isPro: false
          name: LuciferYagami
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;macadeliccc&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/macadeliccc\"\
          >@<span class=\"underline\">macadeliccc</span></a></span>\n\n\t</span></span>\
          \ , Can we run this on 12GB VRAM ? (RTX 3060ti) If yes, can you please tell\
          \ me what would be ideal configuration settings</p>\n"
        raw: Hey @macadeliccc , Can we run this on 12GB VRAM ? (RTX 3060ti) If yes,
          can you please tell me what would be ideal configuration settings
        updatedAt: '2023-11-29T14:16:26.144Z'
      numEdits: 0
      reactions: []
    id: 656747ba4f87f5f4aeea7df6
    type: comment
  author: LuciferYagami
  content: Hey @macadeliccc , Can we run this on 12GB VRAM ? (RTX 3060ti) If yes,
    can you please tell me what would be ideal configuration settings
  created_at: 2023-11-29 14:16:26+00:00
  edited: false
  hidden: false
  id: 656747ba4f87f5f4aeea7df6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
      fullname: Tim Dolan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: macadeliccc
      type: user
    createdAt: '2023-11-29T16:27:51.000Z'
    data:
      edited: true
      editors:
      - macadeliccc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9710211157798767
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
          fullname: Tim Dolan
          isHf: false
          isPro: false
          name: macadeliccc
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;LuciferYagami&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LuciferYagami\"\
          >@<span class=\"underline\">LuciferYagami</span></a></span>\n\n\t</span></span>\
          \ I havent tested how low the VRAM usage is with only 1 for the decode t\
          \ frames, but I would say its very unlikely. I will try to put standard\
          \ svd in the container but its still pushing it.</p>\n"
        raw: '@LuciferYagami I havent tested how low the VRAM usage is with only 1
          for the decode t frames, but I would say its very unlikely. I will try to
          put standard svd in the container but its still pushing it.'
        updatedAt: '2023-11-29T16:53:32.839Z'
      numEdits: 1
      reactions: []
    id: 6567668739af099876d415b6
    type: comment
  author: macadeliccc
  content: '@LuciferYagami I havent tested how low the VRAM usage is with only 1 for
    the decode t frames, but I would say its very unlikely. I will try to put standard
    svd in the container but its still pushing it.'
  created_at: 2023-11-29 16:27:51+00:00
  edited: true
  hidden: false
  id: 6567668739af099876d415b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7e1c6a14d79eb7fe7b64eed7127a0ca.svg
      fullname: Lucifer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LuciferYagami
      type: user
    createdAt: '2023-11-30T14:29:36.000Z'
    data:
      edited: false
      editors:
      - LuciferYagami
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9729084968566895
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7e1c6a14d79eb7fe7b64eed7127a0ca.svg
          fullname: Lucifer
          isHf: false
          isPro: false
          name: LuciferYagami
          type: user
        html: '<p>Thanks for the response. Hoped I could run it with a 3060ti</p>

          '
        raw: Thanks for the response. Hoped I could run it with a 3060ti
        updatedAt: '2023-11-30T14:29:36.496Z'
      numEdits: 0
      reactions: []
    id: 65689c506fcc82e5e803ec00
    type: comment
  author: LuciferYagami
  content: Thanks for the response. Hoped I could run it with a 3060ti
  created_at: 2023-11-30 14:29:36+00:00
  edited: false
  hidden: false
  id: 65689c506fcc82e5e803ec00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c47229bb7be43f6e1bab8c4d3b31a32e.svg
      fullname: Aaron Acosta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sk8aaron
      type: user
    createdAt: '2023-12-03T06:27:58.000Z'
    data:
      edited: false
      editors:
      - Sk8aaron
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7167591452598572
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c47229bb7be43f6e1bab8c4d3b31a32e.svg
          fullname: Aaron Acosta
          isHf: false
          isPro: false
          name: Sk8aaron
          type: user
        html: '<p>currently running on rtx 3070 8GB VRAM generating 512x512 pretty
          good. watch this tutorial : <a rel="nofollow" href="https://www.youtube.com/watch?v=HOVYu2UbgEE">https://www.youtube.com/watch?v=HOVYu2UbgEE</a></p>

          '
        raw: 'currently running on rtx 3070 8GB VRAM generating 512x512 pretty good.
          watch this tutorial : https://www.youtube.com/watch?v=HOVYu2UbgEE

          '
        updatedAt: '2023-12-03T06:27:58.697Z'
      numEdits: 0
      reactions: []
    id: 656c1fee9dcedd16d51f54e4
    type: comment
  author: Sk8aaron
  content: 'currently running on rtx 3070 8GB VRAM generating 512x512 pretty good.
    watch this tutorial : https://www.youtube.com/watch?v=HOVYu2UbgEE

    '
  created_at: 2023-12-03 06:27:58+00:00
  edited: false
  hidden: false
  id: 656c1fee9dcedd16d51f54e4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: stabilityai/stable-video-diffusion-img2vid-xt
repo_type: model
status: open
target_branch: null
title: Low VRAM mode docker image of the SVD_XT demo
