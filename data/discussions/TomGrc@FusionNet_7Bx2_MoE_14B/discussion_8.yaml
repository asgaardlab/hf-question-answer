!!python/object:huggingface_hub.community.DiscussionWithDetails
author: markyfsun
conflicting_files: null
created_at: 2024-01-23 10:40:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6468ac8872d9180d4ac9d8d8/EPL2U9EdNVNQkPRHEf2Mn.jpeg?w=200&h=200&f=face
      fullname: Mark Sun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: markyfsun
      type: user
    createdAt: '2024-01-23T10:40:51.000Z'
    data:
      edited: false
      editors:
      - markyfsun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9533568620681763
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6468ac8872d9180d4ac9d8d8/EPL2U9EdNVNQkPRHEf2Mn.jpeg?w=200&h=200&f=face
          fullname: Mark Sun
          isHf: false
          isPro: false
          name: markyfsun
          type: user
        html: '<p>Thank you for sharing this amazing work! I found something really
          interesting and wonder if you would like to share the trick behind:</p>

          <ol>

          <li>In zero-shot, this model looks like an instruct model. However, when
          I put it into a role-play preset, it continues the conversation well. Is
          this an instruct -finetuned model?</li>

          <li>I tested Chinese role-play as well, the model generates gorgeous Chinese
          words. I have never seen such wonderful output, not even from a 34B+ models.
          Did you build your own high-quality datasets?</li>

          </ol>

          <p>After all, I would greatly appreciate it if you are kind to share your
          finetuning process, especially the idea of "FusionNet". </p>

          '
        raw: "Thank you for sharing this amazing work! I found something really interesting\
          \ and wonder if you would like to share the trick behind:\r\n1. In zero-shot,\
          \ this model looks like an instruct model. However, when I put it into a\
          \ role-play preset, it continues the conversation well. Is this an instruct\
          \ -finetuned model?\r\n2. I tested Chinese role-play as well, the model\
          \ generates gorgeous Chinese words. I have never seen such wonderful output,\
          \ not even from a 34B+ models. Did you build your own high-quality datasets?\r\
          \n\r\nAfter all, I would greatly appreciate it if you are kind to share\
          \ your finetuning process, especially the idea of \"FusionNet\". "
        updatedAt: '2024-01-23T10:40:51.714Z'
      numEdits: 0
      reactions: []
    id: 65af97b3bff3ae9f33a17c54
    type: comment
  author: markyfsun
  content: "Thank you for sharing this amazing work! I found something really interesting\
    \ and wonder if you would like to share the trick behind:\r\n1. In zero-shot,\
    \ this model looks like an instruct model. However, when I put it into a role-play\
    \ preset, it continues the conversation well. Is this an instruct -finetuned model?\r\
    \n2. I tested Chinese role-play as well, the model generates gorgeous Chinese\
    \ words. I have never seen such wonderful output, not even from a 34B+ models.\
    \ Did you build your own high-quality datasets?\r\n\r\nAfter all, I would greatly\
    \ appreciate it if you are kind to share your finetuning process, especially the\
    \ idea of \"FusionNet\". "
  created_at: 2024-01-23 10:40:51+00:00
  edited: false
  hidden: false
  id: 65af97b3bff3ae9f33a17c54
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TomGrc/FusionNet_7Bx2_MoE_14B
repo_type: model
status: open
target_branch: null
title: Query for Magic Explaination
