!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Xenova
conflicting_files: []
created_at: 2023-09-09 23:47:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-09-10T00:47:23.000Z'
    data:
      edited: false
      editors:
      - Xenova
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11044929921627045
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
          fullname: Joshua
          isHf: true
          isPro: false
          name: Xenova
          type: user
        html: ''
        raw: ''
        updatedAt: '2023-09-10T00:47:23.025Z'
      numEdits: 0
      reactions: []
    id: 64fd121b60017eeec9bfdf2f
    type: comment
  author: Xenova
  content: ''
  created_at: 2023-09-09 23:47:23+00:00
  edited: false
  hidden: false
  id: 64fd121b60017eeec9bfdf2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-09-10T00:47:23.000Z'
    data:
      oid: 72c5d4c2e562b64045e4994bb1ea49356d7112f0
      parents:
      - eaaf64e3be20ad1f1fb0bdf689565ba52c97eafe
      subject: 'Fix typo (num_encoder_layers: 2 -> 12)'
    id: 64fd121b0000000000000000
    type: commit
  author: Xenova
  created_at: 2023-09-09 23:47:23+00:00
  id: 64fd121b0000000000000000
  oid: 72c5d4c2e562b64045e4994bb1ea49356d7112f0
  summary: 'Fix typo (num_encoder_layers: 2 -> 12)'
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-09-10T00:51:06.000Z'
    data:
      edited: false
      editors:
      - Xenova
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5166730880737305
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
          fullname: Joshua
          isHf: true
          isPro: false
          name: Xenova
          type: user
        html: '<p>See here for more info: <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/95b374952dc27d8511541d6f5a4e22c9ec11fb24/src/transformers/models/blenderbot/configuration_blenderbot.py#L53-L56">https://github.com/huggingface/transformers/blob/95b374952dc27d8511541d6f5a4e22c9ec11fb24/src/transformers/models/blenderbot/configuration_blenderbot.py#L53-L56</a></p>

          '
        raw: 'See here for more info: https://github.com/huggingface/transformers/blob/95b374952dc27d8511541d6f5a4e22c9ec11fb24/src/transformers/models/blenderbot/configuration_blenderbot.py#L53-L56'
        updatedAt: '2023-09-10T00:51:06.015Z'
      numEdits: 0
      reactions: []
    id: 64fd12fa8f76d4506948d1db
    type: comment
  author: Xenova
  content: 'See here for more info: https://github.com/huggingface/transformers/blob/95b374952dc27d8511541d6f5a4e22c9ec11fb24/src/transformers/models/blenderbot/configuration_blenderbot.py#L53-L56'
  created_at: 2023-09-09 23:51:06+00:00
  edited: false
  hidden: false
  id: 64fd12fa8f76d4506948d1db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-09-11T20:31:20.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2332923412322998
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Xenova&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Xenova\">@<span class=\"\
          underline\">Xenova</span></a></span>\n\n\t</span></span>, are you sure about\
          \ this change?</p>\n<p>This is the 400M-distil model, not the 3B model.\
          \ See the docs above:</p>\n<blockquote>\n<p>Instantiating a configuration\
          \ with the defaults will yield a similar configuration to that of the Blenderbot\
          \ <a href=\"https://huggingface.co/facebook/blenderbot-3B\">facebook/blenderbot-3B</a>\
          \ architecture.</p>\n</blockquote>\n<p>The checkpoint seems to contain only\
          \ two encoder layers as advertised by the configuration.</p>\n<pre><code\
          \ class=\"language-py\"><span class=\"hljs-meta\">&gt;&gt;&gt; </span><span\
          \ class=\"hljs-keyword\">from</span> huggingface_hub <span class=\"hljs-keyword\"\
          >import</span> hf_hub_download\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span><span\
          \ class=\"hljs-keyword\">import</span> torch\n\n<span class=\"hljs-meta\"\
          >&gt;&gt;&gt; </span> file = hf_hub_download(<span class=\"hljs-string\"\
          >\"facebook/blenderbot-400M-distill\"</span>, <span class=\"hljs-string\"\
          >\"pytorch_model.bin\"</span>)\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>checkpoint\
          \  = torch.load(file)\n\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span><span\
          \ class=\"hljs-built_in\">list</span>(checkpoint.keys())\n[<span class=\"\
          hljs-string\">'final_logits_bias'</span>,\n <span class=\"hljs-string\"\
          >'model.shared.weight'</span>,\n <span class=\"hljs-string\">'model.encoder.embed_tokens.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.embed_positions.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.self_attn.k_proj.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.self_attn.k_proj.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.self_attn.v_proj.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.self_attn.v_proj.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.self_attn.q_proj.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.self_attn.q_proj.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.self_attn.out_proj.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.self_attn.out_proj.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.self_attn_layer_norm.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.self_attn_layer_norm.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.fc1.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.fc1.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.fc2.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.fc2.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.final_layer_norm.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.0.final_layer_norm.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.self_attn.k_proj.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.self_attn.k_proj.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.self_attn.v_proj.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.self_attn.v_proj.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.self_attn.q_proj.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.self_attn.q_proj.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.self_attn.out_proj.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.self_attn.out_proj.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.self_attn_layer_norm.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.self_attn_layer_norm.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.fc1.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.fc1.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.fc2.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.fc2.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.final_layer_norm.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layers.1.final_layer_norm.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layer_norm.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.encoder.layer_norm.bias'</span>,\n\
          \ <span class=\"hljs-string\">'model.decoder.embed_tokens.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.decoder.embed_positions.weight'</span>,\n\
          \ <span class=\"hljs-string\">'model.decoder.layers.0.self_attn.k_proj.weight'</span>,\n\
          \n[...]\n</code></pre>\n"
        raw: "Hey @Xenova, are you sure about this change?\n\nThis is the 400M-distil\
          \ model, not the 3B model. See the docs above:\n\n> Instantiating a configuration\
          \ with the defaults will yield a similar configuration to that of the Blenderbot\
          \ [facebook/blenderbot-3B](https://huggingface.co/facebook/blenderbot-3B)\
          \ architecture.\n\nThe checkpoint seems to contain only two encoder layers\
          \ as advertised by the configuration.\n\n```py\n>>> from huggingface_hub\
          \ import hf_hub_download\n>>> import torch\n\n>>>  file = hf_hub_download(\"\
          facebook/blenderbot-400M-distill\", \"pytorch_model.bin\")\n>>> checkpoint\
          \  = torch.load(file)\n\n>>> list(checkpoint.keys())\n['final_logits_bias',\n\
          \ 'model.shared.weight',\n 'model.encoder.embed_tokens.weight',\n 'model.encoder.embed_positions.weight',\n\
          \ 'model.encoder.layers.0.self_attn.k_proj.weight',\n 'model.encoder.layers.0.self_attn.k_proj.bias',\n\
          \ 'model.encoder.layers.0.self_attn.v_proj.weight',\n 'model.encoder.layers.0.self_attn.v_proj.bias',\n\
          \ 'model.encoder.layers.0.self_attn.q_proj.weight',\n 'model.encoder.layers.0.self_attn.q_proj.bias',\n\
          \ 'model.encoder.layers.0.self_attn.out_proj.weight',\n 'model.encoder.layers.0.self_attn.out_proj.bias',\n\
          \ 'model.encoder.layers.0.self_attn_layer_norm.weight',\n 'model.encoder.layers.0.self_attn_layer_norm.bias',\n\
          \ 'model.encoder.layers.0.fc1.weight',\n 'model.encoder.layers.0.fc1.bias',\n\
          \ 'model.encoder.layers.0.fc2.weight',\n 'model.encoder.layers.0.fc2.bias',\n\
          \ 'model.encoder.layers.0.final_layer_norm.weight',\n 'model.encoder.layers.0.final_layer_norm.bias',\n\
          \ 'model.encoder.layers.1.self_attn.k_proj.weight',\n 'model.encoder.layers.1.self_attn.k_proj.bias',\n\
          \ 'model.encoder.layers.1.self_attn.v_proj.weight',\n 'model.encoder.layers.1.self_attn.v_proj.bias',\n\
          \ 'model.encoder.layers.1.self_attn.q_proj.weight',\n 'model.encoder.layers.1.self_attn.q_proj.bias',\n\
          \ 'model.encoder.layers.1.self_attn.out_proj.weight',\n 'model.encoder.layers.1.self_attn.out_proj.bias',\n\
          \ 'model.encoder.layers.1.self_attn_layer_norm.weight',\n 'model.encoder.layers.1.self_attn_layer_norm.bias',\n\
          \ 'model.encoder.layers.1.fc1.weight',\n 'model.encoder.layers.1.fc1.bias',\n\
          \ 'model.encoder.layers.1.fc2.weight',\n 'model.encoder.layers.1.fc2.bias',\n\
          \ 'model.encoder.layers.1.final_layer_norm.weight',\n 'model.encoder.layers.1.final_layer_norm.bias',\n\
          \ 'model.encoder.layer_norm.weight',\n 'model.encoder.layer_norm.bias',\n\
          \ 'model.decoder.embed_tokens.weight',\n 'model.decoder.embed_positions.weight',\n\
          \ 'model.decoder.layers.0.self_attn.k_proj.weight',\n\n[...]\n```\n"
        updatedAt: '2023-09-11T20:31:20.985Z'
      numEdits: 0
      reactions: []
    id: 64ff7918520f8d7b4de36aeb
    type: comment
  author: lysandre
  content: "Hey @Xenova, are you sure about this change?\n\nThis is the 400M-distil\
    \ model, not the 3B model. See the docs above:\n\n> Instantiating a configuration\
    \ with the defaults will yield a similar configuration to that of the Blenderbot\
    \ [facebook/blenderbot-3B](https://huggingface.co/facebook/blenderbot-3B) architecture.\n\
    \nThe checkpoint seems to contain only two encoder layers as advertised by the\
    \ configuration.\n\n```py\n>>> from huggingface_hub import hf_hub_download\n>>>\
    \ import torch\n\n>>>  file = hf_hub_download(\"facebook/blenderbot-400M-distill\"\
    , \"pytorch_model.bin\")\n>>> checkpoint  = torch.load(file)\n\n>>> list(checkpoint.keys())\n\
    ['final_logits_bias',\n 'model.shared.weight',\n 'model.encoder.embed_tokens.weight',\n\
    \ 'model.encoder.embed_positions.weight',\n 'model.encoder.layers.0.self_attn.k_proj.weight',\n\
    \ 'model.encoder.layers.0.self_attn.k_proj.bias',\n 'model.encoder.layers.0.self_attn.v_proj.weight',\n\
    \ 'model.encoder.layers.0.self_attn.v_proj.bias',\n 'model.encoder.layers.0.self_attn.q_proj.weight',\n\
    \ 'model.encoder.layers.0.self_attn.q_proj.bias',\n 'model.encoder.layers.0.self_attn.out_proj.weight',\n\
    \ 'model.encoder.layers.0.self_attn.out_proj.bias',\n 'model.encoder.layers.0.self_attn_layer_norm.weight',\n\
    \ 'model.encoder.layers.0.self_attn_layer_norm.bias',\n 'model.encoder.layers.0.fc1.weight',\n\
    \ 'model.encoder.layers.0.fc1.bias',\n 'model.encoder.layers.0.fc2.weight',\n\
    \ 'model.encoder.layers.0.fc2.bias',\n 'model.encoder.layers.0.final_layer_norm.weight',\n\
    \ 'model.encoder.layers.0.final_layer_norm.bias',\n 'model.encoder.layers.1.self_attn.k_proj.weight',\n\
    \ 'model.encoder.layers.1.self_attn.k_proj.bias',\n 'model.encoder.layers.1.self_attn.v_proj.weight',\n\
    \ 'model.encoder.layers.1.self_attn.v_proj.bias',\n 'model.encoder.layers.1.self_attn.q_proj.weight',\n\
    \ 'model.encoder.layers.1.self_attn.q_proj.bias',\n 'model.encoder.layers.1.self_attn.out_proj.weight',\n\
    \ 'model.encoder.layers.1.self_attn.out_proj.bias',\n 'model.encoder.layers.1.self_attn_layer_norm.weight',\n\
    \ 'model.encoder.layers.1.self_attn_layer_norm.bias',\n 'model.encoder.layers.1.fc1.weight',\n\
    \ 'model.encoder.layers.1.fc1.bias',\n 'model.encoder.layers.1.fc2.weight',\n\
    \ 'model.encoder.layers.1.fc2.bias',\n 'model.encoder.layers.1.final_layer_norm.weight',\n\
    \ 'model.encoder.layers.1.final_layer_norm.bias',\n 'model.encoder.layer_norm.weight',\n\
    \ 'model.encoder.layer_norm.bias',\n 'model.decoder.embed_tokens.weight',\n 'model.decoder.embed_positions.weight',\n\
    \ 'model.decoder.layers.0.self_attn.k_proj.weight',\n\n[...]\n```\n"
  created_at: 2023-09-11 19:31:20+00:00
  edited: false
  hidden: false
  id: 64ff7918520f8d7b4de36aeb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-09-11T21:58:12.000Z'
    data:
      edited: true
      editors:
      - Xenova
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.34829601645469666
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
          fullname: Joshua
          isHf: true
          isPro: false
          name: Xenova
          type: user
        html: "<p>I encountered an issue when converting this model to ONNX with optimum\
          \ (see <a href=\"https://huggingface.co/Xenova/blenderbot-400M-distill/tree/main/onnx\"\
          >here</a>). And prematurely made a PR after seeing the config for the 3B\
          \ model (which I did confuse it for). After opening the model in <a rel=\"\
          nofollow\" href=\"https://netron.app/\">netron</a>, it indicated 12 past\
          \ key values for the encoder, which I misread as 12 layers. My bad - closing\
          \ now!</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/LW-KDY2CTeWTJFdGx1njq.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/LW-KDY2CTeWTJFdGx1njq.png\"\
          ></a></p>\n<p>However, actually looking at the model in pytorch shows 2\
          \ layers:</p>\n<pre><code class=\"language-py\">BlenderbotModel(\n  (shared):\
          \ Embedding(<span class=\"hljs-number\">8008</span>, <span class=\"hljs-number\"\
          >1280</span>, padding_idx=<span class=\"hljs-number\">0</span>)\n  (encoder):\
          \ BlenderbotEncoder(\n    (embed_tokens): Embedding(<span class=\"hljs-number\"\
          >8008</span>, <span class=\"hljs-number\">1280</span>, padding_idx=<span\
          \ class=\"hljs-number\">0</span>)\n    (embed_positions): BlenderbotLearnedPositionalEmbedding(<span\
          \ class=\"hljs-number\">128</span>, <span class=\"hljs-number\">1280</span>)\n\
          \    (layers): ModuleList(\n      (<span class=\"hljs-number\">0</span>-<span\
          \ class=\"hljs-number\">1</span>): <span class=\"hljs-number\">2</span>\
          \ x BlenderbotEncoderLayer(\n        (self_attn): BlenderbotAttention(\n\
          \          (k_proj): Linear(in_features=<span class=\"hljs-number\">1280</span>,\
          \ out_features=<span class=\"hljs-number\">1280</span>, bias=<span class=\"\
          hljs-literal\">True</span>)\n          (v_proj): Linear(in_features=<span\
          \ class=\"hljs-number\">1280</span>, out_features=<span class=\"hljs-number\"\
          >1280</span>, bias=<span class=\"hljs-literal\">True</span>)\n         \
          \ (q_proj): Linear(in_features=<span class=\"hljs-number\">1280</span>,\
          \ out_features=<span class=\"hljs-number\">1280</span>, bias=<span class=\"\
          hljs-literal\">True</span>)\n          (out_proj): Linear(in_features=<span\
          \ class=\"hljs-number\">1280</span>, out_features=<span class=\"hljs-number\"\
          >1280</span>, bias=<span class=\"hljs-literal\">True</span>)\n        )\n\
          \        (self_attn_layer_norm): LayerNorm((<span class=\"hljs-number\"\
          >1280</span>,), eps=<span class=\"hljs-number\">1e-05</span>, elementwise_affine=<span\
          \ class=\"hljs-literal\">True</span>)\n        (activation_fn): GELUActivation()\n\
          \        (fc1): Linear(in_features=<span class=\"hljs-number\">1280</span>,\
          \ out_features=<span class=\"hljs-number\">5120</span>, bias=<span class=\"\
          hljs-literal\">True</span>)\n        (fc2): Linear(in_features=<span class=\"\
          hljs-number\">5120</span>, out_features=<span class=\"hljs-number\">1280</span>,\
          \ bias=<span class=\"hljs-literal\">True</span>)\n        (final_layer_norm):\
          \ LayerNorm((<span class=\"hljs-number\">1280</span>,), eps=<span class=\"\
          hljs-number\">1e-05</span>, elementwise_affine=<span class=\"hljs-literal\"\
          >True</span>)\n      )\n    )\n    (layer_norm): LayerNorm((<span class=\"\
          hljs-number\">1280</span>,), eps=<span class=\"hljs-number\">1e-05</span>,\
          \ elementwise_affine=<span class=\"hljs-literal\">True</span>)\n  )\n  (decoder):\
          \ BlenderbotDecoder(\n    (embed_tokens): Embedding(<span class=\"hljs-number\"\
          >8008</span>, <span class=\"hljs-number\">1280</span>, padding_idx=<span\
          \ class=\"hljs-number\">0</span>)\n    (embed_positions): BlenderbotLearnedPositionalEmbedding(<span\
          \ class=\"hljs-number\">128</span>, <span class=\"hljs-number\">1280</span>)\n\
          \    (layers): ModuleList(\n      (<span class=\"hljs-number\">0</span>-<span\
          \ class=\"hljs-number\">11</span>): <span class=\"hljs-number\">12</span>\
          \ x BlenderbotDecoderLayer(\n        (self_attn): BlenderbotAttention(\n\
          \          (k_proj): Linear(in_features=<span class=\"hljs-number\">1280</span>,\
          \ out_features=<span class=\"hljs-number\">1280</span>, bias=<span class=\"\
          hljs-literal\">True</span>)\n          (v_proj): Linear(in_features=<span\
          \ class=\"hljs-number\">1280</span>, out_features=<span class=\"hljs-number\"\
          >1280</span>, bias=<span class=\"hljs-literal\">True</span>)\n         \
          \ (q_proj): Linear(in_features=<span class=\"hljs-number\">1280</span>,\
          \ out_features=<span class=\"hljs-number\">1280</span>, bias=<span class=\"\
          hljs-literal\">True</span>)\n          (out_proj): Linear(in_features=<span\
          \ class=\"hljs-number\">1280</span>, out_features=<span class=\"hljs-number\"\
          >1280</span>, bias=<span class=\"hljs-literal\">True</span>)\n        )\n\
          \        (activation_fn): GELUActivation()\n        (self_attn_layer_norm):\
          \ LayerNorm((<span class=\"hljs-number\">1280</span>,), eps=<span class=\"\
          hljs-number\">1e-05</span>, elementwise_affine=<span class=\"hljs-literal\"\
          >True</span>)\n        (encoder_attn): BlenderbotAttention(\n          (k_proj):\
          \ Linear(in_features=<span class=\"hljs-number\">1280</span>, out_features=<span\
          \ class=\"hljs-number\">1280</span>, bias=<span class=\"hljs-literal\">True</span>)\n\
          \          (v_proj): Linear(in_features=<span class=\"hljs-number\">1280</span>,\
          \ out_features=<span class=\"hljs-number\">1280</span>, bias=<span class=\"\
          hljs-literal\">True</span>)\n          (q_proj): Linear(in_features=<span\
          \ class=\"hljs-number\">1280</span>, out_features=<span class=\"hljs-number\"\
          >1280</span>, bias=<span class=\"hljs-literal\">True</span>)\n         \
          \ (out_proj): Linear(in_features=<span class=\"hljs-number\">1280</span>,\
          \ out_features=<span class=\"hljs-number\">1280</span>, bias=<span class=\"\
          hljs-literal\">True</span>)\n        )\n        (encoder_attn_layer_norm):\
          \ LayerNorm((<span class=\"hljs-number\">1280</span>,), eps=<span class=\"\
          hljs-number\">1e-05</span>, elementwise_affine=<span class=\"hljs-literal\"\
          >True</span>)\n        (fc1): Linear(in_features=<span class=\"hljs-number\"\
          >1280</span>, out_features=<span class=\"hljs-number\">5120</span>, bias=<span\
          \ class=\"hljs-literal\">True</span>)\n        (fc2): Linear(in_features=<span\
          \ class=\"hljs-number\">5120</span>, out_features=<span class=\"hljs-number\"\
          >1280</span>, bias=<span class=\"hljs-literal\">True</span>)\n        (final_layer_norm):\
          \ LayerNorm((<span class=\"hljs-number\">1280</span>,), eps=<span class=\"\
          hljs-number\">1e-05</span>, elementwise_affine=<span class=\"hljs-literal\"\
          >True</span>)\n      )\n    )\n    (layer_norm): LayerNorm((<span class=\"\
          hljs-number\">1280</span>,), eps=<span class=\"hljs-number\">1e-05</span>,\
          \ elementwise_affine=<span class=\"hljs-literal\">True</span>)\n  )\n)\n\
          </code></pre>\n"
        raw: "I encountered an issue when converting this model to ONNX with optimum\
          \ (see [here](https://huggingface.co/Xenova/blenderbot-400M-distill/tree/main/onnx)).\
          \ And prematurely made a PR after seeing the config for the 3B model (which\
          \ I did confuse it for). After opening the model in [netron](https://netron.app/),\
          \ it indicated 12 past key values for the encoder, which I misread as 12\
          \ layers. My bad - closing now!\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/LW-KDY2CTeWTJFdGx1njq.png)\n\
          \n\nHowever, actually looking at the model in pytorch shows 2 layers:\n\
          ```py\nBlenderbotModel(\n  (shared): Embedding(8008, 1280, padding_idx=0)\n\
          \  (encoder): BlenderbotEncoder(\n    (embed_tokens): Embedding(8008, 1280,\
          \ padding_idx=0)\n    (embed_positions): BlenderbotLearnedPositionalEmbedding(128,\
          \ 1280)\n    (layers): ModuleList(\n      (0-1): 2 x BlenderbotEncoderLayer(\n\
          \        (self_attn): BlenderbotAttention(\n          (k_proj): Linear(in_features=1280,\
          \ out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280,\
          \ out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280,\
          \ out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280,\
          \ out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm):\
          \ LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (activation_fn):\
          \ GELUActivation()\n        (fc1): Linear(in_features=1280, out_features=5120,\
          \ bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280,\
          \ bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05,\
          \ elementwise_affine=True)\n      )\n    )\n    (layer_norm): LayerNorm((1280,),\
          \ eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): BlenderbotDecoder(\n\
          \    (embed_tokens): Embedding(8008, 1280, padding_idx=0)\n    (embed_positions):\
          \ BlenderbotLearnedPositionalEmbedding(128, 1280)\n    (layers): ModuleList(\n\
          \      (0-11): 12 x BlenderbotDecoderLayer(\n        (self_attn): BlenderbotAttention(\n\
          \          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n\
          \          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n\
          \          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n\
          \          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n\
          \        )\n        (activation_fn): GELUActivation()\n        (self_attn_layer_norm):\
          \ LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn):\
          \ BlenderbotAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280,\
          \ bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280,\
          \ bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280,\
          \ bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280,\
          \ bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1280,),\
          \ eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280,\
          \ out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120,\
          \ out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,),\
          \ eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (layer_norm):\
          \ LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  )\n)\n```\n\n"
        updatedAt: '2023-09-11T22:12:20.123Z'
      numEdits: 6
      reactions: []
    id: 64ff8d74dadb8c628f14183d
    type: comment
  author: Xenova
  content: "I encountered an issue when converting this model to ONNX with optimum\
    \ (see [here](https://huggingface.co/Xenova/blenderbot-400M-distill/tree/main/onnx)).\
    \ And prematurely made a PR after seeing the config for the 3B model (which I\
    \ did confuse it for). After opening the model in [netron](https://netron.app/),\
    \ it indicated 12 past key values for the encoder, which I misread as 12 layers.\
    \ My bad - closing now!\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/LW-KDY2CTeWTJFdGx1njq.png)\n\
    \n\nHowever, actually looking at the model in pytorch shows 2 layers:\n```py\n\
    BlenderbotModel(\n  (shared): Embedding(8008, 1280, padding_idx=0)\n  (encoder):\
    \ BlenderbotEncoder(\n    (embed_tokens): Embedding(8008, 1280, padding_idx=0)\n\
    \    (embed_positions): BlenderbotLearnedPositionalEmbedding(128, 1280)\n    (layers):\
    \ ModuleList(\n      (0-1): 2 x BlenderbotEncoderLayer(\n        (self_attn):\
    \ BlenderbotAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05,\
    \ elementwise_affine=True)\n        (activation_fn): GELUActivation()\n      \
    \  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2):\
    \ Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm):\
    \ LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n   \
    \ (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  )\n\
    \  (decoder): BlenderbotDecoder(\n    (embed_tokens): Embedding(8008, 1280, padding_idx=0)\n\
    \    (embed_positions): BlenderbotLearnedPositionalEmbedding(128, 1280)\n    (layers):\
    \ ModuleList(\n      (0-11): 12 x BlenderbotDecoderLayer(\n        (self_attn):\
    \ BlenderbotAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n        )\n        (activation_fn): GELUActivation()\n        (self_attn_layer_norm):\
    \ LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn):\
    \ BlenderbotAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1280,),\
    \ eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280,\
    \ out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280,\
    \ bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \      )\n    )\n    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
    \  )\n)\n```\n\n"
  created_at: 2023-09-11 20:58:12+00:00
  edited: true
  hidden: false
  id: 64ff8d74dadb8c628f14183d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-09-11T22:11:46.000Z'
    data:
      status: closed
    id: 64ff90a273206090a270af3d
    type: status-change
  author: Xenova
  created_at: 2023-09-11 21:11:46+00:00
  id: 64ff90a273206090a270af3d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-09-11T22:27:09.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5992581248283386
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: '<p>Great, thanks for the update!</p>

          '
        raw: Great, thanks for the update!
        updatedAt: '2023-09-11T22:27:09.293Z'
      numEdits: 0
      reactions: []
    id: 64ff943d69219ce3e4821c3f
    type: comment
  author: lysandre
  content: Great, thanks for the update!
  created_at: 2023-09-11 21:27:09+00:00
  edited: false
  hidden: false
  id: 64ff943d69219ce3e4821c3f
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 9
repo_id: facebook/blenderbot-400M-distill
repo_type: model
status: closed
target_branch: refs/heads/main
title: 'Fix typo (num_encoder_layers: 2 -> 12)'
