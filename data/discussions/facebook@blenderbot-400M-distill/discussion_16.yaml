!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AxelPATRON
conflicting_files: null
created_at: 2023-10-19 08:44:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49a7838d326730f12e0a1fad12f4c453.svg
      fullname: Axel PATRON
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AxelPATRON
      type: user
    createdAt: '2023-10-19T09:44:28.000Z'
    data:
      edited: false
      editors:
      - AxelPATRON
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.844363808631897
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49a7838d326730f12e0a1fad12f4c453.svg
          fullname: Axel PATRON
          isHf: false
          isPro: false
          name: AxelPATRON
          type: user
        html: '<p>With this code:</p>

          <p>model_name = ''facebook/blenderbot-400M-distill''<br>tokenizer = BlenderbotTokenizer.from_pretrained(model_name)<br>model
          = BlenderbotForConditionalGeneration.from_pretrained(model_name)<br>chat_pipeline
          = pipeline(''conversational'', model=model, tokenizer=tokenizer)<br>question
          =  "my question"<br>conversation = Conversation(question)<br>chat_pipeline([conversation],max_length=512)<br>response
          = conversation.generated_responses[-1]</p>

          <p>I get "sequence length is longer than the specified maximum sequence
          length for this model (146 &gt; 128). Running this sequence through the
          model will result in indexing errors" when my question is too long, I tried
          to changed the max_length in chat_pipeline but I''m not sure it''s there
          that I should change the parameters, How could I change the code to be able
          to ask huge questions?</p>

          '
        raw: "With this code:\r\n\r\nmodel_name = 'facebook/blenderbot-400M-distill'\r\
          \ntokenizer = BlenderbotTokenizer.from_pretrained(model_name)\r\nmodel =\
          \ BlenderbotForConditionalGeneration.from_pretrained(model_name)\r\nchat_pipeline\
          \ = pipeline('conversational', model=model, tokenizer=tokenizer)\r\nquestion\
          \ =  \"my question\"\r\nconversation = Conversation(question)\r\nchat_pipeline([conversation],max_length=512)\r\
          \nresponse = conversation.generated_responses[-1]\r\n\r\nI get \"sequence\
          \ length is longer than the specified maximum sequence length for this model\
          \ (146 > 128). Running this sequence through the model will result in indexing\
          \ errors\" when my question is too long, I tried to changed the max_length\
          \ in chat_pipeline but I'm not sure it's there that I should change the\
          \ parameters, How could I change the code to be able to ask huge questions?"
        updatedAt: '2023-10-19T09:44:28.596Z'
      numEdits: 0
      reactions: []
    id: 6530fa7c575598d7255b14bc
    type: comment
  author: AxelPATRON
  content: "With this code:\r\n\r\nmodel_name = 'facebook/blenderbot-400M-distill'\r\
    \ntokenizer = BlenderbotTokenizer.from_pretrained(model_name)\r\nmodel = BlenderbotForConditionalGeneration.from_pretrained(model_name)\r\
    \nchat_pipeline = pipeline('conversational', model=model, tokenizer=tokenizer)\r\
    \nquestion =  \"my question\"\r\nconversation = Conversation(question)\r\nchat_pipeline([conversation],max_length=512)\r\
    \nresponse = conversation.generated_responses[-1]\r\n\r\nI get \"sequence length\
    \ is longer than the specified maximum sequence length for this model (146 > 128).\
    \ Running this sequence through the model will result in indexing errors\" when\
    \ my question is too long, I tried to changed the max_length in chat_pipeline\
    \ but I'm not sure it's there that I should change the parameters, How could I\
    \ change the code to be able to ask huge questions?"
  created_at: 2023-10-19 08:44:28+00:00
  edited: false
  hidden: false
  id: 6530fa7c575598d7255b14bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49a7838d326730f12e0a1fad12f4c453.svg
      fullname: Axel PATRON
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AxelPATRON
      type: user
    createdAt: '2023-10-19T13:23:09.000Z'
    data:
      edited: false
      editors:
      - AxelPATRON
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6944798827171326
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49a7838d326730f12e0a1fad12f4c453.svg
          fullname: Axel PATRON
          isHf: false
          isPro: false
          name: AxelPATRON
          type: user
        html: '<p>I changed it to "config = BlenderbotConfig.from_pretrained(model_name,
          max_position_embeddings=1024)<br>tokenizer = BlenderbotTokenizer.from_pretrained(model_name)<br>model
          = BlenderbotForConditionalGeneration.from_pretrained(model_name, ignore_mismatched_sizes=True,
          config = config)"</p>

          <p>But the responses are no longer even intelligble ( I switch to the blenderbot-3B
          btw but it''s the same with both)</p>

          '
        raw: 'I changed it to "config = BlenderbotConfig.from_pretrained(model_name,
          max_position_embeddings=1024)

          tokenizer = BlenderbotTokenizer.from_pretrained(model_name)

          model = BlenderbotForConditionalGeneration.from_pretrained(model_name, ignore_mismatched_sizes=True,
          config = config)"


          But the responses are no longer even intelligble ( I switch to the blenderbot-3B
          btw but it''s the same with both)'
        updatedAt: '2023-10-19T13:23:09.451Z'
      numEdits: 0
      reactions: []
    id: 65312dbd86b57032d756c4bc
    type: comment
  author: AxelPATRON
  content: 'I changed it to "config = BlenderbotConfig.from_pretrained(model_name,
    max_position_embeddings=1024)

    tokenizer = BlenderbotTokenizer.from_pretrained(model_name)

    model = BlenderbotForConditionalGeneration.from_pretrained(model_name, ignore_mismatched_sizes=True,
    config = config)"


    But the responses are no longer even intelligble ( I switch to the blenderbot-3B
    btw but it''s the same with both)'
  created_at: 2023-10-19 12:23:09+00:00
  edited: false
  hidden: false
  id: 65312dbd86b57032d756c4bc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: facebook/blenderbot-400M-distill
repo_type: model
status: open
target_branch: null
title: Problems with questions too long
