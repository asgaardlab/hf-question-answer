!!python/object:huggingface_hub.community.DiscussionWithDetails
author: PerRing
conflicting_files: null
created_at: 2023-12-09 18:12:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae00fde99b95fcc3a95aae8a558eb049.svg
      fullname: HanYoo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PerRing
      type: user
    createdAt: '2023-12-09T18:12:40.000Z'
    data:
      edited: false
      editors:
      - PerRing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.538936972618103
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae00fde99b95fcc3a95aae8a558eb049.svg
          fullname: HanYoo
          isHf: false
          isPro: false
          name: PerRing
          type: user
        html: "<pre><code>WARNING:accelerate.utils.modeling:The model weights are\
          \ not tied. Please use the `tie_weights` method before using the `infer_auto_device`\
          \ function.\nLoading checkpoint shards: 0%\n0/6 [00:00&lt;?, ?it/s]\n---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          &lt;ipython-input-5-c1b45b9f685e&gt; in &lt;cell line: 3&gt;()\n      1\
          \ from transformers import AutoProcessor,AutoTokenizer, AutoModelForCausalLM,\
          \ AutoConfig,CLIPImageProcessor, LlamaForCausalLM, LlamaModel, LlavaForConditionalGeneration,\
          \ LlavaConfig, CLIPVisionConfig, LlamaConfig, CLIPVisionModel, LlavaProcessor\n\
          \      2 import torch\n----&gt; 3 LlavaModel = LlavaForConditionalGeneration.from_pretrained(\"\
          llava-hf/llava-1.5-13b-hf\", torch_dtype=torch.float16, device_map='auto')\n\
          \n3 frames\n/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\
          \ in set_module_tensor_to_device(module, tensor_name, device, value, dtype,\
          \ fp16_statistics)\n    283     if value is not None:\n    284         if\
          \ old_value.shape != value.shape:\n--&gt; 285             raise ValueError(\n\
          \    286                 f'Trying to set a tensor of shape {value.shape}\
          \ in \"{tensor_name}\" (which has shape {old_value.shape}), this look incorrect.'\n\
          \    287             )\n\nValueError: Trying to set a tensor of shape torch.Size([32064,\
          \ 5120]) in \"weight\" (which has shape torch.Size([32128, 5120])), this\
          \ look incorrect.\n</code></pre>\n<p>'llava-hf/llava-1.5-7b-hf' is loaded\
          \ without any error, but when load 'llava-hf/llava-1.5-13b-hf', this error\
          \ happen</p>\n"
        raw: "```\r\nWARNING:accelerate.utils.modeling:The model weights are not tied.\
          \ Please use the `tie_weights` method before using the `infer_auto_device`\
          \ function.\r\nLoading checkpoint shards: 0%\r\n0/6 [00:00<?, ?it/s]\r\n\
          ---------------------------------------------------------------------------\r\
          \nValueError                                Traceback (most recent call\
          \ last)\r\n<ipython-input-5-c1b45b9f685e> in <cell line: 3>()\r\n      1\
          \ from transformers import AutoProcessor,AutoTokenizer, AutoModelForCausalLM,\
          \ AutoConfig,CLIPImageProcessor, LlamaForCausalLM, LlamaModel, LlavaForConditionalGeneration,\
          \ LlavaConfig, CLIPVisionConfig, LlamaConfig, CLIPVisionModel, LlavaProcessor\r\
          \n      2 import torch\r\n----> 3 LlavaModel = LlavaForConditionalGeneration.from_pretrained(\"\
          llava-hf/llava-1.5-13b-hf\", torch_dtype=torch.float16, device_map='auto')\r\
          \n\r\n3 frames\r\n/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\
          \ in set_module_tensor_to_device(module, tensor_name, device, value, dtype,\
          \ fp16_statistics)\r\n    283     if value is not None:\r\n    284     \
          \    if old_value.shape != value.shape:\r\n--> 285             raise ValueError(\r\
          \n    286                 f'Trying to set a tensor of shape {value.shape}\
          \ in \"{tensor_name}\" (which has shape {old_value.shape}), this look incorrect.'\r\
          \n    287             )\r\n\r\nValueError: Trying to set a tensor of shape\
          \ torch.Size([32064, 5120]) in \"weight\" (which has shape torch.Size([32128,\
          \ 5120])), this look incorrect.\r\n```\r\n\r\n'llava-hf/llava-1.5-7b-hf'\
          \ is loaded without any error, but when load 'llava-hf/llava-1.5-13b-hf',\
          \ this error happen"
        updatedAt: '2023-12-09T18:12:40.639Z'
      numEdits: 0
      reactions: []
    id: 6574ae1812ae60c54283cb6a
    type: comment
  author: PerRing
  content: "```\r\nWARNING:accelerate.utils.modeling:The model weights are not tied.\
    \ Please use the `tie_weights` method before using the `infer_auto_device` function.\r\
    \nLoading checkpoint shards: 0%\r\n0/6 [00:00<?, ?it/s]\r\n---------------------------------------------------------------------------\r\
    \nValueError                                Traceback (most recent call last)\r\
    \n<ipython-input-5-c1b45b9f685e> in <cell line: 3>()\r\n      1 from transformers\
    \ import AutoProcessor,AutoTokenizer, AutoModelForCausalLM, AutoConfig,CLIPImageProcessor,\
    \ LlamaForCausalLM, LlamaModel, LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig,\
    \ LlamaConfig, CLIPVisionModel, LlavaProcessor\r\n      2 import torch\r\n---->\
    \ 3 LlavaModel = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-13b-hf\"\
    , torch_dtype=torch.float16, device_map='auto')\r\n\r\n3 frames\r\n/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\
    \ in set_module_tensor_to_device(module, tensor_name, device, value, dtype, fp16_statistics)\r\
    \n    283     if value is not None:\r\n    284         if old_value.shape != value.shape:\r\
    \n--> 285             raise ValueError(\r\n    286                 f'Trying to\
    \ set a tensor of shape {value.shape} in \"{tensor_name}\" (which has shape {old_value.shape}),\
    \ this look incorrect.'\r\n    287             )\r\n\r\nValueError: Trying to\
    \ set a tensor of shape torch.Size([32064, 5120]) in \"weight\" (which has shape\
    \ torch.Size([32128, 5120])), this look incorrect.\r\n```\r\n\r\n'llava-hf/llava-1.5-7b-hf'\
    \ is loaded without any error, but when load 'llava-hf/llava-1.5-13b-hf', this\
    \ error happen"
  created_at: 2023-12-09 18:12:40+00:00
  edited: false
  hidden: false
  id: 6574ae1812ae60c54283cb6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-09T18:37:48.000Z'
    data:
      edited: true
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7865089178085327
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ quick fix is to use set the vocab_size to <code>32064 </code> in the call\
          \ to from_pretrained</p>\n"
        raw: cc @ybelkada quick fix is to use set the vocab_size to `32064 ` in the
          call to from_pretrained
        updatedAt: '2023-12-09T18:39:25.757Z'
      numEdits: 1
      reactions: []
    id: 6574b3fcf9898ed3ab646093
    type: comment
  author: ArthurZ
  content: cc @ybelkada quick fix is to use set the vocab_size to `32064 ` in the
    call to from_pretrained
  created_at: 2023-12-09 18:37:48+00:00
  edited: true
  hidden: false
  id: 6574b3fcf9898ed3ab646093
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-09T18:39:55.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.78470778465271
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;PerRing&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/PerRing\">@<span class=\"\
          underline\">PerRing</span></a></span>\n\n\t</span></span> can you try to\
          \ pass <code>revision=\"refs/pr/3\"</code> in <code>from_pretrained</code>\
          \ ? (before I properly look into it)</p>\n"
        raw: '@PerRing can you try to pass `revision="refs/pr/3"` in `from_pretrained`
          ? (before I properly look into it)'
        updatedAt: '2023-12-09T18:39:55.029Z'
      numEdits: 0
      reactions: []
    id: 6574b47b12ae60c5428523b8
    type: comment
  author: ybelkada
  content: '@PerRing can you try to pass `revision="refs/pr/3"` in `from_pretrained`
    ? (before I properly look into it)'
  created_at: 2023-12-09 18:39:55+00:00
  edited: false
  hidden: false
  id: 6574b47b12ae60c5428523b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae00fde99b95fcc3a95aae8a558eb049.svg
      fullname: HanYoo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PerRing
      type: user
    createdAt: '2023-12-09T18:43:00.000Z'
    data:
      edited: false
      editors:
      - PerRing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.43534573912620544
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae00fde99b95fcc3a95aae8a558eb049.svg
          fullname: HanYoo
          isHf: false
          isPro: false
          name: PerRing
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> </p>\n<pre><code>LlavaModel\
          \ = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-13b-hf\"\
          ,torch_dtype=torch.float16,revision=\"refs/pr/3\",device_map='cpu')\n</code></pre>\n\
          <p>this work!</p>\n"
        raw: "@ybelkada \n```\nLlavaModel = LlavaForConditionalGeneration.from_pretrained(\"\
          llava-hf/llava-1.5-13b-hf\",torch_dtype=torch.float16,revision=\"refs/pr/3\"\
          ,device_map='cpu')\n```\nthis work!"
        updatedAt: '2023-12-09T18:43:00.933Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
        - mjkmain
      - count: 1
        reaction: "\U0001F614"
        users:
        - mjkmain
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mjkmain
      - count: 1
        reaction: "\U0001F91D"
        users:
        - mjkmain
      - count: 1
        reaction: "\U0001F917"
        users:
        - mjkmain
    id: 6574b5347184b6d4082686bc
    type: comment
  author: PerRing
  content: "@ybelkada \n```\nLlavaModel = LlavaForConditionalGeneration.from_pretrained(\"\
    llava-hf/llava-1.5-13b-hf\",torch_dtype=torch.float16,revision=\"refs/pr/3\",device_map='cpu')\n\
    ```\nthis work!"
  created_at: 2023-12-09 18:43:00+00:00
  edited: false
  hidden: false
  id: 6574b5347184b6d4082686bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-09T18:43:47.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8702434301376343
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Awesome, <span data-props=\"{&quot;user&quot;:&quot;ArthurZ&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ArthurZ\"\
          >@<span class=\"underline\">ArthurZ</span></a></span>\n\n\t</span></span>\
          \ feel free to merge <a href=\"/llava-hf/llava-1.5-13b-hf/discussions/3\"\
          >#3</a></p>\n"
        raw: 'Awesome, @ArthurZ feel free to merge #3'
        updatedAt: '2023-12-09T18:43:47.686Z'
      numEdits: 0
      reactions: []
    id: 6574b563fc88f842e3ddb414
    type: comment
  author: ybelkada
  content: 'Awesome, @ArthurZ feel free to merge #3'
  created_at: 2023-12-09 18:43:47+00:00
  edited: false
  hidden: false
  id: 6574b563fc88f842e3ddb414
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-09T18:43:56.000Z'
    data:
      status: closed
    id: 6574b56c8b44ef012b3a6076
    type: status-change
  author: ybelkada
  created_at: 2023-12-09 18:43:56+00:00
  id: 6574b56c8b44ef012b3a6076
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: llava-hf/llava-1.5-13b-hf
repo_type: model
status: closed
target_branch: null
title: error when loading the model
