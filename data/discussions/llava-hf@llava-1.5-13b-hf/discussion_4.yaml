!!python/object:huggingface_hub.community.DiscussionWithDetails
author: PerRing
conflicting_files: null
created_at: 2023-12-10 05:15:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae00fde99b95fcc3a95aae8a558eb049.svg
      fullname: HanYoo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PerRing
      type: user
    createdAt: '2023-12-10T05:15:09.000Z'
    data:
      edited: false
      editors:
      - PerRing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8032360076904297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae00fde99b95fcc3a95aae8a558eb049.svg
          fullname: HanYoo
          isHf: false
          isPro: false
          name: PerRing
          type: user
        html: '<p>I have trained my own LLaVA model with previous LLaVA code (<a rel="nofollow"
          href="https://github.com/haotian-liu/LLaVA">https://github.com/haotian-liu/LLaVA</a>)<br>Is
          there a way to use a previous version of LLaVA with Transformers(using below
          code)?</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoProcessor, LlavaForConditionalGeneration

          model = LlavaForConditionalGeneration.from_pretrained(PATH)

          processor = AutoProcessor.from_pretrained(PATH)

          </code></pre>

          '
        raw: "I have trained my own LLaVA model with previous LLaVA code (https://github.com/haotian-liu/LLaVA)\r\
          \nIs there a way to use a previous version of LLaVA with Transformers(using\
          \ below code)?\r\n```python\r\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\r\
          \nmodel = LlavaForConditionalGeneration.from_pretrained(PATH)\r\nprocessor\
          \ = AutoProcessor.from_pretrained(PATH)\r\n```"
        updatedAt: '2023-12-10T05:15:09.514Z'
      numEdits: 0
      reactions: []
    id: 6575495de09de6aa74302a88
    type: comment
  author: PerRing
  content: "I have trained my own LLaVA model with previous LLaVA code (https://github.com/haotian-liu/LLaVA)\r\
    \nIs there a way to use a previous version of LLaVA with Transformers(using below\
    \ code)?\r\n```python\r\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\r\
    \nmodel = LlavaForConditionalGeneration.from_pretrained(PATH)\r\nprocessor = AutoProcessor.from_pretrained(PATH)\r\
    \n```"
  created_at: 2023-12-10 05:15:09+00:00
  edited: false
  hidden: false
  id: 6575495de09de6aa74302a88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-12-10T08:29:40.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6948648691177368
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Yes, you can use the conversion script for that: <a rel="nofollow"
          href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llava/convert_llava_weights_to_hf.py">https://github.com/huggingface/transformers/blob/main/src/transformers/models/llava/convert_llava_weights_to_hf.py</a>.</p>

          '
        raw: 'Yes, you can use the conversion script for that: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llava/convert_llava_weights_to_hf.py.'
        updatedAt: '2023-12-10T08:29:40.050Z'
      numEdits: 0
      reactions: []
    id: 657576f45f7efec0ae534f8f
    type: comment
  author: nielsr
  content: 'Yes, you can use the conversion script for that: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llava/convert_llava_weights_to_hf.py.'
  created_at: 2023-12-10 08:29:40+00:00
  edited: false
  hidden: false
  id: 657576f45f7efec0ae534f8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-10T09:14:22.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9069331288337708
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;PerRing&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/PerRing\">@<span class=\"\
          underline\">PerRing</span></a></span>\n\n\t</span></span> let us know when\
          \ you managed to convert the weights and push them on the Hub !</p>\n"
        raw: '@PerRing let us know when you managed to convert the weights and push
          them on the Hub !'
        updatedAt: '2023-12-10T09:14:22.767Z'
      numEdits: 0
      reactions: []
    id: 6575816e9fe27c093d95eab7
    type: comment
  author: ybelkada
  content: '@PerRing let us know when you managed to convert the weights and push
    them on the Hub !'
  created_at: 2023-12-10 09:14:22+00:00
  edited: false
  hidden: false
  id: 6575816e9fe27c093d95eab7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-10T09:14:44.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9435376524925232
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>You need to save the raw state dict of your previous model on the
          Hub in order for that script to work</p>

          '
        raw: You need to save the raw state dict of your previous model on the Hub
          in order for that script to work
        updatedAt: '2023-12-10T09:14:44.234Z'
      numEdits: 0
      reactions: []
    id: 657581846da136b50faf4658
    type: comment
  author: ybelkada
  content: You need to save the raw state dict of your previous model on the Hub in
    order for that script to work
  created_at: 2023-12-10 09:14:44+00:00
  edited: false
  hidden: false
  id: 657581846da136b50faf4658
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-10T09:15:58.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6240086555480957
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>For example here: <a href="https://huggingface.co/ybelkada/test-llava-13b/tree/main">https://huggingface.co/ybelkada/test-llava-13b/tree/main</a>
          I stored the raw state dict of the previous llava 13B model, if you do the
          same for your model the conversion script should work smoothly</p>

          '
        raw: 'For example here: https://huggingface.co/ybelkada/test-llava-13b/tree/main
          I stored the raw state dict of the previous llava 13B model, if you do the
          same for your model the conversion script should work smoothly'
        updatedAt: '2023-12-10T09:15:58.786Z'
      numEdits: 0
      reactions: []
    id: 657581ce30a4401dfb83b6c2
    type: comment
  author: ybelkada
  content: 'For example here: https://huggingface.co/ybelkada/test-llava-13b/tree/main
    I stored the raw state dict of the previous llava 13B model, if you do the same
    for your model the conversion script should work smoothly'
  created_at: 2023-12-10 09:15:58+00:00
  edited: false
  hidden: false
  id: 657581ce30a4401dfb83b6c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae00fde99b95fcc3a95aae8a558eb049.svg
      fullname: HanYoo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PerRing
      type: user
    createdAt: '2023-12-10T11:27:48.000Z'
    data:
      edited: false
      editors:
      - PerRing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6090967655181885
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae00fde99b95fcc3a95aae8a558eb049.svg
          fullname: HanYoo
          isHf: false
          isPro: false
          name: PerRing
          type: user
        html: '<p>I''m not sure, but I think line 58 should be changed.</p>

          <pre><code>tokenizer.add_tokens(AddedToken("&lt;image&gt;", special=True,
          normalized=False), special=True)


          tokenizer.add_tokens(AddedToken("&lt;image&gt;", special=True, normalized=False))

          </code></pre>

          <p>because of </p>

          <pre><code>TypeError: SpecialTokensMixin.add_tokens() got an unexpected
          keyword argument ''special''

          </code></pre>

          <p>+)I fix line 58. but error happend in line 69.<br>ValueError: Unrecognized
          configuration class &lt;class ''transformers.models.llava.configuration_llava.LlavaConfig''&gt;
          for this kind of AutoModel: AutoModelForCausalLM.</p>

          <p>++)My model is based on LLaMA2 13B, but i expended vocab size(vocab size
          is 39479). Do you think this script works on my model?</p>

          '
        raw: "I'm not sure, but I think line 58 should be changed.\n```\ntokenizer.add_tokens(AddedToken(\"\
          <image>\", special=True, normalized=False), special=True)\n\ntokenizer.add_tokens(AddedToken(\"\
          <image>\", special=True, normalized=False))\n```\nbecause of \n```\nTypeError:\
          \ SpecialTokensMixin.add_tokens() got an unexpected keyword argument 'special'\n\
          ```\n\n+)I fix line 58. but error happend in line 69.\nValueError: Unrecognized\
          \ configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'>\
          \ for this kind of AutoModel: AutoModelForCausalLM.\n\n++)My model is based\
          \ on LLaMA2 13B, but i expended vocab size(vocab size is 39479). Do you\
          \ think this script works on my model?"
        updatedAt: '2023-12-10T11:27:48.754Z'
      numEdits: 0
      reactions: []
    id: 6575a0b4d7f487de5ff408f1
    type: comment
  author: PerRing
  content: "I'm not sure, but I think line 58 should be changed.\n```\ntokenizer.add_tokens(AddedToken(\"\
    <image>\", special=True, normalized=False), special=True)\n\ntokenizer.add_tokens(AddedToken(\"\
    <image>\", special=True, normalized=False))\n```\nbecause of \n```\nTypeError:\
    \ SpecialTokensMixin.add_tokens() got an unexpected keyword argument 'special'\n\
    ```\n\n+)I fix line 58. but error happend in line 69.\nValueError: Unrecognized\
    \ configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'>\
    \ for this kind of AutoModel: AutoModelForCausalLM.\n\n++)My model is based on\
    \ LLaMA2 13B, but i expended vocab size(vocab size is 39479). Do you think this\
    \ script works on my model?"
  created_at: 2023-12-10 11:27:48+00:00
  edited: false
  hidden: false
  id: 6575a0b4d7f487de5ff408f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-11T11:28:34.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8398443460464478
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<ol>

          <li>Okay we nee to use <code>special_tokens = True</code> </li>

          <li>You need to install fromn source</li>

          <li>The script should work, if not we will make the vocab size be retrieved
          from the model but it is I think</li>

          </ol>

          '
        raw: "1) Okay we nee to use `special_tokens = True` \n2) You need to install\
          \ fromn source\n3) The script should work, if not we will make the vocab\
          \ size be retrieved from the model but it is I think"
        updatedAt: '2023-12-11T11:28:34.144Z'
      numEdits: 0
      reactions: []
    id: 6576f26217ee44ff546c0a77
    type: comment
  author: ArthurZ
  content: "1) Okay we nee to use `special_tokens = True` \n2) You need to install\
    \ fromn source\n3) The script should work, if not we will make the vocab size\
    \ be retrieved from the model but it is I think"
  created_at: 2023-12-11 11:28:34+00:00
  edited: false
  hidden: false
  id: 6576f26217ee44ff546c0a77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1dfa6ec0793106448c8112d83bb35f39.svg
      fullname: klAS13
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SS12444
      type: user
    createdAt: '2023-12-22T20:55:01.000Z'
    data:
      edited: false
      editors:
      - SS12444
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9423380494117737
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1dfa6ec0793106448c8112d83bb35f39.svg
          fullname: klAS13
          isHf: false
          isPro: false
          name: SS12444
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ArthurZ&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ArthurZ\">@<span class=\"\
          underline\">ArthurZ</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;nielsr&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/nielsr\">@<span class=\"underline\">nielsr</span></a></span>\n\
          \n\t</span></span> I got the same problem as <span data-props=\"{&quot;user&quot;:&quot;PerRing&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/PerRing\"\
          >@<span class=\"underline\">PerRing</span></a></span>\n\n\t</span></span>\
          \ and the weights cannot be converted as the script fails with those errors.\
          \ My transformers lib is installed from source and it seems that there is\
          \ no LlavaModelForCausalLM registered at all. Is it possible to fix this?\
          \ If this can be fixed and we know that the performance is the same as the\
          \ original llava, users will have more incentive to use the hf version over\
          \ the original one. Otherwise, all previous improvements in llava cannot\
          \ be used in HF unfortunately. I see there is some discussion here <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/transformers/pull/27662#discussion_r1416880859\"\
          >https://github.com/huggingface/transformers/pull/27662#discussion_r1416880859</a>\
          \ would it be possible to come up with a fix? Thanks!</p>\n"
        raw: '@ArthurZ @nielsr I got the same problem as @PerRing and the weights
          cannot be converted as the script fails with those errors. My transformers
          lib is installed from source and it seems that there is no LlavaModelForCausalLM
          registered at all. Is it possible to fix this? If this can be fixed and
          we know that the performance is the same as the original llava, users will
          have more incentive to use the hf version over the original one. Otherwise,
          all previous improvements in llava cannot be used in HF unfortunately. I
          see there is some discussion here https://github.com/huggingface/transformers/pull/27662#discussion_r1416880859
          would it be possible to come up with a fix? Thanks!'
        updatedAt: '2023-12-22T20:55:01.096Z'
      numEdits: 0
      reactions: []
    id: 6585f7a56f21a20591cc56b9
    type: comment
  author: SS12444
  content: '@ArthurZ @nielsr I got the same problem as @PerRing and the weights cannot
    be converted as the script fails with those errors. My transformers lib is installed
    from source and it seems that there is no LlavaModelForCausalLM registered at
    all. Is it possible to fix this? If this can be fixed and we know that the performance
    is the same as the original llava, users will have more incentive to use the hf
    version over the original one. Otherwise, all previous improvements in llava cannot
    be used in HF unfortunately. I see there is some discussion here https://github.com/huggingface/transformers/pull/27662#discussion_r1416880859
    would it be possible to come up with a fix? Thanks!'
  created_at: 2023-12-22 20:55:01+00:00
  edited: false
  hidden: false
  id: 6585f7a56f21a20591cc56b9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: llava-hf/llava-1.5-13b-hf
repo_type: model
status: open
target_branch: null
title: Is it not possible to load and use a previous version of LLaVA with Transformers?
