!!python/object:huggingface_hub.community.DiscussionWithDetails
author: scall
conflicting_files: null
created_at: 2023-06-23 00:12:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/25f1fb77da555423a4a5b2a3ed40c7cc.svg
      fullname: zhanglezhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: scall
      type: user
    createdAt: '2023-06-23T01:12:33.000Z'
    data:
      edited: false
      editors:
      - scall
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.17545807361602783
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/25f1fb77da555423a4a5b2a3ed40c7cc.svg
          fullname: zhanglezhong
          isHf: false
          isPro: false
          name: scall
          type: user
        html: "<pre><code>Epoch_0:   0%|          | 0/16 [00:04&lt;?, ?it/s]\n\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256E\n\u2502 /export/App/training_platform/PinoModel/applications/VisualGLM/visual_chatgl\
          \ \u2502\n\u2502 m_instructing_mergeclose_v1.py:229 in &lt;module&gt;  \
          \                             \u2502\n\u2502                           \
          \                                                   \u2502\n\u2502   226\
          \ \u2502   parser.add_argument('--lr', type=float, default=5e-6)       \
          \       \u2502\n\u2502   227 \u2502   parser.add_argument('--accimulation_steps',\
          \ type=int, default=4)   \u2502\n\u2502   228 \u2502   args = parser.parse_args()\
          \                                         \u2502\n\u2502 \u2771 229 \u2502\
          \   train(args)                                                        \u2502\
          \n\u2502   230                                                         \
          \               \u2502\n\u2502                                         \
          \                                     \u2502\n\u2502 /export/App/training_platform/PinoModel/applications/VisualGLM/visual_chatgl\
          \ \u2502\n\u2502 m_instructing_mergeclose_v1.py:203 in train           \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   200 \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502    model_save_path='/media/cfs/zhanglezhong/LLMS\
          \ \u2502\n\u2502   201 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \    tensorboard_writer=tensorboard_writer)        \u2502\n\u2502   202\
          \ \u2502                                                               \
          \       \u2502\n\u2502 \u2771 203 \u2502   trainer.fit(logger=logger, log_interval=args.log_interval)\
          \         \u2502\n\u2502   204                                         \
          \                               \u2502\n\u2502   205 #     # save model\
          \ checkpoint after fitting on only rank0              \u2502\n\u2502   206\
          \ #     trainer.save_model(path=args.save_path, only_rank0=True, tokeniz\
          \ \u2502\n\u2502                                                       \
          \                       \u2502\n\u2502 /export/App/training_platform/PinoModel/applications/VisualGLM/coati/trainer\
          \ \u2502\n\u2502 /visual_sft_glm.py:134 in fit                         \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   131 \u2502\
          \   \u2502   \u2502   \u2502   labels = batch[\"labels\"].to(torch.cuda.current_device(\
          \ \u2502\n\u2502   132 \u2502   \u2502   \u2502   \u2502   image = batch[\"\
          img\"].to(torch.cuda.current_device())   \u2502\n\u2502   133 \u2502   \u2502\
          \   \u2502   \u2502   pre_image = batch[\"pre_image\"]                 \
          \        \u2502\n\u2502 \u2771 134 \u2502   \u2502   \u2502   \u2502   outputs\
          \ = self.model(input_ids=prompt_ids, images=imag \u2502\n\u2502   135 \u2502\
          \   \u2502   \u2502   \u2502                                           \
          \               \u2502\n\u2502   136 \u2502   \u2502   \u2502   \u2502 \
          \  loss = outputs.loss                                    \u2502\n\u2502\
          \   137 #                 if loss &gt;= 2.5 and is_rank_0() :          \
          \           \u2502\n\u2502                                             \
          \                                 \u2502\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
          \ \u2502\n\u2502 1110 in _call_impl                                    \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   1107 \u2502\
          \   \u2502   # this function, and just call forward.                   \
          \    \u2502\n\u2502   1108 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._forward_hooks or self._ \u2502\n\u2502   1109 \u2502   \u2502\
          \   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks\
          \ \u2502\n\u2502 \u2771 1110 \u2502   \u2502   \u2502   return forward_call(*input,\
          \ **kwargs)                     \u2502\n\u2502   1111 \u2502   \u2502  \
          \ # Do not call functions when jit is used                      \u2502\n\
          \u2502   1112 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []         \u2502\n\u2502   1113 \u2502   \u2502   if self._backward_hooks\
          \ or _global_backward_hooks:            \u2502\n\u2502                 \
          \                                                             \u2502\n\u2502\
          \ /root/.cache/huggingface/modules/transformers_modules/visualglm/modeling_cha\
          \ \u2502\n\u2502 tglm.py:1462 in forward                               \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   1459 \u2502\
          \   \u2502   \u2502   return_dict: Optional[bool] = None,              \
          \         \u2502\n\u2502   1460 \u2502   ):                            \
          \                                    \u2502\n\u2502   1461 \u2502   \u2502\
          \   if inputs_embeds is None and past_key_values is None and imag \u2502\
          \n\u2502 \u2771 1462 \u2502   \u2502   \u2502   image_embeds = self.image_encoder(images)\
          \                 \u2502\n\u2502   1463 \u2502   \u2502   \u2502   pre_id,\
          \ pads, post_id = torch.tensor_split(input_ids,     \u2502\n\u2502   1464\
          \ \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502      [pre_image_len \u2502\
          \n\u2502   1465 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      dim=1)  # imag\
          \ \u2502\n\u2502                                                       \
          \                       \u2502\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
          \ \u2502\n\u2502 1110 in _call_impl                                    \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   1107 \u2502\
          \   \u2502   # this function, and just call forward.                   \
          \    \u2502\n\u2502   1108 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._forward_hooks or self._ \u2502\n\u2502   1109 \u2502   \u2502\
          \   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks\
          \ \u2502\n\u2502 \u2771 1110 \u2502   \u2502   \u2502   return forward_call(*input,\
          \ **kwargs)                     \u2502\n\u2502   1111 \u2502   \u2502  \
          \ # Do not call functions when jit is used                      \u2502\n\
          \u2502   1112 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []         \u2502\n\u2502   1113 \u2502   \u2502   if self._backward_hooks\
          \ or _global_backward_hooks:            \u2502\n\u2502                 \
          \                                                             \u2502\n\u2502\
          \ /root/.cache/huggingface/modules/transformers_modules/visualglm/visual.py:69\
          \ \u2502\n\u2502 in forward                                            \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502    66 \u2502\
          \   \u2502   \u2502   self.qformer.parameters().__next__().dtype)      \
          \          \u2502\n\u2502    67 \u2502                                 \
          \                                     \u2502\n\u2502    68 \u2502   def\
          \ forward(self, image, **kwargs):                                \u2502\n\
          \u2502 \u2771  69 \u2502   \u2502   enc = self.vit(image)[0]           \
          \                            \u2502\n\u2502    70 \u2502   \u2502   out\
          \ = self.qformer(enc)[0]                                     \u2502\n\u2502\
          \    71 \u2502   \u2502   return self.glm_proj(out)                    \
          \                  \u2502\n\u2502    72                                \
          \                                        \u2502\n\u2502                \
          \                                                              \u2502\n\u2502\
          \ /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
          \ \u2502\n\u2502 1110 in _call_impl                                    \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   1107 \u2502\
          \   \u2502   # this function, and just call forward.                   \
          \    \u2502\n\u2502   1108 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._forward_hooks or self._ \u2502\n\u2502   1109 \u2502   \u2502\
          \   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks\
          \ \u2502\n\u2502 \u2771 1110 \u2502   \u2502   \u2502   return forward_call(*input,\
          \ **kwargs)                     \u2502\n\u2502   1111 \u2502   \u2502  \
          \ # Do not call functions when jit is used                      \u2502\n\
          \u2502   1112 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []         \u2502\n\u2502   1113 \u2502   \u2502   if self._backward_hooks\
          \ or _global_backward_hooks:            \u2502\n\u2502                 \
          \                                                             \u2502\n\u2502\
          \ /root/.cache/huggingface/modules/transformers_modules/visualglm/visual.py:28\
          \ \u2502\n\u2502 in forward                                            \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502    25 \u2502\
          \   \u2502   batch_size = image.size(0)                                \
          \     \u2502\n\u2502    26 \u2502   \u2502   input_ids = torch.zeros(batch_size,\
          \ 1, dtype=torch.long, devic \u2502\n\u2502    27 \u2502   \u2502   attention_mask\
          \ = torch.tensor([[1.]], dtype=image.dtype, devic \u2502\n\u2502 \u2771\
          \  28 \u2502   \u2502   return super().forward(input_ids=input_ids, position_ids=None,\
          \ \u2502\n\u2502    29                                                 \
          \                       \u2502\n\u2502    30                           \
          \                                             \u2502\n\u2502    31 class\
          \ QFormer(BaseModel):                                              \u2502\
          \n\u2502                                                               \
          \               \u2502\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/model/base_model.py:144\
          \ \u2502\n\u2502 in forward                                            \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   141 \u2502\
          \   \u2502   # Attention! the transformer might be shared by multiple model\
          \ \u2502\n\u2502   142 \u2502   \u2502   self.transformer.hooks.clear()\
          \                                 \u2502\n\u2502   143 \u2502   \u2502 \
          \  self.transformer.hooks.update(self.hooks)                      \u2502\
          \n\u2502 \u2771 144 \u2502   \u2502   return self.transformer(*args, **kwargs)\
          \                       \u2502\n\u2502   145 \u2502                    \
          \                                                  \u2502\n\u2502   146\
          \ \u2502   def collect_hooks_(self):                                   \
          \       \u2502\n\u2502   147 \u2502   \u2502   names = list(HOOKS_DEFAULT.keys())\
          \                             \u2502\n\u2502                           \
          \                                                   \u2502\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
          \ \u2502\n\u2502 1110 in _call_impl                                    \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   1107 \u2502\
          \   \u2502   # this function, and just call forward.                   \
          \    \u2502\n\u2502   1108 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._forward_hooks or self._ \u2502\n\u2502   1109 \u2502   \u2502\
          \   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks\
          \ \u2502\n\u2502 \u2771 1110 \u2502   \u2502   \u2502   return forward_call(*input,\
          \ **kwargs)                     \u2502\n\u2502   1111 \u2502   \u2502  \
          \ # Do not call functions when jit is used                      \u2502\n\
          \u2502   1112 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []         \u2502\n\u2502   1113 \u2502   \u2502   if self._backward_hooks\
          \ or _global_backward_hooks:            \u2502\n\u2502                 \
          \                                                             \u2502\n\u2502\
          \ /usr/local/anaconda3/lib/python3.8/site-packages/sat/model/transformer.py:56\
          \ \u2502\n\u2502 9 in forward                                          \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   566 \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502   output_this_layer=output_this_layer_obj,\
          \ outpu \u2502\n\u2502   567 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   )                                                  \u2502\n\u2502  \
          \ 568 \u2502   \u2502   \u2502   \u2502   else:                        \
          \                          \u2502\n\u2502 \u2771 569 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   layer_ret = layer(*args, layer_id=torch.tensor(i),\
          \ \u2502\n\u2502   570 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \   output_this_layer=output_this_layer_obj, outpu \u2502\n\u2502   571\
          \ \u2502   \u2502   \u2502   \u2502   if isinstance(layer_ret, tuple): \
          \                      \u2502\n\u2502   572 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   layer_ret = layer_ret[0] # for legacy API          \u2502\n\
          \u2502                                                                 \
          \             \u2502\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
          \ \u2502\n\u2502 1110 in _call_impl                                    \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   1107 \u2502\
          \   \u2502   # this function, and just call forward.                   \
          \    \u2502\n\u2502   1108 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._forward_hooks or self._ \u2502\n\u2502   1109 \u2502   \u2502\
          \   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks\
          \ \u2502\n\u2502 \u2771 1110 \u2502   \u2502   \u2502   return forward_call(*input,\
          \ **kwargs)                     \u2502\n\u2502   1111 \u2502   \u2502  \
          \ # Do not call functions when jit is used                      \u2502\n\
          \u2502   1112 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []         \u2502\n\u2502   1113 \u2502   \u2502   if self._backward_hooks\
          \ or _global_backward_hooks:            \u2502\n\u2502                 \
          \                                                             \u2502\n\u2502\
          \ /usr/local/anaconda3/lib/python3.8/site-packages/sat/model/transformer.py:33\
          \ \u2502\n\u2502 0 in forward                                          \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   327 \u2502\
          \   \u2502   )                                                         \
          \     \u2502\n\u2502   328 \u2502                                      \
          \                                \u2502\n\u2502   329 \u2502   def forward(self,\
          \ hidden_states, mask, *args, **kw_args):          \u2502\n\u2502 \u2771\
          \ 330 \u2502   \u2502   return HOOKS_DEFAULT['layer_forward'](self, hidden_states,\
          \ mas \u2502\n\u2502   331                                             \
          \                           \u2502\n\u2502   332                       \
          \                                                 \u2502\n\u2502   333 class\
          \ BaseTransformer(torch.nn.Module):                                \u2502\
          \n\u2502                                                               \
          \               \u2502\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/transformer_defaults.py\
          \ \u2502\n\u2502 :127 in layer_forward_default                         \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   124 \u2502\
          \   # Layer norm at the begining of the transformer layer.             \u2502\
          \n\u2502   125 \u2502   attention_input = self.input_layernorm(hidden_states)\
          \              \u2502\n\u2502   126 \u2502   # Self attention.         \
          \                                         \u2502\n\u2502 \u2771 127 \u2502\
          \   attention_output = self.attention(attention_input, mask, **kw_args \u2502\
          \n\u2502   128 \u2502                                                  \
          \                    \u2502\n\u2502   129 \u2502   # Third LayerNorm   \
          \                                               \u2502\n\u2502   130 \u2502\
          \   if self.layernorm_order == 'sandwich':                             \u2502\
          \n\u2502                                                               \
          \               \u2502\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
          \ \u2502\n\u2502 1110 in _call_impl                                    \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   1107 \u2502\
          \   \u2502   # this function, and just call forward.                   \
          \    \u2502\n\u2502   1108 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._forward_hooks or self._ \u2502\n\u2502   1109 \u2502   \u2502\
          \   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks\
          \ \u2502\n\u2502 \u2771 1110 \u2502   \u2502   \u2502   return forward_call(*input,\
          \ **kwargs)                     \u2502\n\u2502   1111 \u2502   \u2502  \
          \ # Do not call functions when jit is used                      \u2502\n\
          \u2502   1112 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []         \u2502\n\u2502   1113 \u2502   \u2502   if self._backward_hooks\
          \ or _global_backward_hooks:            \u2502\n\u2502                 \
          \                                                             \u2502\n\u2502\
          \ /usr/local/anaconda3/lib/python3.8/site-packages/sat/model/transformer.py:10\
          \ \u2502\n\u2502 3 in forward                                          \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   100 \u2502\
          \   \u2502   if 'attention_forward' in self.hooks:                     \
          \     \u2502\n\u2502   101 \u2502   \u2502   \u2502   return self.hooks['attention_forward'](hidden_states,\
          \ mask \u2502\n\u2502   102 \u2502   \u2502   else:                    \
          \                                      \u2502\n\u2502 \u2771 103 \u2502\
          \   \u2502   \u2502   return HOOKS_DEFAULT['attention_forward'](self, hidden_sta\
          \ \u2502\n\u2502   104                                                 \
          \                       \u2502\n\u2502   105                           \
          \                                             \u2502\n\u2502   106 class\
          \ CrossAttention(torch.nn.Module):                                 \u2502\
          \n\u2502                                                               \
          \               \u2502\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/transformer_defaults.py\
          \ \u2502\n\u2502 :63 in attention_forward_default                      \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502    60 \u2502\
          \   key_layer = self._transpose_for_scores(mixed_key_layer)            \u2502\
          \n\u2502    61 \u2502   value_layer = self._transpose_for_scores(mixed_value_layer)\
          \        \u2502\n\u2502    62 \u2502                                   \
          \                                   \u2502\n\u2502 \u2771  63 \u2502   context_layer\
          \ = attention_fn(query_layer, key_layer, value_layer,  \u2502\n\u2502  \
          \  64 \u2502                                                           \
          \           \u2502\n\u2502    65 \u2502   context_layer = context_layer.permute(0,\
          \ 2, 1, 3).contiguous()     \u2502\n\u2502    66 \u2502   new_context_layer_shape\
          \ = context_layer.size()[:-2] + (self.hidden \u2502\n\u2502            \
          \                                                                  \u2502\
          \n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/transformer_defaults.py\
          \ \u2502\n\u2502 :38 in standard_attention                             \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502    35 \u2502\
          \                                                                      \u2502\
          \n\u2502    36 \u2502   if attention_dropout is not None:              \
          \                    \u2502\n\u2502    37 \u2502   \u2502   if mpu.get_cuda_rng_tracker\
          \ is not None:                       \u2502\n\u2502 \u2771  38 \u2502  \
          \ \u2502   \u2502   with mpu.get_cuda_rng_tracker().fork():            \
          \        \u2502\n\u2502    39 \u2502   \u2502   \u2502   \u2502   attention_probs\
          \ = attention_dropout(attention_probs)   \u2502\n\u2502    40 \u2502   \u2502\
          \   else:                                                          \u2502\
          \n\u2502    41 \u2502   \u2502   \u2502   attention_probs = attention_dropout(attention_probs)\
          \       \u2502\n\u2502                                                 \
          \                             \u2502\n\u2502 /usr/local/anaconda3/lib/python3.8/contextlib.py:113\
          \ in __enter__            \u2502\n\u2502                               \
          \                                               \u2502\n\u2502   110 \u2502\
          \   \u2502   # they are only needed for recreation, which is not possible\
          \ a \u2502\n\u2502   111 \u2502   \u2502   del self.args, self.kwds, self.func\
          \                            \u2502\n\u2502   112 \u2502   \u2502   try:\
          \                                                           \u2502\n\u2502\
          \ \u2771 113 \u2502   \u2502   \u2502   return next(self.gen)          \
          \                            \u2502\n\u2502   114 \u2502   \u2502   except\
          \ StopIteration:                                          \u2502\n\u2502\
          \   115 \u2502   \u2502   \u2502   raise RuntimeError(\"generator didn't\
          \ yield\") from None     \u2502\n\u2502   116                          \
          \                                              \u2502\n\u2502          \
          \                                                                    \u2502\
          \n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/deepspeed/runtime/activatio\
          \ \u2502\n\u2502 n_checkpointing/checkpointing.py:174 in fork          \
          \                       \u2502\n\u2502                                 \
          \                                             \u2502\n\u2502   171 \u2502\
          \   \u2502   the original state.\"\"\"                                 \
          \        \u2502\n\u2502   172 \u2502   \u2502   # Check if we have added\
          \ the state                             \u2502\n\u2502   173 \u2502   \u2502\
          \   if name not in self.states_:                                   \u2502\
          \n\u2502 \u2771 174 \u2502   \u2502   \u2502   raise Exception('cuda rng\
          \ state {} is not added'.format(na \u2502\n\u2502   175 \u2502   \u2502\
          \   # Store current rng state.                                     \u2502\
          \n\u2502   176 \u2502   \u2502   orig_cuda_rng_state = get_accelerator().get_rng_state()\
          \        \u2502\n\u2502   177 \u2502   \u2502   # Set rng state to the desired\
          \ one                             \u2502\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256F\nException: cuda rng state model-parallel-rng is not\
          \ added\n</code></pre>\n"
        raw: "```\r\nEpoch_0:   0%|          | 0/16 [00:04<?, ?it/s]\r\n\u256D\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256E\r\n\u2502 /export/App/training_platform/PinoModel/applications/VisualGLM/visual_chatgl\
          \ \u2502\r\n\u2502 m_instructing_mergeclose_v1.py:229 in <module>      \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   226\
          \ \u2502   parser.add_argument('--lr', type=float, default=5e-6)       \
          \       \u2502\r\n\u2502   227 \u2502   parser.add_argument('--accimulation_steps',\
          \ type=int, default=4)   \u2502\r\n\u2502   228 \u2502   args = parser.parse_args()\
          \                                         \u2502\r\n\u2502 \u2771 229 \u2502\
          \   train(args)                                                        \u2502\
          \r\n\u2502   230                                                       \
          \                 \u2502\r\n\u2502                                     \
          \                                         \u2502\r\n\u2502 /export/App/training_platform/PinoModel/applications/VisualGLM/visual_chatgl\
          \ \u2502\r\n\u2502 m_instructing_mergeclose_v1.py:203 in train         \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   200\
          \ \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    model_save_path='/media/cfs/zhanglezhong/LLMS\
          \ \u2502\r\n\u2502   201 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \    tensorboard_writer=tensorboard_writer)        \u2502\r\n\u2502   202\
          \ \u2502                                                               \
          \       \u2502\r\n\u2502 \u2771 203 \u2502   trainer.fit(logger=logger,\
          \ log_interval=args.log_interval)         \u2502\r\n\u2502   204       \
          \                                                                 \u2502\
          \r\n\u2502   205 #     # save model checkpoint after fitting on only rank0\
          \              \u2502\r\n\u2502   206 #     trainer.save_model(path=args.save_path,\
          \ only_rank0=True, tokeniz \u2502\r\n\u2502                            \
          \                                                  \u2502\r\n\u2502 /export/App/training_platform/PinoModel/applications/VisualGLM/coati/trainer\
          \ \u2502\r\n\u2502 /visual_sft_glm.py:134 in fit                       \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   131\
          \ \u2502   \u2502   \u2502   \u2502   labels = batch[\"labels\"].to(torch.cuda.current_device(\
          \ \u2502\r\n\u2502   132 \u2502   \u2502   \u2502   \u2502   image = batch[\"\
          img\"].to(torch.cuda.current_device())   \u2502\r\n\u2502   133 \u2502 \
          \  \u2502   \u2502   \u2502   pre_image = batch[\"pre_image\"]         \
          \                \u2502\r\n\u2502 \u2771 134 \u2502   \u2502   \u2502  \
          \ \u2502   outputs = self.model(input_ids=prompt_ids, images=imag \u2502\
          \r\n\u2502   135 \u2502   \u2502   \u2502   \u2502                     \
          \                                     \u2502\r\n\u2502   136 \u2502   \u2502\
          \   \u2502   \u2502   loss = outputs.loss                              \
          \      \u2502\r\n\u2502   137 #                 if loss >= 2.5 and is_rank_0()\
          \ :                     \u2502\r\n\u2502                               \
          \                                               \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
          \ \u2502\r\n\u2502 1110 in _call_impl                                  \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   1107\
          \ \u2502   \u2502   # this function, and just call forward.            \
          \           \u2502\r\n\u2502   1108 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._forward_hooks or self._ \u2502\r\n\u2502   1109 \u2502   \u2502\
          \   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks\
          \ \u2502\r\n\u2502 \u2771 1110 \u2502   \u2502   \u2502   return forward_call(*input,\
          \ **kwargs)                     \u2502\r\n\u2502   1111 \u2502   \u2502\
          \   # Do not call functions when jit is used                      \u2502\
          \r\n\u2502   1112 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []         \u2502\r\n\u2502   1113 \u2502   \u2502   if self._backward_hooks\
          \ or _global_backward_hooks:            \u2502\r\n\u2502               \
          \                                                               \u2502\r\
          \n\u2502 /root/.cache/huggingface/modules/transformers_modules/visualglm/modeling_cha\
          \ \u2502\r\n\u2502 tglm.py:1462 in forward                             \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   1459\
          \ \u2502   \u2502   \u2502   return_dict: Optional[bool] = None,       \
          \                \u2502\r\n\u2502   1460 \u2502   ):                   \
          \                                             \u2502\r\n\u2502   1461 \u2502\
          \   \u2502   if inputs_embeds is None and past_key_values is None and imag\
          \ \u2502\r\n\u2502 \u2771 1462 \u2502   \u2502   \u2502   image_embeds =\
          \ self.image_encoder(images)                 \u2502\r\n\u2502   1463 \u2502\
          \   \u2502   \u2502   pre_id, pads, post_id = torch.tensor_split(input_ids,\
          \     \u2502\r\n\u2502   1464 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \      [pre_image_len \u2502\r\n\u2502   1465 \u2502   \u2502   \u2502 \
          \  \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502      dim=1)  # imag \u2502\r\n\u2502               \
          \                                                               \u2502\r\
          \n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
          \ \u2502\r\n\u2502 1110 in _call_impl                                  \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   1107\
          \ \u2502   \u2502   # this function, and just call forward.            \
          \           \u2502\r\n\u2502   1108 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._forward_hooks or self._ \u2502\r\n\u2502   1109 \u2502   \u2502\
          \   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks\
          \ \u2502\r\n\u2502 \u2771 1110 \u2502   \u2502   \u2502   return forward_call(*input,\
          \ **kwargs)                     \u2502\r\n\u2502   1111 \u2502   \u2502\
          \   # Do not call functions when jit is used                      \u2502\
          \r\n\u2502   1112 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []         \u2502\r\n\u2502   1113 \u2502   \u2502   if self._backward_hooks\
          \ or _global_backward_hooks:            \u2502\r\n\u2502               \
          \                                                               \u2502\r\
          \n\u2502 /root/.cache/huggingface/modules/transformers_modules/visualglm/visual.py:69\
          \ \u2502\r\n\u2502 in forward                                          \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502    66\
          \ \u2502   \u2502   \u2502   self.qformer.parameters().__next__().dtype)\
          \                \u2502\r\n\u2502    67 \u2502                         \
          \                                             \u2502\r\n\u2502    68 \u2502\
          \   def forward(self, image, **kwargs):                                \u2502\
          \r\n\u2502 \u2771  69 \u2502   \u2502   enc = self.vit(image)[0]       \
          \                                \u2502\r\n\u2502    70 \u2502   \u2502\
          \   out = self.qformer(enc)[0]                                     \u2502\
          \r\n\u2502    71 \u2502   \u2502   return self.glm_proj(out)           \
          \                           \u2502\r\n\u2502    72                     \
          \                                                   \u2502\r\n\u2502   \
          \                                                                      \
          \     \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
          \ \u2502\r\n\u2502 1110 in _call_impl                                  \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   1107\
          \ \u2502   \u2502   # this function, and just call forward.            \
          \           \u2502\r\n\u2502   1108 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._forward_hooks or self._ \u2502\r\n\u2502   1109 \u2502   \u2502\
          \   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks\
          \ \u2502\r\n\u2502 \u2771 1110 \u2502   \u2502   \u2502   return forward_call(*input,\
          \ **kwargs)                     \u2502\r\n\u2502   1111 \u2502   \u2502\
          \   # Do not call functions when jit is used                      \u2502\
          \r\n\u2502   1112 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []         \u2502\r\n\u2502   1113 \u2502   \u2502   if self._backward_hooks\
          \ or _global_backward_hooks:            \u2502\r\n\u2502               \
          \                                                               \u2502\r\
          \n\u2502 /root/.cache/huggingface/modules/transformers_modules/visualglm/visual.py:28\
          \ \u2502\r\n\u2502 in forward                                          \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502    25\
          \ \u2502   \u2502   batch_size = image.size(0)                         \
          \            \u2502\r\n\u2502    26 \u2502   \u2502   input_ids = torch.zeros(batch_size,\
          \ 1, dtype=torch.long, devic \u2502\r\n\u2502    27 \u2502   \u2502   attention_mask\
          \ = torch.tensor([[1.]], dtype=image.dtype, devic \u2502\r\n\u2502 \u2771\
          \  28 \u2502   \u2502   return super().forward(input_ids=input_ids, position_ids=None,\
          \ \u2502\r\n\u2502    29                                               \
          \                         \u2502\r\n\u2502    30                       \
          \                                                 \u2502\r\n\u2502    31\
          \ class QFormer(BaseModel):                                            \
          \  \u2502\r\n\u2502                                                    \
          \                          \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/model/base_model.py:144\
          \ \u2502\r\n\u2502 in forward                                          \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   141\
          \ \u2502   \u2502   # Attention! the transformer might be shared by multiple\
          \ model \u2502\r\n\u2502   142 \u2502   \u2502   self.transformer.hooks.clear()\
          \                                 \u2502\r\n\u2502   143 \u2502   \u2502\
          \   self.transformer.hooks.update(self.hooks)                      \u2502\
          \r\n\u2502 \u2771 144 \u2502   \u2502   return self.transformer(*args, **kwargs)\
          \                       \u2502\r\n\u2502   145 \u2502                  \
          \                                                    \u2502\r\n\u2502  \
          \ 146 \u2502   def collect_hooks_(self):                               \
          \           \u2502\r\n\u2502   147 \u2502   \u2502   names = list(HOOKS_DEFAULT.keys())\
          \                             \u2502\r\n\u2502                         \
          \                                                     \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
          \ \u2502\r\n\u2502 1110 in _call_impl                                  \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   1107\
          \ \u2502   \u2502   # this function, and just call forward.            \
          \           \u2502\r\n\u2502   1108 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._forward_hooks or self._ \u2502\r\n\u2502   1109 \u2502   \u2502\
          \   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks\
          \ \u2502\r\n\u2502 \u2771 1110 \u2502   \u2502   \u2502   return forward_call(*input,\
          \ **kwargs)                     \u2502\r\n\u2502   1111 \u2502   \u2502\
          \   # Do not call functions when jit is used                      \u2502\
          \r\n\u2502   1112 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []         \u2502\r\n\u2502   1113 \u2502   \u2502   if self._backward_hooks\
          \ or _global_backward_hooks:            \u2502\r\n\u2502               \
          \                                                               \u2502\r\
          \n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/model/transformer.py:56\
          \ \u2502\r\n\u2502 9 in forward                                        \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   566\
          \ \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   output_this_layer=output_this_layer_obj,\
          \ outpu \u2502\r\n\u2502   567 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   )                                                  \u2502\r\n\u2502\
          \   568 \u2502   \u2502   \u2502   \u2502   else:                      \
          \                            \u2502\r\n\u2502 \u2771 569 \u2502   \u2502\
          \   \u2502   \u2502   \u2502   layer_ret = layer(*args, layer_id=torch.tensor(i),\
          \ \u2502\r\n\u2502   570 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \   output_this_layer=output_this_layer_obj, outpu \u2502\r\n\u2502   571\
          \ \u2502   \u2502   \u2502   \u2502   if isinstance(layer_ret, tuple): \
          \                      \u2502\r\n\u2502   572 \u2502   \u2502   \u2502 \
          \  \u2502   \u2502   layer_ret = layer_ret[0] # for legacy API         \
          \ \u2502\r\n\u2502                                                     \
          \                         \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
          \ \u2502\r\n\u2502 1110 in _call_impl                                  \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   1107\
          \ \u2502   \u2502   # this function, and just call forward.            \
          \           \u2502\r\n\u2502   1108 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._forward_hooks or self._ \u2502\r\n\u2502   1109 \u2502   \u2502\
          \   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks\
          \ \u2502\r\n\u2502 \u2771 1110 \u2502   \u2502   \u2502   return forward_call(*input,\
          \ **kwargs)                     \u2502\r\n\u2502   1111 \u2502   \u2502\
          \   # Do not call functions when jit is used                      \u2502\
          \r\n\u2502   1112 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []         \u2502\r\n\u2502   1113 \u2502   \u2502   if self._backward_hooks\
          \ or _global_backward_hooks:            \u2502\r\n\u2502               \
          \                                                               \u2502\r\
          \n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/model/transformer.py:33\
          \ \u2502\r\n\u2502 0 in forward                                        \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   327\
          \ \u2502   \u2502   )                                                  \
          \            \u2502\r\n\u2502   328 \u2502                             \
          \                                         \u2502\r\n\u2502   329 \u2502\
          \   def forward(self, hidden_states, mask, *args, **kw_args):          \u2502\
          \r\n\u2502 \u2771 330 \u2502   \u2502   return HOOKS_DEFAULT['layer_forward'](self,\
          \ hidden_states, mas \u2502\r\n\u2502   331                            \
          \                                            \u2502\r\n\u2502   332    \
          \                                                                    \u2502\
          \r\n\u2502   333 class BaseTransformer(torch.nn.Module):               \
          \                 \u2502\r\n\u2502                                     \
          \                                         \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/transformer_defaults.py\
          \ \u2502\r\n\u2502 :127 in layer_forward_default                       \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   124\
          \ \u2502   # Layer norm at the begining of the transformer layer.      \
          \       \u2502\r\n\u2502   125 \u2502   attention_input = self.input_layernorm(hidden_states)\
          \              \u2502\r\n\u2502   126 \u2502   # Self attention.       \
          \                                           \u2502\r\n\u2502 \u2771 127\
          \ \u2502   attention_output = self.attention(attention_input, mask, **kw_args\
          \ \u2502\r\n\u2502   128 \u2502                                        \
          \                              \u2502\r\n\u2502   129 \u2502   # Third LayerNorm\
          \                                                  \u2502\r\n\u2502   130\
          \ \u2502   if self.layernorm_order == 'sandwich':                      \
          \       \u2502\r\n\u2502                                               \
          \                               \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
          \ \u2502\r\n\u2502 1110 in _call_impl                                  \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   1107\
          \ \u2502   \u2502   # this function, and just call forward.            \
          \           \u2502\r\n\u2502   1108 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._forward_hooks or self._ \u2502\r\n\u2502   1109 \u2502   \u2502\
          \   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks\
          \ \u2502\r\n\u2502 \u2771 1110 \u2502   \u2502   \u2502   return forward_call(*input,\
          \ **kwargs)                     \u2502\r\n\u2502   1111 \u2502   \u2502\
          \   # Do not call functions when jit is used                      \u2502\
          \r\n\u2502   1112 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []         \u2502\r\n\u2502   1113 \u2502   \u2502   if self._backward_hooks\
          \ or _global_backward_hooks:            \u2502\r\n\u2502               \
          \                                                               \u2502\r\
          \n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/model/transformer.py:10\
          \ \u2502\r\n\u2502 3 in forward                                        \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   100\
          \ \u2502   \u2502   if 'attention_forward' in self.hooks:              \
          \            \u2502\r\n\u2502   101 \u2502   \u2502   \u2502   return self.hooks['attention_forward'](hidden_states,\
          \ mask \u2502\r\n\u2502   102 \u2502   \u2502   else:                  \
          \                                        \u2502\r\n\u2502 \u2771 103 \u2502\
          \   \u2502   \u2502   return HOOKS_DEFAULT['attention_forward'](self, hidden_sta\
          \ \u2502\r\n\u2502   104                                               \
          \                         \u2502\r\n\u2502   105                       \
          \                                                 \u2502\r\n\u2502   106\
          \ class CrossAttention(torch.nn.Module):                               \
          \  \u2502\r\n\u2502                                                    \
          \                          \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/transformer_defaults.py\
          \ \u2502\r\n\u2502 :63 in attention_forward_default                    \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502    60\
          \ \u2502   key_layer = self._transpose_for_scores(mixed_key_layer)     \
          \       \u2502\r\n\u2502    61 \u2502   value_layer = self._transpose_for_scores(mixed_value_layer)\
          \        \u2502\r\n\u2502    62 \u2502                                 \
          \                                     \u2502\r\n\u2502 \u2771  63 \u2502\
          \   context_layer = attention_fn(query_layer, key_layer, value_layer,  \u2502\
          \r\n\u2502    64 \u2502                                                \
          \                      \u2502\r\n\u2502    65 \u2502   context_layer = context_layer.permute(0,\
          \ 2, 1, 3).contiguous()     \u2502\r\n\u2502    66 \u2502   new_context_layer_shape\
          \ = context_layer.size()[:-2] + (self.hidden \u2502\r\n\u2502          \
          \                                                                    \u2502\
          \r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/transformer_defaults.py\
          \ \u2502\r\n\u2502 :38 in standard_attention                           \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502    35\
          \ \u2502                                                               \
          \       \u2502\r\n\u2502    36 \u2502   if attention_dropout is not None:\
          \                                  \u2502\r\n\u2502    37 \u2502   \u2502\
          \   if mpu.get_cuda_rng_tracker is not None:                       \u2502\
          \r\n\u2502 \u2771  38 \u2502   \u2502   \u2502   with mpu.get_cuda_rng_tracker().fork():\
          \                    \u2502\r\n\u2502    39 \u2502   \u2502   \u2502   \u2502\
          \   attention_probs = attention_dropout(attention_probs)   \u2502\r\n\u2502\
          \    40 \u2502   \u2502   else:                                        \
          \                  \u2502\r\n\u2502    41 \u2502   \u2502   \u2502   attention_probs\
          \ = attention_dropout(attention_probs)       \u2502\r\n\u2502          \
          \                                                                    \u2502\
          \r\n\u2502 /usr/local/anaconda3/lib/python3.8/contextlib.py:113 in __enter__\
          \            \u2502\r\n\u2502                                          \
          \                                    \u2502\r\n\u2502   110 \u2502   \u2502\
          \   # they are only needed for recreation, which is not possible a \u2502\
          \r\n\u2502   111 \u2502   \u2502   del self.args, self.kwds, self.func \
          \                           \u2502\r\n\u2502   112 \u2502   \u2502   try:\
          \                                                           \u2502\r\n\u2502\
          \ \u2771 113 \u2502   \u2502   \u2502   return next(self.gen)          \
          \                            \u2502\r\n\u2502   114 \u2502   \u2502   except\
          \ StopIteration:                                          \u2502\r\n\u2502\
          \   115 \u2502   \u2502   \u2502   raise RuntimeError(\"generator didn't\
          \ yield\") from None     \u2502\r\n\u2502   116                        \
          \                                                \u2502\r\n\u2502      \
          \                                                                      \
          \  \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/deepspeed/runtime/activatio\
          \ \u2502\r\n\u2502 n_checkpointing/checkpointing.py:174 in fork        \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502   171\
          \ \u2502   \u2502   the original state.\"\"\"                          \
          \               \u2502\r\n\u2502   172 \u2502   \u2502   # Check if we have\
          \ added the state                             \u2502\r\n\u2502   173 \u2502\
          \   \u2502   if name not in self.states_:                              \
          \     \u2502\r\n\u2502 \u2771 174 \u2502   \u2502   \u2502   raise Exception('cuda\
          \ rng state {} is not added'.format(na \u2502\r\n\u2502   175 \u2502   \u2502\
          \   # Store current rng state.                                     \u2502\
          \r\n\u2502   176 \u2502   \u2502   orig_cuda_rng_state = get_accelerator().get_rng_state()\
          \        \u2502\r\n\u2502   177 \u2502   \u2502   # Set rng state to the\
          \ desired one                             \u2502\r\n\u2570\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u256F\r\nException: cuda rng state model-parallel-rng\
          \ is not added\r\n```"
        updatedAt: '2023-06-23T01:12:33.842Z'
      numEdits: 0
      reactions: []
    id: 6494f181c01e0e65a8ef1b11
    type: comment
  author: scall
  content: "```\r\nEpoch_0:   0%|          | 0/16 [00:04<?, ?it/s]\r\n\u256D\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502 /export/App/training_platform/PinoModel/applications/VisualGLM/visual_chatgl\
    \ \u2502\r\n\u2502 m_instructing_mergeclose_v1.py:229 in <module>            \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   226 \u2502   parser.add_argument('--lr',\
    \ type=float, default=5e-6)              \u2502\r\n\u2502   227 \u2502   parser.add_argument('--accimulation_steps',\
    \ type=int, default=4)   \u2502\r\n\u2502   228 \u2502   args = parser.parse_args()\
    \                                         \u2502\r\n\u2502 \u2771 229 \u2502 \
    \  train(args)                                                        \u2502\r\
    \n\u2502   230                                                               \
    \         \u2502\r\n\u2502                                                   \
    \                           \u2502\r\n\u2502 /export/App/training_platform/PinoModel/applications/VisualGLM/visual_chatgl\
    \ \u2502\r\n\u2502 m_instructing_mergeclose_v1.py:203 in train               \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   200 \u2502   \u2502 \
    \  \u2502   \u2502   \u2502   \u2502    model_save_path='/media/cfs/zhanglezhong/LLMS\
    \ \u2502\r\n\u2502   201 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502 \
    \   tensorboard_writer=tensorboard_writer)        \u2502\r\n\u2502   202 \u2502\
    \                                                                      \u2502\r\
    \n\u2502 \u2771 203 \u2502   trainer.fit(logger=logger, log_interval=args.log_interval)\
    \         \u2502\r\n\u2502   204                                             \
    \                           \u2502\r\n\u2502   205 #     # save model checkpoint\
    \ after fitting on only rank0              \u2502\r\n\u2502   206 #     trainer.save_model(path=args.save_path,\
    \ only_rank0=True, tokeniz \u2502\r\n\u2502                                  \
    \                                            \u2502\r\n\u2502 /export/App/training_platform/PinoModel/applications/VisualGLM/coati/trainer\
    \ \u2502\r\n\u2502 /visual_sft_glm.py:134 in fit                             \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   131 \u2502   \u2502 \
    \  \u2502   \u2502   labels = batch[\"labels\"].to(torch.cuda.current_device(\
    \ \u2502\r\n\u2502   132 \u2502   \u2502   \u2502   \u2502   image = batch[\"\
    img\"].to(torch.cuda.current_device())   \u2502\r\n\u2502   133 \u2502   \u2502\
    \   \u2502   \u2502   pre_image = batch[\"pre_image\"]                       \
    \  \u2502\r\n\u2502 \u2771 134 \u2502   \u2502   \u2502   \u2502   outputs = self.model(input_ids=prompt_ids,\
    \ images=imag \u2502\r\n\u2502   135 \u2502   \u2502   \u2502   \u2502       \
    \                                                   \u2502\r\n\u2502   136 \u2502\
    \   \u2502   \u2502   \u2502   loss = outputs.loss                           \
    \         \u2502\r\n\u2502   137 #                 if loss >= 2.5 and is_rank_0()\
    \ :                     \u2502\r\n\u2502                                     \
    \                                         \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
    \ \u2502\r\n\u2502 1110 in _call_impl                                        \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   1107 \u2502   \u2502\
    \   # this function, and just call forward.                       \u2502\r\n\u2502\
    \   1108 \u2502   \u2502   if not (self._backward_hooks or self._forward_hooks\
    \ or self._ \u2502\r\n\u2502   1109 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks \u2502\r\n\u2502 \u2771 1110 \u2502   \u2502  \
    \ \u2502   return forward_call(*input, **kwargs)                     \u2502\r\n\
    \u2502   1111 \u2502   \u2502   # Do not call functions when jit is used     \
    \                 \u2502\r\n\u2502   1112 \u2502   \u2502   full_backward_hooks,\
    \ non_full_backward_hooks = [], []         \u2502\r\n\u2502   1113 \u2502   \u2502\
    \   if self._backward_hooks or _global_backward_hooks:            \u2502\r\n\u2502\
    \                                                                            \
    \  \u2502\r\n\u2502 /root/.cache/huggingface/modules/transformers_modules/visualglm/modeling_cha\
    \ \u2502\r\n\u2502 tglm.py:1462 in forward                                   \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   1459 \u2502   \u2502\
    \   \u2502   return_dict: Optional[bool] = None,                       \u2502\r\
    \n\u2502   1460 \u2502   ):                                                  \
    \              \u2502\r\n\u2502   1461 \u2502   \u2502   if inputs_embeds is None\
    \ and past_key_values is None and imag \u2502\r\n\u2502 \u2771 1462 \u2502   \u2502\
    \   \u2502   image_embeds = self.image_encoder(images)                 \u2502\r\
    \n\u2502   1463 \u2502   \u2502   \u2502   pre_id, pads, post_id = torch.tensor_split(input_ids,\
    \     \u2502\r\n\u2502   1464 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
    \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      [pre_image_len\
    \ \u2502\r\n\u2502   1465 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
    \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      dim=1) \
    \ # imag \u2502\r\n\u2502                                                    \
    \                          \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
    \ \u2502\r\n\u2502 1110 in _call_impl                                        \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   1107 \u2502   \u2502\
    \   # this function, and just call forward.                       \u2502\r\n\u2502\
    \   1108 \u2502   \u2502   if not (self._backward_hooks or self._forward_hooks\
    \ or self._ \u2502\r\n\u2502   1109 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks \u2502\r\n\u2502 \u2771 1110 \u2502   \u2502  \
    \ \u2502   return forward_call(*input, **kwargs)                     \u2502\r\n\
    \u2502   1111 \u2502   \u2502   # Do not call functions when jit is used     \
    \                 \u2502\r\n\u2502   1112 \u2502   \u2502   full_backward_hooks,\
    \ non_full_backward_hooks = [], []         \u2502\r\n\u2502   1113 \u2502   \u2502\
    \   if self._backward_hooks or _global_backward_hooks:            \u2502\r\n\u2502\
    \                                                                            \
    \  \u2502\r\n\u2502 /root/.cache/huggingface/modules/transformers_modules/visualglm/visual.py:69\
    \ \u2502\r\n\u2502 in forward                                                \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502    66 \u2502   \u2502 \
    \  \u2502   self.qformer.parameters().__next__().dtype)                \u2502\r\
    \n\u2502    67 \u2502                                                        \
    \              \u2502\r\n\u2502    68 \u2502   def forward(self, image, **kwargs):\
    \                                \u2502\r\n\u2502 \u2771  69 \u2502   \u2502 \
    \  enc = self.vit(image)[0]                                       \u2502\r\n\u2502\
    \    70 \u2502   \u2502   out = self.qformer(enc)[0]                         \
    \            \u2502\r\n\u2502    71 \u2502   \u2502   return self.glm_proj(out)\
    \                                      \u2502\r\n\u2502    72                \
    \                                                        \u2502\r\n\u2502    \
    \                                                                          \u2502\
    \r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
    \ \u2502\r\n\u2502 1110 in _call_impl                                        \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   1107 \u2502   \u2502\
    \   # this function, and just call forward.                       \u2502\r\n\u2502\
    \   1108 \u2502   \u2502   if not (self._backward_hooks or self._forward_hooks\
    \ or self._ \u2502\r\n\u2502   1109 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks \u2502\r\n\u2502 \u2771 1110 \u2502   \u2502  \
    \ \u2502   return forward_call(*input, **kwargs)                     \u2502\r\n\
    \u2502   1111 \u2502   \u2502   # Do not call functions when jit is used     \
    \                 \u2502\r\n\u2502   1112 \u2502   \u2502   full_backward_hooks,\
    \ non_full_backward_hooks = [], []         \u2502\r\n\u2502   1113 \u2502   \u2502\
    \   if self._backward_hooks or _global_backward_hooks:            \u2502\r\n\u2502\
    \                                                                            \
    \  \u2502\r\n\u2502 /root/.cache/huggingface/modules/transformers_modules/visualglm/visual.py:28\
    \ \u2502\r\n\u2502 in forward                                                \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502    25 \u2502   \u2502 \
    \  batch_size = image.size(0)                                     \u2502\r\n\u2502\
    \    26 \u2502   \u2502   input_ids = torch.zeros(batch_size, 1, dtype=torch.long,\
    \ devic \u2502\r\n\u2502    27 \u2502   \u2502   attention_mask = torch.tensor([[1.]],\
    \ dtype=image.dtype, devic \u2502\r\n\u2502 \u2771  28 \u2502   \u2502   return\
    \ super().forward(input_ids=input_ids, position_ids=None, \u2502\r\n\u2502   \
    \ 29                                                                        \u2502\
    \r\n\u2502    30                                                             \
    \           \u2502\r\n\u2502    31 class QFormer(BaseModel):                 \
    \                             \u2502\r\n\u2502                               \
    \                                               \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/model/base_model.py:144\
    \ \u2502\r\n\u2502 in forward                                                \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   141 \u2502   \u2502 \
    \  # Attention! the transformer might be shared by multiple model \u2502\r\n\u2502\
    \   142 \u2502   \u2502   self.transformer.hooks.clear()                     \
    \            \u2502\r\n\u2502   143 \u2502   \u2502   self.transformer.hooks.update(self.hooks)\
    \                      \u2502\r\n\u2502 \u2771 144 \u2502   \u2502   return self.transformer(*args,\
    \ **kwargs)                       \u2502\r\n\u2502   145 \u2502              \
    \                                                        \u2502\r\n\u2502   146\
    \ \u2502   def collect_hooks_(self):                                         \
    \ \u2502\r\n\u2502   147 \u2502   \u2502   names = list(HOOKS_DEFAULT.keys())\
    \                             \u2502\r\n\u2502                               \
    \                                               \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
    \ \u2502\r\n\u2502 1110 in _call_impl                                        \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   1107 \u2502   \u2502\
    \   # this function, and just call forward.                       \u2502\r\n\u2502\
    \   1108 \u2502   \u2502   if not (self._backward_hooks or self._forward_hooks\
    \ or self._ \u2502\r\n\u2502   1109 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks \u2502\r\n\u2502 \u2771 1110 \u2502   \u2502  \
    \ \u2502   return forward_call(*input, **kwargs)                     \u2502\r\n\
    \u2502   1111 \u2502   \u2502   # Do not call functions when jit is used     \
    \                 \u2502\r\n\u2502   1112 \u2502   \u2502   full_backward_hooks,\
    \ non_full_backward_hooks = [], []         \u2502\r\n\u2502   1113 \u2502   \u2502\
    \   if self._backward_hooks or _global_backward_hooks:            \u2502\r\n\u2502\
    \                                                                            \
    \  \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/model/transformer.py:56\
    \ \u2502\r\n\u2502 9 in forward                                              \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   566 \u2502   \u2502 \
    \  \u2502   \u2502   \u2502   \u2502   output_this_layer=output_this_layer_obj,\
    \ outpu \u2502\r\n\u2502   567 \u2502   \u2502   \u2502   \u2502   \u2502   )\
    \                                                  \u2502\r\n\u2502   568 \u2502\
    \   \u2502   \u2502   \u2502   else:                                         \
    \         \u2502\r\n\u2502 \u2771 569 \u2502   \u2502   \u2502   \u2502   \u2502\
    \   layer_ret = layer(*args, layer_id=torch.tensor(i), \u2502\r\n\u2502   570\
    \ \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   output_this_layer=output_this_layer_obj,\
    \ outpu \u2502\r\n\u2502   571 \u2502   \u2502   \u2502   \u2502   if isinstance(layer_ret,\
    \ tuple):                       \u2502\r\n\u2502   572 \u2502   \u2502   \u2502\
    \   \u2502   \u2502   layer_ret = layer_ret[0] # for legacy API          \u2502\
    \r\n\u2502                                                                   \
    \           \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
    \ \u2502\r\n\u2502 1110 in _call_impl                                        \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   1107 \u2502   \u2502\
    \   # this function, and just call forward.                       \u2502\r\n\u2502\
    \   1108 \u2502   \u2502   if not (self._backward_hooks or self._forward_hooks\
    \ or self._ \u2502\r\n\u2502   1109 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks \u2502\r\n\u2502 \u2771 1110 \u2502   \u2502  \
    \ \u2502   return forward_call(*input, **kwargs)                     \u2502\r\n\
    \u2502   1111 \u2502   \u2502   # Do not call functions when jit is used     \
    \                 \u2502\r\n\u2502   1112 \u2502   \u2502   full_backward_hooks,\
    \ non_full_backward_hooks = [], []         \u2502\r\n\u2502   1113 \u2502   \u2502\
    \   if self._backward_hooks or _global_backward_hooks:            \u2502\r\n\u2502\
    \                                                                            \
    \  \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/model/transformer.py:33\
    \ \u2502\r\n\u2502 0 in forward                                              \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   327 \u2502   \u2502 \
    \  )                                                              \u2502\r\n\u2502\
    \   328 \u2502                                                               \
    \       \u2502\r\n\u2502   329 \u2502   def forward(self, hidden_states, mask,\
    \ *args, **kw_args):          \u2502\r\n\u2502 \u2771 330 \u2502   \u2502   return\
    \ HOOKS_DEFAULT['layer_forward'](self, hidden_states, mas \u2502\r\n\u2502   331\
    \                                                                        \u2502\
    \r\n\u2502   332                                                             \
    \           \u2502\r\n\u2502   333 class BaseTransformer(torch.nn.Module):   \
    \                             \u2502\r\n\u2502                               \
    \                                               \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/transformer_defaults.py\
    \ \u2502\r\n\u2502 :127 in layer_forward_default                             \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   124 \u2502   # Layer\
    \ norm at the begining of the transformer layer.             \u2502\r\n\u2502\
    \   125 \u2502   attention_input = self.input_layernorm(hidden_states)       \
    \       \u2502\r\n\u2502   126 \u2502   # Self attention.                    \
    \                              \u2502\r\n\u2502 \u2771 127 \u2502   attention_output\
    \ = self.attention(attention_input, mask, **kw_args \u2502\r\n\u2502   128 \u2502\
    \                                                                      \u2502\r\
    \n\u2502   129 \u2502   # Third LayerNorm                                    \
    \              \u2502\r\n\u2502   130 \u2502   if self.layernorm_order == 'sandwich':\
    \                             \u2502\r\n\u2502                               \
    \                                               \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:\
    \ \u2502\r\n\u2502 1110 in _call_impl                                        \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   1107 \u2502   \u2502\
    \   # this function, and just call forward.                       \u2502\r\n\u2502\
    \   1108 \u2502   \u2502   if not (self._backward_hooks or self._forward_hooks\
    \ or self._ \u2502\r\n\u2502   1109 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks \u2502\r\n\u2502 \u2771 1110 \u2502   \u2502  \
    \ \u2502   return forward_call(*input, **kwargs)                     \u2502\r\n\
    \u2502   1111 \u2502   \u2502   # Do not call functions when jit is used     \
    \                 \u2502\r\n\u2502   1112 \u2502   \u2502   full_backward_hooks,\
    \ non_full_backward_hooks = [], []         \u2502\r\n\u2502   1113 \u2502   \u2502\
    \   if self._backward_hooks or _global_backward_hooks:            \u2502\r\n\u2502\
    \                                                                            \
    \  \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/model/transformer.py:10\
    \ \u2502\r\n\u2502 3 in forward                                              \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   100 \u2502   \u2502 \
    \  if 'attention_forward' in self.hooks:                          \u2502\r\n\u2502\
    \   101 \u2502   \u2502   \u2502   return self.hooks['attention_forward'](hidden_states,\
    \ mask \u2502\r\n\u2502   102 \u2502   \u2502   else:                        \
    \                                  \u2502\r\n\u2502 \u2771 103 \u2502   \u2502\
    \   \u2502   return HOOKS_DEFAULT['attention_forward'](self, hidden_sta \u2502\
    \r\n\u2502   104                                                             \
    \           \u2502\r\n\u2502   105                                           \
    \                             \u2502\r\n\u2502   106 class CrossAttention(torch.nn.Module):\
    \                                 \u2502\r\n\u2502                           \
    \                                                   \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/sat/transformer_defaults.py\
    \ \u2502\r\n\u2502 :63 in attention_forward_default                          \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502    60 \u2502   key_layer\
    \ = self._transpose_for_scores(mixed_key_layer)            \u2502\r\n\u2502  \
    \  61 \u2502   value_layer = self._transpose_for_scores(mixed_value_layer)   \
    \     \u2502\r\n\u2502    62 \u2502                                          \
    \                            \u2502\r\n\u2502 \u2771  63 \u2502   context_layer\
    \ = attention_fn(query_layer, key_layer, value_layer,  \u2502\r\n\u2502    64\
    \ \u2502                                                                     \
    \ \u2502\r\n\u2502    65 \u2502   context_layer = context_layer.permute(0, 2,\
    \ 1, 3).contiguous()     \u2502\r\n\u2502    66 \u2502   new_context_layer_shape\
    \ = context_layer.size()[:-2] + (self.hidden \u2502\r\n\u2502                \
    \                                                              \u2502\r\n\u2502\
    \ /usr/local/anaconda3/lib/python3.8/site-packages/sat/transformer_defaults.py\
    \ \u2502\r\n\u2502 :38 in standard_attention                                 \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502    35 \u2502          \
    \                                                            \u2502\r\n\u2502\
    \    36 \u2502   if attention_dropout is not None:                           \
    \       \u2502\r\n\u2502    37 \u2502   \u2502   if mpu.get_cuda_rng_tracker is\
    \ not None:                       \u2502\r\n\u2502 \u2771  38 \u2502   \u2502\
    \   \u2502   with mpu.get_cuda_rng_tracker().fork():                    \u2502\
    \r\n\u2502    39 \u2502   \u2502   \u2502   \u2502   attention_probs = attention_dropout(attention_probs)\
    \   \u2502\r\n\u2502    40 \u2502   \u2502   else:                           \
    \                               \u2502\r\n\u2502    41 \u2502   \u2502   \u2502\
    \   attention_probs = attention_dropout(attention_probs)       \u2502\r\n\u2502\
    \                                                                            \
    \  \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/contextlib.py:113 in __enter__\
    \            \u2502\r\n\u2502                                                \
    \                              \u2502\r\n\u2502   110 \u2502   \u2502   # they\
    \ are only needed for recreation, which is not possible a \u2502\r\n\u2502   111\
    \ \u2502   \u2502   del self.args, self.kwds, self.func                      \
    \      \u2502\r\n\u2502   112 \u2502   \u2502   try:                         \
    \                                  \u2502\r\n\u2502 \u2771 113 \u2502   \u2502\
    \   \u2502   return next(self.gen)                                      \u2502\
    \r\n\u2502   114 \u2502   \u2502   except StopIteration:                     \
    \                     \u2502\r\n\u2502   115 \u2502   \u2502   \u2502   raise\
    \ RuntimeError(\"generator didn't yield\") from None     \u2502\r\n\u2502   116\
    \                                                                        \u2502\
    \r\n\u2502                                                                   \
    \           \u2502\r\n\u2502 /usr/local/anaconda3/lib/python3.8/site-packages/deepspeed/runtime/activatio\
    \ \u2502\r\n\u2502 n_checkpointing/checkpointing.py:174 in fork              \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502   171 \u2502   \u2502 \
    \  the original state.\"\"\"                                         \u2502\r\n\
    \u2502   172 \u2502   \u2502   # Check if we have added the state            \
    \                 \u2502\r\n\u2502   173 \u2502   \u2502   if name not in self.states_:\
    \                                   \u2502\r\n\u2502 \u2771 174 \u2502   \u2502\
    \   \u2502   raise Exception('cuda rng state {} is not added'.format(na \u2502\
    \r\n\u2502   175 \u2502   \u2502   # Store current rng state.                \
    \                     \u2502\r\n\u2502   176 \u2502   \u2502   orig_cuda_rng_state\
    \ = get_accelerator().get_rng_state()        \u2502\r\n\u2502   177 \u2502   \u2502\
    \   # Set rng state to the desired one                             \u2502\r\n\u2570\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u256F\r\nException: cuda rng state model-parallel-rng is not added\r\n```"
  created_at: 2023-06-23 00:12:33+00:00
  edited: false
  hidden: false
  id: 6494f181c01e0e65a8ef1b11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/25f1fb77da555423a4a5b2a3ed40c7cc.svg
      fullname: zhanglezhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: scall
      type: user
    createdAt: '2023-06-23T01:20:38.000Z'
    data:
      from: "huggingface\u7248\u672C\u5728\u8BAD\u7EC3\u65F6\u5019sat\u4E0Edeepspeed\u9002\
        \u914D\u6709\u95EE\u9898\uFF0C\u65B9\u4FBF\u63D0\u9AD8\u4F60\u4EEC\u4F7F\u7528\
        huggingface\u7684\u8BAD\u7EC3demo\u4E48"
      to: "huggingface\u7248\u672C\u5728\u8BAD\u7EC3\u65F6\u5019sat\u4E0Edeepspeed\u9002\
        \u914D\u6709\u95EE\u9898\uFF0C\u65B9\u4FBF\u63D0\u4F9B\u4F60\u4EEC\u4F7F\u7528\
        huggingface\u7684\u8BAD\u7EC3demo\u4E48"
    id: 6494f36632f2c0d7ef27ea81
    type: title-change
  author: scall
  created_at: 2023-06-23 00:20:38+00:00
  id: 6494f36632f2c0d7ef27ea81
  new_title: "huggingface\u7248\u672C\u5728\u8BAD\u7EC3\u65F6\u5019sat\u4E0Edeepspeed\u9002\
    \u914D\u6709\u95EE\u9898\uFF0C\u65B9\u4FBF\u63D0\u4F9B\u4F60\u4EEC\u4F7F\u7528\
    huggingface\u7684\u8BAD\u7EC3demo\u4E48"
  old_title: "huggingface\u7248\u672C\u5728\u8BAD\u7EC3\u65F6\u5019sat\u4E0Edeepspeed\u9002\
    \u914D\u6709\u95EE\u9898\uFF0C\u65B9\u4FBF\u63D0\u9AD8\u4F60\u4EEC\u4F7F\u7528\
    huggingface\u7684\u8BAD\u7EC3demo\u4E48"
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: THUDM/visualglm-6b
repo_type: model
status: open
target_branch: null
title: "huggingface\u7248\u672C\u5728\u8BAD\u7EC3\u65F6\u5019sat\u4E0Edeepspeed\u9002\
  \u914D\u6709\u95EE\u9898\uFF0C\u65B9\u4FBF\u63D0\u4F9B\u4F60\u4EEC\u4F7F\u7528huggingface\u7684\
  \u8BAD\u7EC3demo\u4E48"
