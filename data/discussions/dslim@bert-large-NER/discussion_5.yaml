!!python/object:huggingface_hub.community.DiscussionWithDetails
author: diana-onutu
conflicting_files: null
created_at: 2023-09-11 21:49:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ea68d84b78adb7d6b63a3ffa2ed7210.svg
      fullname: Diana Onutu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: diana-onutu
      type: user
    createdAt: '2023-09-11T22:49:51.000Z'
    data:
      edited: true
      editors:
      - diana-onutu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9804844260215759
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ea68d84b78adb7d6b63a3ffa2ed7210.svg
          fullname: Diana Onutu
          isHf: false
          isPro: false
          name: diana-onutu
          type: user
        html: '<p>How did you deal with the misalignment that appears after tokenization
          between the tokens and the ner tags? If the word "Japan" has as ner tag
          "B-LOC", how does it look like after it is tokenized as follows: "JA", "#PA",
          "#N"? Do you for example re-align the ner tags as "B-LOC", "I-LOC", "I-LOC"?
          I''m trying to reproduce your evaluation results, but most of them are between
          0.5-0.7 (except accuracy). In the calculation of these metrics, do we also
          evaluate the performance on the "O" label?</p>

          '
        raw: 'How did you deal with the misalignment that appears after tokenization
          between the tokens and the ner tags? If the word "Japan" has as ner tag
          "B-LOC", how does it look like after it is tokenized as follows: "JA", "#PA",
          "#N"? Do you for example re-align the ner tags as "B-LOC", "I-LOC", "I-LOC"?
          I''m trying to reproduce your evaluation results, but most of them are between
          0.5-0.7 (except accuracy). In the calculation of these metrics, do we also
          evaluate the performance on the "O" label?'
        updatedAt: '2023-09-12T06:37:14.985Z'
      numEdits: 1
      reactions: []
    id: 64ff998f520f8d7b4de84d5d
    type: comment
  author: diana-onutu
  content: 'How did you deal with the misalignment that appears after tokenization
    between the tokens and the ner tags? If the word "Japan" has as ner tag "B-LOC",
    how does it look like after it is tokenized as follows: "JA", "#PA", "#N"? Do
    you for example re-align the ner tags as "B-LOC", "I-LOC", "I-LOC"? I''m trying
    to reproduce your evaluation results, but most of them are between 0.5-0.7 (except
    accuracy). In the calculation of these metrics, do we also evaluate the performance
    on the "O" label?'
  created_at: 2023-09-11 21:49:51+00:00
  edited: true
  hidden: false
  id: 64ff998f520f8d7b4de84d5d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: dslim/bert-large-NER
repo_type: model
status: open
target_branch: null
title: eval results reproducibility
