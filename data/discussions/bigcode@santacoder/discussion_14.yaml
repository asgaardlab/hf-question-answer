!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tlphams
conflicting_files: null
created_at: 2023-01-26 09:05:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b23d4d74d332d19a79ad0d2afe8f7f9f.svg
      fullname: Pham Tung Lam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tlphams
      type: user
    createdAt: '2023-01-26T09:05:21.000Z'
    data:
      edited: false
      editors:
      - tlphams
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b23d4d74d332d19a79ad0d2afe8f7f9f.svg
          fullname: Pham Tung Lam
          isHf: false
          isPro: false
          name: tlphams
          type: user
        html: '<p>I have tried to run the model with Pytorch=1.9.0+cu111, but its
          generated text is bizarre with duplicated words. So I want to know about
          the requirement of torch version and other libraries. Thank you. </p>

          '
        raw: 'I have tried to run the model with Pytorch=1.9.0+cu111, but its generated
          text is bizarre with duplicated words. So I want to know about the requirement
          of torch version and other libraries. Thank you. '
        updatedAt: '2023-01-26T09:05:21.912Z'
      numEdits: 0
      reactions: []
    id: 63d242518700bc77c8e68047
    type: comment
  author: tlphams
  content: 'I have tried to run the model with Pytorch=1.9.0+cu111, but its generated
    text is bizarre with duplicated words. So I want to know about the requirement
    of torch version and other libraries. Thank you. '
  created_at: 2023-01-26 09:05:21+00:00
  edited: false
  hidden: false
  id: 63d242518700bc77c8e68047
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b23d4d74d332d19a79ad0d2afe8f7f9f.svg
      fullname: Pham Tung Lam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tlphams
      type: user
    createdAt: '2023-01-26T09:26:04.000Z'
    data:
      status: closed
    id: 63d2472caa0afab5e87844a0
    type: status-change
  author: tlphams
  created_at: 2023-01-26 09:26:04+00:00
  id: 63d2472caa0afab5e87844a0
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-01-26T09:55:10.000Z'
    data:
      edited: true
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: "<p>Can you please share the code you used to generate text, Pytorch\
          \ version shouldn't impact the generation. Something to pay attention to\
          \ is not passing <code>token_type_ids</code> returned by the tokenizer to\
          \ the model. Here's a working example to use the model both in standard\
          \ and FIM settings:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"bigcode/santacoder\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"bigcode/santacoder\"</span>)\n\n<span class=\"\
          hljs-comment\">#standard example</span>\ninput_text =<span class=\"hljs-string\"\
          >\"def all_odd_elements((L):\\n\"</span>\n<span class=\"hljs-comment\">#\
          \ example to do FIM, add fim special tokens: &lt;fim-prefix&gt;, &lt;fim-middle&gt;\
          \ and &lt;fim-suffix&gt; </span>\ninput_text_fim = <span class=\"hljs-string\"\
          >\"&lt;fim-prefix&gt;def fib(n):&lt;fim-suffix&gt;    else:\\n        return\
          \ fib(n - 2) + fib(n - 1)&lt;fim-middle&gt;\"</span>\n\n<span class=\"hljs-comment\"\
          ># tokenizer(inputs) returns inputs_ids, attention_mask and token_types_ids,\
          \ the latter shouldn't be fed to the model</span>\n<span class=\"hljs-comment\"\
          ># so if you want to use model(**inputs) or model.generate(**inputs) make\
          \ sure you add return_token_type_ids=False to not have it returned </span>\n\
          \ninputs = tokenizer(input_text, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>) <span class=\"hljs-comment\"># add return_token_type_ids=False\
          \ for model(**inputs) </span>\ninputs_fim = tokenizer(input_text_fim, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>)  <span class=\"hljs-comment\"># add\
          \ return_token_type_ids=False for model(**inputs) </span>\n\noutputs = model.generate(inputs[<span\
          \ class=\"hljs-string\">\"input_ids\"</span>], max_new_tokens=<span class=\"\
          hljs-number\">18</span>)\noutputs_fim = model.generate(inputs_fim[<span\
          \ class=\"hljs-string\">\"input_ids\"</span>], max_new_tokens=<span class=\"\
          hljs-number\">25</span>)\n\ngeneration = [tokenizer.decode(tensor, skip_special_tokens=<span\
          \ class=\"hljs-literal\">False</span>) <span class=\"hljs-keyword\">for</span>\
          \ tensor <span class=\"hljs-keyword\">in</span> outputs]\ngeneration_fim\
          \ = [tokenizer.decode(tensor, skip_special_tokens=<span class=\"hljs-literal\"\
          >False</span>) <span class=\"hljs-keyword\">for</span> tensor <span class=\"\
          hljs-keyword\">in</span> outputs_fim]\n\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"Standard example:\\n <span class=\"hljs-subst\"\
          >{generation[<span class=\"hljs-number\">0</span>]}</span>\"</span>)\n<span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"FIM\
          \ example:\\n <span class=\"hljs-subst\">{generation_fim[<span class=\"\
          hljs-number\">0</span>]}</span>\"</span>)\n</code></pre>\n<pre><code>Standard\
          \ example:\n def all_odd_elements((L):\n    return all(x % 2!= 0 for x in\
          \ L)\n\n\nFIM example:\n &lt;fim-prefix&gt;def fib(n):&lt;fim-suffix&gt;\
          \    else:\n        return fib(n - 2) + fib(n - 1)&lt;fim-middle&gt;\n \
          \   if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n&lt;|endoftext|&gt;&lt;fim-prefix&gt;\n\
          </code></pre>\n"
        raw: "Can you please share the code you used to generate text, Pytorch version\
          \ shouldn't impact the generation. Something to pay attention to is not\
          \ passing `token_type_ids` returned by the tokenizer to the model. Here's\
          \ a working example to use the model both in standard and FIM settings:\n\
          ```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/santacoder\", trust_remote_code=True)\n\
          tokenizer = AutoTokenizer.from_pretrained(\"bigcode/santacoder\")\n\n#standard\
          \ example\ninput_text =\"def all_odd_elements((L):\\n\"\n# example to do\
          \ FIM, add fim special tokens: <fim-prefix>, <fim-middle> and <fim-suffix>\
          \ \ninput_text_fim = \"<fim-prefix>def fib(n):<fim-suffix>    else:\\n \
          \       return fib(n - 2) + fib(n - 1)<fim-middle>\"\n\n# tokenizer(inputs)\
          \ returns inputs_ids, attention_mask and token_types_ids, the latter shouldn't\
          \ be fed to the model\n# so if you want to use model(**inputs) or model.generate(**inputs)\
          \ make sure you add return_token_type_ids=False to not have it returned\
          \ \n\ninputs = tokenizer(input_text, return_tensors=\"pt\") # add return_token_type_ids=False\
          \ for model(**inputs) \ninputs_fim = tokenizer(input_text_fim, return_tensors=\"\
          pt\")  # add return_token_type_ids=False for model(**inputs) \n\noutputs\
          \ = model.generate(inputs[\"input_ids\"], max_new_tokens=18)\noutputs_fim\
          \ = model.generate(inputs_fim[\"input_ids\"], max_new_tokens=25)\n\ngeneration\
          \ = [tokenizer.decode(tensor, skip_special_tokens=False) for tensor in outputs]\n\
          generation_fim = [tokenizer.decode(tensor, skip_special_tokens=False) for\
          \ tensor in outputs_fim]\n\nprint(f\"Standard example:\\n {generation[0]}\"\
          )\nprint(f\"FIM example:\\n {generation_fim[0]}\")\n```\n``` \nStandard\
          \ example:\n def all_odd_elements((L):\n\treturn all(x % 2!= 0 for x in\
          \ L)\n\n\nFIM example:\n <fim-prefix>def fib(n):<fim-suffix>    else:\n\
          \        return fib(n - 2) + fib(n - 1)<fim-middle>\n    if n == 0:\n  \
          \      return 0\n    elif n == 1:\n        return 1\n<|endoftext|><fim-prefix>\n\
          ```"
        updatedAt: '2023-01-26T09:55:41.443Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - tlphams
    id: 63d24dfe1cfb0d40cbf63dc1
    type: comment
  author: loubnabnl
  content: "Can you please share the code you used to generate text, Pytorch version\
    \ shouldn't impact the generation. Something to pay attention to is not passing\
    \ `token_type_ids` returned by the tokenizer to the model. Here's a working example\
    \ to use the model both in standard and FIM settings:\n```python\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    bigcode/santacoder\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"\
    bigcode/santacoder\")\n\n#standard example\ninput_text =\"def all_odd_elements((L):\\\
    n\"\n# example to do FIM, add fim special tokens: <fim-prefix>, <fim-middle> and\
    \ <fim-suffix> \ninput_text_fim = \"<fim-prefix>def fib(n):<fim-suffix>    else:\\\
    n        return fib(n - 2) + fib(n - 1)<fim-middle>\"\n\n# tokenizer(inputs) returns\
    \ inputs_ids, attention_mask and token_types_ids, the latter shouldn't be fed\
    \ to the model\n# so if you want to use model(**inputs) or model.generate(**inputs)\
    \ make sure you add return_token_type_ids=False to not have it returned \n\ninputs\
    \ = tokenizer(input_text, return_tensors=\"pt\") # add return_token_type_ids=False\
    \ for model(**inputs) \ninputs_fim = tokenizer(input_text_fim, return_tensors=\"\
    pt\")  # add return_token_type_ids=False for model(**inputs) \n\noutputs = model.generate(inputs[\"\
    input_ids\"], max_new_tokens=18)\noutputs_fim = model.generate(inputs_fim[\"input_ids\"\
    ], max_new_tokens=25)\n\ngeneration = [tokenizer.decode(tensor, skip_special_tokens=False)\
    \ for tensor in outputs]\ngeneration_fim = [tokenizer.decode(tensor, skip_special_tokens=False)\
    \ for tensor in outputs_fim]\n\nprint(f\"Standard example:\\n {generation[0]}\"\
    )\nprint(f\"FIM example:\\n {generation_fim[0]}\")\n```\n``` \nStandard example:\n\
    \ def all_odd_elements((L):\n\treturn all(x % 2!= 0 for x in L)\n\n\nFIM example:\n\
    \ <fim-prefix>def fib(n):<fim-suffix>    else:\n        return fib(n - 2) + fib(n\
    \ - 1)<fim-middle>\n    if n == 0:\n        return 0\n    elif n == 1:\n     \
    \   return 1\n<|endoftext|><fim-prefix>\n```"
  created_at: 2023-01-26 09:55:10+00:00
  edited: true
  hidden: false
  id: 63d24dfe1cfb0d40cbf63dc1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b23d4d74d332d19a79ad0d2afe8f7f9f.svg
      fullname: Pham Tung Lam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tlphams
      type: user
    createdAt: '2023-01-27T01:03:31.000Z'
    data:
      edited: false
      editors:
      - tlphams
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b23d4d74d332d19a79ad0d2afe8f7f9f.svg
          fullname: Pham Tung Lam
          isHf: false
          isPro: false
          name: tlphams
          type: user
        html: '<p>Yeah there is a mistake in my text generation code ^^ I have changed
          the code and it is working well now<br>Previously:</p>

          <pre><code>inputs = tokenizer(input_text, return_tensors="pt")

          outputs = model.generate(**inputs, max_new_tokens=64)

          </code></pre>

          <p>I checked the README.md again and have changed it into</p>

          <pre><code>inputs = tokenizer.encode(input_text, return_tensors="pt")

          outputs = model.generate(inputs, max_new_tokens=64)

          </code></pre>

          <p>I have just tried to add return_token_type_ids=False in the first case,
          too, and it also works. Thank you ^^</p>

          '
        raw: 'Yeah there is a mistake in my text generation code ^^ I have changed
          the code and it is working well now

          Previously:

          ```

          inputs = tokenizer(input_text, return_tensors="pt")

          outputs = model.generate(**inputs, max_new_tokens=64)

          ```

          I checked the README.md again and have changed it into

          ```

          inputs = tokenizer.encode(input_text, return_tensors="pt")

          outputs = model.generate(inputs, max_new_tokens=64)

          ```

          I have just tried to add return_token_type_ids=False in the first case,
          too, and it also works. Thank you ^^'
        updatedAt: '2023-01-27T01:03:31.898Z'
      numEdits: 0
      reactions: []
    id: 63d322e3c3eb22526cb61bd2
    type: comment
  author: tlphams
  content: 'Yeah there is a mistake in my text generation code ^^ I have changed the
    code and it is working well now

    Previously:

    ```

    inputs = tokenizer(input_text, return_tensors="pt")

    outputs = model.generate(**inputs, max_new_tokens=64)

    ```

    I checked the README.md again and have changed it into

    ```

    inputs = tokenizer.encode(input_text, return_tensors="pt")

    outputs = model.generate(inputs, max_new_tokens=64)

    ```

    I have just tried to add return_token_type_ids=False in the first case, too, and
    it also works. Thank you ^^'
  created_at: 2023-01-27 01:03:31+00:00
  edited: false
  hidden: false
  id: 63d322e3c3eb22526cb61bd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-01-28T16:26:12.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>Great, you don''t even need to specify <code>return_token_type_ids=False</code>
          now, we turned it off by default</p>

          '
        raw: Great, you don't even need to specify `return_token_type_ids=False` now,
          we turned it off by default
        updatedAt: '2023-01-28T16:26:12.253Z'
      numEdits: 0
      reactions: []
    id: 63d54ca443a3934c5de3d221
    type: comment
  author: loubnabnl
  content: Great, you don't even need to specify `return_token_type_ids=False` now,
    we turned it off by default
  created_at: 2023-01-28 16:26:12+00:00
  edited: false
  hidden: false
  id: 63d54ca443a3934c5de3d221
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: bigcode/santacoder
repo_type: model
status: closed
target_branch: null
title: Require Pytorch version
