!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LakshyAAAgrawal
conflicting_files: null
created_at: 2023-01-11 19:30:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3bb13e989db31820734cbe6b13be130a.svg
      fullname: Lakshya A Agrawal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LakshyAAAgrawal
      type: user
    createdAt: '2023-01-11T19:30:18.000Z'
    data:
      edited: false
      editors:
      - LakshyAAAgrawal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3bb13e989db31820734cbe6b13be130a.svg
          fullname: Lakshya A Agrawal
          isHf: false
          isPro: false
          name: LakshyAAAgrawal
          type: user
        html: '<p>I have been able to get the model to generate autoregressively,
          however, when I try to tokenize a sequence consisting of special tokens
          as shown in the FIM example in the model card - "", "", "" - I see multiple
          token ids being generated corresponding to each of the tokens, further,
          I am not able to get good generations in the FIM setting, even with the
          example provided in the Model card. I do not see the FIM tokens being part
          of the special symbols in the tokenizer either.</p>

          <p>Kindly suggest how to use the fill-in-the-middle setting of Santacoder</p>

          '
        raw: "I have been able to get the model to generate autoregressively, however,\
          \ when I try to tokenize a sequence consisting of special tokens as shown\
          \ in the FIM example in the model card - \"<fim-prefix>\", \"<fim-middle>\"\
          , \"<fim-suffix>\" - I see multiple token ids being generated corresponding\
          \ to each of the tokens, further, I am not able to get good generations\
          \ in the FIM setting, even with the example provided in the Model card.\
          \ I do not see the FIM tokens being part of the special symbols in the tokenizer\
          \ either.\r\n\r\nKindly suggest how to use the fill-in-the-middle setting\
          \ of Santacoder"
        updatedAt: '2023-01-11T19:30:18.934Z'
      numEdits: 0
      reactions: []
    id: 63bf0e4a87619d1458cb0b7a
    type: comment
  author: LakshyAAAgrawal
  content: "I have been able to get the model to generate autoregressively, however,\
    \ when I try to tokenize a sequence consisting of special tokens as shown in the\
    \ FIM example in the model card - \"<fim-prefix>\", \"<fim-middle>\", \"<fim-suffix>\"\
    \ - I see multiple token ids being generated corresponding to each of the tokens,\
    \ further, I am not able to get good generations in the FIM setting, even with\
    \ the example provided in the Model card. I do not see the FIM tokens being part\
    \ of the special symbols in the tokenizer either.\r\n\r\nKindly suggest how to\
    \ use the fill-in-the-middle setting of Santacoder"
  created_at: 2023-01-11 19:30:18+00:00
  edited: false
  hidden: false
  id: 63bf0e4a87619d1458cb0b7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-01-14T21:55:44.000Z'
    data:
      edited: true
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: "<p>Hi, you need to manually add the FIM special tokens to the vocab,\
          \ you will also need to specify <code>return_token_type_ids=False</code>\
          \ when tokenizing to not get the token ids that might confuse the order.\
          \ We will try to make the  model card more clear about this. Here's a functioning\
          \ example. You can also find more details in this <a rel=\"nofollow\" href=\"\
          https://github.com/arjunguha/BigCode-demos/blob/main/bigcode.ipynb\">notebook</a>.</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-comment\"># pip\
          \ install -q transformers</span>\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(<span class=\"\
          hljs-string\">\"bigcode/santacoder\"</span>, revision=<span class=\"hljs-string\"\
          >\"fim\"</span>, trust_remote_code=<span class=\"hljs-literal\">True</span>)\n\
          tokenizer_fim = AutoTokenizer.from_pretrained(<span class=\"hljs-string\"\
          >\"bigcode/santacoder\"</span>, padding_side=<span class=\"hljs-string\"\
          >\"left\"</span>)\n\nFIM_PREFIX = <span class=\"hljs-string\">\"&lt;fim-prefix&gt;\"\
          </span>\nFIM_MIDDLE = <span class=\"hljs-string\">\"&lt;fim-middle&gt;\"\
          </span>\nFIM_SUFFIX = <span class=\"hljs-string\">\"&lt;fim-suffix&gt;\"\
          </span>\nFIM_PAD = <span class=\"hljs-string\">\"&lt;fim-pad&gt;\"</span>\n\
          EOD = <span class=\"hljs-string\">\"&lt;|endoftext|&gt;\"</span>\n\ntokenizer_fim.add_special_tokens({\n\
          \  <span class=\"hljs-string\">\"additional_special_tokens\"</span>: [EOD,\
          \ FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, FIM_PAD],\n  <span class=\"hljs-string\"\
          >\"pad_token\"</span>: EOD,\n})\n\ninput_text = <span class=\"hljs-string\"\
          >\"&lt;fim-prefix&gt;def fib(n):&lt;fim-suffix&gt;    else:\\n        return\
          \ fib(n - 2) + fib(n - 1)&lt;fim-middle&gt;\"</span>\ninputs = tokenizer_fim(input_text,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>, padding=<span\
          \ class=\"hljs-literal\">True</span>, return_token_type_ids=<span class=\"\
          hljs-literal\">False</span>)\noutputs = model.generate(**inputs, max_new_tokens=<span\
          \ class=\"hljs-number\">25</span>)\ngeneration = [tokenizer_fim.decode(tensor,\
          \ skip_special_tokens=<span class=\"hljs-literal\">False</span>) <span class=\"\
          hljs-keyword\">for</span> tensor <span class=\"hljs-keyword\">in</span>\
          \ outputs]\n\n<span class=\"hljs-built_in\">print</span>(generation[<span\
          \ class=\"hljs-number\">0</span>])\n</code></pre>\n<pre><code>&lt;fim-prefix&gt;def\
          \ fib(n):&lt;fim-suffix&gt;    else:\n        return fib(n - 2) + fib(n\
          \ - 1)&lt;fim-middle&gt;\n    if n == 0:\n        return 0\n    elif n ==\
          \ 1:\n        return 1\n&lt;|endoftext|&gt;&lt;fim-prefix&gt;\n</code></pre>\n"
        raw: "Hi, you need to manually add the FIM special tokens to the vocab, you\
          \ will also need to specify `return_token_type_ids=False` when tokenizing\
          \ to not get the token ids that might confuse the order. We will try to\
          \ make the  model card more clear about this. Here's a functioning example.\
          \ You can also find more details in this [notebook](https://github.com/arjunguha/BigCode-demos/blob/main/bigcode.ipynb).\n\
          ```python\n# pip install -q transformers\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/santacoder\"\
          , revision=\"fim\", trust_remote_code=True)\ntokenizer_fim = AutoTokenizer.from_pretrained(\"\
          bigcode/santacoder\", padding_side=\"left\")\n\nFIM_PREFIX = \"<fim-prefix>\"\
          \nFIM_MIDDLE = \"<fim-middle>\"\nFIM_SUFFIX = \"<fim-suffix>\"\nFIM_PAD\
          \ = \"<fim-pad>\"\nEOD = \"<|endoftext|>\"\n\ntokenizer_fim.add_special_tokens({\n\
          \  \"additional_special_tokens\": [EOD, FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX,\
          \ FIM_PAD],\n  \"pad_token\": EOD,\n})\n\ninput_text = \"<fim-prefix>def\
          \ fib(n):<fim-suffix>    else:\\n        return fib(n - 2) + fib(n - 1)<fim-middle>\"\
          \ninputs = tokenizer_fim(input_text, return_tensors=\"pt\", padding=True,\
          \ return_token_type_ids=False)\noutputs = model.generate(**inputs, max_new_tokens=25)\n\
          generation = [tokenizer_fim.decode(tensor, skip_special_tokens=False) for\
          \ tensor in outputs]\n\nprint(generation[0])\n```\n```\n<fim-prefix>def\
          \ fib(n):<fim-suffix>    else:\n        return fib(n - 2) + fib(n - 1)<fim-middle>\n\
          \    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n\
          <|endoftext|><fim-prefix>\n```"
        updatedAt: '2023-01-14T21:58:58.873Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - LakshyAAAgrawal
        - NerdyRodent
        - thekizoch
    id: 63c324e0e25fc176427928aa
    type: comment
  author: loubnabnl
  content: "Hi, you need to manually add the FIM special tokens to the vocab, you\
    \ will also need to specify `return_token_type_ids=False` when tokenizing to not\
    \ get the token ids that might confuse the order. We will try to make the  model\
    \ card more clear about this. Here's a functioning example. You can also find\
    \ more details in this [notebook](https://github.com/arjunguha/BigCode-demos/blob/main/bigcode.ipynb).\n\
    ```python\n# pip install -q transformers\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/santacoder\"\
    , revision=\"fim\", trust_remote_code=True)\ntokenizer_fim = AutoTokenizer.from_pretrained(\"\
    bigcode/santacoder\", padding_side=\"left\")\n\nFIM_PREFIX = \"<fim-prefix>\"\n\
    FIM_MIDDLE = \"<fim-middle>\"\nFIM_SUFFIX = \"<fim-suffix>\"\nFIM_PAD = \"<fim-pad>\"\
    \nEOD = \"<|endoftext|>\"\n\ntokenizer_fim.add_special_tokens({\n  \"additional_special_tokens\"\
    : [EOD, FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, FIM_PAD],\n  \"pad_token\": EOD,\n\
    })\n\ninput_text = \"<fim-prefix>def fib(n):<fim-suffix>    else:\\n        return\
    \ fib(n - 2) + fib(n - 1)<fim-middle>\"\ninputs = tokenizer_fim(input_text, return_tensors=\"\
    pt\", padding=True, return_token_type_ids=False)\noutputs = model.generate(**inputs,\
    \ max_new_tokens=25)\ngeneration = [tokenizer_fim.decode(tensor, skip_special_tokens=False)\
    \ for tensor in outputs]\n\nprint(generation[0])\n```\n```\n<fim-prefix>def fib(n):<fim-suffix>\
    \    else:\n        return fib(n - 2) + fib(n - 1)<fim-middle>\n    if n == 0:\n\
    \        return 0\n    elif n == 1:\n        return 1\n<|endoftext|><fim-prefix>\n\
    ```"
  created_at: 2023-01-14 21:55:44+00:00
  edited: true
  hidden: false
  id: 63c324e0e25fc176427928aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-01-24T22:10:04.000Z'
    data:
      status: closed
    id: 63d0573c2a72b1e24997a9d4
    type: status-change
  author: loubnabnl
  created_at: 2023-01-24 22:10:04+00:00
  id: 63d0573c2a72b1e24997a9d4
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-01-25T11:27:37.000Z'
    data:
      edited: true
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>FYI the special tokens are now in the tokenizer by default: <a href="https://huggingface.co/bigcode/santacoder/discussions/11">https://huggingface.co/bigcode/santacoder/discussions/11</a></p>

          <p>And you don''t even need to specify return_token_type_ids=False now,
          we turned it off by default</p>

          '
        raw: 'FYI the special tokens are now in the tokenizer by default: https://huggingface.co/bigcode/santacoder/discussions/11


          And you don''t even need to specify return_token_type_ids=False now, we
          turned it off by default'
        updatedAt: '2023-01-28T16:27:01.119Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - LakshyAAAgrawal
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - LakshyAAAgrawal
    id: 63d11229119416cdbe0e20a3
    type: comment
  author: loubnabnl
  content: 'FYI the special tokens are now in the tokenizer by default: https://huggingface.co/bigcode/santacoder/discussions/11


    And you don''t even need to specify return_token_type_ids=False now, we turned
    it off by default'
  created_at: 2023-01-25 11:27:37+00:00
  edited: true
  hidden: false
  id: 63d11229119416cdbe0e20a3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: bigcode/santacoder
repo_type: model
status: closed
target_branch: null
title: How to run the Fill-in-the-middle setting
