!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mh
conflicting_files: null
created_at: 2023-03-13 10:30:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b86f80d8e495ec2fbfe7ed9a9d92b384.svg
      fullname: munhyong.kim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mh
      type: user
    createdAt: '2023-03-13T11:30:33.000Z'
    data:
      edited: true
      editors:
      - mh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b86f80d8e495ec2fbfe7ed9a9d92b384.svg
          fullname: munhyong.kim
          isHf: false
          isPro: false
          name: mh
          type: user
        html: '<p>I tried to evaluate human_eval set with santacoder, but the performance
          of santacoder was found to be different with the results of the paper.<br>When
          looking through the example, I see sometimes it generates java code though
          the task is to generate python code in human eval.<br>I loaded the model
          and infered following model card directions.<br>Anything I need to check
          to ensure the human_eval evaluation score?</p>

          '
        raw: 'I tried to evaluate human_eval set with santacoder, but the performance
          of santacoder was found to be different with the results of the paper.

          When looking through the example, I see sometimes it generates java code
          though the task is to generate python code in human eval.

          I loaded the model and infered following model card directions.

          Anything I need to check to ensure the human_eval evaluation score?'
        updatedAt: '2023-03-13T11:36:12.963Z'
      numEdits: 1
      reactions: []
    id: 640f0959f2d7c41a1e9be4a5
    type: comment
  author: mh
  content: 'I tried to evaluate human_eval set with santacoder, but the performance
    of santacoder was found to be different with the results of the paper.

    When looking through the example, I see sometimes it generates java code though
    the task is to generate python code in human eval.

    I loaded the model and infered following model card directions.

    Anything I need to check to ensure the human_eval evaluation score?'
  created_at: 2023-03-13 10:30:33+00:00
  edited: true
  hidden: false
  id: 640f0959f2d7c41a1e9be4a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664332914111-62d8315bad693a1a962864b3.png?w=200&h=200&f=face
      fullname: Arjun Guha
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: arjunguha
      type: user
    createdAt: '2023-03-13T22:11:29.000Z'
    data:
      edited: false
      editors:
      - arjunguha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664332914111-62d8315bad693a1a962864b3.png?w=200&h=200&f=face
          fullname: Arjun Guha
          isHf: false
          isPro: false
          name: arjunguha
          type: user
        html: '<p>Thanks for your interest in SantaCoder. Could you say more about
          how you''re running evaluation and the results you get?</p>

          <p>Note that for the SantaCoder paper, the evaluation results were produced
          with MultiPL-E directly (github.com/nuprl/MultiPL-E). We did not use the
          BigCode evaluation harness: MultiPL-E integration into the evaluation harness
          is a WIP.</p>

          '
        raw: 'Thanks for your interest in SantaCoder. Could you say more about how
          you''re running evaluation and the results you get?


          Note that for the SantaCoder paper, the evaluation results were produced
          with MultiPL-E directly (github.com/nuprl/MultiPL-E). We did not use the
          BigCode evaluation harness: MultiPL-E integration into the evaluation harness
          is a WIP.'
        updatedAt: '2023-03-13T22:11:29.419Z'
      numEdits: 0
      reactions: []
    id: 640f9f91df68b86bf8ed5b41
    type: comment
  author: arjunguha
  content: 'Thanks for your interest in SantaCoder. Could you say more about how you''re
    running evaluation and the results you get?


    Note that for the SantaCoder paper, the evaluation results were produced with
    MultiPL-E directly (github.com/nuprl/MultiPL-E). We did not use the BigCode evaluation
    harness: MultiPL-E integration into the evaluation harness is a WIP.'
  created_at: 2023-03-13 21:11:29+00:00
  edited: false
  hidden: false
  id: 640f9f91df68b86bf8ed5b41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b86f80d8e495ec2fbfe7ed9a9d92b384.svg
      fullname: munhyong.kim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mh
      type: user
    createdAt: '2023-03-14T02:30:59.000Z'
    data:
      edited: false
      editors:
      - mh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b86f80d8e495ec2fbfe7ed9a9d92b384.svg
          fullname: munhyong.kim
          isHf: false
          isPro: false
          name: mh
          type: user
        html: '<p>Thank you for the quick reply.<br>I just used <a rel="nofollow"
          href="https://github.com/openai/human-eval">https://github.com/openai/human-eval</a>
          data and made the evaluation code by myself. Maybe this is why the numbers
          are different. I will test with MultiPL-E, too. Thanks again.</p>

          '
        raw: 'Thank you for the quick reply.

          I just used https://github.com/openai/human-eval data and made the evaluation
          code by myself. Maybe this is why the numbers are different. I will test
          with MultiPL-E, too. Thanks again.'
        updatedAt: '2023-03-14T02:30:59.363Z'
      numEdits: 0
      reactions: []
    id: 640fdc63df68b86bf8ef63a1
    type: comment
  author: mh
  content: 'Thank you for the quick reply.

    I just used https://github.com/openai/human-eval data and made the evaluation
    code by myself. Maybe this is why the numbers are different. I will test with
    MultiPL-E, too. Thanks again.'
  created_at: 2023-03-14 01:30:59+00:00
  edited: false
  hidden: false
  id: 640fdc63df68b86bf8ef63a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-03-14T08:22:44.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>Can you also report your HumanEval numbers? We also evaluated on
          the original HumanEval with the <a rel="nofollow" href="https://github.com/bigcode-project/bigcode-evaluation-harness">evaluation-harness</a>
          and the numbers aren''t very far from MultiPL-E HumanEval version.</p>

          '
        raw: Can you also report your HumanEval numbers? We also evaluated on the
          original HumanEval with the [evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness)
          and the numbers aren't very far from MultiPL-E HumanEval version.
        updatedAt: '2023-03-14T08:22:44.816Z'
      numEdits: 0
      reactions: []
    id: 64102ed430ef3512cd046d77
    type: comment
  author: loubnabnl
  content: Can you also report your HumanEval numbers? We also evaluated on the original
    HumanEval with the [evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness)
    and the numbers aren't very far from MultiPL-E HumanEval version.
  created_at: 2023-03-14 07:22:44+00:00
  edited: false
  hidden: false
  id: 64102ed430ef3512cd046d77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2dbbe226ec070d9a39d578e0a7cd4ac6.svg
      fullname: Hande Dong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: donghd
      type: user
    createdAt: '2023-03-31T02:53:30.000Z'
    data:
      edited: false
      editors:
      - donghd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2dbbe226ec070d9a39d578e0a7cd4ac6.svg
          fullname: Hande Dong
          isHf: false
          isPro: false
          name: donghd
          type: user
        html: '<p>I also encounter this problem. </p>

          <p>There are three evaluation modules to evaluate on HumanEval dataset:<br>(1)
          Official: The official module provided by OpenAI (Codex), <a rel="nofollow"
          href="https://github.com/openai/human-eval">https://github.com/openai/human-eval</a>;<br>(2)
          bigcode_evaluation_harness: <a rel="nofollow" href="https://github.com/bigcode-project/bigcode-evaluation-harness">https://github.com/bigcode-project/bigcode-evaluation-harness</a>;<br>(3)
          MultiPL-E: <a rel="nofollow" href="https://github.com/nuprl/MultiPL-E">https://github.com/nuprl/MultiPL-E</a>.<br>I
          find that bigcode_evaluation_harness is the same as the Official with 164
          programming questions, but MultiPL-E is slightly different from the previous
          two modules with 161 questions. </p>

          <p>With the MultiPL-E module, I can reproduce the result of HumanEval-Python
          with 0.49 PASS@100, but PASS@100 of bigcode_evaluation_harness and Official
          is 0.46, a bit lower than MultiPL-E.</p>

          '
        raw: "I also encounter this problem. \n\nThere are three evaluation modules\
          \ to evaluate on HumanEval dataset: \n(1) Official: The official module\
          \ provided by OpenAI (Codex), https://github.com/openai/human-eval; \n(2)\
          \ bigcode_evaluation_harness: https://github.com/bigcode-project/bigcode-evaluation-harness;\
          \ \n(3) MultiPL-E: https://github.com/nuprl/MultiPL-E. \nI find that bigcode_evaluation_harness\
          \ is the same as the Official with 164 programming questions, but MultiPL-E\
          \ is slightly different from the previous two modules with 161 questions.\
          \ \n\nWith the MultiPL-E module, I can reproduce the result of HumanEval-Python\
          \ with 0.49 PASS@100, but PASS@100 of bigcode_evaluation_harness and Official\
          \ is 0.46, a bit lower than MultiPL-E."
        updatedAt: '2023-03-31T02:53:30.673Z'
      numEdits: 0
      reactions: []
    id: 64264b2a1454fbdb601bb281
    type: comment
  author: donghd
  content: "I also encounter this problem. \n\nThere are three evaluation modules\
    \ to evaluate on HumanEval dataset: \n(1) Official: The official module provided\
    \ by OpenAI (Codex), https://github.com/openai/human-eval; \n(2) bigcode_evaluation_harness:\
    \ https://github.com/bigcode-project/bigcode-evaluation-harness; \n(3) MultiPL-E:\
    \ https://github.com/nuprl/MultiPL-E. \nI find that bigcode_evaluation_harness\
    \ is the same as the Official with 164 programming questions, but MultiPL-E is\
    \ slightly different from the previous two modules with 161 questions. \n\nWith\
    \ the MultiPL-E module, I can reproduce the result of HumanEval-Python with 0.49\
    \ PASS@100, but PASS@100 of bigcode_evaluation_harness and Official is 0.46, a\
    \ bit lower than MultiPL-E."
  created_at: 2023-03-31 01:53:30+00:00
  edited: false
  hidden: false
  id: 64264b2a1454fbdb601bb281
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-04-03T16:23:20.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>The MultiPL-E Humaneval version is slightly different from the original
          implementation which could explain the difference you are observing, the
          authors add type annotations and doctests to prompts where they are missing.
          You can refer to the <a rel="nofollow" href="https://arxiv.org/abs/2208.08227">paper</a>
          for more details</p>

          '
        raw: The MultiPL-E Humaneval version is slightly different from the original
          implementation which could explain the difference you are observing, the
          authors add type annotations and doctests to prompts where they are missing.
          You can refer to the [paper](https://arxiv.org/abs/2208.08227) for more
          details
        updatedAt: '2023-04-03T16:23:20.035Z'
      numEdits: 0
      reactions: []
    id: 642afd7896cd8ffa39e6f330
    type: comment
  author: loubnabnl
  content: The MultiPL-E Humaneval version is slightly different from the original
    implementation which could explain the difference you are observing, the authors
    add type annotations and doctests to prompts where they are missing. You can refer
    to the [paper](https://arxiv.org/abs/2208.08227) for more details
  created_at: 2023-04-03 15:23:20+00:00
  edited: false
  hidden: false
  id: 642afd7896cd8ffa39e6f330
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b86f80d8e495ec2fbfe7ed9a9d92b384.svg
      fullname: munhyong.kim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mh
      type: user
    createdAt: '2023-04-04T05:25:26.000Z'
    data:
      edited: false
      editors:
      - mh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b86f80d8e495ec2fbfe7ed9a9d92b384.svg
          fullname: munhyong.kim
          isHf: false
          isPro: false
          name: mh
          type: user
        html: '<p>I made a mistake of mentioning human eval data. It was a problem
          of mbpp dataset. After I changed the mbpp set to MultiPle-E version. I see
          the close result from the reported score. I guess the number can vary slightly
          due to the randomness of generation. Thanks</p>

          '
        raw: I made a mistake of mentioning human eval data. It was a problem of mbpp
          dataset. After I changed the mbpp set to MultiPle-E version. I see the close
          result from the reported score. I guess the number can vary slightly due
          to the randomness of generation. Thanks
        updatedAt: '2023-04-04T05:25:26.278Z'
      numEdits: 0
      reactions: []
    id: 642bb4c6f8692638b03c378b
    type: comment
  author: mh
  content: I made a mistake of mentioning human eval data. It was a problem of mbpp
    dataset. After I changed the mbpp set to MultiPle-E version. I see the close result
    from the reported score. I guess the number can vary slightly due to the randomness
    of generation. Thanks
  created_at: 2023-04-04 04:25:26+00:00
  edited: false
  hidden: false
  id: 642bb4c6f8692638b03c378b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2023-04-04T09:32:51.000Z'
    data:
      status: closed
    id: 642beec3be5c7d989e605f90
    type: status-change
  author: cakiki
  created_at: 2023-04-04 08:32:51+00:00
  id: 642beec3be5c7d989e605f90
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: bigcode/santacoder
repo_type: model
status: closed
target_branch: null
title: reproducibility of santacoder
