!!python/object:huggingface_hub.community.DiscussionWithDetails
author: oceangod
conflicting_files: null
created_at: 2022-12-30 01:45:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d451a79f3e68da006f5317dc641a6fdf.svg
      fullname: wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oceangod
      type: user
    createdAt: '2022-12-30T01:45:31.000Z'
    data:
      edited: false
      editors:
      - oceangod
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d451a79f3e68da006f5317dc641a6fdf.svg
          fullname: wang
          isHf: false
          isPro: false
          name: oceangod
          type: user
        html: '<p>I am trying to accelerate your 1.1B model. I tried these ways on
          A-100<br>80G GPUI:  (1) deepspeed. (2) optimum (3) onnx-tensorrt (4)<br>torch-dynamo.  I
          set the max_new_token = 256 or max_length = 256 for<br>one batch.  However,
          the generation time is more than 10 seconds.  Do<br>you know how to accelerate
          the inference? I want the latency to be as<br>low as possible. It is better
          to have a latency around 500ms.</p>

          '
        raw: "I am trying to accelerate your 1.1B model. I tried these ways on A-100\r\
          \n80G GPUI:  (1) deepspeed. (2) optimum (3) onnx-tensorrt (4)\r\ntorch-dynamo.\
          \  I set the max_new_token = 256 or max_length = 256 for\r\none batch. \
          \ However, the generation time is more than 10 seconds.  Do\r\nyou know\
          \ how to accelerate the inference? I want the latency to be as\r\nlow as\
          \ possible. It is better to have a latency around 500ms."
        updatedAt: '2022-12-30T01:45:31.159Z'
      numEdits: 0
      reactions: []
    id: 63ae42bb20176b2d216911be
    type: comment
  author: oceangod
  content: "I am trying to accelerate your 1.1B model. I tried these ways on A-100\r\
    \n80G GPUI:  (1) deepspeed. (2) optimum (3) onnx-tensorrt (4)\r\ntorch-dynamo.\
    \  I set the max_new_token = 256 or max_length = 256 for\r\none batch.  However,\
    \ the generation time is more than 10 seconds.  Do\r\nyou know how to accelerate\
    \ the inference? I want the latency to be as\r\nlow as possible. It is better\
    \ to have a latency around 500ms."
  created_at: 2022-12-30 01:45:31+00:00
  edited: false
  hidden: false
  id: 63ae42bb20176b2d216911be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-01-14T22:10:11.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>We are working on accelerating the inference, if you''re interested
          in following our inference efforts more closely, you can Join the project
          through this <a rel="nofollow" href="https://docs.google.com/forms/d/e/1FAIpQLSdVF-TxIs-TM51WluA1SuY174tzzVctF4LkFYGlOW8QtgVlaQ/viewform">form</a>.</p>

          '
        raw: We are working on accelerating the inference, if you're interested in
          following our inference efforts more closely, you can Join the project through
          this [form](https://docs.google.com/forms/d/e/1FAIpQLSdVF-TxIs-TM51WluA1SuY174tzzVctF4LkFYGlOW8QtgVlaQ/viewform).
        updatedAt: '2023-01-14T22:10:11.255Z'
      numEdits: 0
      reactions: []
    id: 63c32843e25fc176427965ef
    type: comment
  author: loubnabnl
  content: We are working on accelerating the inference, if you're interested in following
    our inference efforts more closely, you can Join the project through this [form](https://docs.google.com/forms/d/e/1FAIpQLSdVF-TxIs-TM51WluA1SuY174tzzVctF4LkFYGlOW8QtgVlaQ/viewform).
  created_at: 2023-01-14 22:10:11+00:00
  edited: false
  hidden: false
  id: 63c32843e25fc176427965ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-01-24T22:10:32.000Z'
    data:
      status: closed
    id: 63d057589f1643d247170dab
    type: status-change
  author: loubnabnl
  created_at: 2023-01-24 22:10:32+00:00
  id: 63d057589f1643d247170dab
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: bigcode/santacoder
repo_type: model
status: closed
target_branch: null
title: How to accelerate this bigcode
