!!python/object:huggingface_hub.community.DiscussionWithDetails
author: juliuscheng
conflicting_files: null
created_at: 2023-03-27 21:26:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6d34c2d49cb8dfdccc6681aacf47cd4.svg
      fullname: Julius Cheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: juliuscheng
      type: user
    createdAt: '2023-03-27T22:26:05.000Z'
    data:
      edited: false
      editors:
      - juliuscheng
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6d34c2d49cb8dfdccc6681aacf47cd4.svg
          fullname: Julius Cheng
          isHf: false
          isPro: false
          name: juliuscheng
          type: user
        html: "<p>Hi, I'm trying to reproduce SantaCoder test results on MBPP from\
          \ the paper, and I'm wondering what is the recommended way to prompt the\
          \ model.</p>\n<p>MBPP provides text instructions, e.g. \"Write a function\
          \ to reverse words in a given string.\", which the SantaCoder model card\
          \ explicitly advises against using. Nevertheless, I try to prompt the model\
          \ in one of two ways (in Python):</p>\n<ol>\n<li>Function signature, followed\
          \ by docstring:</li>\n</ol>\n<pre><code>def reverse_words(s):\n    \"\"\"\
          Write a function to reverse words in a given string.\"\"\"\n</code></pre>\n\
          <ol start=\"2\">\n<li>Comment, followed by function signature</li>\n</ol>\n\
          <pre><code># Write a function to reverse words in a given string.\ndef reverse_words(s):\n\
          </code></pre>\n<p>In both cases I get reasonable output, except that after\
          \ defining the function, generation repeats until max_length without terminating\
          \ in the following manner:</p>\n<pre><code>def reverse_words(s):\n    \"\
          \"\"Write a function to reverse words in a given string.\"\"\"\n    return''.join(s.split()[::-1])\n\
          \n\ndef reverse_words_2(s):\n    \"\"\"Write a function to reverse words\
          \ in a given string.\"\"\"\n    return''.join(s.split()[::-1])\n\n\ndef\
          \ reverse_words_3(s):\n    \"\"\"Write a function to reverse words in a\
          \ given string.\"\"\"\n    return''.join(s.split()[::-1])\n</code></pre>\n\
          <p>Should I change the prompting method, or is this output acceptable and\
          \ I should just truncate the output manually? I am trying to reproduce the\
          \ eval results from the paper as closely as possible. Thanks for your help.</p>\n"
        raw: "Hi, I'm trying to reproduce SantaCoder test results on MBPP from the\
          \ paper, and I'm wondering what is the recommended way to prompt the model.\r\
          \n\r\nMBPP provides text instructions, e.g. \"Write a function to reverse\
          \ words in a given string.\", which the SantaCoder model card explicitly\
          \ advises against using. Nevertheless, I try to prompt the model in one\
          \ of two ways (in Python):\r\n\r\n1) Function signature, followed by docstring:\r\
          \n```\r\ndef reverse_words(s):\r\n    \"\"\"Write a function to reverse\
          \ words in a given string.\"\"\"\r\n```\r\n2) Comment, followed by function\
          \ signature\r\n```\r\n# Write a function to reverse words in a given string.\r\
          \ndef reverse_words(s):\r\n```\r\n\r\nIn both cases I get reasonable output,\
          \ except that after defining the function, generation repeats until max_length\
          \ without terminating in the following manner:\r\n\r\n```\r\ndef reverse_words(s):\r\
          \n    \"\"\"Write a function to reverse words in a given string.\"\"\"\r\
          \n    return''.join(s.split()[::-1])\r\n\r\n\r\ndef reverse_words_2(s):\r\
          \n    \"\"\"Write a function to reverse words in a given string.\"\"\"\r\
          \n    return''.join(s.split()[::-1])\r\n\r\n\r\ndef reverse_words_3(s):\r\
          \n    \"\"\"Write a function to reverse words in a given string.\"\"\"\r\
          \n    return''.join(s.split()[::-1])\r\n```\r\n\r\nShould I change the prompting\
          \ method, or is this output acceptable and I should just truncate the output\
          \ manually? I am trying to reproduce the eval results from the paper as\
          \ closely as possible. Thanks for your help."
        updatedAt: '2023-03-27T22:26:05.256Z'
      numEdits: 0
      reactions: []
    id: 642217fdeaad1bcb28b2f5a2
    type: comment
  author: juliuscheng
  content: "Hi, I'm trying to reproduce SantaCoder test results on MBPP from the paper,\
    \ and I'm wondering what is the recommended way to prompt the model.\r\n\r\nMBPP\
    \ provides text instructions, e.g. \"Write a function to reverse words in a given\
    \ string.\", which the SantaCoder model card explicitly advises against using.\
    \ Nevertheless, I try to prompt the model in one of two ways (in Python):\r\n\r\
    \n1) Function signature, followed by docstring:\r\n```\r\ndef reverse_words(s):\r\
    \n    \"\"\"Write a function to reverse words in a given string.\"\"\"\r\n```\r\
    \n2) Comment, followed by function signature\r\n```\r\n# Write a function to reverse\
    \ words in a given string.\r\ndef reverse_words(s):\r\n```\r\n\r\nIn both cases\
    \ I get reasonable output, except that after defining the function, generation\
    \ repeats until max_length without terminating in the following manner:\r\n\r\n\
    ```\r\ndef reverse_words(s):\r\n    \"\"\"Write a function to reverse words in\
    \ a given string.\"\"\"\r\n    return''.join(s.split()[::-1])\r\n\r\n\r\ndef reverse_words_2(s):\r\
    \n    \"\"\"Write a function to reverse words in a given string.\"\"\"\r\n   \
    \ return''.join(s.split()[::-1])\r\n\r\n\r\ndef reverse_words_3(s):\r\n    \"\"\
    \"Write a function to reverse words in a given string.\"\"\"\r\n    return''.join(s.split()[::-1])\r\
    \n```\r\n\r\nShould I change the prompting method, or is this output acceptable\
    \ and I should just truncate the output manually? I am trying to reproduce the\
    \ eval results from the paper as closely as possible. Thanks for your help."
  created_at: 2023-03-27 21:26:05+00:00
  edited: false
  hidden: false
  id: 642217fdeaad1bcb28b2f5a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-03-28T08:33:55.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>Hi we evaluated using the <a href="https://huggingface.co/datasets/nuprl/MultiPL-E">MultiPL-E
          version of MBPP</a> which already implements functions signatures, so evaluation
          is very similar to Human-Eval</p>

          '
        raw: Hi we evaluated using the [MultiPL-E version of MBPP](https://huggingface.co/datasets/nuprl/MultiPL-E)
          which already implements functions signatures, so evaluation is very similar
          to Human-Eval
        updatedAt: '2023-03-28T08:33:55.832Z'
      numEdits: 0
      reactions: []
    id: 6422a673073ae6ae2c8257fa
    type: comment
  author: loubnabnl
  content: Hi we evaluated using the [MultiPL-E version of MBPP](https://huggingface.co/datasets/nuprl/MultiPL-E)
    which already implements functions signatures, so evaluation is very similar to
    Human-Eval
  created_at: 2023-03-28 07:33:55+00:00
  edited: false
  hidden: false
  id: 6422a673073ae6ae2c8257fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6d34c2d49cb8dfdccc6681aacf47cd4.svg
      fullname: Julius Cheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: juliuscheng
      type: user
    createdAt: '2023-03-29T00:46:01.000Z'
    data:
      edited: false
      editors:
      - juliuscheng
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6d34c2d49cb8dfdccc6681aacf47cd4.svg
          fullname: Julius Cheng
          isHf: false
          isPro: false
          name: juliuscheng
          type: user
        html: '<p>Thank you! And regarding the other part of my question, generation
          with greedy search or sampling with temperature=0.2 does not terminate in
          the way shown above. Should I manually truncate the output?</p>

          '
        raw: Thank you! And regarding the other part of my question, generation with
          greedy search or sampling with temperature=0.2 does not terminate in the
          way shown above. Should I manually truncate the output?
        updatedAt: '2023-03-29T00:46:01.315Z'
      numEdits: 0
      reactions: []
    id: 64238a49e37bdd6e30be5976
    type: comment
  author: juliuscheng
  content: Thank you! And regarding the other part of my question, generation with
    greedy search or sampling with temperature=0.2 does not terminate in the way shown
    above. Should I manually truncate the output?
  created_at: 2023-03-28 23:46:01+00:00
  edited: false
  hidden: false
  id: 64238a49e37bdd6e30be5976
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-03-29T01:22:12.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>How are you doing the generations? If you use <code>model.generate()</code>
          it should stop at <code>eos</code> token if comes up, if it doesn''t come
          up often you can add a stopping criteria like it''s done <a rel="nofollow"
          href="https://github.com/huggingface/transformers/blob/b29fd6971d9cd6ba2a824628effe243f543b8f61/examples/research_projects/codeparrot/scripts/human_eval.py#L170">here</a>.
          Note that then you need to post-process the output to only keep the first
          function like it''s done <a rel="nofollow" href="https://github.com/bigcode-project/bigcode-evaluation-harness/blob/d60f52d11a4dd599f88f6dbdaf1ec018c973a4be/lm_eval/tasks/mbpp.py#L65">here</a>.
          You can also find more examples in our <a rel="nofollow" href="https://github.com/bigcode-project/bigcode-evaluation-harness">evaluation
          harness</a></p>

          '
        raw: How are you doing the generations? If you use `model.generate()` it should
          stop at `eos` token if comes up, if it doesn't come up often you can add
          a stopping criteria like it's done [here](https://github.com/huggingface/transformers/blob/b29fd6971d9cd6ba2a824628effe243f543b8f61/examples/research_projects/codeparrot/scripts/human_eval.py#L170).
          Note that then you need to post-process the output to only keep the first
          function like it's done [here](https://github.com/bigcode-project/bigcode-evaluation-harness/blob/d60f52d11a4dd599f88f6dbdaf1ec018c973a4be/lm_eval/tasks/mbpp.py#L65).
          You can also find more examples in our [evaluation harness](https://github.com/bigcode-project/bigcode-evaluation-harness)
        updatedAt: '2023-03-29T01:22:12.054Z'
      numEdits: 0
      reactions: []
    id: 642392c443c77fef90f3d90a
    type: comment
  author: loubnabnl
  content: How are you doing the generations? If you use `model.generate()` it should
    stop at `eos` token if comes up, if it doesn't come up often you can add a stopping
    criteria like it's done [here](https://github.com/huggingface/transformers/blob/b29fd6971d9cd6ba2a824628effe243f543b8f61/examples/research_projects/codeparrot/scripts/human_eval.py#L170).
    Note that then you need to post-process the output to only keep the first function
    like it's done [here](https://github.com/bigcode-project/bigcode-evaluation-harness/blob/d60f52d11a4dd599f88f6dbdaf1ec018c973a4be/lm_eval/tasks/mbpp.py#L65).
    You can also find more examples in our [evaluation harness](https://github.com/bigcode-project/bigcode-evaluation-harness)
  created_at: 2023-03-29 00:22:12+00:00
  edited: false
  hidden: false
  id: 642392c443c77fef90f3d90a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6d34c2d49cb8dfdccc6681aacf47cd4.svg
      fullname: Julius Cheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: juliuscheng
      type: user
    createdAt: '2023-03-29T08:30:37.000Z'
    data:
      edited: false
      editors:
      - juliuscheng
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6d34c2d49cb8dfdccc6681aacf47cd4.svg
          fullname: Julius Cheng
          isHf: false
          isPro: false
          name: juliuscheng
          type: user
        html: '<p>This answers my question, thank you for the great and prompt responses!</p>

          '
        raw: This answers my question, thank you for the great and prompt responses!
        updatedAt: '2023-03-29T08:30:37.265Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - loubnabnl
      relatedEventId: 6423f72d46fb3eac158e7ff1
    id: 6423f72d46fb3eac158e7ff0
    type: comment
  author: juliuscheng
  content: This answers my question, thank you for the great and prompt responses!
  created_at: 2023-03-29 07:30:37+00:00
  edited: false
  hidden: false
  id: 6423f72d46fb3eac158e7ff0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d6d34c2d49cb8dfdccc6681aacf47cd4.svg
      fullname: Julius Cheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: juliuscheng
      type: user
    createdAt: '2023-03-29T08:30:37.000Z'
    data:
      status: closed
    id: 6423f72d46fb3eac158e7ff1
    type: status-change
  author: juliuscheng
  created_at: 2023-03-29 07:30:37+00:00
  id: 6423f72d46fb3eac158e7ff1
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: bigcode/santacoder
repo_type: model
status: closed
target_branch: null
title: Prompting to reproduce MBPP test results
