!!python/object:huggingface_hub.community.DiscussionWithDetails
author: harithushan
conflicting_files: null
created_at: 2023-08-31 12:20:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YqMKUtAhfx9q5OxxLu5nB.jpeg?w=200&h=200&f=face
      fullname: Paramanathan Thushanthan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harithushan
      type: user
    createdAt: '2023-08-31T13:20:10.000Z'
    data:
      edited: false
      editors:
      - harithushan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.270550400018692
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YqMKUtAhfx9q5OxxLu5nB.jpeg?w=200&h=200&f=face
          fullname: Paramanathan Thushanthan
          isHf: false
          isPro: false
          name: harithushan
          type: user
        html: '<h5 id="is-there-any-way-to-load-this-model-for-qa-with-a-faiss-db-retriver">is
          there any way to load this model? for QA with a faiss db retriver</h5>

          <p>use_triton = False<br>model_file = "TheBloke/Llama-2-7B-Chat-GPTQ"</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_file , use_fast=True)<br>logging.info("Loaded
          Tokenizer")</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(model_file,<br>        use_safetensors=True,<br>        trust_remote_code=True,<br>        device="cuda:0",<br>        use_triton=use_triton<br>        )</p>

          <p>logging.info("Initialize model")<br>logging.info("*** Pipeline:")</p>

          <p>pipe = pipeline(<br>    "question-answering",<br>    model=model,<br>    tokenizer=tokenizer,<br>    max_new_tokens=256,<br>    temperature=0.1,<br>    top_p=0.95,<br>    repetition_penalty=1.15,<br>    do_sample
          =True,<br>    device_map= "auto"<br>    )</p>

          <p>llm = HuggingFacePipeline(pipeline=pipe)</p>

          <p>embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2",<br>                                    model_kwargs={''device'':
          ''cpu''})</p>

          <p>prompt = PromptTemplate(template=qa_template,<br>                        input_variables=[''context'',
          ''question''])</p>

          <p>answer = get_answer(question, llm, prompt, embeddings)</p>

          <p>The model ''LlamaGPTQForCausalLM'' is not supported for question-answering.
          Supported models are [''AlbertForQuestionAnswering'', ''BartForQuestionAnswering'',
          ''BertForQuestionAnswering'', ''BigBirdForQuestionAnswering'', ''BigBirdPegasusForQuestionAnswering'',
          ''BloomForQuestionAnswering'', ''CamembertForQuestionAnswering'', ''CanineForQuestionAnswering'',
          ''ConvBertForQuestionAnswering'', ''Data2VecTextForQuestionAnswering'',
          ''DebertaForQuestionAnswering'', ''DebertaV2ForQuestionAnswering'', ''DistilBertForQuestionAnswering'',
          ''ElectraForQuestionAnswering'', ''ErnieForQuestionAnswering'', ''ErnieMForQuestionAnswering'',
          ''FalconForQuestionAnswering'', ''FlaubertForQuestionAnsweringSimple'',
          ''FNetForQuestionAnswering'', ''FunnelForQuestionAnswering'', ''GPT2ForQuestionAnswering'',
          ''GPTNeoForQuestionAnswering'', ''GPTNeoXForQuestionAnswering'', ''GPTJForQuestionAnswering'',
          ''IBertForQuestionAnswering'', ''LayoutLMv2ForQuestionAnswering'', ''LayoutLMv3ForQuestionAnswering'',
          ''LEDForQuestionAnswering'', ''LiltForQuestionAnswering'', ''LongformerForQuestionAnswering'',
          ''LukeForQuestionAnswering'', ''LxmertForQuestionAnswering'', ''MarkupLMForQuestionAnswering'',
          ''MBartForQuestionAnswering'', ''MegaForQuestionAnswering'', ''MegatronBertForQuestionAnswering'',
          ''MobileBertForQuestionAnswering'', ''MPNetForQuestionAnswering'', ''MptForQuestionAnswering'',
          ''MraForQuestionAnswering'', ''MT5ForQuestionAnswering'', ''MvpForQuestionAnswering'',
          ''NezhaForQuestionAnswering'', ''NystromformerForQuestionAnswering'', ''OPTForQuestionAnswering'',
          ''QDQBertForQuestionAnswering'', ''ReformerForQuestionAnswering'', ''RemBertForQuestionAnswering'',
          ''RobertaForQuestionAnswering'', ''RobertaPreLayerNormForQuestionAnswering'',
          ''RoCBertForQuestionAnswering'', ''RoFormerForQuestionAnswering'', ''SplinterForQuestionAnswering'',
          ''SqueezeBertForQuestionAnswering'', ''T5ForQuestionAnswering'', ''UMT5ForQuestionAnswering'',
          ''XLMForQuestionAnsweringSimple'', ''XLMRobertaForQuestionAnswering'', ''XLMRobertaXLForQuestionAnswering'',
          ''XLNetForQuestionAnsweringSimple'', ''XmodForQuestionAnswering'', ''YosoForQuestionAnswering''].</p>

          '
        raw: "#####  is there any way to load this model? for QA with a faiss db retriver\r\
          \n\r\nuse_triton = False\r\nmodel_file = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\
          \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_file , use_fast=True)\r\
          \nlogging.info(\"Loaded Tokenizer\")\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_file,\r\
          \n        use_safetensors=True,\r\n        trust_remote_code=True,\r\n \
          \       device=\"cuda:0\",\r\n        use_triton=use_triton\r\n        )\r\
          \n\r\nlogging.info(\"Initialize model\")\r\nlogging.info(\"*** Pipeline:\"\
          )\r\n\r\npipe = pipeline(\r\n    \"question-answering\",\r\n    model=model,\r\
          \n    tokenizer=tokenizer,\r\n    max_new_tokens=256,\r\n    temperature=0.1,\r\
          \n    top_p=0.95,\r\n    repetition_penalty=1.15,\r\n    do_sample =True,\r\
          \n    device_map= \"auto\"\r\n    )\r\n\r\n\r\nllm = HuggingFacePipeline(pipeline=pipe)\r\
          \n\r\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\
          ,\r\n                                    model_kwargs={'device': 'cpu'})\r\
          \n\r\nprompt = PromptTemplate(template=qa_template,\r\n                \
          \        input_variables=['context', 'question'])\r\n\r\nanswer = get_answer(question,\
          \ llm, prompt, embeddings)\r\n\r\n\r\n\r\n\r\nThe model 'LlamaGPTQForCausalLM'\
          \ is not supported for question-answering. Supported models are ['AlbertForQuestionAnswering',\
          \ 'BartForQuestionAnswering', 'BertForQuestionAnswering', 'BigBirdForQuestionAnswering',\
          \ 'BigBirdPegasusForQuestionAnswering', 'BloomForQuestionAnswering', 'CamembertForQuestionAnswering',\
          \ 'CanineForQuestionAnswering', 'ConvBertForQuestionAnswering', 'Data2VecTextForQuestionAnswering',\
          \ 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'DistilBertForQuestionAnswering',\
          \ 'ElectraForQuestionAnswering', 'ErnieForQuestionAnswering', 'ErnieMForQuestionAnswering',\
          \ 'FalconForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FNetForQuestionAnswering',\
          \ 'FunnelForQuestionAnswering', 'GPT2ForQuestionAnswering', 'GPTNeoForQuestionAnswering',\
          \ 'GPTNeoXForQuestionAnswering', 'GPTJForQuestionAnswering', 'IBertForQuestionAnswering',\
          \ 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv3ForQuestionAnswering', 'LEDForQuestionAnswering',\
          \ 'LiltForQuestionAnswering', 'LongformerForQuestionAnswering', 'LukeForQuestionAnswering',\
          \ 'LxmertForQuestionAnswering', 'MarkupLMForQuestionAnswering', 'MBartForQuestionAnswering',\
          \ 'MegaForQuestionAnswering', 'MegatronBertForQuestionAnswering', 'MobileBertForQuestionAnswering',\
          \ 'MPNetForQuestionAnswering', 'MptForQuestionAnswering', 'MraForQuestionAnswering',\
          \ 'MT5ForQuestionAnswering', 'MvpForQuestionAnswering', 'NezhaForQuestionAnswering',\
          \ 'NystromformerForQuestionAnswering', 'OPTForQuestionAnswering', 'QDQBertForQuestionAnswering',\
          \ 'ReformerForQuestionAnswering', 'RemBertForQuestionAnswering', 'RobertaForQuestionAnswering',\
          \ 'RobertaPreLayerNormForQuestionAnswering', 'RoCBertForQuestionAnswering',\
          \ 'RoFormerForQuestionAnswering', 'SplinterForQuestionAnswering', 'SqueezeBertForQuestionAnswering',\
          \ 'T5ForQuestionAnswering', 'UMT5ForQuestionAnswering', 'XLMForQuestionAnsweringSimple',\
          \ 'XLMRobertaForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering',\
          \ 'XLNetForQuestionAnsweringSimple', 'XmodForQuestionAnswering', 'YosoForQuestionAnswering'].\r\
          \n"
        updatedAt: '2023-08-31T13:20:10.287Z'
      numEdits: 0
      reactions: []
    id: 64f0938a82f9f9b408374fe9
    type: comment
  author: harithushan
  content: "#####  is there any way to load this model? for QA with a faiss db retriver\r\
    \n\r\nuse_triton = False\r\nmodel_file = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\r\n\
    \r\ntokenizer = AutoTokenizer.from_pretrained(model_file , use_fast=True)\r\n\
    logging.info(\"Loaded Tokenizer\")\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_file,\r\
    \n        use_safetensors=True,\r\n        trust_remote_code=True,\r\n       \
    \ device=\"cuda:0\",\r\n        use_triton=use_triton\r\n        )\r\n\r\nlogging.info(\"\
    Initialize model\")\r\nlogging.info(\"*** Pipeline:\")\r\n\r\npipe = pipeline(\r\
    \n    \"question-answering\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
    \n    max_new_tokens=256,\r\n    temperature=0.1,\r\n    top_p=0.95,\r\n    repetition_penalty=1.15,\r\
    \n    do_sample =True,\r\n    device_map= \"auto\"\r\n    )\r\n\r\n\r\nllm = HuggingFacePipeline(pipeline=pipe)\r\
    \n\r\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\
    ,\r\n                                    model_kwargs={'device': 'cpu'})\r\n\r\
    \nprompt = PromptTemplate(template=qa_template,\r\n                        input_variables=['context',\
    \ 'question'])\r\n\r\nanswer = get_answer(question, llm, prompt, embeddings)\r\
    \n\r\n\r\n\r\n\r\nThe model 'LlamaGPTQForCausalLM' is not supported for question-answering.\
    \ Supported models are ['AlbertForQuestionAnswering', 'BartForQuestionAnswering',\
    \ 'BertForQuestionAnswering', 'BigBirdForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering',\
    \ 'BloomForQuestionAnswering', 'CamembertForQuestionAnswering', 'CanineForQuestionAnswering',\
    \ 'ConvBertForQuestionAnswering', 'Data2VecTextForQuestionAnswering', 'DebertaForQuestionAnswering',\
    \ 'DebertaV2ForQuestionAnswering', 'DistilBertForQuestionAnswering', 'ElectraForQuestionAnswering',\
    \ 'ErnieForQuestionAnswering', 'ErnieMForQuestionAnswering', 'FalconForQuestionAnswering',\
    \ 'FlaubertForQuestionAnsweringSimple', 'FNetForQuestionAnswering', 'FunnelForQuestionAnswering',\
    \ 'GPT2ForQuestionAnswering', 'GPTNeoForQuestionAnswering', 'GPTNeoXForQuestionAnswering',\
    \ 'GPTJForQuestionAnswering', 'IBertForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering',\
    \ 'LayoutLMv3ForQuestionAnswering', 'LEDForQuestionAnswering', 'LiltForQuestionAnswering',\
    \ 'LongformerForQuestionAnswering', 'LukeForQuestionAnswering', 'LxmertForQuestionAnswering',\
    \ 'MarkupLMForQuestionAnswering', 'MBartForQuestionAnswering', 'MegaForQuestionAnswering',\
    \ 'MegatronBertForQuestionAnswering', 'MobileBertForQuestionAnswering', 'MPNetForQuestionAnswering',\
    \ 'MptForQuestionAnswering', 'MraForQuestionAnswering', 'MT5ForQuestionAnswering',\
    \ 'MvpForQuestionAnswering', 'NezhaForQuestionAnswering', 'NystromformerForQuestionAnswering',\
    \ 'OPTForQuestionAnswering', 'QDQBertForQuestionAnswering', 'ReformerForQuestionAnswering',\
    \ 'RemBertForQuestionAnswering', 'RobertaForQuestionAnswering', 'RobertaPreLayerNormForQuestionAnswering',\
    \ 'RoCBertForQuestionAnswering', 'RoFormerForQuestionAnswering', 'SplinterForQuestionAnswering',\
    \ 'SqueezeBertForQuestionAnswering', 'T5ForQuestionAnswering', 'UMT5ForQuestionAnswering',\
    \ 'XLMForQuestionAnsweringSimple', 'XLMRobertaForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering',\
    \ 'XLNetForQuestionAnsweringSimple', 'XmodForQuestionAnswering', 'YosoForQuestionAnswering'].\r\
    \n"
  created_at: 2023-08-31 12:20:10+00:00
  edited: false
  hidden: false
  id: 64f0938a82f9f9b408374fe9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/772d99d0261b85626101f8534aff4ce8.svg
      fullname: Nicolas Iglesias
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: webpolis
      type: user
    createdAt: '2023-08-31T13:25:17.000Z'
    data:
      edited: false
      editors:
      - webpolis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9487633109092712
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/772d99d0261b85626101f8534aff4ce8.svg
          fullname: Nicolas Iglesias
          isHf: false
          isPro: false
          name: webpolis
          type: user
        html: '<p>Can you make sure to format your code when asking for help?</p>

          '
        raw: Can you make sure to format your code when asking for help?
        updatedAt: '2023-08-31T13:25:17.179Z'
      numEdits: 0
      reactions: []
    id: 64f094bd46fdb0c3c69b4525
    type: comment
  author: webpolis
  content: Can you make sure to format your code when asking for help?
  created_at: 2023-08-31 12:25:17+00:00
  edited: false
  hidden: false
  id: 64f094bd46fdb0c3c69b4525
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: TheBloke/Llama-2-7B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: how  to  load the GPTQ model using any pipeline method
