!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aarnphm
conflicting_files: []
created_at: 2023-09-04 16:38:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656529339923-609b62a5f7c1303cfd1760f8.jpeg?w=200&h=200&f=face
      fullname: Aaron Pham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aarnphm
      type: user
    createdAt: '2023-09-04T17:38:13.000Z'
    data:
      edited: true
      editors:
      - aarnphm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7088046669960022
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656529339923-609b62a5f7c1303cfd1760f8.jpeg?w=200&h=200&f=face
          fullname: Aaron Pham
          isHf: false
          isPro: false
          name: aarnphm
          type: user
        html: '<p>It seems like <code>optimum.gptq.load_quantized_model</code> loads
          quantization_config from quantization_config.json</p>

          '
        raw: It seems like `optimum.gptq.load_quantized_model` loads quantization_config
          from quantization_config.json
        updatedAt: '2023-09-04T17:39:12.390Z'
      numEdits: 1
      reactions: []
    id: 64f61605a8bdfe3eb2721b14
    type: comment
  author: aarnphm
  content: It seems like `optimum.gptq.load_quantized_model` loads quantization_config
    from quantization_config.json
  created_at: 2023-09-04 16:38:13+00:00
  edited: true
  hidden: false
  id: 64f61605a8bdfe3eb2721b14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656529339923-609b62a5f7c1303cfd1760f8.jpeg?w=200&h=200&f=face
      fullname: Aaron Pham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aarnphm
      type: user
    createdAt: '2023-09-04T17:38:14.000Z'
    data:
      oid: 55e6dd085b40a7006c8aec946b8b039ada2be538
      parents:
      - 35469a667a7e1197dba6fc898c4c368c74a9b8ce
      subject: Rename quantize_config.json to quantization_config.json
    id: 64f616060000000000000000
    type: commit
  author: aarnphm
  created_at: 2023-09-04 16:38:14+00:00
  id: 64f616060000000000000000
  oid: 55e6dd085b40a7006c8aec946b8b039ada2be538
  summary: Rename quantize_config.json to quantization_config.json
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656529339923-609b62a5f7c1303cfd1760f8.jpeg?w=200&h=200&f=face
      fullname: Aaron Pham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aarnphm
      type: user
    createdAt: '2023-09-04T17:38:41.000Z'
    data:
      edited: false
      editors:
      - aarnphm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5107676982879639
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656529339923-609b62a5f7c1303cfd1760f8.jpeg?w=200&h=200&f=face
          fullname: Aaron Pham
          isHf: false
          isPro: false
          name: aarnphm
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/609b62a5f7c1303cfd1760f8/hN_XC0fKEBMcSHZhRKRPY.png"><img
          alt="Screenshot 2023-09-04 at 13.38.33.png" src="https://cdn-uploads.huggingface.co/production/uploads/609b62a5f7c1303cfd1760f8/hN_XC0fKEBMcSHZhRKRPY.png"></a></p>

          '
        raw: '![Screenshot 2023-09-04 at 13.38.33.png](https://cdn-uploads.huggingface.co/production/uploads/609b62a5f7c1303cfd1760f8/hN_XC0fKEBMcSHZhRKRPY.png)

          '
        updatedAt: '2023-09-04T17:38:41.307Z'
      numEdits: 0
      reactions: []
    id: 64f616215207c759d08a6a8e
    type: comment
  author: aarnphm
  content: '![Screenshot 2023-09-04 at 13.38.33.png](https://cdn-uploads.huggingface.co/production/uploads/609b62a5f7c1303cfd1760f8/hN_XC0fKEBMcSHZhRKRPY.png)

    '
  created_at: 2023-09-04 16:38:41+00:00
  edited: false
  hidden: false
  id: 64f616215207c759d08a6a8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656529339923-609b62a5f7c1303cfd1760f8.jpeg?w=200&h=200&f=face
      fullname: Aaron Pham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aarnphm
      type: user
    createdAt: '2023-09-04T17:39:44.000Z'
    data:
      edited: false
      editors:
      - aarnphm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41213399171829224
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656529339923-609b62a5f7c1303cfd1760f8.jpeg?w=200&h=200&f=face
          fullname: Aaron Pham
          isHf: false
          isPro: false
          name: aarnphm
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'cc @TheBloke '
        updatedAt: '2023-09-04T17:39:44.740Z'
      numEdits: 0
      reactions: []
    id: 64f61660a88ce22a1023e716
    type: comment
  author: aarnphm
  content: 'cc @TheBloke '
  created_at: 2023-09-04 16:39:44+00:00
  edited: false
  hidden: false
  id: 64f61660a88ce22a1023e716
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-04T18:50:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7754924893379211
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>No it loads it from config.json: <a href="https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ/blob/main/config.json#L23-L32">https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ/blob/main/config.json#L23-L32</a></p>

          <p>quantize_config.json is for AutoGPTQ.  The files have been tested with
          Transformers and Optimum and are fine.</p>

          '
        raw: 'No it loads it from config.json: https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ/blob/main/config.json#L23-L32


          quantize_config.json is for AutoGPTQ.  The files have been tested with Transformers
          and Optimum and are fine.'
        updatedAt: '2023-09-04T18:50:59.108Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64f627137cff6e8fe02f95a4
    id: 64f627137cff6e8fe02f95a3
    type: comment
  author: TheBloke
  content: 'No it loads it from config.json: https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ/blob/main/config.json#L23-L32


    quantize_config.json is for AutoGPTQ.  The files have been tested with Transformers
    and Optimum and are fine.'
  created_at: 2023-09-04 17:50:59+00:00
  edited: false
  hidden: false
  id: 64f627137cff6e8fe02f95a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-04T18:50:59.000Z'
    data:
      status: closed
    id: 64f627137cff6e8fe02f95a4
    type: status-change
  author: TheBloke
  created_at: 2023-09-04 17:50:59+00:00
  id: 64f627137cff6e8fe02f95a4
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656529339923-609b62a5f7c1303cfd1760f8.jpeg?w=200&h=200&f=face
      fullname: Aaron Pham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aarnphm
      type: user
    createdAt: '2023-09-07T01:31:20.000Z'
    data:
      edited: false
      editors:
      - aarnphm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4927394390106201
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656529339923-609b62a5f7c1303cfd1760f8.jpeg?w=200&h=200&f=face
          fullname: Aaron Pham
          isHf: false
          isPro: false
          name: aarnphm
          type: user
        html: "<p>Hmm I don't think you can load this from a custom path. The path\
          \ for loading the model into memory is fine, but then <code>model.save_pretrained()</code>\
          \ to a path, the following:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">from</span> accelerate <span class=\"hljs-keyword\"\
          >import</span> init_empty_weights\n<span class=\"hljs-keyword\">from</span>\
          \ optimum.gptq <span class=\"hljs-keyword\">import</span> load_quantized_model\n\
          <span class=\"hljs-comment\"># disable exllama if gptq is loaded on CPU</span>\n\
          disable_exllama = <span class=\"hljs-keyword\">not</span> torch.cuda.is_available()\n\
          <span class=\"hljs-keyword\">with</span> init_empty_weights():\n  empty\
          \ = auto_class.from_pretrained(llm.model_id, torch_dtype=torch.float16 <span\
          \ class=\"hljs-keyword\">if</span> torch.cuda.is_available() <span class=\"\
          hljs-keyword\">else</span> torch.float32, device_map=<span class=\"hljs-string\"\
          >'auto'</span>)\nempty.tie_weights()\nmodel = load_quantized_model(empty,\
          \ save_folder=<span class=\"hljs-string\">\"/path/to/saved\"</span>, device_map=<span\
          \ class=\"hljs-string\">'auto'</span>, disable_exllama=disable_exllama)\n\
          </code></pre>\n<p>runs into the following issue:</p>\n<pre><code class=\"\
          language-prolog\">    model = load_quantized_model(empty, save_folder=<span\
          \ class=\"hljs-string\">\"/home/ubuntu/gptq-13b-local\"</span>, device_map=<span\
          \ class=\"hljs-string\">'auto'</span>, disable_exllama=disable_exllama)\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  <span class=\"hljs-symbol\">File</span> <span class=\"hljs-string\">\"\
          /home/ubuntu/.pyenv/versions/3.11.4/lib/python3.11/site-packages/optimum/gptq/quantizer.py\"\
          </span>, line <span class=\"hljs-number\">614</span>, in load_quantized_model\n\
          \    with open(os.path.join(save_folder, quant_config_name), <span class=\"\
          hljs-string\">\"r\"</span>, encoding=<span class=\"hljs-string\">\"utf-8\"\
          </span>) as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          <span class=\"hljs-symbol\">FileNotFoundError</span>: [<span class=\"hljs-symbol\"\
          >Errno</span> <span class=\"hljs-number\">2</span>] <span class=\"hljs-symbol\"\
          >No</span> such file or directory: <span class=\"hljs-string\">'/home/ubuntu/gptq-13b-local/quantization_config.json'</span>\n\
          </code></pre>\n<p>While I do believe this should also be fixed on optimum's\
          \ <code>load_quantized_model</code> to check config.json, idk the release\
          \ schedule from the optimum team so would be nice to also have a quantization_config.json</p>\n"
        raw: "Hmm I don't think you can load this from a custom path. The path for\
          \ loading the model into memory is fine, but then `model.save_pretrained()`\
          \ to a path, the following:\n```python\nfrom accelerate import init_empty_weights\n\
          from optimum.gptq import load_quantized_model\n# disable exllama if gptq\
          \ is loaded on CPU\ndisable_exllama = not torch.cuda.is_available()\nwith\
          \ init_empty_weights():\n  empty = auto_class.from_pretrained(llm.model_id,\
          \ torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\
          \ device_map='auto')\nempty.tie_weights()\nmodel = load_quantized_model(empty,\
          \ save_folder=\"/path/to/saved\", device_map='auto', disable_exllama=disable_exllama)\n\
          ```\n\nruns into the following issue:\n```prolog\n    model = load_quantized_model(empty,\
          \ save_folder=\"/home/ubuntu/gptq-13b-local\", device_map='auto', disable_exllama=disable_exllama)\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/ubuntu/.pyenv/versions/3.11.4/lib/python3.11/site-packages/optimum/gptq/quantizer.py\"\
          , line 614, in load_quantized_model\n    with open(os.path.join(save_folder,\
          \ quant_config_name), \"r\", encoding=\"utf-8\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/gptq-13b-local/quantization_config.json'\n\
          ```\n\nWhile I do believe this should also be fixed on optimum's `load_quantized_model`\
          \ to check config.json, idk the release schedule from the optimum team so\
          \ would be nice to also have a quantization_config.json"
        updatedAt: '2023-09-07T01:31:20.187Z'
      numEdits: 0
      reactions: []
    id: 64f927e820ca770b6ecd58f4
    type: comment
  author: aarnphm
  content: "Hmm I don't think you can load this from a custom path. The path for loading\
    \ the model into memory is fine, but then `model.save_pretrained()` to a path,\
    \ the following:\n```python\nfrom accelerate import init_empty_weights\nfrom optimum.gptq\
    \ import load_quantized_model\n# disable exllama if gptq is loaded on CPU\ndisable_exllama\
    \ = not torch.cuda.is_available()\nwith init_empty_weights():\n  empty = auto_class.from_pretrained(llm.model_id,\
    \ torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, device_map='auto')\n\
    empty.tie_weights()\nmodel = load_quantized_model(empty, save_folder=\"/path/to/saved\"\
    , device_map='auto', disable_exllama=disable_exllama)\n```\n\nruns into the following\
    \ issue:\n```prolog\n    model = load_quantized_model(empty, save_folder=\"/home/ubuntu/gptq-13b-local\"\
    , device_map='auto', disable_exllama=disable_exllama)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/ubuntu/.pyenv/versions/3.11.4/lib/python3.11/site-packages/optimum/gptq/quantizer.py\"\
    , line 614, in load_quantized_model\n    with open(os.path.join(save_folder, quant_config_name),\
    \ \"r\", encoding=\"utf-8\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/gptq-13b-local/quantization_config.json'\n\
    ```\n\nWhile I do believe this should also be fixed on optimum's `load_quantized_model`\
    \ to check config.json, idk the release schedule from the optimum team so would\
    \ be nice to also have a quantization_config.json"
  created_at: 2023-09-07 00:31:20+00:00
  edited: false
  hidden: false
  id: 64f927e820ca770b6ecd58f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-07T07:08:52.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9660558700561523
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Could you raise this as an issue on the Optimum Github. They''re
          doing a release soon to fix another issue related to GPTQ so maybe they''ll
          look at this soon, or have already fixed it </p>

          '
        raw: 'Could you raise this as an issue on the Optimum Github. They''re doing
          a release soon to fix another issue related to GPTQ so maybe they''ll look
          at this soon, or have already fixed it '
        updatedAt: '2023-09-07T07:09:13.707Z'
      numEdits: 1
      reactions: []
    id: 64f97704baa3b4ec4e751137
    type: comment
  author: TheBloke
  content: 'Could you raise this as an issue on the Optimum Github. They''re doing
    a release soon to fix another issue related to GPTQ so maybe they''ll look at
    this soon, or have already fixed it '
  created_at: 2023-09-07 06:08:52+00:00
  edited: true
  hidden: false
  id: 64f97704baa3b4ec4e751137
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656529339923-609b62a5f7c1303cfd1760f8.jpeg?w=200&h=200&f=face
      fullname: Aaron Pham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aarnphm
      type: user
    createdAt: '2023-09-07T14:25:09.000Z'
    data:
      edited: false
      editors:
      - aarnphm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9915966391563416
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656529339923-609b62a5f7c1303cfd1760f8.jpeg?w=200&h=200&f=face
          fullname: Aaron Pham
          isHf: false
          isPro: false
          name: aarnphm
          type: user
        html: '<p>Yes I have sent them a issue wrt to this</p>

          '
        raw: Yes I have sent them a issue wrt to this
        updatedAt: '2023-09-07T14:25:09.253Z'
      numEdits: 0
      reactions: []
    id: 64f9dd45b550b098b3fd10ee
    type: comment
  author: aarnphm
  content: Yes I have sent them a issue wrt to this
  created_at: 2023-09-07 13:25:09+00:00
  edited: false
  hidden: false
  id: 64f9dd45b550b098b3fd10ee
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 19
repo_id: TheBloke/Llama-2-7B-Chat-GPTQ
repo_type: model
status: closed
target_branch: refs/heads/main
title: Rename quantize_config.json to quantization_config.json
