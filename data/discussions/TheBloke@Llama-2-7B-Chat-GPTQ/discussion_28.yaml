!!python/object:huggingface_hub.community.DiscussionWithDetails
author: notmax123
conflicting_files: null
created_at: 2023-11-16 17:21:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63453ab89ad67b3d069effdf/gUqqoYO43PQch0iQZ4IGL.jpeg?w=200&h=200&f=face
      fullname: Maxim Melichov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: notmax123
      type: user
    createdAt: '2023-11-16T17:21:02.000Z'
    data:
      edited: false
      editors:
      - notmax123
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5116441249847412
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63453ab89ad67b3d069effdf/gUqqoYO43PQch0iQZ4IGL.jpeg?w=200&h=200&f=face
          fullname: Maxim Melichov
          isHf: false
          isPro: false
          name: notmax123
          type: user
        html: "<p>When I'm running the model it prints the next warning:\" This is\
          \ a friendly reminder - the current text generation call will exceed the\
          \ model's predefined maximum length (4096). Depending on the model, you\
          \ may observe exceptions, performance degradation, or nothing at all\"<br>and\
          \ the result that I get is garbage  :\"  \u2022 \u2022          \u2022 \
          \ \u2022    \u2022  \u2022                             \u2022          \
          \  \\n \\n\u2022  \u2022    \u2022                          \u2022    \u2022\
          \ ................ \u2022  \u25C4 \u2022  \xDC \u2022 \u2022 \u2022  ...\
          \ \"<br>How to Address and Improve Results?<br>This is my code:</p>\n<pre><code>import\
          \ torch\nfrom langchain import HuggingFacePipeline\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\nfrom\
          \ auto_gptq import AutoGPTQForCausalLM\nMODEL_NAME = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\
          \n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(\n    MODEL_NAME, torch_dtype=torch.float16,\
          \ trust_remote_code=True, device_map=\"auto\"\n)\n\ngeneration_config =\
          \ GenerationConfig.from_pretrained(MODEL_NAME)\ngeneration_config.max_new_tokens\
          \ = 1024\ngeneration_config.temperature = 0.0001\ngeneration_config.top_p\
          \ = 0.95\ngeneration_config.do_sample = True\ngeneration_config.repetition_penalty\
          \ = 1.15\n\ntext_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    return_full_text=True,\n    generation_config=generation_config,\n\
          )\n\nllm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"\
          temperature\": 0})\n</code></pre>\n"
        raw: "When I'm running the model it prints the next warning:\" This is a friendly\
          \ reminder - the current text generation call will exceed the model's predefined\
          \ maximum length (4096). Depending on the model, you may observe exceptions,\
          \ performance degradation, or nothing at all\"\r\nand the result that I\
          \ get is garbage  :\"  \u2022 \u2022          \u2022  \u2022    \u2022 \
          \ \u2022                             \u2022            \\n \\n\u2022  \u2022\
          \    \u2022                          \u2022    \u2022 ................ \u2022\
          \  \u25C4 \u2022  \xDC \u2022 \u2022 \u2022  ... \"\r\nHow to Address and\
          \ Improve Results?\r\nThis is my code:\r\n```\r\nimport torch\r\nfrom langchain\
          \ import HuggingFacePipeline\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer, GenerationConfig, pipeline\r\nfrom auto_gptq import AutoGPTQForCausalLM\r\
          \nMODEL_NAME = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,\
          \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(\r\n\
          \    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"\
          auto\"\r\n)\r\n\r\ngeneration_config = GenerationConfig.from_pretrained(MODEL_NAME)\r\
          \ngeneration_config.max_new_tokens = 1024\r\ngeneration_config.temperature\
          \ = 0.0001\r\ngeneration_config.top_p = 0.95\r\ngeneration_config.do_sample\
          \ = True\r\ngeneration_config.repetition_penalty = 1.15\r\n\r\ntext_pipeline\
          \ = pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    return_full_text=True,\r\n    generation_config=generation_config,\r\
          \n)\r\n\r\nllm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"\
          temperature\": 0})\r\n```\r\n"
        updatedAt: '2023-11-16T17:21:02.573Z'
      numEdits: 0
      reactions: []
    id: 65564f7e66423b57b2cab607
    type: comment
  author: notmax123
  content: "When I'm running the model it prints the next warning:\" This is a friendly\
    \ reminder - the current text generation call will exceed the model's predefined\
    \ maximum length (4096). Depending on the model, you may observe exceptions, performance\
    \ degradation, or nothing at all\"\r\nand the result that I get is garbage  :\"\
    \  \u2022 \u2022          \u2022  \u2022    \u2022  \u2022                   \
    \          \u2022            \\n \\n\u2022  \u2022    \u2022                 \
    \         \u2022    \u2022 ................ \u2022  \u25C4 \u2022  \xDC \u2022\
    \ \u2022 \u2022  ... \"\r\nHow to Address and Improve Results?\r\nThis is my code:\r\
    \n```\r\nimport torch\r\nfrom langchain import HuggingFacePipeline\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\r\nfrom\
    \ auto_gptq import AutoGPTQForCausalLM\r\nMODEL_NAME = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\
    \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\r\
    \n\r\nmodel = AutoGPTQForCausalLM.from_quantized(\r\n    MODEL_NAME, torch_dtype=torch.float16,\
    \ trust_remote_code=True, device_map=\"auto\"\r\n)\r\n\r\ngeneration_config =\
    \ GenerationConfig.from_pretrained(MODEL_NAME)\r\ngeneration_config.max_new_tokens\
    \ = 1024\r\ngeneration_config.temperature = 0.0001\r\ngeneration_config.top_p\
    \ = 0.95\r\ngeneration_config.do_sample = True\r\ngeneration_config.repetition_penalty\
    \ = 1.15\r\n\r\ntext_pipeline = pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
    \n    tokenizer=tokenizer,\r\n    return_full_text=True,\r\n    generation_config=generation_config,\r\
    \n)\r\n\r\nllm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"\
    temperature\": 0})\r\n```\r\n"
  created_at: 2023-11-16 17:21:02+00:00
  edited: false
  hidden: false
  id: 65564f7e66423b57b2cab607
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: TheBloke/Llama-2-7B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: How to overcoming bad output for better results?
