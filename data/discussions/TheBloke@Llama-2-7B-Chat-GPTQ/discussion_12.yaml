!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tongwuhugging
conflicting_files: null
created_at: 2023-08-14 15:36:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1a8e9d15356e8f09cb009d567e51595.svg
      fullname: Tong Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tongwuhugging
      type: user
    createdAt: '2023-08-14T16:36:07.000Z'
    data:
      edited: false
      editors:
      - tongwuhugging
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4556760787963867
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1a8e9d15356e8f09cb009d567e51595.svg
          fullname: Tong Wu
          isHf: false
          isPro: false
          name: tongwuhugging
          type: user
        html: "<h1 id=\"hi-thanks-for-releasing-this-model-im-using-text-generation-web-ui-to-fine-tune-a-lora-the-model-loader-is-autogptq-ive-formatted-the-dataset-in-alpaca-format-however-it-gets-this-error\"\
          >Hi, thanks for releasing this model. I'm using text generation web-ui to\
          \ fine-tune a LoRA. The model loader is AutoGPTQ. I've formatted the dataset\
          \ in Alpaca format. However, it gets this error:</h1>\n<p>2023-08-14 09:51:01\
          \ INFO:Loading TheBloke_Llama-2-7b-Chat-GPTQ...<br>2023-08-14 09:51:02 INFO:The\
          \ AutoGPTQ params are: {'model_basename': 'gptq_model-4bit-128g', 'device':\
          \ 'cuda:0', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp':\
          \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory':\
          \ {0: '30000MiB', 1: '30000MiB', 'cpu': '2000MiB'}, 'quantize_config': None,\
          \ 'use_cuda_fp16': True}<br>2023-08-14 09:51:02 WARNING:CUDA extension not\
          \ installed.<br>2023-08-14 09:51:03 WARNING:The safetensors archive passed\
          \ at models/TheBloke_Llama-2-7b-Chat-GPTQ/gptq_model-4bit-128g.safetensors\
          \ does not contain metadata. Make sure to save your model with the <code>save_pretrained</code>\
          \ method. Defaulting to 'pt' metadata.<br>2023-08-14 09:51:10 WARNING:skip\
          \ module injection for FusedLlamaMLPForQuantizedModel not support integrate\
          \ without triton yet.<br>2023-08-14 09:51:10 INFO:Loaded the model in 8.85\
          \ seconds.</p>\n<p>2023-08-14 09:51:10 INFO:Loading the extension \"gallery\"\
          ...<br>Running on local URL:  <a rel=\"nofollow\" href=\"http://0.0.0.0:7860\"\
          >http://0.0.0.0:7860</a></p>\n<p>To create a public link, set <code>share=True</code>\
          \ in <code>launch()</code>.<br>2023-08-14 09:52:55 WARNING:LoRA training\
          \ has only currently been validated for LLaMA, OPT, GPT-J, and GPT-NeoX\
          \ models. (Found model type: LlamaGPTQForCausalLM)<br>2023-08-14 09:53:00\
          \ INFO:Loading JSON datasets...<br>Map: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 18980/18980 [00:16&lt;00:00, 1163.44 examples/s]<br>2023-08-14\
          \ 09:53:17 INFO:Getting model ready...<br>/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/peft/utils/other.py:102:\
          \ FutureWarning: prepare_model_for_int8_training is deprecated and will\
          \ be removed in a future version. Use prepare_model_for_kbit_training instead.<br>\
          \  warnings.warn(<br>2023-08-14 09:53:17 INFO:Prepping for training...<br>2023-08-14\
          \ 09:53:17 INFO:Backing up existing LoRA adapter...</p>\n<ul>\n<li>Backup\
          \ already exists. Skipping backup process.<br>2023-08-14 09:53:17 INFO:Creating\
          \ LoRA model...<br>2023-08-14 09:53:17 INFO:Loading existing LoRA data...<br>2023-08-14\
          \ 09:53:17 INFO:Starting training...<br>Training 'llama' model using (q,\
          \ v) projections<br>Trainable params: 16,777,216 (1.5391 %), All params:\
          \ 1,090,048,000 (Model: 1,073,270,784)<br>2023-08-14 09:53:17 INFO:Log file\
          \ 'train_dataset_sample.json' created in the 'logs' directory.<br>wandb:\
          \ Tracking run with wandb version 0.15.7<br>wandb: W&amp;B syncing is set\
          \ to <code>offline</code> in this directory.<br>wandb: Run <code>wandb online</code>\
          \ or set WANDB_MODE=online to enable cloud syncing.<br>Exception in thread\
          \ Thread-3 (threaded_run):<br>Traceback (most recent call last):<br>  File\
          \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/threading.py\"\
          , line 1016, in _bootstrap_inner<br> self.run()<br>  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/threading.py\"\
          , line 953, in run<br> self._target(*self._args, **self._kwargs)<br>  File\
          \ \"/data/01/dv/data/ve2/dtl/ecp1/phi/no_gbd/r000/work/llama-2/text-generation-webui/modules/training.py\"\
          , line 665, in threaded_run<br> trainer.train()<br>  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1539, in train<br> return inner_training_loop(<br>  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1809, in _inner_training_loop<br> tr_loss_step = self.training_step(model,\
          \ inputs)<br>  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2654, in training_step<br> loss = self.compute_loss(model, inputs)<br>\
          \  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2679, in compute_loss<br> outputs = model(**inputs)<br>  File \"\
          /local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br> return forward_call(*args, **kwargs)<br>\
          \  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/operations.py\"\
          , line 581, in forward<br> return model_forward(*args, **kwargs)<br>  File\
          \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/operations.py\"\
          , line 569, in <strong>call</strong><br> return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))<br>  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/amp/autocast_mode.py\"\
          , line 14, in decorate_autocast<br> return func(*args, **kwargs)<br>  File\
          \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/peft/peft_model.py\"\
          , line 947, in forward<br> return self.base_model(<br>  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br> return forward_call(*args, **kwargs)<br>\
          \  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          , line 433, in forward<br> return self.model(*args, **kwargs)<br>  File\
          \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br> return forward_call(*args, **kwargs)<br>\
          \  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 806, in forward<br> outputs = self.model(<br>  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br> return forward_call(*args, **kwargs)<br>\
          \  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 693, in forward<br> layer_outputs = decoder_layer(<br>  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br> return forward_call(*args, **kwargs)<br>\
          \  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br> output = old_forward(*args, **kwargs)<br>\
          \  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 408, in forward<br> hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(<br>  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br> return forward_call(*args, **kwargs)<br>\
          \  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py\"\
          , line 53, in forward<br> qkv_states = self.qkv_proj(hidden_states)<br>\
          \  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br> return forward_call(*args, **kwargs)<br>\
          \  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/peft/tuners/lora.py\"\
          , line 840, in forward<br> result = F.linear(x, transpose(self.weight, self.fan_in_fan_out),\
          \ bias=self.bias)<br>RuntimeError: self and mat2 must have the same dtype<br>2023-08-14\
          \ 09:53:26 INFO:Training complete, saving...<br>2023-08-14 09:53:26 INFO:Training\
          \ complete!</li>\n</ul>\n<h1 id=\"any-advice-and-help-is-appreciated\">Any\
          \ advice and help is appreciated!</h1>\n"
        raw: "# Hi, thanks for releasing this model. I'm using text generation web-ui\
          \ to fine-tune a LoRA. The model loader is AutoGPTQ. I've formatted the\
          \ dataset in Alpaca format. However, it gets this error:\r\n\r\n\r\n2023-08-14\
          \ 09:51:01 INFO:Loading TheBloke_Llama-2-7b-Chat-GPTQ...\r\n2023-08-14 09:51:02\
          \ INFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit-128g',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': {0: '30000MiB', 1: '30000MiB', 'cpu': '2000MiB'},\
          \ 'quantize_config': None, 'use_cuda_fp16': True}\r\n2023-08-14 09:51:02\
          \ WARNING:CUDA extension not installed.\r\n2023-08-14 09:51:03 WARNING:The\
          \ safetensors archive passed at models/TheBloke_Llama-2-7b-Chat-GPTQ/gptq_model-4bit-128g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\r\n2023-08-14 09:51:10 WARNING:skip\
          \ module injection for FusedLlamaMLPForQuantizedModel not support integrate\
          \ without triton yet.             \r\n2023-08-14 09:51:10 INFO:Loaded the\
          \ model in 8.85 seconds.\r\n\r\n2023-08-14 09:51:10 INFO:Loading the extension\
          \ \"gallery\"...\r\nRunning on local URL:  http://0.0.0.0:7860\r\n\r\nTo\
          \ create a public link, set `share=True` in `launch()`.\r\n2023-08-14 09:52:55\
          \ WARNING:LoRA training has only currently been validated for LLaMA, OPT,\
          \ GPT-J, and GPT-NeoX models. (Found model type: LlamaGPTQForCausalLM)\r\
          \n2023-08-14 09:53:00 INFO:Loading JSON datasets...\r\nMap: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18980/18980 [00:16<00:00,\
          \ 1163.44 examples/s]\r\n2023-08-14 09:53:17 INFO:Getting model ready...\r\
          \n/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/peft/utils/other.py:102:\
          \ FutureWarning: prepare_model_for_int8_training is deprecated and will\
          \ be removed in a future version. Use prepare_model_for_kbit_training instead.\r\
          \n  warnings.warn(\r\n2023-08-14 09:53:17 INFO:Prepping for training...\r\
          \n2023-08-14 09:53:17 INFO:Backing up existing LoRA adapter...\r\n - Backup\
          \ already exists. Skipping backup process.\r\n2023-08-14 09:53:17 INFO:Creating\
          \ LoRA model...\r\n2023-08-14 09:53:17 INFO:Loading existing LoRA data...\r\
          \n2023-08-14 09:53:17 INFO:Starting training...\r\nTraining 'llama' model\
          \ using (q, v) projections\r\nTrainable params: 16,777,216 (1.5391 %), All\
          \ params: 1,090,048,000 (Model: 1,073,270,784)\r\n2023-08-14 09:53:17 INFO:Log\
          \ file 'train_dataset_sample.json' created in the 'logs' directory.\r\n\
          wandb: Tracking run with wandb version 0.15.7\r\nwandb: W&B syncing is set\
          \ to `offline` in this directory.  \r\nwandb: Run `wandb online` or set\
          \ WANDB_MODE=online to enable cloud syncing.\r\nException in thread Thread-3\
          \ (threaded_run):\r\nTraceback (most recent call last):\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/threading.py\"\
          , line 1016, in _bootstrap_inner\r\n    self.run()\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/threading.py\"\
          , line 953, in run\r\n    self._target(*self._args, **self._kwargs)\r\n\
          \  File \"/data/01/dv/data/ve2/dtl/ecp1/phi/no_gbd/r000/work/llama-2/text-generation-webui/modules/training.py\"\
          , line 665, in threaded_run\r\n    trainer.train()\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model,\
          \ inputs)\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\
          \n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File\
          \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/operations.py\"\
          , line 581, in forward\r\n    return model_forward(*args, **kwargs)\r\n\
          \  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/operations.py\"\
          , line 569, in __call__\r\n    return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/amp/autocast_mode.py\"\
          , line 14, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/peft/peft_model.py\"\
          , line 947, in forward\r\n    return self.base_model(\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          , line 433, in forward\r\n    return self.model(*args, **kwargs)\r\n  File\
          \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 806, in forward\r\n    outputs = self.model(\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 693, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"\
          /local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 408, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py\"\
          , line 53, in forward\r\n    qkv_states = self.qkv_proj(hidden_states)\r\
          \n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/peft/tuners/lora.py\"\
          , line 840, in forward\r\n    result = F.linear(x, transpose(self.weight,\
          \ self.fan_in_fan_out), bias=self.bias)\r\nRuntimeError: self and mat2 must\
          \ have the same dtype\r\n2023-08-14 09:53:26 INFO:Training complete, saving...\r\
          \n2023-08-14 09:53:26 INFO:Training complete!\r\n\r\n# Any advice and help\
          \ is appreciated!"
        updatedAt: '2023-08-14T16:36:07.872Z'
      numEdits: 0
      reactions: []
    id: 64da57f7888b7e9c40fdf8ee
    type: comment
  author: tongwuhugging
  content: "# Hi, thanks for releasing this model. I'm using text generation web-ui\
    \ to fine-tune a LoRA. The model loader is AutoGPTQ. I've formatted the dataset\
    \ in Alpaca format. However, it gets this error:\r\n\r\n\r\n2023-08-14 09:51:01\
    \ INFO:Loading TheBloke_Llama-2-7b-Chat-GPTQ...\r\n2023-08-14 09:51:02 INFO:The\
    \ AutoGPTQ params are: {'model_basename': 'gptq_model-4bit-128g', 'device': 'cuda:0',\
    \ 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp': True,\
    \ 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': {0: '30000MiB',\
    \ 1: '30000MiB', 'cpu': '2000MiB'}, 'quantize_config': None, 'use_cuda_fp16':\
    \ True}\r\n2023-08-14 09:51:02 WARNING:CUDA extension not installed.\r\n2023-08-14\
    \ 09:51:03 WARNING:The safetensors archive passed at models/TheBloke_Llama-2-7b-Chat-GPTQ/gptq_model-4bit-128g.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\r\n2023-08-14 09:51:10 WARNING:skip module\
    \ injection for FusedLlamaMLPForQuantizedModel not support integrate without triton\
    \ yet.             \r\n2023-08-14 09:51:10 INFO:Loaded the model in 8.85 seconds.\r\
    \n\r\n2023-08-14 09:51:10 INFO:Loading the extension \"gallery\"...\r\nRunning\
    \ on local URL:  http://0.0.0.0:7860\r\n\r\nTo create a public link, set `share=True`\
    \ in `launch()`.\r\n2023-08-14 09:52:55 WARNING:LoRA training has only currently\
    \ been validated for LLaMA, OPT, GPT-J, and GPT-NeoX models. (Found model type:\
    \ LlamaGPTQForCausalLM)\r\n2023-08-14 09:53:00 INFO:Loading JSON datasets...\r\
    \nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 18980/18980 [00:16<00:00, 1163.44 examples/s]\r\
    \n2023-08-14 09:53:17 INFO:Getting model ready...\r\n/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/peft/utils/other.py:102:\
    \ FutureWarning: prepare_model_for_int8_training is deprecated and will be removed\
    \ in a future version. Use prepare_model_for_kbit_training instead.\r\n  warnings.warn(\r\
    \n2023-08-14 09:53:17 INFO:Prepping for training...\r\n2023-08-14 09:53:17 INFO:Backing\
    \ up existing LoRA adapter...\r\n - Backup already exists. Skipping backup process.\r\
    \n2023-08-14 09:53:17 INFO:Creating LoRA model...\r\n2023-08-14 09:53:17 INFO:Loading\
    \ existing LoRA data...\r\n2023-08-14 09:53:17 INFO:Starting training...\r\nTraining\
    \ 'llama' model using (q, v) projections\r\nTrainable params: 16,777,216 (1.5391\
    \ %), All params: 1,090,048,000 (Model: 1,073,270,784)\r\n2023-08-14 09:53:17\
    \ INFO:Log file 'train_dataset_sample.json' created in the 'logs' directory.\r\
    \nwandb: Tracking run with wandb version 0.15.7\r\nwandb: W&B syncing is set to\
    \ `offline` in this directory.  \r\nwandb: Run `wandb online` or set WANDB_MODE=online\
    \ to enable cloud syncing.\r\nException in thread Thread-3 (threaded_run):\r\n\
    Traceback (most recent call last):\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/threading.py\"\
    , line 1016, in _bootstrap_inner\r\n    self.run()\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/threading.py\"\
    , line 953, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File\
    \ \"/data/01/dv/data/ve2/dtl/ecp1/phi/no_gbd/r000/work/llama-2/text-generation-webui/modules/training.py\"\
    , line 665, in threaded_run\r\n    trainer.train()\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model,\
    \ inputs)\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\
    \n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/operations.py\"\
    , line 581, in forward\r\n    return model_forward(*args, **kwargs)\r\n  File\
    \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/operations.py\"\
    , line 569, in __call__\r\n    return convert_to_fp32(self.model_forward(*args,\
    \ **kwargs))\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/amp/autocast_mode.py\"\
    , line 14, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/peft/peft_model.py\"\
    , line 947, in forward\r\n    return self.base_model(\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
    , line 433, in forward\r\n    return self.model(*args, **kwargs)\r\n  File \"\
    /local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 806, in forward\r\n    outputs = self.model(\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 693, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 408, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
    \ = self.self_attn(\r\n  File \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py\"\
    , line 53, in forward\r\n    qkv_states = self.qkv_proj(hidden_states)\r\n  File\
    \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/local_home/al50860/anaconda3/envs/textgen/lib/python3.10/site-packages/peft/tuners/lora.py\"\
    , line 840, in forward\r\n    result = F.linear(x, transpose(self.weight, self.fan_in_fan_out),\
    \ bias=self.bias)\r\nRuntimeError: self and mat2 must have the same dtype\r\n\
    2023-08-14 09:53:26 INFO:Training complete, saving...\r\n2023-08-14 09:53:26 INFO:Training\
    \ complete!\r\n\r\n# Any advice and help is appreciated!"
  created_at: 2023-08-14 15:36:07+00:00
  edited: false
  hidden: false
  id: 64da57f7888b7e9c40fdf8ee
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: TheBloke/Llama-2-7B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: LORA fine tuning error
