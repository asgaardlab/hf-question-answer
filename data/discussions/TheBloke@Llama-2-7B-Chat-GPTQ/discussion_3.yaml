!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HAvietisov
conflicting_files: null
created_at: 2023-07-25 15:15:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
      fullname: Hlib Avietisov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HAvietisov
      type: user
    createdAt: '2023-07-25T16:15:52.000Z'
    data:
      edited: false
      editors:
      - HAvietisov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5964851379394531
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
          fullname: Hlib Avietisov
          isHf: false
          isPro: false
          name: HAvietisov
          type: user
        html: "<p>I want to try 32g version, but can't get it to work. I get \"FileNotFoundError:\
          \ Could not find model in TheBloke/Llama-2-7b-Chat-GPTQ\"<br>the code below\
          \ : </p>\n<pre><code>from transformers import AutoTokenizer, pipeline, logging\n\
          from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path\
          \ = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\nmodel_basename = \"gptq_model-4bit-32g\"\
          \nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        revision=\"gptq-4bit-32g-actorder_True\",\n        model_basename=model_basename,\n\
          \        use_safetensors=True,\n        trust_remote_code=True,\n      \
          \  device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None,\n\
          \        rope_scaling={\"type\": \"dynamic\", \"factor\": 2})\n</code></pre>\n"
        raw: "I want to try 32g version, but can't get it to work. I get \"FileNotFoundError:\
          \ Could not find model in TheBloke/Llama-2-7b-Chat-GPTQ\"\r\nthe code below\
          \ : \r\n```\r\nfrom transformers import AutoTokenizer, pipeline, logging\r\
          \nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\n\r\n\
          model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\r\nmodel_basename\
          \ = \"gptq_model-4bit-32g\"\r\nuse_triton = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n        revision=\"gptq-4bit-32g-actorder_True\",\r\n        model_basename=model_basename,\r\
          \n        use_safetensors=True,\r\n        trust_remote_code=True,\r\n \
          \       device=\"cuda:0\",\r\n        use_triton=use_triton,\r\n       \
          \ quantize_config=None,\r\n        rope_scaling={\"type\": \"dynamic\",\
          \ \"factor\": 2})\r\n```"
        updatedAt: '2023-07-25T16:15:52.581Z'
      numEdits: 0
      reactions: []
    id: 64bff538b878d31de52f18b7
    type: comment
  author: HAvietisov
  content: "I want to try 32g version, but can't get it to work. I get \"FileNotFoundError:\
    \ Could not find model in TheBloke/Llama-2-7b-Chat-GPTQ\"\r\nthe code below :\
    \ \r\n```\r\nfrom transformers import AutoTokenizer, pipeline, logging\r\nfrom\
    \ auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\n\r\nmodel_name_or_path\
    \ = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\r\nmodel_basename = \"gptq_model-4bit-32g\"\
    \r\nuse_triton = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n        revision=\"gptq-4bit-32g-actorder_True\",\r\n        model_basename=model_basename,\r\
    \n        use_safetensors=True,\r\n        trust_remote_code=True,\r\n       \
    \ device=\"cuda:0\",\r\n        use_triton=use_triton,\r\n        quantize_config=None,\r\
    \n        rope_scaling={\"type\": \"dynamic\", \"factor\": 2})\r\n```"
  created_at: 2023-07-25 15:15:52+00:00
  edited: false
  hidden: false
  id: 64bff538b878d31de52f18b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T16:37:47.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7394211888313293
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah sorry it turns out there''s a bug in AutoGPTQ with the revision
          parameter at the moment.  I have just pushed a PR to AutoGPTQ to fix it,
          which you can see here: <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/pull/205">https://github.com/PanQiWei/AutoGPTQ/pull/205</a></p>

          <p>Also I discovered recently that there''s a bug in AutoGPTQ 0.3.0 which
          breaks inference with group_size + desc_act together.  So currently you
          can''t do inference with the model you want, unless you downgrade to 0.2.2.  Both
          the inference bug and the revision bug should be fixed in AutoGPTQ 0.3.1,
          which I hope will come out in the next 24 hours.</p>

          <p>Your options are:</p>

          <ol>

          <li>Wait until AutoGPTQ 0.3.1 is released which will fix both bugs, or</li>

          <li>Downgrade to AutoGPTQ 0.2.2, then use this code instead, which first
          downloads the branch locally and then does inference from there:</li>

          </ol>

          <p>Change <code>local_base_folder</code> to a suitable base path where you
          want the model folder to be created. Note that they will still mostly be
          stored in the Huggingface Cache folder, but symlinks will be created in
          the path specified which you can then point AutoGPTQ to.</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          os

          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          AutoTokenizer, pipeline, logging

          <span class="hljs-keyword">from</span> auto_gptq <span class="hljs-keyword">import</span>
          AutoGPTQForCausalLM, BaseQuantizeConfig

          <span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span>
          snapshot_download


          model_name = <span class="hljs-string">"TheBloke/Llama-2-7b-Chat-GPTQ"</span>

          branch = <span class="hljs-string">"gptq-4bit-32g-actorder_True"</span>

          local_base_folder = <span class="hljs-string">"/workspace"</span>

          local_folder = os.path.join(local_base_folder, <span class="hljs-string">f"<span
          class="hljs-subst">{model_name.replace(<span class="hljs-string">''/''</span>,
          <span class="hljs-string">''_''</span>)}</span>_<span class="hljs-subst">{branch}</span>"</span>)


          snapshot_download(repo_id=model_name, local_dir=local_folder, revision=branch)


          model_basename = <span class="hljs-string">"gptq_model-4bit-32g"</span>


          use_triton = <span class="hljs-literal">False</span>


          tokenizer = AutoTokenizer.from_pretrained(local_folder, use_fast=<span class="hljs-literal">True</span>)


          model = AutoGPTQForCausalLM.from_quantized(local_folder,

          model_basename=model_basename,

          use_safetensors=<span class="hljs-literal">True</span>,

          trust_remote_code=<span class="hljs-literal">False</span>,

          device=<span class="hljs-string">"cuda:0"</span>,

          use_triton=use_triton,

          quantize_config=<span class="hljs-literal">None</span>)


          input_ids = tokenizer(<span class="hljs-string">"Llamas are"</span>, return_tensors=<span
          class="hljs-string">''pt''</span>).input_ids.cuda()

          output = model.generate(inputs=input_ids, temperature=<span class="hljs-number">0.7</span>,
          max_new_tokens=<span class="hljs-number">512</span>)

          <span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>]))

          </code></pre>

          '
        raw: 'Yeah sorry it turns out there''s a bug in AutoGPTQ with the revision
          parameter at the moment.  I have just pushed a PR to AutoGPTQ to fix it,
          which you can see here: https://github.com/PanQiWei/AutoGPTQ/pull/205


          Also I discovered recently that there''s a bug in AutoGPTQ 0.3.0 which breaks
          inference with group_size + desc_act together.  So currently you can''t
          do inference with the model you want, unless you downgrade to 0.2.2.  Both
          the inference bug and the revision bug should be fixed in AutoGPTQ 0.3.1,
          which I hope will come out in the next 24 hours.


          Your options are:

          1. Wait until AutoGPTQ 0.3.1 is released which will fix both bugs, or

          2. Downgrade to AutoGPTQ 0.2.2, then use this code instead, which first
          downloads the branch locally and then does inference from there:


          Change `local_base_folder` to a suitable base path where you want the model
          folder to be created. Note that they will still mostly be stored in the
          Huggingface Cache folder, but symlinks will be created in the path specified
          which you can then point AutoGPTQ to.


          ```python

          import os

          from transformers import AutoTokenizer, pipeline, logging

          from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

          from huggingface_hub import snapshot_download


          model_name = "TheBloke/Llama-2-7b-Chat-GPTQ"

          branch = "gptq-4bit-32g-actorder_True"

          local_base_folder = "/workspace"

          local_folder = os.path.join(local_base_folder, f"{model_name.replace(''/'',
          ''_'')}_{branch}")


          snapshot_download(repo_id=model_name, local_dir=local_folder, revision=branch)


          model_basename = "gptq_model-4bit-32g"


          use_triton = False


          tokenizer = AutoTokenizer.from_pretrained(local_folder, use_fast=True)


          model = AutoGPTQForCausalLM.from_quantized(local_folder,

          model_basename=model_basename,

          use_safetensors=True,

          trust_remote_code=False,

          device="cuda:0",

          use_triton=use_triton,

          quantize_config=None)


          input_ids = tokenizer("Llamas are", return_tensors=''pt'').input_ids.cuda()

          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)

          print(tokenizer.decode(output[0]))

          ```'
        updatedAt: '2023-07-25T16:37:47.002Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - HAvietisov
    id: 64bffa5b2263348d85c1c662
    type: comment
  author: TheBloke
  content: 'Yeah sorry it turns out there''s a bug in AutoGPTQ with the revision parameter
    at the moment.  I have just pushed a PR to AutoGPTQ to fix it, which you can see
    here: https://github.com/PanQiWei/AutoGPTQ/pull/205


    Also I discovered recently that there''s a bug in AutoGPTQ 0.3.0 which breaks
    inference with group_size + desc_act together.  So currently you can''t do inference
    with the model you want, unless you downgrade to 0.2.2.  Both the inference bug
    and the revision bug should be fixed in AutoGPTQ 0.3.1, which I hope will come
    out in the next 24 hours.


    Your options are:

    1. Wait until AutoGPTQ 0.3.1 is released which will fix both bugs, or

    2. Downgrade to AutoGPTQ 0.2.2, then use this code instead, which first downloads
    the branch locally and then does inference from there:


    Change `local_base_folder` to a suitable base path where you want the model folder
    to be created. Note that they will still mostly be stored in the Huggingface Cache
    folder, but symlinks will be created in the path specified which you can then
    point AutoGPTQ to.


    ```python

    import os

    from transformers import AutoTokenizer, pipeline, logging

    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

    from huggingface_hub import snapshot_download


    model_name = "TheBloke/Llama-2-7b-Chat-GPTQ"

    branch = "gptq-4bit-32g-actorder_True"

    local_base_folder = "/workspace"

    local_folder = os.path.join(local_base_folder, f"{model_name.replace(''/'', ''_'')}_{branch}")


    snapshot_download(repo_id=model_name, local_dir=local_folder, revision=branch)


    model_basename = "gptq_model-4bit-32g"


    use_triton = False


    tokenizer = AutoTokenizer.from_pretrained(local_folder, use_fast=True)


    model = AutoGPTQForCausalLM.from_quantized(local_folder,

    model_basename=model_basename,

    use_safetensors=True,

    trust_remote_code=False,

    device="cuda:0",

    use_triton=use_triton,

    quantize_config=None)


    input_ids = tokenizer("Llamas are", return_tensors=''pt'').input_ids.cuda()

    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)

    print(tokenizer.decode(output[0]))

    ```'
  created_at: 2023-07-25 15:37:47+00:00
  edited: false
  hidden: false
  id: 64bffa5b2263348d85c1c662
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9fe0dd972ea3a4e37106f5093e370bd.svg
      fullname: Vithika Pungliya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vithika
      type: user
    createdAt: '2023-07-27T08:28:22.000Z'
    data:
      edited: false
      editors:
      - Vithika
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9934273362159729
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9fe0dd972ea3a4e37106f5093e370bd.svg
          fullname: Vithika Pungliya
          isHf: false
          isPro: false
          name: Vithika
          type: user
        html: '<p>Thanks, this helped me.</p>

          '
        raw: Thanks, this helped me.
        updatedAt: '2023-07-27T08:28:22.089Z'
      numEdits: 0
      reactions: []
    id: 64c22aa6c5a3e0880365e4e7
    type: comment
  author: Vithika
  content: Thanks, this helped me.
  created_at: 2023-07-27 07:28:22+00:00
  edited: false
  hidden: false
  id: 64c22aa6c5a3e0880365e4e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-27T08:29:31.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8872183561325073
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great. By the way, AutoGPTQ 0.3.2 released yesterday and fixes this
          issue, so you can now upgrade AutoGPTQ to the latest version again.</p>

          '
        raw: Great. By the way, AutoGPTQ 0.3.2 released yesterday and fixes this issue,
          so you can now upgrade AutoGPTQ to the latest version again.
        updatedAt: '2023-07-27T08:29:31.012Z'
      numEdits: 0
      reactions: []
    id: 64c22aeb0f9c5d5e24716edb
    type: comment
  author: TheBloke
  content: Great. By the way, AutoGPTQ 0.3.2 released yesterday and fixes this issue,
    so you can now upgrade AutoGPTQ to the latest version again.
  created_at: 2023-07-27 07:29:31+00:00
  edited: false
  hidden: false
  id: 64c22aeb0f9c5d5e24716edb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9fe0dd972ea3a4e37106f5093e370bd.svg
      fullname: Vithika Pungliya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vithika
      type: user
    createdAt: '2023-07-27T09:09:24.000Z'
    data:
      edited: false
      editors:
      - Vithika
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4749611020088196
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9fe0dd972ea3a4e37106f5093e370bd.svg
          fullname: Vithika Pungliya
          isHf: false
          isPro: false
          name: Vithika
          type: user
        html: "<p>Ohkay thanks!\U0001F60A</p>\n"
        raw: "Ohkay thanks!\U0001F60A"
        updatedAt: '2023-07-27T09:09:24.662Z'
      numEdits: 0
      reactions: []
    id: 64c2344411c6194a55e4d959
    type: comment
  author: Vithika
  content: "Ohkay thanks!\U0001F60A"
  created_at: 2023-07-27 08:09:24+00:00
  edited: false
  hidden: false
  id: 64c2344411c6194a55e4d959
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
      fullname: Hlib Avietisov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HAvietisov
      type: user
    createdAt: '2023-07-27T10:55:50.000Z'
    data:
      edited: true
      editors:
      - HAvietisov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.775425136089325
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
          fullname: Hlib Avietisov
          isHf: false
          isPro: false
          name: HAvietisov
          type: user
        html: '<p>Ohhhh, thanks a lot!<br>p.s. inference bug on 0.3.1 is still not
          fixed.<br>for some reason 0.3.2 is not available for me atm<br>and the inference
          speed is just terrible : update from 2.2 to 3.1 changed speed of inference
          from 8 t/s to 1.7 t/s</p>

          '
        raw: 'Ohhhh, thanks a lot!

          p.s. inference bug on 0.3.1 is still not fixed.

          for some reason 0.3.2 is not available for me atm

          and the inference speed is just terrible : update from 2.2 to 3.1 changed
          speed of inference from 8 t/s to 1.7 t/s'
        updatedAt: '2023-07-27T11:36:18.069Z'
      numEdits: 4
      reactions: []
    id: 64c24d36886f7db857f80342
    type: comment
  author: HAvietisov
  content: 'Ohhhh, thanks a lot!

    p.s. inference bug on 0.3.1 is still not fixed.

    for some reason 0.3.2 is not available for me atm

    and the inference speed is just terrible : update from 2.2 to 3.1 changed speed
    of inference from 8 t/s to 1.7 t/s'
  created_at: 2023-07-27 09:55:50+00:00
  edited: true
  hidden: false
  id: 64c24d36886f7db857f80342
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9fe0dd972ea3a4e37106f5093e370bd.svg
      fullname: Vithika Pungliya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vithika
      type: user
    createdAt: '2023-07-28T08:27:32.000Z'
    data:
      edited: false
      editors:
      - Vithika
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5887457132339478
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9fe0dd972ea3a4e37106f5093e370bd.svg
          fullname: Vithika Pungliya
          isHf: false
          isPro: false
          name: Vithika
          type: user
        html: '<p>I am using this model to fine tune on my dataset and using the method
          above- <a href="https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ/discussions/3#64bffa5b2263348d85c1c662">https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ/discussions/3#64bffa5b2263348d85c1c662</a>.<br>I
          am using the SFTTrainer and getting this issue- HFValidationError: Repo
          id must use alphanumeric chars..<br>Any solution?</p>

          '
        raw: 'I am using this model to fine tune on my dataset and using the method
          above- https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ/discussions/3#64bffa5b2263348d85c1c662.

          I am using the SFTTrainer and getting this issue- HFValidationError: Repo
          id must use alphanumeric chars..

          Any solution?


          '
        updatedAt: '2023-07-28T08:27:32.802Z'
      numEdits: 0
      reactions: []
    id: 64c37bf4d67e744b78aa2806
    type: comment
  author: Vithika
  content: 'I am using this model to fine tune on my dataset and using the method
    above- https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ/discussions/3#64bffa5b2263348d85c1c662.

    I am using the SFTTrainer and getting this issue- HFValidationError: Repo id must
    use alphanumeric chars..

    Any solution?


    '
  created_at: 2023-07-28 07:27:32+00:00
  edited: false
  hidden: false
  id: 64c37bf4d67e744b78aa2806
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xSREzaGLP9cNZfMajrQGN.jpeg?w=200&h=200&f=face
      fullname: Lucas Alvarez Lacasa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucasalvarezlacasa
      type: user
    createdAt: '2023-08-01T14:34:49.000Z'
    data:
      edited: false
      editors:
      - lucasalvarezlacasa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9619742631912231
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xSREzaGLP9cNZfMajrQGN.jpeg?w=200&h=200&f=face
          fullname: Lucas Alvarez Lacasa
          isHf: false
          isPro: false
          name: lucasalvarezlacasa
          type: user
        html: '<p>I''m running auto-gptq version 0.3.1 and I have the same issue,
          even when trying to use the "main" branch model. How can I get around it?</p>

          '
        raw: I'm running auto-gptq version 0.3.1 and I have the same issue, even when
          trying to use the "main" branch model. How can I get around it?
        updatedAt: '2023-08-01T14:34:49.731Z'
      numEdits: 0
      reactions: []
    id: 64c918095381684d3eae0527
    type: comment
  author: lucasalvarezlacasa
  content: I'm running auto-gptq version 0.3.1 and I have the same issue, even when
    trying to use the "main" branch model. How can I get around it?
  created_at: 2023-08-01 13:34:49+00:00
  edited: false
  hidden: false
  id: 64c918095381684d3eae0527
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
      fullname: Hlib Avietisov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HAvietisov
      type: user
    createdAt: '2023-08-01T15:03:20.000Z'
    data:
      edited: false
      editors:
      - HAvietisov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8581512570381165
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
          fullname: Hlib Avietisov
          isHf: false
          isPro: false
          name: HAvietisov
          type: user
        html: "<p>the method from <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ post above works. But you'l have to downgrade to 0.2.2, <span data-props=\"\
          {&quot;user&quot;:&quot;lucasalvarezlacasa&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/lucasalvarezlacasa\">@<span class=\"\
          underline\">lucasalvarezlacasa</span></a></span>\n\n\t</span></span> </p>\n"
        raw: 'the method from @TheBloke post above works. But you''l have to downgrade
          to 0.2.2, @lucasalvarezlacasa '
        updatedAt: '2023-08-01T15:03:20.044Z'
      numEdits: 0
      reactions: []
    id: 64c91eb836c11430f32510cb
    type: comment
  author: HAvietisov
  content: 'the method from @TheBloke post above works. But you''l have to downgrade
    to 0.2.2, @lucasalvarezlacasa '
  created_at: 2023-08-01 14:03:20+00:00
  edited: false
  hidden: false
  id: 64c91eb836c11430f32510cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-01T17:18:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4788946807384491
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;HAvietisov&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/HAvietisov\">@<span class=\"\
          underline\">HAvietisov</span></a></span>\n\n\t</span></span> terrible inference\
          \ performance means you very likely don't have the AutoGPTQ CUDA kernel\
          \ compiled. This is a common problem at the moment.</p>\n<p>All: AutoGPTQ\
          \ 0.3.2 is working fine in general for me.  However you may need to build\
          \ it from source, as the PyPi package has multiple problems at the moment\
          \ that the AutoGPTQ author still has not been able to look at.</p>\n<p>Can\
          \ you all try this:</p>\n<pre><code>pip3 uninstall -y auto-gptq\ngit clone\
          \ https://github.com/PanQiWei/AutoGPTQ\ncd AutoGPTQ\npip3 install  .\n</code></pre>\n\
          <p>and report back.  If you're using any kind of UI, like text-generation-webui,\
          \ you must do the above in the Python environment of that UI. The text-generation-webui\
          \ one click installer creates its own Conda environment and you would need\
          \ to run the above commands with that conda environment activated.   </p>\n\
          <p>I also just realised that there's still a bug with the AutoGPTQ <code>revision</code>\
          \ parameter, which means that if you request eg the 32g model, it will download\
          \ it OK, but it downloads the quantize_config.json from the main branch.\
          \  So you get this error:</p>\n<pre><code>  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda_old.py\"\
          , line 261, in forward\n    weight = (scales * (weight - zeros))\nRuntimeError:\
          \ The size of tensor a (32) must match the size of tensor b (128) at non-singleton\
          \ dimension 0\n</code></pre>\n<p>That is my fault and I will need to fix\
          \ it in AutoGPTQ. I'll try to do that soon.</p>\n<p>So if you want to use\
          \ an alternative branch version with AutoGPTQ, please download it rather\
          \ than fetching it straight from the hub in the AutoGPTQ call. The following\
          \ test code shows doing that, and running it successfully on a random pod\
          \ with CUDA 11.8 and pytorch 2.0.1:</p>\n<pre><code class=\"language-python3\"\
          >from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom huggingface_hub import\
          \ snapshot_download\n\nmodel_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\
          \n\nuse_triton = False\n\n# We can download the tokenizer from the main\
          \ branch as they're all the same\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\n#To download from a specific branch, use the revision\
          \ parameter, as in this example:\n\n# First download the model, from the\
          \ desired branch to the specified local_folder - change this location to\
          \ where you want the model to download to\nlocal_folder=\"/workspace/llama-2-7b-gptq-32g\"\
          \nsnapshot_download(repo_id=model_name_or_path, local_dir=local_folder,\
          \ revision=\"gptq-4bit-32g-actorder_True\")\n\nmodel = AutoGPTQForCausalLM.from_quantized(local_folder,\n\
          \        use_safetensors=True,\n        trust_remote_code=True,\n      \
          \  device=\"cuda:0\",\n        quantize_config=None)\n\nprompt = \"Tell\
          \ me about AI\"\nsystem_message = \"You are a helpful, respectful and honest\
          \ assistant. Always answer as helpfully as possible, while being safe. \
          \ Your answers should not include any harmful, unethical, racist, sexist,\
          \ toxic, dangerous, or illegal content. Please ensure that your responses\
          \ are socially unbiased and positive in nature. If a question does not make\
          \ any sense, or is not factually coherent, explain why instead of answering\
          \ something not correct. If you don't know the answer to a question, please\
          \ don't share false information.\"\nprompt_template=f'''[INST] &lt;&lt;SYS&gt;&gt;\n\
          {system_message}\n&lt;&lt;/SYS&gt;&gt;\n\n{prompt} [/INST]'''\n\nprint(\"\
          \\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n</code></pre>\n<p>And here's the output\
          \ from me running it:</p>\n<pre><code>root@34ea00540a00:~# python3 test.py\n\
          Downloading (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 727/727 [00:00&lt;00:00, 4.04MB/s]\nDownloading tokenizer.model:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 500k/500k [00:00&lt;00:00, 4.29MB/s]\nDownloading (\u2026)/main/tokenizer.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 1.84M/1.84M [00:00&lt;00:00, 8.35MB/s]\nDownloading\
          \ (\u2026)cial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 411/411 [00:00&lt;00:00, 2.69MB/s]\nDownloading (\u2026)b06239e96013b/Notice:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:00&lt;00:00,\
          \ 553kB/s]\nDownloading (\u2026)9e96013b/config.json: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 548/548 [00:00&lt;00:00, 3.15MB/s]\nDownloading (\u2026\
          )06239e96013b/LICENSE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 50.3k/50.3k [00:00&lt;00:00, 3.04MB/s]\n\
          Downloading (\u2026)6013b/.gitattributes: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.52k/1.52k [00:00&lt;00:00,\
          \ 7.07MB/s]\nDownloading (\u2026)239e96013b/README.md: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 20.1k/20.1k [00:00&lt;00:00, 69.2MB/s]\nDownloading (\u2026)quantize_config.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 183/183 [00:00&lt;00:00,\
          \ 432kB/s]\nDownloading (\u2026)neration_config.json: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 137/137 [00:00&lt;00:00, 651kB/s]\nDownloading\
          \ (\u2026)4bit-32g.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.28G/4.28G [00:43&lt;00:00,\
          \ 98.7MB/s]\nFetching 12 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:44&lt;00:00,  3.74s/it]\n\
          The safetensors archive passed at /workspace/llama-2-7b-gptq-32g/gptq_model-4bit-32g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\nskip module injection for FusedLlamaMLPForQuantizedModel\
          \ not support integrate without triton yet.\n\n\n*** Generate:\n&lt;s&gt;\
          \ [INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful, respectful and honest assistant.\
          \ Always answer as helpfully as possible, while being safe.  Your answers\
          \ should not include any harmful, unethical, racist, sexist, toxic, dangerous,\
          \ or illegal content. Please ensure that your responses are socially unbiased\
          \ and positive in nature. If a question does not make any sense, or is not\
          \ factually coherent, explain why instead of answering something not correct.\
          \ If you don't know the answer to a question, please don't share false information.\n\
          &lt;&lt;/SYS&gt;&gt;\n\nTell me about AI [/INST]  Hello! I'm here to help\
          \ you with any questions you may have. AI, or Artificial Intelligence, refers\
          \ to the development of computer systems that can perform tasks that typically\
          \ require human intelligence, such as visual perception, speech recognition,\
          \ and decision-making. AI technology has been rapidly advancing in recent\
          \ years, and it has the potential to revolutionize many industries, including\
          \ healthcare, finance, transportation, and education.\nThere are several\
          \ types of AI, including:\n1. Narrow or weak AI: This type of AI is designed\
          \ to perform a specific task, such as playing chess or recognizing faces.\n\
          2. General or strong AI: This type of AI is designed to perform any intellectual\
          \ task that a human can, and it is still a topic of ongoing research and\
          \ development.\n3. Superintelligence: This type of AI is significantly more\
          \ intelligent than the best human minds, and it is still a topic of debate\
          \ and speculation.\nIt's important to note that AI is not a single entity,\
          \ but rather a collection of technologies and techniques that are being\
          \ developed and improved upon by researchers and developers around the world.\n\
          I hope this helps! Is there anything else you would like to know about AI?&lt;/s&gt;\n\
          \nroot@34ea00540a00:~#\n</code></pre>\n"
        raw: "@HAvietisov terrible inference performance means you very likely don't\
          \ have the AutoGPTQ CUDA kernel compiled. This is a common problem at the\
          \ moment.\n\nAll: AutoGPTQ 0.3.2 is working fine in general for me.  However\
          \ you may need to build it from source, as the PyPi package has multiple\
          \ problems at the moment that the AutoGPTQ author still has not been able\
          \ to look at.\n\nCan you all try this:\n```\npip3 uninstall -y auto-gptq\n\
          git clone https://github.com/PanQiWei/AutoGPTQ\ncd AutoGPTQ\npip3 install\
          \  .\n```\n\nand report back.  If you're using any kind of UI, like text-generation-webui,\
          \ you must do the above in the Python environment of that UI. The text-generation-webui\
          \ one click installer creates its own Conda environment and you would need\
          \ to run the above commands with that conda environment activated.   \n\n\
          I also just realised that there's still a bug with the AutoGPTQ `revision`\
          \ parameter, which means that if you request eg the 32g model, it will download\
          \ it OK, but it downloads the quantize_config.json from the main branch.\
          \  So you get this error:\n```\n  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda_old.py\"\
          , line 261, in forward\n    weight = (scales * (weight - zeros))\nRuntimeError:\
          \ The size of tensor a (32) must match the size of tensor b (128) at non-singleton\
          \ dimension 0\n```\n\nThat is my fault and I will need to fix it in AutoGPTQ.\
          \ I'll try to do that soon.\n\nSo if you want to use an alternative branch\
          \ version with AutoGPTQ, please download it rather than fetching it straight\
          \ from the hub in the AutoGPTQ call. The following test code shows doing\
          \ that, and running it successfully on a random pod with CUDA 11.8 and pytorch\
          \ 2.0.1:\n\n```python3\nfrom transformers import AutoTokenizer, pipeline,\
          \ logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          from huggingface_hub import snapshot_download\n\nmodel_name_or_path = \"\
          TheBloke/Llama-2-7b-Chat-GPTQ\"\n\nuse_triton = False\n\n# We can download\
          \ the tokenizer from the main branch as they're all the same\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\n\
          #To download from a specific branch, use the revision parameter, as in this\
          \ example:\n\n# First download the model, from the desired branch to the\
          \ specified local_folder - change this location to where you want the model\
          \ to download to\nlocal_folder=\"/workspace/llama-2-7b-gptq-32g\"\nsnapshot_download(repo_id=model_name_or_path,\
          \ local_dir=local_folder, revision=\"gptq-4bit-32g-actorder_True\")\n\n\
          model = AutoGPTQForCausalLM.from_quantized(local_folder,\n        use_safetensors=True,\n\
          \        trust_remote_code=True,\n        device=\"cuda:0\",\n        quantize_config=None)\n\
          \nprompt = \"Tell me about AI\"\nsystem_message = \"You are a helpful, respectful\
          \ and honest assistant. Always answer as helpfully as possible, while being\
          \ safe.  Your answers should not include any harmful, unethical, racist,\
          \ sexist, toxic, dangerous, or illegal content. Please ensure that your\
          \ responses are socially unbiased and positive in nature. If a question\
          \ does not make any sense, or is not factually coherent, explain why instead\
          \ of answering something not correct. If you don't know the answer to a\
          \ question, please don't share false information.\"\nprompt_template=f'''[INST]\
          \ <<SYS>>\n{system_message}\n<</SYS>>\n\n{prompt} [/INST]'''\n\nprint(\"\
          \\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n```\n\nAnd here's the output from me\
          \ running it:\n\n```\nroot@34ea00540a00:~# python3 test.py\nDownloading\
          \ (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 727/727 [00:00<00:00, 4.04MB/s]\nDownloading tokenizer.model: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500k/500k\
          \ [00:00<00:00, 4.29MB/s]\nDownloading (\u2026)/main/tokenizer.json: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 1.84M/1.84M [00:00<00:00, 8.35MB/s]\nDownloading (\u2026)cial_tokens_map.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 411/411 [00:00<00:00, 2.69MB/s]\n\
          Downloading (\u2026)b06239e96013b/Notice: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 112/112 [00:00<00:00, 553kB/s]\nDownloading (\u2026)9e96013b/config.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 548/548 [00:00<00:00, 3.15MB/s]\n\
          Downloading (\u2026)06239e96013b/LICENSE: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50.3k/50.3k [00:00<00:00,\
          \ 3.04MB/s]\nDownloading (\u2026)6013b/.gitattributes: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 1.52k/1.52k [00:00<00:00, 7.07MB/s]\nDownloading (\u2026)239e96013b/README.md:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 20.1k/20.1k [00:00<00:00, 69.2MB/s]\nDownloading (\u2026\
          )quantize_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 183/183\
          \ [00:00<00:00, 432kB/s]\nDownloading (\u2026)neration_config.json: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 137/137 [00:00<00:00, 651kB/s]\nDownloading\
          \ (\u2026)4bit-32g.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.28G/4.28G [00:43<00:00,\
          \ 98.7MB/s]\nFetching 12 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:44<00:00,  3.74s/it]\n\
          The safetensors archive passed at /workspace/llama-2-7b-gptq-32g/gptq_model-4bit-32g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\nskip module injection for FusedLlamaMLPForQuantizedModel\
          \ not support integrate without triton yet.\n\n\n*** Generate:\n<s> [INST]\
          \ <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer\
          \ as helpfully as possible, while being safe.  Your answers should not include\
          \ any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\
          \ Please ensure that your responses are socially unbiased and positive in\
          \ nature. If a question does not make any sense, or is not factually coherent,\
          \ explain why instead of answering something not correct. If you don't know\
          \ the answer to a question, please don't share false information.\n<</SYS>>\n\
          \nTell me about AI [/INST]  Hello! I'm here to help you with any questions\
          \ you may have. AI, or Artificial Intelligence, refers to the development\
          \ of computer systems that can perform tasks that typically require human\
          \ intelligence, such as visual perception, speech recognition, and decision-making.\
          \ AI technology has been rapidly advancing in recent years, and it has the\
          \ potential to revolutionize many industries, including healthcare, finance,\
          \ transportation, and education.\nThere are several types of AI, including:\n\
          1. Narrow or weak AI: This type of AI is designed to perform a specific\
          \ task, such as playing chess or recognizing faces.\n2. General or strong\
          \ AI: This type of AI is designed to perform any intellectual task that\
          \ a human can, and it is still a topic of ongoing research and development.\n\
          3. Superintelligence: This type of AI is significantly more intelligent\
          \ than the best human minds, and it is still a topic of debate and speculation.\n\
          It's important to note that AI is not a single entity, but rather a collection\
          \ of technologies and techniques that are being developed and improved upon\
          \ by researchers and developers around the world.\nI hope this helps! Is\
          \ there anything else you would like to know about AI?</s>\n\nroot@34ea00540a00:~#\n\
          ```"
        updatedAt: '2023-08-01T17:18:22.629Z'
      numEdits: 0
      reactions: []
    id: 64c93e5e904317f42dcb8472
    type: comment
  author: TheBloke
  content: "@HAvietisov terrible inference performance means you very likely don't\
    \ have the AutoGPTQ CUDA kernel compiled. This is a common problem at the moment.\n\
    \nAll: AutoGPTQ 0.3.2 is working fine in general for me.  However you may need\
    \ to build it from source, as the PyPi package has multiple problems at the moment\
    \ that the AutoGPTQ author still has not been able to look at.\n\nCan you all\
    \ try this:\n```\npip3 uninstall -y auto-gptq\ngit clone https://github.com/PanQiWei/AutoGPTQ\n\
    cd AutoGPTQ\npip3 install  .\n```\n\nand report back.  If you're using any kind\
    \ of UI, like text-generation-webui, you must do the above in the Python environment\
    \ of that UI. The text-generation-webui one click installer creates its own Conda\
    \ environment and you would need to run the above commands with that conda environment\
    \ activated.   \n\nI also just realised that there's still a bug with the AutoGPTQ\
    \ `revision` parameter, which means that if you request eg the 32g model, it will\
    \ download it OK, but it downloads the quantize_config.json from the main branch.\
    \  So you get this error:\n```\n  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda_old.py\"\
    , line 261, in forward\n    weight = (scales * (weight - zeros))\nRuntimeError:\
    \ The size of tensor a (32) must match the size of tensor b (128) at non-singleton\
    \ dimension 0\n```\n\nThat is my fault and I will need to fix it in AutoGPTQ.\
    \ I'll try to do that soon.\n\nSo if you want to use an alternative branch version\
    \ with AutoGPTQ, please download it rather than fetching it straight from the\
    \ hub in the AutoGPTQ call. The following test code shows doing that, and running\
    \ it successfully on a random pod with CUDA 11.8 and pytorch 2.0.1:\n\n```python3\n\
    from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq import\
    \ AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom huggingface_hub import snapshot_download\n\
    \nmodel_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n\nuse_triton = False\n\
    \n# We can download the tokenizer from the main branch as they're all the same\n\
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
    \n#To download from a specific branch, use the revision parameter, as in this\
    \ example:\n\n# First download the model, from the desired branch to the specified\
    \ local_folder - change this location to where you want the model to download\
    \ to\nlocal_folder=\"/workspace/llama-2-7b-gptq-32g\"\nsnapshot_download(repo_id=model_name_or_path,\
    \ local_dir=local_folder, revision=\"gptq-4bit-32g-actorder_True\")\n\nmodel =\
    \ AutoGPTQForCausalLM.from_quantized(local_folder,\n        use_safetensors=True,\n\
    \        trust_remote_code=True,\n        device=\"cuda:0\",\n        quantize_config=None)\n\
    \nprompt = \"Tell me about AI\"\nsystem_message = \"You are a helpful, respectful\
    \ and honest assistant. Always answer as helpfully as possible, while being safe.\
    \  Your answers should not include any harmful, unethical, racist, sexist, toxic,\
    \ dangerous, or illegal content. Please ensure that your responses are socially\
    \ unbiased and positive in nature. If a question does not make any sense, or is\
    \ not factually coherent, explain why instead of answering something not correct.\
    \ If you don't know the answer to a question, please don't share false information.\"\
    \nprompt_template=f'''[INST] <<SYS>>\n{system_message}\n<</SYS>>\n\n{prompt} [/INST]'''\n\
    \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n```\n\nAnd here's the output from me running\
    \ it:\n\n```\nroot@34ea00540a00:~# python3 test.py\nDownloading (\u2026)okenizer_config.json:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 727/727 [00:00<00:00,\
    \ 4.04MB/s]\nDownloading tokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500k/500k [00:00<00:00,\
    \ 4.29MB/s]\nDownloading (\u2026)/main/tokenizer.json: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 1.84M/1.84M [00:00<00:00, 8.35MB/s]\nDownloading (\u2026)cial_tokens_map.json:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 411/411 [00:00<00:00,\
    \ 2.69MB/s]\nDownloading (\u2026)b06239e96013b/Notice: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:00<00:00, 553kB/s]\n\
    Downloading (\u2026)9e96013b/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 548/548 [00:00<00:00, 3.15MB/s]\nDownloading (\u2026\
    )06239e96013b/LICENSE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50.3k/50.3k [00:00<00:00,\
    \ 3.04MB/s]\nDownloading (\u2026)6013b/.gitattributes: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 1.52k/1.52k [00:00<00:00, 7.07MB/s]\nDownloading (\u2026)239e96013b/README.md:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.1k/20.1k [00:00<00:00, 69.2MB/s]\n\
    Downloading (\u2026)quantize_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 183/183 [00:00<00:00, 432kB/s]\nDownloading (\u2026\
    )neration_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 137/137 [00:00<00:00, 651kB/s]\nDownloading (\u2026)4bit-32g.safetensors:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.28G/4.28G [00:43<00:00, 98.7MB/s]\n\
    Fetching 12 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:44<00:00,  3.74s/it]\nThe\
    \ safetensors archive passed at /workspace/llama-2-7b-gptq-32g/gptq_model-4bit-32g.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\nskip module injection for FusedLlamaMLPForQuantizedModel\
    \ not support integrate without triton yet.\n\n\n*** Generate:\n<s> [INST] <<SYS>>\n\
    You are a helpful, respectful and honest assistant. Always answer as helpfully\
    \ as possible, while being safe.  Your answers should not include any harmful,\
    \ unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure\
    \ that your responses are socially unbiased and positive in nature. If a question\
    \ does not make any sense, or is not factually coherent, explain why instead of\
    \ answering something not correct. If you don't know the answer to a question,\
    \ please don't share false information.\n<</SYS>>\n\nTell me about AI [/INST]\
    \  Hello! I'm here to help you with any questions you may have. AI, or Artificial\
    \ Intelligence, refers to the development of computer systems that can perform\
    \ tasks that typically require human intelligence, such as visual perception,\
    \ speech recognition, and decision-making. AI technology has been rapidly advancing\
    \ in recent years, and it has the potential to revolutionize many industries,\
    \ including healthcare, finance, transportation, and education.\nThere are several\
    \ types of AI, including:\n1. Narrow or weak AI: This type of AI is designed to\
    \ perform a specific task, such as playing chess or recognizing faces.\n2. General\
    \ or strong AI: This type of AI is designed to perform any intellectual task that\
    \ a human can, and it is still a topic of ongoing research and development.\n\
    3. Superintelligence: This type of AI is significantly more intelligent than the\
    \ best human minds, and it is still a topic of debate and speculation.\nIt's important\
    \ to note that AI is not a single entity, but rather a collection of technologies\
    \ and techniques that are being developed and improved upon by researchers and\
    \ developers around the world.\nI hope this helps! Is there anything else you\
    \ would like to know about AI?</s>\n\nroot@34ea00540a00:~#\n```"
  created_at: 2023-08-01 16:18:22+00:00
  edited: false
  hidden: false
  id: 64c93e5e904317f42dcb8472
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xSREzaGLP9cNZfMajrQGN.jpeg?w=200&h=200&f=face
      fullname: Lucas Alvarez Lacasa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucasalvarezlacasa
      type: user
    createdAt: '2023-08-01T17:30:42.000Z'
    data:
      edited: true
      editors:
      - lucasalvarezlacasa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9062532186508179
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xSREzaGLP9cNZfMajrQGN.jpeg?w=200&h=200&f=face
          fullname: Lucas Alvarez Lacasa
          isHf: false
          isPro: false
          name: lucasalvarezlacasa
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> what is the expected\
          \ inference time when using GPTQ models? I found it to be extremely slow\
          \ (40/50s in average) compared to just using the raw official models from\
          \ meta-llama (15/20s in average) for 7B CHAT model. Is this the case or\
          \ might there be something wrong on my side?</p>\n<p>Thanks for your support.</p>\n"
        raw: '@TheBloke what is the expected inference time when using GPTQ models?
          I found it to be extremely slow (40/50s in average) compared to just using
          the raw official models from meta-llama (15/20s in average) for 7B CHAT
          model. Is this the case or might there be something wrong on my side?


          Thanks for your support.'
        updatedAt: '2023-08-01T17:54:47.348Z'
      numEdits: 2
      reactions: []
    id: 64c941426a26cddbecc26e9c
    type: comment
  author: lucasalvarezlacasa
  content: '@TheBloke what is the expected inference time when using GPTQ models?
    I found it to be extremely slow (40/50s in average) compared to just using the
    raw official models from meta-llama (15/20s in average) for 7B CHAT model. Is
    this the case or might there be something wrong on my side?


    Thanks for your support.'
  created_at: 2023-08-01 16:30:42+00:00
  edited: true
  hidden: false
  id: 64c941426a26cddbecc26e9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9fe0dd972ea3a4e37106f5093e370bd.svg
      fullname: Vithika Pungliya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vithika
      type: user
    createdAt: '2023-08-02T04:54:18.000Z'
    data:
      edited: false
      editors:
      - Vithika
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7578824162483215
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9fe0dd972ea3a4e37106f5093e370bd.svg
          fullname: Vithika Pungliya
          isHf: false
          isPro: false
          name: Vithika
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;lucasalvarezlacasa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lucasalvarezlacasa\"\
          >@<span class=\"underline\">lucasalvarezlacasa</span></a></span>\n\n\t</span></span>\
          \ I have come across this same problem.</p>\n"
        raw: '@lucasalvarezlacasa I have come across this same problem.

          '
        updatedAt: '2023-08-02T04:54:18.384Z'
      numEdits: 0
      reactions: []
    id: 64c9e17ac58a251a16bb42bd
    type: comment
  author: Vithika
  content: '@lucasalvarezlacasa I have come across this same problem.

    '
  created_at: 2023-08-02 03:54:18+00:00
  edited: false
  hidden: false
  id: 64c9e17ac58a251a16bb42bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xSREzaGLP9cNZfMajrQGN.jpeg?w=200&h=200&f=face
      fullname: Lucas Alvarez Lacasa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucasalvarezlacasa
      type: user
    createdAt: '2023-08-07T19:16:23.000Z'
    data:
      edited: false
      editors:
      - lucasalvarezlacasa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7152709364891052
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xSREzaGLP9cNZfMajrQGN.jpeg?w=200&h=200&f=face
          fullname: Lucas Alvarez Lacasa
          isHf: false
          isPro: false
          name: lucasalvarezlacasa
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Vithika&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Vithika\">@<span class=\"\
          underline\">Vithika</span></a></span>\n\n\t</span></span> any solutions?</p>\n"
        raw: '@Vithika any solutions?'
        updatedAt: '2023-08-07T19:16:23.119Z'
      numEdits: 0
      reactions: []
    id: 64d1430715b26cc7f71501c4
    type: comment
  author: lucasalvarezlacasa
  content: '@Vithika any solutions?'
  created_at: 2023-08-07 18:16:23+00:00
  edited: false
  hidden: false
  id: 64d1430715b26cc7f71501c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eedd380b9d6dae685251c722e0460cfc.svg
      fullname: Pankaj Barai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PankajB
      type: user
    createdAt: '2023-08-26T08:32:12.000Z'
    data:
      edited: false
      editors:
      - PankajB
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7444545030593872
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eedd380b9d6dae685251c722e0460cfc.svg
          fullname: Pankaj Barai
          isHf: false
          isPro: false
          name: PankajB
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;lucasalvarezlacasa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lucasalvarezlacasa\"\
          >@<span class=\"underline\">lucasalvarezlacasa</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;Vithika&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Vithika\">@<span class=\"\
          underline\">Vithika</span></a></span>\n\n\t</span></span>  Did you guys\
          \ got any solutions</p>\n"
        raw: '@lucasalvarezlacasa @Vithika  Did you guys got any solutions'
        updatedAt: '2023-08-26T08:32:12.610Z'
      numEdits: 0
      reactions: []
    id: 64e9b88cd014af2062d621b3
    type: comment
  author: PankajB
  content: '@lucasalvarezlacasa @Vithika  Did you guys got any solutions'
  created_at: 2023-08-26 07:32:12+00:00
  edited: false
  hidden: false
  id: 64e9b88cd014af2062d621b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c044d9479e9621eb60165b939d52128f.svg
      fullname: kyky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kyky0099
      type: user
    createdAt: '2023-09-08T06:45:30.000Z'
    data:
      edited: false
      editors:
      - kyky0099
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7450014352798462
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c044d9479e9621eb60165b939d52128f.svg
          fullname: kyky
          isHf: false
          isPro: false
          name: kyky0099
          type: user
        html: '<blockquote>

          <p>Yeah sorry it turns out there''s a bug in AutoGPTQ with the revision
          parameter at the moment.  I have just pushed a PR to AutoGPTQ to fix it,
          which you can see here: <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/pull/205">https://github.com/PanQiWei/AutoGPTQ/pull/205</a></p>

          <p>Also I discovered recently that there''s a bug in AutoGPTQ 0.3.0 which
          breaks inference with group_size + desc_act together.  So currently you
          can''t do inference with the model you want, unless you downgrade to 0.2.2.  Both
          the inference bug and the revision bug should be fixed in AutoGPTQ 0.3.1,
          which I hope will come out in the next 24 hours.</p>

          <p>Your options are:</p>

          <ol>

          <li>Wait until AutoGPTQ 0.3.1 is released which will fix both bugs, or</li>

          <li>Downgrade to AutoGPTQ 0.2.2, then use this code instead, which first
          downloads the branch locally and then does inference from there:</li>

          </ol>

          <p>Change <code>local_base_folder</code> to a suitable base path where you
          want the model folder to be created. Note that they will still mostly be
          stored in the Huggingface Cache folder, but symlinks will be created in
          the path specified which you can then point AutoGPTQ to.</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          os

          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          AutoTokenizer, pipeline, logging

          <span class="hljs-keyword">from</span> auto_gptq <span class="hljs-keyword">import</span>
          AutoGPTQForCausalLM, BaseQuantizeConfig

          <span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span>
          snapshot_download


          model_name = <span class="hljs-string">"TheBloke/Llama-2-7b-Chat-GPTQ"</span>

          branch = <span class="hljs-string">"gptq-4bit-32g-actorder_True"</span>

          local_base_folder = <span class="hljs-string">"/workspace"</span>

          local_folder = os.path.join(local_base_folder, <span class="hljs-string">f"<span
          class="hljs-subst">{model_name.replace(<span class="hljs-string">''/''</span>,
          <span class="hljs-string">''_''</span>)}</span>_<span class="hljs-subst">{branch}</span>"</span>)


          snapshot_download(repo_id=model_name, local_dir=local_folder, revision=branch)


          model_basename = <span class="hljs-string">"gptq_model-4bit-32g"</span>


          use_triton = <span class="hljs-literal">False</span>


          tokenizer = AutoTokenizer.from_pretrained(local_folder, use_fast=<span class="hljs-literal">True</span>)


          model = AutoGPTQForCausalLM.from_quantized(local_folder,

          model_basename=model_basename,

          use_safetensors=<span class="hljs-literal">True</span>,

          trust_remote_code=<span class="hljs-literal">False</span>,

          device=<span class="hljs-string">"cuda:0"</span>,

          use_triton=use_triton,

          quantize_config=<span class="hljs-literal">None</span>)


          input_ids = tokenizer(<span class="hljs-string">"Llamas are"</span>, return_tensors=<span
          class="hljs-string">''pt''</span>).input_ids.cuda()

          output = model.generate(inputs=input_ids, temperature=<span class="hljs-number">0.7</span>,
          max_new_tokens=<span class="hljs-number">512</span>)

          <span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>]))

          </code></pre>

          <p>Hi all, I tried to install the latest AutoGPTQ version or downgraded
          to AutoGPTQ 0.2.2 (as advised by bing using: pip install auto-gptq --extra-index-url
          <a rel="nofollow" href="https://huggingface.github.io/autogptq-index/whl/cu118/">https://huggingface.github.io/autogptq-index/whl/cu118/</a>),
          however, it still shows that AutoGPTQ is not installed. How can I fix this
          problem?</p>

          </blockquote>

          '
        raw: "> Yeah sorry it turns out there's a bug in AutoGPTQ with the revision\
          \ parameter at the moment.  I have just pushed a PR to AutoGPTQ to fix it,\
          \ which you can see here: https://github.com/PanQiWei/AutoGPTQ/pull/205\n\
          > \n> Also I discovered recently that there's a bug in AutoGPTQ 0.3.0 which\
          \ breaks inference with group_size + desc_act together.  So currently you\
          \ can't do inference with the model you want, unless you downgrade to 0.2.2.\
          \  Both the inference bug and the revision bug should be fixed in AutoGPTQ\
          \ 0.3.1, which I hope will come out in the next 24 hours.\n> \n> Your options\
          \ are:\n> 1. Wait until AutoGPTQ 0.3.1 is released which will fix both bugs,\
          \ or\n> 2. Downgrade to AutoGPTQ 0.2.2, then use this code instead, which\
          \ first downloads the branch locally and then does inference from there:\n\
          > \n> Change `local_base_folder` to a suitable base path where you want\
          \ the model folder to be created. Note that they will still mostly be stored\
          \ in the Huggingface Cache folder, but symlinks will be created in the path\
          \ specified which you can then point AutoGPTQ to.\n> \n> ```python\n> import\
          \ os\n> from transformers import AutoTokenizer, pipeline, logging\n> from\
          \ auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n> from huggingface_hub\
          \ import snapshot_download\n> \n> model_name = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\
          \n> branch = \"gptq-4bit-32g-actorder_True\"\n> local_base_folder = \"/workspace\"\
          \n> local_folder = os.path.join(local_base_folder, f\"{model_name.replace('/',\
          \ '_')}_{branch}\")\n> \n> snapshot_download(repo_id=model_name, local_dir=local_folder,\
          \ revision=branch)\n> \n> model_basename = \"gptq_model-4bit-32g\"\n> \n\
          > use_triton = False\n> \n> tokenizer = AutoTokenizer.from_pretrained(local_folder,\
          \ use_fast=True)\n> \n> model = AutoGPTQForCausalLM.from_quantized(local_folder,\n\
          > model_basename=model_basename,\n> use_safetensors=True,\n> trust_remote_code=False,\n\
          > device=\"cuda:0\",\n> use_triton=use_triton,\n> quantize_config=None)\n\
          > \n> input_ids = tokenizer(\"Llamas are\", return_tensors='pt').input_ids.cuda()\n\
          > output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          > print(tokenizer.decode(output[0]))\n> ```\nHi all, I tried to install\
          \ the latest AutoGPTQ version or downgraded to AutoGPTQ 0.2.2 (as advised\
          \ by bing using: pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/),\
          \ however, it still shows that AutoGPTQ is not installed. How can I fix\
          \ this problem?\n"
        updatedAt: '2023-09-08T06:45:30.948Z'
      numEdits: 0
      reactions: []
    id: 64fac30ad82fc6977d5ba361
    type: comment
  author: kyky0099
  content: "> Yeah sorry it turns out there's a bug in AutoGPTQ with the revision\
    \ parameter at the moment.  I have just pushed a PR to AutoGPTQ to fix it, which\
    \ you can see here: https://github.com/PanQiWei/AutoGPTQ/pull/205\n> \n> Also\
    \ I discovered recently that there's a bug in AutoGPTQ 0.3.0 which breaks inference\
    \ with group_size + desc_act together.  So currently you can't do inference with\
    \ the model you want, unless you downgrade to 0.2.2.  Both the inference bug and\
    \ the revision bug should be fixed in AutoGPTQ 0.3.1, which I hope will come out\
    \ in the next 24 hours.\n> \n> Your options are:\n> 1. Wait until AutoGPTQ 0.3.1\
    \ is released which will fix both bugs, or\n> 2. Downgrade to AutoGPTQ 0.2.2,\
    \ then use this code instead, which first downloads the branch locally and then\
    \ does inference from there:\n> \n> Change `local_base_folder` to a suitable base\
    \ path where you want the model folder to be created. Note that they will still\
    \ mostly be stored in the Huggingface Cache folder, but symlinks will be created\
    \ in the path specified which you can then point AutoGPTQ to.\n> \n> ```python\n\
    > import os\n> from transformers import AutoTokenizer, pipeline, logging\n> from\
    \ auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n> from huggingface_hub\
    \ import snapshot_download\n> \n> model_name = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\
    \n> branch = \"gptq-4bit-32g-actorder_True\"\n> local_base_folder = \"/workspace\"\
    \n> local_folder = os.path.join(local_base_folder, f\"{model_name.replace('/',\
    \ '_')}_{branch}\")\n> \n> snapshot_download(repo_id=model_name, local_dir=local_folder,\
    \ revision=branch)\n> \n> model_basename = \"gptq_model-4bit-32g\"\n> \n> use_triton\
    \ = False\n> \n> tokenizer = AutoTokenizer.from_pretrained(local_folder, use_fast=True)\n\
    > \n> model = AutoGPTQForCausalLM.from_quantized(local_folder,\n> model_basename=model_basename,\n\
    > use_safetensors=True,\n> trust_remote_code=False,\n> device=\"cuda:0\",\n> use_triton=use_triton,\n\
    > quantize_config=None)\n> \n> input_ids = tokenizer(\"Llamas are\", return_tensors='pt').input_ids.cuda()\n\
    > output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    > print(tokenizer.decode(output[0]))\n> ```\nHi all, I tried to install the latest\
    \ AutoGPTQ version or downgraded to AutoGPTQ 0.2.2 (as advised by bing using:\
    \ pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/),\
    \ however, it still shows that AutoGPTQ is not installed. How can I fix this problem?\n"
  created_at: 2023-09-08 05:45:30+00:00
  edited: false
  hidden: false
  id: 64fac30ad82fc6977d5ba361
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Llama-2-7B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Cannot use anything but what's in the main branch
