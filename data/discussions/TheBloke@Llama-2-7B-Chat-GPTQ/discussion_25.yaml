!!python/object:huggingface_hub.community.DiscussionWithDetails
author: echogit
conflicting_files: null
created_at: 2023-10-13 22:19:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/27431fcfdd56e210efe6ba219c3460a0.svg
      fullname: C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: echogit
      type: user
    createdAt: '2023-10-13T23:19:32.000Z'
    data:
      edited: false
      editors:
      - echogit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5080863833427429
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/27431fcfdd56e210efe6ba219c3460a0.svg
          fullname: C
          isHf: false
          isPro: false
          name: echogit
          type: user
        html: '<p>When trying to load the model in google colab, I get the error:</p>

          <p>ImportError: Loading a GPTQ quantized model requires optimum (<code>pip
          install optimum</code>) and auto-gptq library (<code>pip install auto-gptq</code>)</p>

          <p>My code has following:</p>

          <p>!pip install -q -U transformers peft accelerate optimum<br>!pip install
          auto-gptq</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>tokenizer
          = AutoTokenizer.from_pretrained("TheBloke/Llama-2-7b-Chat-GPTQ")<br>model
          = AutoModelForCausalLM.from_pretrained("TheBloke/Llama-2-7b-Chat-GPTQ")
          #ERROR HAPPENS HERE</p>

          <p>If I try a different 7b gptq model it does''t give the error, for example:<br>model
          = AutoModelForCausalLM.from_pretrained("edumunozsala/llama-2-7b-int4-python-code-20k")</p>

          '
        raw: "When trying to load the model in google colab, I get the error:\r\n\r\
          \nImportError: Loading a GPTQ quantized model requires optimum (`pip install\
          \ optimum`) and auto-gptq library (`pip install auto-gptq`)\r\n\r\nMy code\
          \ has following:\r\n\r\n!pip install -q -U transformers peft accelerate\
          \ optimum\r\n!pip install auto-gptq\r\n\r\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\r\ntokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GPTQ\"\
          )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GPTQ\"\
          ) #ERROR HAPPENS HERE\r\n\r\nIf I try a different 7b gptq model it does't\
          \ give the error, for example:\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          edumunozsala/llama-2-7b-int4-python-code-20k\")"
        updatedAt: '2023-10-13T23:19:32.798Z'
      numEdits: 0
      reactions: []
    id: 6529d0844267f8c80230e2f9
    type: comment
  author: echogit
  content: "When trying to load the model in google colab, I get the error:\r\n\r\n\
    ImportError: Loading a GPTQ quantized model requires optimum (`pip install optimum`)\
    \ and auto-gptq library (`pip install auto-gptq`)\r\n\r\nMy code has following:\r\
    \n\r\n!pip install -q -U transformers peft accelerate optimum\r\n!pip install\
    \ auto-gptq\r\n\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
    \ntokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GPTQ\"\
    )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GPTQ\"\
    ) #ERROR HAPPENS HERE\r\n\r\nIf I try a different 7b gptq model it does't give\
    \ the error, for example:\r\nmodel = AutoModelForCausalLM.from_pretrained(\"edumunozsala/llama-2-7b-int4-python-code-20k\"\
    )"
  created_at: 2023-10-13 22:19:32+00:00
  edited: false
  hidden: false
  id: 6529d0844267f8c80230e2f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-14T07:08:32.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8265402317047119
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Not sure why it's working with that other model and not this one.\
          \  But please try installing AutoGPTQ as follows:</p>\n<pre><code>!pip install\
          \ auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\
          \ \n</code></pre>\n"
        raw: "Not sure why it's working with that other model and not this one.  But\
          \ please try installing AutoGPTQ as follows:\n```\n!pip install auto-gptq\
          \ --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\
          \ \n```"
        updatedAt: '2023-10-14T07:08:32.849Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - aClicheName
    id: 652a3e7062a488518964f02c
    type: comment
  author: TheBloke
  content: "Not sure why it's working with that other model and not this one.  But\
    \ please try installing AutoGPTQ as follows:\n```\n!pip install auto-gptq --extra-index-url\
    \ https://huggingface.github.io/autogptq-index/whl/cu118/ \n```"
  created_at: 2023-10-14 06:08:32+00:00
  edited: false
  hidden: false
  id: 652a3e7062a488518964f02c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07f6a62438fdc5c9eefcfe92b07436b9.svg
      fullname: Grigory Trofimov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GrigoryT22
      type: user
    createdAt: '2023-10-15T11:02:51.000Z'
    data:
      edited: false
      editors:
      - GrigoryT22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7593129873275757
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07f6a62438fdc5c9eefcfe92b07436b9.svg
          fullname: Grigory Trofimov
          isHf: false
          isPro: false
          name: GrigoryT22
          type: user
        html: '<p>as i understand error might be because transformers lib can not
          check existance of auto-gpt lib<br>_is_package_available function in transformers
          uses this code: "package_exists = importlib.util.find_spec(pkg_name) is
          not None"<br>the error might me in importlib lib, I cant find "util" module
          in it (Python 3.10.12, kaggle notebook)</p>

          '
        raw: 'as i understand error might be because transformers lib can not check
          existance of auto-gpt lib

          _is_package_available function in transformers uses this code: "package_exists
          = importlib.util.find_spec(pkg_name) is not None"

          the error might me in importlib lib, I cant find "util" module in it (Python
          3.10.12, kaggle notebook)'
        updatedAt: '2023-10-15T11:02:51.694Z'
      numEdits: 0
      reactions: []
    id: 652bc6dbec10d7e481f96858
    type: comment
  author: GrigoryT22
  content: 'as i understand error might be because transformers lib can not check
    existance of auto-gpt lib

    _is_package_available function in transformers uses this code: "package_exists
    = importlib.util.find_spec(pkg_name) is not None"

    the error might me in importlib lib, I cant find "util" module in it (Python 3.10.12,
    kaggle notebook)'
  created_at: 2023-10-15 10:02:51+00:00
  edited: false
  hidden: false
  id: 652bc6dbec10d7e481f96858
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ad840cad2fff5371fd3d7d882e80a9b.svg
      fullname: Omkar Malpure
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Omkar008
      type: user
    createdAt: '2023-10-25T13:09:12.000Z'
    data:
      edited: false
      editors:
      - Omkar008
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7924649715423584
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ad840cad2fff5371fd3d7d882e80a9b.svg
          fullname: Omkar Malpure
          isHf: false
          isPro: false
          name: Omkar008
          type: user
        html: '<p>yeah I am facing the same error ? But I got it working by using
          langchain Ctransformers .CTransformers(model="TheBloke/Llama-2-7b-Chat-GPTQ").
          But still I want to download this model using pretrained and then use it
          like on a local hardware . </p>

          '
        raw: 'yeah I am facing the same error ? But I got it working by using langchain
          Ctransformers .CTransformers(model="TheBloke/Llama-2-7b-Chat-GPTQ"). But
          still I want to download this model using pretrained and then use it like
          on a local hardware . '
        updatedAt: '2023-10-25T13:09:12.787Z'
      numEdits: 0
      reactions: []
    id: 65391378f84e87099cd540e5
    type: comment
  author: Omkar008
  content: 'yeah I am facing the same error ? But I got it working by using langchain
    Ctransformers .CTransformers(model="TheBloke/Llama-2-7b-Chat-GPTQ"). But still
    I want to download this model using pretrained and then use it like on a local
    hardware . '
  created_at: 2023-10-25 12:09:12+00:00
  edited: false
  hidden: false
  id: 65391378f84e87099cd540e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ad840cad2fff5371fd3d7d882e80a9b.svg
      fullname: Omkar Malpure
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Omkar008
      type: user
    createdAt: '2023-10-25T13:09:29.000Z'
    data:
      edited: false
      editors:
      - Omkar008
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9518961310386658
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ad840cad2fff5371fd3d7d882e80a9b.svg
          fullname: Omkar Malpure
          isHf: false
          isPro: false
          name: Omkar008
          type: user
        html: '<p>is there any solution ? </p>

          '
        raw: "is there any solution ? \n"
        updatedAt: '2023-10-25T13:09:29.896Z'
      numEdits: 0
      reactions: []
    id: 653913891d28ea99dcdde7ee
    type: comment
  author: Omkar008
  content: "is there any solution ? \n"
  created_at: 2023-10-25 12:09:29+00:00
  edited: false
  hidden: false
  id: 653913891d28ea99dcdde7ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/27431fcfdd56e210efe6ba219c3460a0.svg
      fullname: C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: echogit
      type: user
    createdAt: '2023-10-26T09:37:23.000Z'
    data:
      edited: false
      editors:
      - echogit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9734509587287903
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/27431fcfdd56e210efe6ba219c3460a0.svg
          fullname: C
          isHf: false
          isPro: false
          name: echogit
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span>  solution worked\
          \ for me </p>\n"
        raw: '@TheBloke  solution worked for me '
        updatedAt: '2023-10-26T09:37:23.297Z'
      numEdits: 0
      reactions: []
    id: 653a335348518d0b3a996f51
    type: comment
  author: echogit
  content: '@TheBloke  solution worked for me '
  created_at: 2023-10-26 08:37:23+00:00
  edited: false
  hidden: false
  id: 653a335348518d0b3a996f51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f1314bce85de9c244b4f9b2effdc3d40.svg
      fullname: Aritra Sen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aritrasen
      type: user
    createdAt: '2023-11-03T18:48:06.000Z'
    data:
      edited: false
      editors:
      - aritrasen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42939913272857666
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f1314bce85de9c244b4f9b2effdc3d40.svg
          fullname: Aritra Sen
          isHf: false
          isPro: false
          name: aritrasen
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64c36638027e6d96667b67e1/mzihpMH-fplxNHADWNFg2.png"><img
          alt="Screenshot_2023-11-04-00-16-40-446_com.android.chrome.png" src="https://cdn-uploads.huggingface.co/production/uploads/64c36638027e6d96667b67e1/mzihpMH-fplxNHADWNFg2.png"></a></p>

          <p>Please help with this error.</p>

          <p>from transformers import AutoTokenizer, pipeline, logging, AutoModelForCausalLM<br>#from
          auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig</p>

          <p>model_name_or_path = "TheBloke/Llama-2-7b-Chat-GPTQ"<br>model_basename
          = "model"</p>

          <p>use_triton = False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)</p>

          <p>model = AutoModelForCausalLM.from_pretrained(model_name_or_path,<br>        model_basename=model_basename,<br>        use_safetensors=True,<br>        trust_remote_code=True,<br>        device="cuda:0",<br>        use_triton=use_triton,<br>        quantize_config=None)</p>

          '
        raw: "\n![Screenshot_2023-11-04-00-16-40-446_com.android.chrome.png](https://cdn-uploads.huggingface.co/production/uploads/64c36638027e6d96667b67e1/mzihpMH-fplxNHADWNFg2.png)\n\
          \nPlease help with this error.\n\nfrom transformers import AutoTokenizer,\
          \ pipeline, logging, AutoModelForCausalLM\n#from auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\n\nmodel_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\
          \nmodel_basename = \"model\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n\n"
        updatedAt: '2023-11-03T18:48:06.041Z'
      numEdits: 0
      reactions: []
    id: 654540662a2a483042c92ff7
    type: comment
  author: aritrasen
  content: "\n![Screenshot_2023-11-04-00-16-40-446_com.android.chrome.png](https://cdn-uploads.huggingface.co/production/uploads/64c36638027e6d96667b67e1/mzihpMH-fplxNHADWNFg2.png)\n\
    \nPlease help with this error.\n\nfrom transformers import AutoTokenizer, pipeline,\
    \ logging, AutoModelForCausalLM\n#from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
    \nmodel_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\nmodel_basename = \"\
    model\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
    \        model_basename=model_basename,\n        use_safetensors=True,\n     \
    \   trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
    \        quantize_config=None)\n\n"
  created_at: 2023-11-03 17:48:06+00:00
  edited: false
  hidden: false
  id: 654540662a2a483042c92ff7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: TheBloke/Llama-2-7B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: AUTOGPTQ Error in Google Colab
