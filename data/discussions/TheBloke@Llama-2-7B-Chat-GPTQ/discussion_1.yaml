!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pseudotensor
conflicting_files: null
created_at: 2023-07-22 05:07:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
      fullname: Jonathan McKinney
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pseudotensor
      type: user
    createdAt: '2023-07-22T06:07:43.000Z'
    data:
      edited: false
      editors:
      - pseudotensor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7063394784927368
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
          fullname: Jonathan McKinney
          isHf: false
          isPro: false
          name: pseudotensor
          type: user
        html: '<p>Why do these have no ''max_length'' and have max_position_embeddings=2048
          while meta-llama ones have  "max_length": 4096,<br>  "max_position_embeddings":
          4096</p>

          <p>Can these not be properly used for 4k context?</p>

          '
        raw: "Why do these have no 'max_length' and have max_position_embeddings=2048\
          \ while meta-llama ones have  \"max_length\": 4096,\r\n  \"max_position_embeddings\"\
          : 4096\r\n\r\nCan these not be properly used for 4k context?"
        updatedAt: '2023-07-22T06:07:43.721Z'
      numEdits: 0
      reactions: []
    id: 64bb722fe38420aabaa1df23
    type: comment
  author: pseudotensor
  content: "Why do these have no 'max_length' and have max_position_embeddings=2048\
    \ while meta-llama ones have  \"max_length\": 4096,\r\n  \"max_position_embeddings\"\
    : 4096\r\n\r\nCan these not be properly used for 4k context?"
  created_at: 2023-07-22 05:07:43+00:00
  edited: false
  hidden: false
  id: 64bb722fe38420aabaa1df23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-22T08:15:11.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9140186905860901
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>They can be used for 4K context.  The difference in the config.json
          is because the original Meta config.json  files didn''t have max_length:
          4096 and I''ve not yet gone back to fix my config.json files.  But it shouldn''t
          affect inference in most clients, which have their own parameter for specifying
          length.</p>

          <p>Anyway, I will fix these config.json files today.</p>

          '
        raw: 'They can be used for 4K context.  The difference in the config.json
          is because the original Meta config.json  files didn''t have max_length:
          4096 and I''ve not yet gone back to fix my config.json files.  But it shouldn''t
          affect inference in most clients, which have their own parameter for specifying
          length.


          Anyway, I will fix these config.json files today.'
        updatedAt: '2023-07-22T08:15:11.377Z'
      numEdits: 0
      reactions: []
    id: 64bb900f976343e90a33a7fe
    type: comment
  author: TheBloke
  content: 'They can be used for 4K context.  The difference in the config.json is
    because the original Meta config.json  files didn''t have max_length: 4096 and
    I''ve not yet gone back to fix my config.json files.  But it shouldn''t affect
    inference in most clients, which have their own parameter for specifying length.


    Anyway, I will fix these config.json files today.'
  created_at: 2023-07-22 07:15:11+00:00
  edited: false
  hidden: false
  id: 64bb900f976343e90a33a7fe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Llama-2-7B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: '  "max_length": 4096,   "max_position_embeddings": 4096,'
