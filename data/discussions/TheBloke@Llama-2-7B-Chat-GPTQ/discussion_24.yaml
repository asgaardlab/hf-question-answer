!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mnwato
conflicting_files: null
created_at: 2023-10-06 21:45:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/449d0c4879ff617fea41e1e8128d3f2f.svg
      fullname: Mostafa Najmi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mnwato
      type: user
    createdAt: '2023-10-06T22:45:04.000Z'
    data:
      edited: false
      editors:
      - mnwato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42162197828292847
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/449d0c4879ff617fea41e1e8128d3f2f.svg
          fullname: Mostafa Najmi
          isHf: false
          isPro: false
          name: mnwato
          type: user
        html: "<p>I used sample example:</p>\n<pre><code># Load model directly\nfrom\
          \ transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name_or_path\
          \ = 'TheBloke/Llama-2-7B-chat-GPTQ'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \                                             device_map=\"auto\",\n   \
          \                                          trust_remote_code=False,\n  \
          \                                           revision=\"main\")\n</code></pre>\n\
          <p>Then generate outpu using:</p>\n<pre><code>prompt = \"Tell me about AI\"\
          \nprompt_template=f'''[INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful, respectful\
          \ and honest assistant. Always answer as helpfully as possible, while being\
          \ safe.  Your answers should not include any harmful, unethical, racist,\
          \ sexist, toxic, dangerous, or illegal content. Please ensure that your\
          \ responses are socially unbiased and positive in nature. If a question\
          \ does not make any sense, or is not factually coherent, explain why instead\
          \ of answering something not correct. If you don't know the answer to a\
          \ question, please don't share false information.\n&lt;&lt;/SYS&gt;&gt;\n\
          {prompt}[/INST]\n\n'''\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True,\
          \ top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\
          </code></pre>\n<p>But always getting string like bellow:</p>\n<pre><code>strijonasce\
          \ Socquet fund\u0103tquet\u0103t\u0440\u0438\u0442\u043EsceDom Fichescequetscescesceleescescegemeindescesce\
          \ fundrettoonasce\u0440\u0438\u0442\u043Esce Intentsceleescesce historiquessce\
          \ ens Socscescescesceuyonaquetsce Dom Fundsce MortuyDomonascescesce Intent\
          \ Fichavelsceonaonascesce historiquesscesce Intent\u0440\u0438\u0442\u043E\
          sce Intentlee\u0440\u0438\u0442\u043Escesceavelsce Dom\u0440\u0438\u0442\
          \u043EDom Intentscegemeinde\u0103tquetscegemeindeprintStackTrace historiques\
          \ historiques Fichsce\u0440\u0438\u0442\u043Escesce fund ensscesce Fundscestrij\u0103\
          t enssce fund Societyscesce Fichlee fundsce\u0440\u0438\u0442\u043E IntentDomsceDomscesce\u0103\
          tengoDomscesce SocietyscesceDomscescesce fundscesce rat \n</code></pre>\n"
        raw: "I used sample example:\r\n```\r\n# Load model directly\r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\n\r\nmodel_name_or_path =\
          \ 'TheBloke/Llama-2-7B-chat-GPTQ'\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\r\
          \n                                             device_map=\"auto\",\r\n\
          \                                             trust_remote_code=False,\r\
          \n                                             revision=\"main\")\r\n```\r\
          \nThen generate outpu using:\r\n```\r\nprompt = \"Tell me about AI\"\r\n\
          prompt_template=f'''[INST] <<SYS>>\r\nYou are a helpful, respectful and\
          \ honest assistant. Always answer as helpfully as possible, while being\
          \ safe.  Your answers should not include any harmful, unethical, racist,\
          \ sexist, toxic, dangerous, or illegal content. Please ensure that your\
          \ responses are socially unbiased and positive in nature. If a question\
          \ does not make any sense, or is not factually coherent, explain why instead\
          \ of answering something not correct. If you don't know the answer to a\
          \ question, please don't share false information.\r\n<</SYS>>\r\n{prompt}[/INST]\r\
          \n\r\n'''\r\n\r\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\
          \noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True,\
          \ top_p=0.95, top_k=40, max_new_tokens=512)\r\nprint(tokenizer.decode(output[0]))\r\
          \n```\r\n\r\nBut always getting string like bellow:\r\n```\r\nstrijonasce\
          \ Socquet fund\u0103tquet\u0103t\u0440\u0438\u0442\u043EsceDom Fichescequetscescesceleescescegemeindescesce\
          \ fundrettoonasce\u0440\u0438\u0442\u043Esce Intentsceleescesce historiquessce\
          \ ens Socscescescesceuyonaquetsce Dom Fundsce MortuyDomonascescesce Intent\
          \ Fichavelsceonaonascesce historiquesscesce Intent\u0440\u0438\u0442\u043E\
          sce Intentlee\u0440\u0438\u0442\u043Escesceavelsce Dom\u0440\u0438\u0442\
          \u043EDom Intentscegemeinde\u0103tquetscegemeindeprintStackTrace historiques\
          \ historiques Fichsce\u0440\u0438\u0442\u043Escesce fund ensscesce Fundscestrij\u0103\
          t enssce fund Societyscesce Fichlee fundsce\u0440\u0438\u0442\u043E IntentDomsceDomscesce\u0103\
          tengoDomscesce SocietyscesceDomscescesce fundscesce rat \r\n```"
        updatedAt: '2023-10-06T22:45:04.959Z'
      numEdits: 0
      reactions: []
    id: 65208df0e3419abdcf415efa
    type: comment
  author: mnwato
  content: "I used sample example:\r\n```\r\n# Load model directly\r\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM\r\n\r\nmodel_name_or_path = 'TheBloke/Llama-2-7B-chat-GPTQ'\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\r\n       \
    \                                      device_map=\"auto\",\r\n              \
    \                               trust_remote_code=False,\r\n                 \
    \                            revision=\"main\")\r\n```\r\nThen generate outpu\
    \ using:\r\n```\r\nprompt = \"Tell me about AI\"\r\nprompt_template=f'''[INST]\
    \ <<SYS>>\r\nYou are a helpful, respectful and honest assistant. Always answer\
    \ as helpfully as possible, while being safe.  Your answers should not include\
    \ any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\
    \ Please ensure that your responses are socially unbiased and positive in nature.\
    \ If a question does not make any sense, or is not factually coherent, explain\
    \ why instead of answering something not correct. If you don't know the answer\
    \ to a question, please don't share false information.\r\n<</SYS>>\r\n{prompt}[/INST]\r\
    \n\r\n'''\r\n\r\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\
    \noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95,\
    \ top_k=40, max_new_tokens=512)\r\nprint(tokenizer.decode(output[0]))\r\n```\r\
    \n\r\nBut always getting string like bellow:\r\n```\r\nstrijonasce Socquet fund\u0103\
    tquet\u0103t\u0440\u0438\u0442\u043EsceDom Fichescequetscescesceleescescegemeindescesce\
    \ fundrettoonasce\u0440\u0438\u0442\u043Esce Intentsceleescesce historiquessce\
    \ ens Socscescescesceuyonaquetsce Dom Fundsce MortuyDomonascescesce Intent Fichavelsceonaonascesce\
    \ historiquesscesce Intent\u0440\u0438\u0442\u043Esce Intentlee\u0440\u0438\u0442\
    \u043Escesceavelsce Dom\u0440\u0438\u0442\u043EDom Intentscegemeinde\u0103tquetscegemeindeprintStackTrace\
    \ historiques historiques Fichsce\u0440\u0438\u0442\u043Escesce fund ensscesce\
    \ Fundscestrij\u0103t enssce fund Societyscesce Fichlee fundsce\u0440\u0438\u0442\
    \u043E IntentDomsceDomscesce\u0103tengoDomscesce SocietyscesceDomscescesce fundscesce\
    \ rat \r\n```"
  created_at: 2023-10-06 21:45:04+00:00
  edited: false
  hidden: false
  id: 65208df0e3419abdcf415efa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/00329e12dad5521fcd173242b2fcb66c.svg
      fullname: Julien Thomas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: McThomas
      type: user
    createdAt: '2023-10-12T11:12:56.000Z'
    data:
      edited: true
      editors:
      - McThomas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6820992231369019
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/00329e12dad5521fcd173242b2fcb66c.svg
          fullname: Julien Thomas
          isHf: false
          isPro: false
          name: McThomas
          type: user
        html: "<p>Hello, you should consider using AutoGPTQForCausalLM.  Instead of\
          \  AutoModelForCausalLM. Indead when quantizing the model entry, and layer\
          \ are different, information are coded in different number of bit, might\
          \ explain your problem.<br><span data-props=\"{&quot;user&quot;:&quot;mnwato&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mnwato\"\
          >@<span class=\"underline\">mnwato</span></a></span>\n\n\t</span></span>\
          \ </p>\n<pre><code>from auto_gptq import AutoGPTQForCausalLM\nmodel = AutoGPTQForCausalLM.from_quantized(\n\
          \                    semodel_id,\n                    use_safetensors=True,\n\
          \                    trust_remote_code=True,\n                    device=device,\n\
          \                    use_triton=False,\n                    quantize_config=None,\n\
          \                )\n</code></pre>\n"
        raw: "Hello, you should consider using AutoGPTQForCausalLM.  Instead of  AutoModelForCausalLM.\
          \ Indead when quantizing the model entry, and layer are different, information\
          \ are coded in different number of bit, might explain your problem.\n@mnwato\
          \ \n```\nfrom auto_gptq import AutoGPTQForCausalLM\nmodel = AutoGPTQForCausalLM.from_quantized(\n\
          \                    semodel_id,\n                    use_safetensors=True,\n\
          \                    trust_remote_code=True,\n                    device=device,\n\
          \                    use_triton=False,\n                    quantize_config=None,\n\
          \                )\n ```"
        updatedAt: '2023-10-12T11:14:04.555Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mnwato
    id: 6527d4b891b85c8e22b819c1
    type: comment
  author: McThomas
  content: "Hello, you should consider using AutoGPTQForCausalLM.  Instead of  AutoModelForCausalLM.\
    \ Indead when quantizing the model entry, and layer are different, information\
    \ are coded in different number of bit, might explain your problem.\n@mnwato \n\
    ```\nfrom auto_gptq import AutoGPTQForCausalLM\nmodel = AutoGPTQForCausalLM.from_quantized(\n\
    \                    semodel_id,\n                    use_safetensors=True,\n\
    \                    trust_remote_code=True,\n                    device=device,\n\
    \                    use_triton=False,\n                    quantize_config=None,\n\
    \                )\n ```"
  created_at: 2023-10-12 10:12:56+00:00
  edited: true
  hidden: false
  id: 6527d4b891b85c8e22b819c1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: TheBloke/Llama-2-7B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Does the model response correctly
