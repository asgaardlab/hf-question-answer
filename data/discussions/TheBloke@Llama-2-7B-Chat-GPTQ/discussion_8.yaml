!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RonanMcGovern
conflicting_files: null
created_at: 2023-08-01 10:24:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-01T11:24:26.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6736367344856262
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Hi, and thanks for making this repo.</p>

          <p>Is there a way to specify a max_length for generation?</p>

          <p>My motivation is measuring tokens per second on generation of 100 tokens
          of output.</p>

          <p>BaseGPTQForCausalLM.generate doesn''t support max_length.</p>

          <p>Here is how I measure toks for models that accept max_length:</p>

          <pre><code>import time


          # Initialize the prompt

          prompt = "Once upon a time"


          # Encode the prompt

          input_ids = tokenizer.encode(prompt, return_tensors=''pt'').to(device)


          # Set the maximum length for generation

          max_length = 100


          # Record the start time

          start_time = time.time()


          # Generate the sequence

          output = model.generate(input_ids, max_length=max_length)


          # Record the end time

          end_time = time.time()


          # Calculate the tokens per second

          elapsed_time = end_time - start_time

          tokens_per_sec = max_length / elapsed_time


          print(f"Tokens per second: {tokens_per_sec}")

          </code></pre>

          '
        raw: "Hi, and thanks for making this repo.\r\n\r\nIs there a way to specify\
          \ a max_length for generation?\r\n\r\nMy motivation is measuring tokens\
          \ per second on generation of 100 tokens of output.\r\n\r\nBaseGPTQForCausalLM.generate\
          \ doesn't support max_length.\r\n\r\nHere is how I measure toks for models\
          \ that accept max_length:\r\n\r\n```\r\nimport time\r\n\r\n# Initialize\
          \ the prompt\r\nprompt = \"Once upon a time\"\r\n\r\n# Encode the prompt\r\
          \ninput_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\r\
          \n\r\n# Set the maximum length for generation\r\nmax_length = 100\r\n\r\n\
          # Record the start time\r\nstart_time = time.time()\r\n\r\n# Generate the\
          \ sequence\r\noutput = model.generate(input_ids, max_length=max_length)\r\
          \n\r\n# Record the end time\r\nend_time = time.time()\r\n\r\n# Calculate\
          \ the tokens per second\r\nelapsed_time = end_time - start_time\r\ntokens_per_sec\
          \ = max_length / elapsed_time\r\n\r\nprint(f\"Tokens per second: {tokens_per_sec}\"\
          )\r\n```"
        updatedAt: '2023-08-01T11:24:26.070Z'
      numEdits: 0
      reactions: []
    id: 64c8eb6ad418013c77f5ddf5
    type: comment
  author: RonanMcGovern
  content: "Hi, and thanks for making this repo.\r\n\r\nIs there a way to specify\
    \ a max_length for generation?\r\n\r\nMy motivation is measuring tokens per second\
    \ on generation of 100 tokens of output.\r\n\r\nBaseGPTQForCausalLM.generate doesn't\
    \ support max_length.\r\n\r\nHere is how I measure toks for models that accept\
    \ max_length:\r\n\r\n```\r\nimport time\r\n\r\n# Initialize the prompt\r\nprompt\
    \ = \"Once upon a time\"\r\n\r\n# Encode the prompt\r\ninput_ids = tokenizer.encode(prompt,\
    \ return_tensors='pt').to(device)\r\n\r\n# Set the maximum length for generation\r\
    \nmax_length = 100\r\n\r\n# Record the start time\r\nstart_time = time.time()\r\
    \n\r\n# Generate the sequence\r\noutput = model.generate(input_ids, max_length=max_length)\r\
    \n\r\n# Record the end time\r\nend_time = time.time()\r\n\r\n# Calculate the tokens\
    \ per second\r\nelapsed_time = end_time - start_time\r\ntokens_per_sec = max_length\
    \ / elapsed_time\r\n\r\nprint(f\"Tokens per second: {tokens_per_sec}\")\r\n```"
  created_at: 2023-08-01 10:24:26+00:00
  edited: false
  hidden: false
  id: 64c8eb6ad418013c77f5ddf5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-01T11:30:16.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6580144166946411
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>I've always used <code>max_new_tokens</code> - I thought that was\
          \ the correct option for <code>model.generate()</code> </p>\n<p>If you want\
          \ to benchmark AutoGPTQ (or any Transformers model), check out this benchmark\
          \ script that is provided with AutoGPTQ: <a rel=\"nofollow\" href=\"https://github.com/PanQiWei/AutoGPTQ/blob/main/examples/benchmark/generation_speed.py\"\
          >https://github.com/PanQiWei/AutoGPTQ/blob/main/examples/benchmark/generation_speed.py</a></p>\n\
          <p>Here is an example execution:</p>\n<pre><code>git clone https://github.com/PanQiWei/AutoGPTQ\n\
          cd AutoGPTQ\ncd examples/benchmark\npython3 generation_speed.py --model_name_or_path\
          \ TheBloke/Llama-2-7b-Chat-GPTQ  --use_safetensors --max_new_tokens 100\n\
          </code></pre>\n<p>This will automatically download this model (default branch)\
          \ and then test 10 executions with 100 tokens.   Run <code>python3 generation_speed.py\
          \ -h</code> to see other options, or just edit the script to do what you\
          \ need.</p>\n<p>If you already downloaded the model, you can pass its path\
          \ to <code>--model_name_or_path</code></p>\n<p>Example output:</p>\n<pre><code>\
          \ [pytorch2] tomj@d442126f7dde:/workspace/git/AutoGPTQ/examples/benchmark\
          \ (main \u2714) \u1405 CUDA_VISIBLE_DEVICES=0 python3 generation_speed.py\
          \ --model_name_or_path TheBloke/Llama-2-7b-Chat-GPTQ  --use_safetensors\
          \ --max_new_tokens 100\n2023-08-01 11:30:03 INFO [__main__] max_memory:\
          \ None\n2023-08-01 11:30:03 INFO [__main__] loading model and tokenizer\n\
          You are using the legacy behaviour of the &lt;class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'&gt;.\
          \ This means that tokens that come after special tokens will not be properly\
          \ handled. We recommend you to read the related pull request available at\
          \ https://github.com/huggingface/transformers/pull/24565\n2023-08-01 11:30:05\
          \ INFO [auto_gptq.modeling._base] lm_head not been quantized, will be ignored\
          \ when make_quant.\n2023-08-01 11:30:08 WARNING [accelerate.utils.modeling]\
          \ The safetensors archive passed at /workspace/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GPTQ/snapshots/b7ee6c20ac0bba85a310dc699d6bb4c845811608/gptq_model-4bit-128g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\n2023-08-01 11:30:09 WARNING [auto_gptq.nn_modules.fused_llama_mlp]\
          \ skip module injection for FusedLlamaMLPForQuantizedModel not support integrate\
          \ without triton yet.\n2023-08-01 11:30:09 INFO [__main__] model and tokenizer\
          \ loading time: 5.7821s\n2023-08-01 11:30:09 INFO [__main__] model quantized:\
          \ True\n2023-08-01 11:30:09 INFO [__main__] quantize config: {'bits': 4,\
          \ 'group_size': 128, 'damp_percent': 0.01, 'desc_act': False, 'sym': True,\
          \ 'true_sequential': True, 'model_name_or_path': 'TheBloke/Llama-2-7b-Chat-GPTQ',\
          \ 'model_file_base_name': 'gptq_model-4bit-128g'}\n2023-08-01 11:30:09 INFO\
          \ [__main__] model device map: {'': 0}\n2023-08-01 11:30:09 INFO [__main__]\
          \ loading data\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10\
          \ [00:00&lt;00:00, 509.02 examples/s]\n2023-08-01 11:30:09 INFO [__main__]\
          \ generation config: {'max_length': 20, 'max_new_tokens': 100, 'min_length':\
          \ 0, 'min_new_tokens': 100, 'early_stopping': False, 'max_time': None, 'do_sample':\
          \ False, 'num_beams': 1, 'num_beam_groups': 1, 'penalty_alpha': None, 'use_cache':\
          \ True, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0,\
          \ 'epsilon_cutoff': 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty':\
          \ 1.0, 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size':\
          \ 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits':\
          \ False, 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id':\
          \ None, 'remove_invalid_values': False, 'exponential_decay_length_penalty':\
          \ None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'forced_decoder_ids':\
          \ None, 'sequence_bias': None, 'guidance_scale': None, 'num_return_sequences':\
          \ 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores':\
          \ False, 'return_dict_in_generate': False, 'pad_token_id': 2, 'bos_token_id':\
          \ None, 'eos_token_id': None, 'encoder_no_repeat_ngram_size': 0, 'decoder_start_token_id':\
          \ None, 'generation_kwargs': {}, '_from_model_config': False, 'transformers_version':\
          \ '4.31.0'}\n2023-08-01 11:30:09 INFO [__main__] benchmark generation speed\n\
          100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:34&lt;00:00,  3.45s/it,\
          \ num_tokens=100, speed=30.1477tokens/s, time=3.32]\n2023-08-01 11:30:44\
          \ INFO [__main__] generated 1000 tokens using 34.478087186813354 seconds,\
          \ generation speed: 29.00392920818602tokens/s\n</code></pre>\n"
        raw: "I've always used `max_new_tokens` - I thought that was the correct option\
          \ for `model.generate()` \n\nIf you want to benchmark AutoGPTQ (or any Transformers\
          \ model), check out this benchmark script that is provided with AutoGPTQ:\
          \ https://github.com/PanQiWei/AutoGPTQ/blob/main/examples/benchmark/generation_speed.py\n\
          \nHere is an example execution:\n```\ngit clone https://github.com/PanQiWei/AutoGPTQ\n\
          cd AutoGPTQ\ncd examples/benchmark\npython3 generation_speed.py --model_name_or_path\
          \ TheBloke/Llama-2-7b-Chat-GPTQ  --use_safetensors --max_new_tokens 100\n\
          ```\n\nThis will automatically download this model (default branch) and\
          \ then test 10 executions with 100 tokens.   Run `python3 generation_speed.py\
          \ -h` to see other options, or just edit the script to do what you need.\n\
          \nIf you already downloaded the model, you can pass its path to `--model_name_or_path`\n\
          \nExample output:\n```\n [pytorch2] tomj@d442126f7dde:/workspace/git/AutoGPTQ/examples/benchmark\
          \ (main \u2714) \u1405 CUDA_VISIBLE_DEVICES=0 python3 generation_speed.py\
          \ --model_name_or_path TheBloke/Llama-2-7b-Chat-GPTQ  --use_safetensors\
          \ --max_new_tokens 100\n2023-08-01 11:30:03 INFO [__main__] max_memory:\
          \ None\n2023-08-01 11:30:03 INFO [__main__] loading model and tokenizer\n\
          You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>.\
          \ This means that tokens that come after special tokens will not be properly\
          \ handled. We recommend you to read the related pull request available at\
          \ https://github.com/huggingface/transformers/pull/24565\n2023-08-01 11:30:05\
          \ INFO [auto_gptq.modeling._base] lm_head not been quantized, will be ignored\
          \ when make_quant.\n2023-08-01 11:30:08 WARNING [accelerate.utils.modeling]\
          \ The safetensors archive passed at /workspace/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GPTQ/snapshots/b7ee6c20ac0bba85a310dc699d6bb4c845811608/gptq_model-4bit-128g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\n2023-08-01 11:30:09 WARNING [auto_gptq.nn_modules.fused_llama_mlp]\
          \ skip module injection for FusedLlamaMLPForQuantizedModel not support integrate\
          \ without triton yet.\n2023-08-01 11:30:09 INFO [__main__] model and tokenizer\
          \ loading time: 5.7821s\n2023-08-01 11:30:09 INFO [__main__] model quantized:\
          \ True\n2023-08-01 11:30:09 INFO [__main__] quantize config: {'bits': 4,\
          \ 'group_size': 128, 'damp_percent': 0.01, 'desc_act': False, 'sym': True,\
          \ 'true_sequential': True, 'model_name_or_path': 'TheBloke/Llama-2-7b-Chat-GPTQ',\
          \ 'model_file_base_name': 'gptq_model-4bit-128g'}\n2023-08-01 11:30:09 INFO\
          \ [__main__] model device map: {'': 0}\n2023-08-01 11:30:09 INFO [__main__]\
          \ loading data\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10\
          \ [00:00<00:00, 509.02 examples/s]\n2023-08-01 11:30:09 INFO [__main__]\
          \ generation config: {'max_length': 20, 'max_new_tokens': 100, 'min_length':\
          \ 0, 'min_new_tokens': 100, 'early_stopping': False, 'max_time': None, 'do_sample':\
          \ False, 'num_beams': 1, 'num_beam_groups': 1, 'penalty_alpha': None, 'use_cache':\
          \ True, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0,\
          \ 'epsilon_cutoff': 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty':\
          \ 1.0, 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size':\
          \ 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits':\
          \ False, 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id':\
          \ None, 'remove_invalid_values': False, 'exponential_decay_length_penalty':\
          \ None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'forced_decoder_ids':\
          \ None, 'sequence_bias': None, 'guidance_scale': None, 'num_return_sequences':\
          \ 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores':\
          \ False, 'return_dict_in_generate': False, 'pad_token_id': 2, 'bos_token_id':\
          \ None, 'eos_token_id': None, 'encoder_no_repeat_ngram_size': 0, 'decoder_start_token_id':\
          \ None, 'generation_kwargs': {}, '_from_model_config': False, 'transformers_version':\
          \ '4.31.0'}\n2023-08-01 11:30:09 INFO [__main__] benchmark generation speed\n\
          100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:34<00:00,  3.45s/it, num_tokens=100,\
          \ speed=30.1477tokens/s, time=3.32]\n2023-08-01 11:30:44 INFO [__main__]\
          \ generated 1000 tokens using 34.478087186813354 seconds, generation speed:\
          \ 29.00392920818602tokens/s\n```"
        updatedAt: '2023-08-01T11:31:51.089Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RonanMcGovern
    id: 64c8ecc86c2d4ce2c929c5da
    type: comment
  author: TheBloke
  content: "I've always used `max_new_tokens` - I thought that was the correct option\
    \ for `model.generate()` \n\nIf you want to benchmark AutoGPTQ (or any Transformers\
    \ model), check out this benchmark script that is provided with AutoGPTQ: https://github.com/PanQiWei/AutoGPTQ/blob/main/examples/benchmark/generation_speed.py\n\
    \nHere is an example execution:\n```\ngit clone https://github.com/PanQiWei/AutoGPTQ\n\
    cd AutoGPTQ\ncd examples/benchmark\npython3 generation_speed.py --model_name_or_path\
    \ TheBloke/Llama-2-7b-Chat-GPTQ  --use_safetensors --max_new_tokens 100\n```\n\
    \nThis will automatically download this model (default branch) and then test 10\
    \ executions with 100 tokens.   Run `python3 generation_speed.py -h` to see other\
    \ options, or just edit the script to do what you need.\n\nIf you already downloaded\
    \ the model, you can pass its path to `--model_name_or_path`\n\nExample output:\n\
    ```\n [pytorch2] tomj@d442126f7dde:/workspace/git/AutoGPTQ/examples/benchmark\
    \ (main \u2714) \u1405 CUDA_VISIBLE_DEVICES=0 python3 generation_speed.py --model_name_or_path\
    \ TheBloke/Llama-2-7b-Chat-GPTQ  --use_safetensors --max_new_tokens 100\n2023-08-01\
    \ 11:30:03 INFO [__main__] max_memory: None\n2023-08-01 11:30:03 INFO [__main__]\
    \ loading model and tokenizer\nYou are using the legacy behaviour of the <class\
    \ 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that\
    \ tokens that come after special tokens will not be properly handled. We recommend\
    \ you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n\
    2023-08-01 11:30:05 INFO [auto_gptq.modeling._base] lm_head not been quantized,\
    \ will be ignored when make_quant.\n2023-08-01 11:30:08 WARNING [accelerate.utils.modeling]\
    \ The safetensors archive passed at /workspace/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GPTQ/snapshots/b7ee6c20ac0bba85a310dc699d6bb4c845811608/gptq_model-4bit-128g.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\n2023-08-01 11:30:09 WARNING [auto_gptq.nn_modules.fused_llama_mlp]\
    \ skip module injection for FusedLlamaMLPForQuantizedModel not support integrate\
    \ without triton yet.\n2023-08-01 11:30:09 INFO [__main__] model and tokenizer\
    \ loading time: 5.7821s\n2023-08-01 11:30:09 INFO [__main__] model quantized:\
    \ True\n2023-08-01 11:30:09 INFO [__main__] quantize config: {'bits': 4, 'group_size':\
    \ 128, 'damp_percent': 0.01, 'desc_act': False, 'sym': True, 'true_sequential':\
    \ True, 'model_name_or_path': 'TheBloke/Llama-2-7b-Chat-GPTQ', 'model_file_base_name':\
    \ 'gptq_model-4bit-128g'}\n2023-08-01 11:30:09 INFO [__main__] model device map:\
    \ {'': 0}\n2023-08-01 11:30:09 INFO [__main__] loading data\nMap: 100%|\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 10/10 [00:00<00:00, 509.02 examples/s]\n2023-08-01 11:30:09 INFO\
    \ [__main__] generation config: {'max_length': 20, 'max_new_tokens': 100, 'min_length':\
    \ 0, 'min_new_tokens': 100, 'early_stopping': False, 'max_time': None, 'do_sample':\
    \ False, 'num_beams': 1, 'num_beam_groups': 1, 'penalty_alpha': None, 'use_cache':\
    \ True, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'epsilon_cutoff':\
    \ 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty': 1.0,\
    \ 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size':\
    \ 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits': False,\
    \ 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None,\
    \ 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens':\
    \ None, 'begin_suppress_tokens': None, 'forced_decoder_ids': None, 'sequence_bias':\
    \ None, 'guidance_scale': None, 'num_return_sequences': 1, 'output_attentions':\
    \ False, 'output_hidden_states': False, 'output_scores': False, 'return_dict_in_generate':\
    \ False, 'pad_token_id': 2, 'bos_token_id': None, 'eos_token_id': None, 'encoder_no_repeat_ngram_size':\
    \ 0, 'decoder_start_token_id': None, 'generation_kwargs': {}, '_from_model_config':\
    \ False, 'transformers_version': '4.31.0'}\n2023-08-01 11:30:09 INFO [__main__]\
    \ benchmark generation speed\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:34<00:00,\
    \  3.45s/it, num_tokens=100, speed=30.1477tokens/s, time=3.32]\n2023-08-01 11:30:44\
    \ INFO [__main__] generated 1000 tokens using 34.478087186813354 seconds, generation\
    \ speed: 29.00392920818602tokens/s\n```"
  created_at: 2023-08-01 10:30:16+00:00
  edited: true
  hidden: false
  id: 64c8ecc86c2d4ce2c929c5da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-01T11:30:46.000Z'
    data:
      edited: true
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8857638239860535
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>sorry, found that the issue was that I needed this syntax:</p>

          <pre><code>output = model.generate(**input_data, max_new_tokens=100)

          </code></pre>

          <p>And thanks for the tips on using a better approach for perplexity evaluation.
          I''ll dig into that.</p>

          '
        raw: 'sorry, found that the issue was that I needed this syntax:


          ```

          output = model.generate(**input_data, max_new_tokens=100)


          ```

          And thanks for the tips on using a better approach for perplexity evaluation.
          I''ll dig into that.'
        updatedAt: '2023-08-01T18:22:54.945Z'
      numEdits: 1
      reactions: []
      relatedEventId: 64c8ece61c25d2c581ae5472
    id: 64c8ece61c25d2c581ae546f
    type: comment
  author: RonanMcGovern
  content: 'sorry, found that the issue was that I needed this syntax:


    ```

    output = model.generate(**input_data, max_new_tokens=100)


    ```

    And thanks for the tips on using a better approach for perplexity evaluation.
    I''ll dig into that.'
  created_at: 2023-08-01 10:30:46+00:00
  edited: true
  hidden: false
  id: 64c8ece61c25d2c581ae546f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-01T11:30:46.000Z'
    data:
      status: closed
    id: 64c8ece61c25d2c581ae5472
    type: status-change
  author: RonanMcGovern
  created_at: 2023-08-01 10:30:46+00:00
  id: 64c8ece61c25d2c581ae5472
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/Llama-2-7B-Chat-GPTQ
repo_type: model
status: closed
target_branch: null
title: How to use max_length when generating
