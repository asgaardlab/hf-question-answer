!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Strider221B
conflicting_files: null
created_at: 2023-08-06 04:00:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9cdf4a5d15dcff261d9d803d8c6c7bb4.svg
      fullname: SC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Strider221B
      type: user
    createdAt: '2023-08-06T05:00:43.000Z'
    data:
      edited: false
      editors:
      - Strider221B
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.636012613773346
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9cdf4a5d15dcff261d9d803d8c6c7bb4.svg
          fullname: SC
          isHf: false
          isPro: false
          name: Strider221B
          type: user
        html: '<p>Hi,</p>

          <p>I am new to chat GPTQ models, so forgive me if I am asking a very basic
          question.</p>

          <p>Can you please tell me what is the GPU requirement to run this model?<br>I
          was trying to run this (for inference) on a NVIDIA Quadro P2000 GPU (Dedicated
          GPU memory: 5GB) and I keep getting CUDA out of memory error.</p>

          <p>If 5GB VRAM is too less, then can you suggest a chat GPTQ model which
          would run with this kind of GPU?<br>Also, is there any location in the documentation
          of these models which refer the basic GPU requirements for these models?</p>

          <p>Thanks!</p>

          <p>NVIDIA SMI output:<br>+-----------------------------------------------------------------------------+<br>|
          NVIDIA-SMI 522.06       Driver Version: 522.06       CUDA Version: 11.8     |<br>|-------------------------------+----------------------+----------------------+<br>|
          GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr.
          ECC |<br>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute
          M. |<br>|                               |                      |               MIG
          M. |<br>|===============================+======================+======================|<br>|   0  Quadro
          P2000       WDDM  | 00000000:2D:00.0 Off |                  N/A |<br>| 46%   34C    P8     5W
          /  75W |    399MiB /  5120MiB |      2%      Default |<br>|                               |                      |                  N/A
          |<br>+-------------------------------+----------------------+----------------------+</p>

          <p>+-----------------------------------------------------------------------------+<br>|
          Processes:                                                                  |<br>|  GPU   GI   CI        PID   Type   Process
          name                  GPU Memory |<br>|        ID   ID                                                   Usage      |<br>|=============================================================================|<br>|    0   N/A  N/A      5468    C+G   C:\Windows\explorer.exe         N/A      |<br>|    0   N/A  N/A      8748    C+G   ...w5n1h2txyewy\SearchUI.exe    N/A      |<br>|    0   N/A  N/A     11180    C+G   ...y\ShellExperienceHost.exe    N/A      |<br>|    0   N/A  N/A     34156    C+G   ...icrosoft
          VS Code\Code.exe    N/A      |<br>|    0   N/A  N/A     41660    C+G   ...zilla
          Firefox\firefox.exe    N/A      |<br>+-----------------------------------------------------------------------------+</p>

          '
        raw: "Hi,\r\n\r\nI am new to chat GPTQ models, so forgive me if I am asking\
          \ a very basic question.\r\n\r\nCan you please tell me what is the GPU requirement\
          \ to run this model?\r\nI was trying to run this (for inference) on a NVIDIA\
          \ Quadro P2000 GPU (Dedicated GPU memory: 5GB) and I keep getting CUDA out\
          \ of memory error.\r\n\r\nIf 5GB VRAM is too less, then can you suggest\
          \ a chat GPTQ model which would run with this kind of GPU?\r\nAlso, is there\
          \ any location in the documentation of these models which refer the basic\
          \ GPU requirements for these models?\r\n\r\nThanks!\r\n\r\nNVIDIA SMI output:\r\
          \n+-----------------------------------------------------------------------------+\r\
          \n| NVIDIA-SMI 522.06       Driver Version: 522.06       CUDA Version: 11.8\
          \     |\r\n|-------------------------------+----------------------+----------------------+\r\
          \n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr.\
          \ ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util\
          \  Compute M. |\r\n|                               |                   \
          \   |               MIG M. |\r\n|===============================+======================+======================|\r\
          \n|   0  Quadro P2000       WDDM  | 00000000:2D:00.0 Off |             \
          \     N/A |\r\n| 46%   34C    P8     5W /  75W |    399MiB /  5120MiB |\
          \      2%      Default |\r\n|                               |          \
          \            |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\
          \n\r\n+-----------------------------------------------------------------------------+\r\
          \n| Processes:                                                         \
          \         |\r\n|  GPU   GI   CI        PID   Type   Process name       \
          \           GPU Memory |\r\n|        ID   ID                           \
          \                        Usage      |\r\n|=============================================================================|\r\
          \n|    0   N/A  N/A      5468    C+G   C:\\Windows\\explorer.exe       \
          \  N/A      |\r\n|    0   N/A  N/A      8748    C+G   ...w5n1h2txyewy\\\
          SearchUI.exe    N/A      |\r\n|    0   N/A  N/A     11180    C+G   ...y\\\
          ShellExperienceHost.exe    N/A      |\r\n|    0   N/A  N/A     34156   \
          \ C+G   ...icrosoft VS Code\\Code.exe    N/A      |\r\n|    0   N/A  N/A\
          \     41660    C+G   ...zilla Firefox\\firefox.exe    N/A      |\r\n+-----------------------------------------------------------------------------+"
        updatedAt: '2023-08-06T05:00:43.052Z'
      numEdits: 0
      reactions: []
    id: 64cf28fb8749302d8139c3d6
    type: comment
  author: Strider221B
  content: "Hi,\r\n\r\nI am new to chat GPTQ models, so forgive me if I am asking\
    \ a very basic question.\r\n\r\nCan you please tell me what is the GPU requirement\
    \ to run this model?\r\nI was trying to run this (for inference) on a NVIDIA Quadro\
    \ P2000 GPU (Dedicated GPU memory: 5GB) and I keep getting CUDA out of memory\
    \ error.\r\n\r\nIf 5GB VRAM is too less, then can you suggest a chat GPTQ model\
    \ which would run with this kind of GPU?\r\nAlso, is there any location in the\
    \ documentation of these models which refer the basic GPU requirements for these\
    \ models?\r\n\r\nThanks!\r\n\r\nNVIDIA SMI output:\r\n+-----------------------------------------------------------------------------+\r\
    \n| NVIDIA-SMI 522.06       Driver Version: 522.06       CUDA Version: 11.8  \
    \   |\r\n|-------------------------------+----------------------+----------------------+\r\
    \n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC\
    \ |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute\
    \ M. |\r\n|                               |                      |           \
    \    MIG M. |\r\n|===============================+======================+======================|\r\
    \n|   0  Quadro P2000       WDDM  | 00000000:2D:00.0 Off |                  N/A\
    \ |\r\n| 46%   34C    P8     5W /  75W |    399MiB /  5120MiB |      2%      Default\
    \ |\r\n|                               |                      |              \
    \    N/A |\r\n+-------------------------------+----------------------+----------------------+\r\
    \n\r\n+-----------------------------------------------------------------------------+\r\
    \n| Processes:                                                               \
    \   |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU\
    \ Memory |\r\n|        ID   ID                                               \
    \    Usage      |\r\n|=============================================================================|\r\
    \n|    0   N/A  N/A      5468    C+G   C:\\Windows\\explorer.exe         N/A \
    \     |\r\n|    0   N/A  N/A      8748    C+G   ...w5n1h2txyewy\\SearchUI.exe\
    \    N/A      |\r\n|    0   N/A  N/A     11180    C+G   ...y\\ShellExperienceHost.exe\
    \    N/A      |\r\n|    0   N/A  N/A     34156    C+G   ...icrosoft VS Code\\\
    Code.exe    N/A      |\r\n|    0   N/A  N/A     41660    C+G   ...zilla Firefox\\\
    firefox.exe    N/A      |\r\n+-----------------------------------------------------------------------------+"
  created_at: 2023-08-06 04:00:43+00:00
  edited: false
  hidden: false
  id: 64cf28fb8749302d8139c3d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-08T10:33:58.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8605352640151978
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>It may just not be possible to run with that small of a GPU.</p>

          <p>Ideas:</p>

          <ul>

          <li>Run on a free Google Colab notebook: <a rel="nofollow" href="https://colab.research.google.com/drive/1iRNIVACiRGjvCrCn4ClAAW9-KoHj9W2z?usp=sharing">https://colab.research.google.com/drive/1iRNIVACiRGjvCrCn4ClAAW9-KoHj9W2z?usp=sharing</a></li>

          <li>Try out the smallest ggml model from TheBloke</li>

          <li>Run on cpu instead of gpu (this will be slower but at least you could
          maybe run)</li>

          </ul>

          <p>Perhaps there are other ideas I''m missing.</p>

          '
        raw: 'It may just not be possible to run with that small of a GPU.


          Ideas:

          - Run on a free Google Colab notebook: https://colab.research.google.com/drive/1iRNIVACiRGjvCrCn4ClAAW9-KoHj9W2z?usp=sharing

          - Try out the smallest ggml model from TheBloke

          - Run on cpu instead of gpu (this will be slower but at least you could
          maybe run)


          Perhaps there are other ideas I''m missing.'
        updatedAt: '2023-08-08T10:33:58.459Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Strider221B
    id: 64d21a1654d87aee8286a27c
    type: comment
  author: RonanMcGovern
  content: 'It may just not be possible to run with that small of a GPU.


    Ideas:

    - Run on a free Google Colab notebook: https://colab.research.google.com/drive/1iRNIVACiRGjvCrCn4ClAAW9-KoHj9W2z?usp=sharing

    - Try out the smallest ggml model from TheBloke

    - Run on cpu instead of gpu (this will be slower but at least you could maybe
    run)


    Perhaps there are other ideas I''m missing.'
  created_at: 2023-08-08 09:33:58+00:00
  edited: false
  hidden: false
  id: 64d21a1654d87aee8286a27c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9cdf4a5d15dcff261d9d803d8c6c7bb4.svg
      fullname: SC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Strider221B
      type: user
    createdAt: '2023-08-08T13:47:58.000Z'
    data:
      edited: false
      editors:
      - Strider221B
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9740949869155884
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9cdf4a5d15dcff261d9d803d8c6c7bb4.svg
          fullname: SC
          isHf: false
          isPro: false
          name: Strider221B
          type: user
        html: '<p>Thanks Ronan for your reply. I''ll take a look into the Colab notebook
          you have shared.<br>I checked the GGML model, that works fine. For now I''ll
          just use that.</p>

          <p>Thanks!</p>

          '
        raw: 'Thanks Ronan for your reply. I''ll take a look into the Colab notebook
          you have shared.

          I checked the GGML model, that works fine. For now I''ll just use that.


          Thanks!'
        updatedAt: '2023-08-08T13:47:58.909Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64d2478e962576caba0e8d8b
    id: 64d2478e962576caba0e8d8a
    type: comment
  author: Strider221B
  content: 'Thanks Ronan for your reply. I''ll take a look into the Colab notebook
    you have shared.

    I checked the GGML model, that works fine. For now I''ll just use that.


    Thanks!'
  created_at: 2023-08-08 12:47:58+00:00
  edited: false
  hidden: false
  id: 64d2478e962576caba0e8d8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9cdf4a5d15dcff261d9d803d8c6c7bb4.svg
      fullname: SC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Strider221B
      type: user
    createdAt: '2023-08-08T13:47:58.000Z'
    data:
      status: closed
    id: 64d2478e962576caba0e8d8b
    type: status-change
  author: Strider221B
  created_at: 2023-08-08 12:47:58+00:00
  id: 64d2478e962576caba0e8d8b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: TheBloke/Llama-2-7B-Chat-GPTQ
repo_type: model
status: closed
target_branch: null
title: GPU requirements
