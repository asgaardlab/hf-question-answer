!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Moneymaker2023
conflicting_files: null
created_at: 2023-08-01 09:18:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
      fullname: Catalin Ciocea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moneymaker2023
      type: user
    createdAt: '2023-08-01T10:18:55.000Z'
    data:
      edited: false
      editors:
      - Moneymaker2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9324275851249695
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
          fullname: Catalin Ciocea
          isHf: false
          isPro: false
          name: Moneymaker2023
          type: user
        html: '<p>Hello, sorry for offtopic but I''m new here and don''t know other
          way to message you. You are doin a very great job for comunity providing
          so much models quantised for allowing ppls with low computing resources
          to run models on their computers. Thank you very much for that! Please quantise
          these models in GPTQ 4 bits (it will be ideally to run on laptops with Nvidia
          4050 6gb vram or higher ) togethercomputer/LLaMA-2-7B-32K  ; cerebras/btlm-3b-8k-base
          ; James-WYang/BigTranslate . Thank you very much if you can help! And thanks
          anyway for the great job you are doing for all !</p>

          '
        raw: Hello, sorry for offtopic but I'm new here and don't know other way to
          message you. You are doin a very great job for comunity providing so much
          models quantised for allowing ppls with low computing resources to run models
          on their computers. Thank you very much for that! Please quantise these
          models in GPTQ 4 bits (it will be ideally to run on laptops with Nvidia
          4050 6gb vram or higher ) togethercomputer/LLaMA-2-7B-32K  ; cerebras/btlm-3b-8k-base
          ; James-WYang/BigTranslate . Thank you very much if you can help! And thanks
          anyway for the great job you are doing for all !
        updatedAt: '2023-08-01T10:18:55.326Z'
      numEdits: 0
      reactions: []
    id: 64c8dc0f3d5a0dfed5c5ebd2
    type: comment
  author: Moneymaker2023
  content: Hello, sorry for offtopic but I'm new here and don't know other way to
    message you. You are doin a very great job for comunity providing so much models
    quantised for allowing ppls with low computing resources to run models on their
    computers. Thank you very much for that! Please quantise these models in GPTQ
    4 bits (it will be ideally to run on laptops with Nvidia 4050 6gb vram or higher
    ) togethercomputer/LLaMA-2-7B-32K  ; cerebras/btlm-3b-8k-base ; James-WYang/BigTranslate
    . Thank you very much if you can help! And thanks anyway for the great job you
    are doing for all !
  created_at: 2023-08-01 09:18:55+00:00
  edited: false
  hidden: false
  id: 64c8dc0f3d5a0dfed5c5ebd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-01T10:25:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9658200740814209
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You''re welcome!</p>

          <p>I''ve already done BigTranslate: <a href="https://huggingface.co/TheBloke/BigTrans-13B-GPTQ">https://huggingface.co/TheBloke/BigTrans-13B-GPTQ</a></p>

          <p>I will look at llama 2 32k later today.  </p>

          <p>I don''t think I can do btlm as it''s a new model format not supported
          by GPTQ or GGML yet.</p>

          '
        raw: "You're welcome!\n\nI've already done BigTranslate: https://huggingface.co/TheBloke/BigTrans-13B-GPTQ\n\
          \nI will look at llama 2 32k later today.  \n\nI don't think I can do btlm\
          \ as it's a new model format not supported by GPTQ or GGML yet."
        updatedAt: '2023-08-01T10:25:16.882Z'
      numEdits: 0
      reactions: []
    id: 64c8dd8cc547ed5243e8071d
    type: comment
  author: TheBloke
  content: "You're welcome!\n\nI've already done BigTranslate: https://huggingface.co/TheBloke/BigTrans-13B-GPTQ\n\
    \nI will look at llama 2 32k later today.  \n\nI don't think I can do btlm as\
    \ it's a new model format not supported by GPTQ or GGML yet."
  created_at: 2023-08-01 09:25:16+00:00
  edited: false
  hidden: false
  id: 64c8dd8cc547ed5243e8071d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
      fullname: Catalin Ciocea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moneymaker2023
      type: user
    createdAt: '2023-08-01T11:12:35.000Z'
    data:
      edited: true
      editors:
      - Moneymaker2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9249517917633057
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
          fullname: Catalin Ciocea
          isHf: false
          isPro: false
          name: Moneymaker2023
          type: user
        html: '<p>I can run btlm on Oobabooga with Transformers and load in 4 bit
          , with a reduced max_new_tokens (model reply size) . This make me belive
          it can be quantised too with GPTQ in 4 bits ( a noob oppinion you are the
          expert here :) ) .  Anyway to reduce the BigTrans-13B model size further
          to fit in a 6gb vram ? Your quantised version in GPTQ have 7,9 gb . Is it
          posible to quatise it in 3bit, 2 bit or anything that can squueze it more
          si that it fit in a videocard with 6gb videoram ?</p>

          '
        raw: I can run btlm on Oobabooga with Transformers and load in 4 bit , with
          a reduced max_new_tokens (model reply size) . This make me belive it can
          be quantised too with GPTQ in 4 bits ( a noob oppinion you are the expert
          here :) ) .  Anyway to reduce the BigTrans-13B model size further to fit
          in a 6gb vram ? Your quantised version in GPTQ have 7,9 gb . Is it posible
          to quatise it in 3bit, 2 bit or anything that can squueze it more si that
          it fit in a videocard with 6gb videoram ?
        updatedAt: '2023-08-01T11:15:57.115Z'
      numEdits: 1
      reactions: []
    id: 64c8e8a31f9614c3e8afb0b4
    type: comment
  author: Moneymaker2023
  content: I can run btlm on Oobabooga with Transformers and load in 4 bit , with
    a reduced max_new_tokens (model reply size) . This make me belive it can be quantised
    too with GPTQ in 4 bits ( a noob oppinion you are the expert here :) ) .  Anyway
    to reduce the BigTrans-13B model size further to fit in a 6gb vram ? Your quantised
    version in GPTQ have 7,9 gb . Is it posible to quatise it in 3bit, 2 bit or anything
    that can squueze it more si that it fit in a videocard with 6gb videoram ?
  created_at: 2023-08-01 10:12:35+00:00
  edited: true
  hidden: false
  id: 64c8e8a31f9614c3e8afb0b4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/Llama-2-7B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Please make this model quantised GPTQ
