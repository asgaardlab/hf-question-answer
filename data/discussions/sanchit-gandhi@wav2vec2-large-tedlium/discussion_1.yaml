!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mutiann
conflicting_files: null
created_at: 2022-07-27 07:46:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-07-27T08:46:49.000Z'
    data:
      edited: false
      editors:
      - mutiann
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
          fullname: Mutian He
          isHf: false
          isPro: false
          name: mutiann
          type: user
        html: '<p>I''m recently fine-tuning W2V2 on TEDLIUM v3 as well, while I could
          only reach ~20% WER, even though I used your hyperparameters and reach similar
          training losses. Is there any special trick during training? Is it possible
          for you to share the code or more details?</p>

          <p>Thanks in advance!</p>

          '
        raw: "I'm recently fine-tuning W2V2 on TEDLIUM v3 as well, while I could only\
          \ reach ~20% WER, even though I used your hyperparameters and reach similar\
          \ training losses. Is there any special trick during training? Is it possible\
          \ for you to share the code or more details?\r\n\r\nThanks in advance!"
        updatedAt: '2022-07-27T08:46:49.612Z'
      numEdits: 0
      reactions: []
    id: 62e0fb793eb0730f621484da
    type: comment
  author: mutiann
  content: "I'm recently fine-tuning W2V2 on TEDLIUM v3 as well, while I could only\
    \ reach ~20% WER, even though I used your hyperparameters and reach similar training\
    \ losses. Is there any special trick during training? Is it possible for you to\
    \ share the code or more details?\r\n\r\nThanks in advance!"
  created_at: 2022-07-27 07:46:49+00:00
  edited: false
  hidden: false
  id: 62e0fb793eb0730f621484da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-07-27T09:15:07.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;mutiann&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mutiann\"\
          >@<span class=\"underline\">mutiann</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>How many train steps did you train for? This model was trained over 50k\
          \ train steps. The WER was still decreasing after 50k train steps, so if\
          \ you had longer you could probably go even better. Also what pre-processing\
          \ steps did you employ? I'd highly recommend removing the <code>&lt;unk&gt;</code>\
          \ token from the training transcriptions: <a rel=\"nofollow\" href=\"https://github.com/sanchit-gandhi/seq2seq-speech/blob/cfc6d73959486f5bd71c623ddd95843d62f5a614/run_flax_speech_recognition_ctc.py#L986\"\
          >https://github.com/sanchit-gandhi/seq2seq-speech/blob/cfc6d73959486f5bd71c623ddd95843d62f5a614/run_flax_speech_recognition_ctc.py#L986</a><br>These\
          \ <code>&lt;unk&gt;</code> tokens only appear in the train split (not the\
          \ dev or test) and are commonly removed in the literature.</p>\n<p>The script\
          \ used to train the model was: <a rel=\"nofollow\" href=\"https://github.com/sanchit-gandhi/seq2seq-speech/blob/main/run_flax_speech_recognition_ctc.py\"\
          >https://github.com/sanchit-gandhi/seq2seq-speech/blob/main/run_flax_speech_recognition_ctc.py</a><br>It\
          \ was trained on a TPU v3-8 for 50k train steps (~28 hours), training logs\
          \ here: <a rel=\"nofollow\" href=\"https://wandb.ai/sanchit-gandhi/tedlium/runs/10c85yc4?workspace=user-sanchit-gandhi\"\
          >https://wandb.ai/sanchit-gandhi/tedlium/runs/10c85yc4?workspace=user-sanchit-gandhi</a></p>\n\
          <p>The command used to train the model:</p>\n<pre><code>python run_flax_speech_recognition_ctc.py\
          \ --model_name_or_path=speech-seq2seq/flax-wav2vec2-large-lv60-scan --tokenizer_name=sanchit-gandhi/wav2vec2_ctc_tedlium_tokenizer\
          \ --dataset_name=LIUM/tedlium --dataset_config_name=release3 --train_split_name=train\
          \ --eval_split_name=validation --test_split_name=test --text_column_name=text\
          \ --hidden_dropout=0.2 --activation_dropout=0.2 --feat_proj_dropout=0.2\
          \ --output_dir=./flax-wav2vec2-ctc-tedlium-hidden-activation-featproj-dropout-0.2\
          \ --wandb_project=tedlium --wandb_name=flax-wav2vec2-ctc-tedlium-hidden-activation-featproj-dropout-0.2\
          \ --dataset_cache_dir=/home/sanchitgandhi/cache/huggingface/datasets --max_steps=50000\
          \ --save_steps=10000 --eval_steps=10000 --learning_rate=3e-4 --logging_steps=25\
          \ --warmup_steps=5000 --preprocessing_num_workers=1 --do_train --do_eval\
          \ --do_predict --overwrite_output_dir --gradient_checkpointing --freeze_feature_encoder\
          \ --push_to_hub --use_auth_token\n</code></pre>\n"
        raw: 'Hey @mutiann!


          How many train steps did you train for? This model was trained over 50k
          train steps. The WER was still decreasing after 50k train steps, so if you
          had longer you could probably go even better. Also what pre-processing steps
          did you employ? I''d highly recommend removing the `<unk>` token from the
          training transcriptions: https://github.com/sanchit-gandhi/seq2seq-speech/blob/cfc6d73959486f5bd71c623ddd95843d62f5a614/run_flax_speech_recognition_ctc.py#L986

          These `<unk>` tokens only appear in the train split (not the dev or test)
          and are commonly removed in the literature.


          The script used to train the model was: https://github.com/sanchit-gandhi/seq2seq-speech/blob/main/run_flax_speech_recognition_ctc.py

          It was trained on a TPU v3-8 for 50k train steps (~28 hours), training logs
          here: https://wandb.ai/sanchit-gandhi/tedlium/runs/10c85yc4?workspace=user-sanchit-gandhi


          The command used to train the model:

          ```

          python run_flax_speech_recognition_ctc.py --model_name_or_path=speech-seq2seq/flax-wav2vec2-large-lv60-scan
          --tokenizer_name=sanchit-gandhi/wav2vec2_ctc_tedlium_tokenizer --dataset_name=LIUM/tedlium
          --dataset_config_name=release3 --train_split_name=train --eval_split_name=validation
          --test_split_name=test --text_column_name=text --hidden_dropout=0.2 --activation_dropout=0.2
          --feat_proj_dropout=0.2 --output_dir=./flax-wav2vec2-ctc-tedlium-hidden-activation-featproj-dropout-0.2
          --wandb_project=tedlium --wandb_name=flax-wav2vec2-ctc-tedlium-hidden-activation-featproj-dropout-0.2
          --dataset_cache_dir=/home/sanchitgandhi/cache/huggingface/datasets --max_steps=50000
          --save_steps=10000 --eval_steps=10000 --learning_rate=3e-4 --logging_steps=25
          --warmup_steps=5000 --preprocessing_num_workers=1 --do_train --do_eval --do_predict
          --overwrite_output_dir --gradient_checkpointing --freeze_feature_encoder
          --push_to_hub --use_auth_token

          ```'
        updatedAt: '2022-07-27T09:16:14.143Z'
      numEdits: 2
      reactions: []
    id: 62e1021b3eb0730f6214b175
    type: comment
  author: sanchit-gandhi
  content: 'Hey @mutiann!


    How many train steps did you train for? This model was trained over 50k train
    steps. The WER was still decreasing after 50k train steps, so if you had longer
    you could probably go even better. Also what pre-processing steps did you employ?
    I''d highly recommend removing the `<unk>` token from the training transcriptions:
    https://github.com/sanchit-gandhi/seq2seq-speech/blob/cfc6d73959486f5bd71c623ddd95843d62f5a614/run_flax_speech_recognition_ctc.py#L986

    These `<unk>` tokens only appear in the train split (not the dev or test) and
    are commonly removed in the literature.


    The script used to train the model was: https://github.com/sanchit-gandhi/seq2seq-speech/blob/main/run_flax_speech_recognition_ctc.py

    It was trained on a TPU v3-8 for 50k train steps (~28 hours), training logs here:
    https://wandb.ai/sanchit-gandhi/tedlium/runs/10c85yc4?workspace=user-sanchit-gandhi


    The command used to train the model:

    ```

    python run_flax_speech_recognition_ctc.py --model_name_or_path=speech-seq2seq/flax-wav2vec2-large-lv60-scan
    --tokenizer_name=sanchit-gandhi/wav2vec2_ctc_tedlium_tokenizer --dataset_name=LIUM/tedlium
    --dataset_config_name=release3 --train_split_name=train --eval_split_name=validation
    --test_split_name=test --text_column_name=text --hidden_dropout=0.2 --activation_dropout=0.2
    --feat_proj_dropout=0.2 --output_dir=./flax-wav2vec2-ctc-tedlium-hidden-activation-featproj-dropout-0.2
    --wandb_project=tedlium --wandb_name=flax-wav2vec2-ctc-tedlium-hidden-activation-featproj-dropout-0.2
    --dataset_cache_dir=/home/sanchitgandhi/cache/huggingface/datasets --max_steps=50000
    --save_steps=10000 --eval_steps=10000 --learning_rate=3e-4 --logging_steps=25
    --warmup_steps=5000 --preprocessing_num_workers=1 --do_train --do_eval --do_predict
    --overwrite_output_dir --gradient_checkpointing --freeze_feature_encoder --push_to_hub
    --use_auth_token

    ```'
  created_at: 2022-07-27 08:15:07+00:00
  edited: true
  hidden: false
  id: 62e1021b3eb0730f6214b175
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-07-28T10:33:16.000Z'
    data:
      status: closed
    id: 62e265ec47678ea5ce1b4cc2
    type: status-change
  author: sanchit-gandhi
  created_at: 2022-07-28 09:33:16+00:00
  id: 62e265ec47678ea5ce1b4cc2
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-07-28T10:59:15.000Z'
    data:
      edited: false
      editors:
      - mutiann
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
          fullname: Mutian He
          isHf: false
          isPro: false
          name: mutiann
          type: user
        html: '<p>Thank you!</p>

          <p>By removing UNKs the WER decreases to ~15%. It does matter!</p>

          <p>I feel that the batch size might be an issue, as you use a total 64 batch
          size, while I don''t have so many GPUs :( I''m now limiting each batch to
          a total 2.4e6, which corresponds to a batch size of 24.6 in average.<br>How
          much impact do you think the batch size could have?</p>

          '
        raw: 'Thank you!


          By removing UNKs the WER decreases to ~15%. It does matter!


          I feel that the batch size might be an issue, as you use a total 64 batch
          size, while I don''t have so many GPUs :( I''m now limiting each batch to
          a total 2.4e6, which corresponds to a batch size of 24.6 in average.

          How much impact do you think the batch size could have?'
        updatedAt: '2022-07-28T10:59:15.933Z'
      numEdits: 0
      reactions: []
    id: 62e26c033715ed86cbc0f5fd
    type: comment
  author: mutiann
  content: 'Thank you!


    By removing UNKs the WER decreases to ~15%. It does matter!


    I feel that the batch size might be an issue, as you use a total 64 batch size,
    while I don''t have so many GPUs :( I''m now limiting each batch to a total 2.4e6,
    which corresponds to a batch size of 24.6 in average.

    How much impact do you think the batch size could have?'
  created_at: 2022-07-28 09:59:15+00:00
  edited: false
  hidden: false
  id: 62e26c033715ed86cbc0f5fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-07-28T11:05:18.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>Great, that''s good to hear!</p>

          <p>In Flax, I struggled to get training to work with a batch size of 16.
          It fared better with 32+. Could you employ gradient accumulation to bump
          your effective batch size? </p>

          '
        raw: 'Great, that''s good to hear!


          In Flax, I struggled to get training to work with a batch size of 16. It
          fared better with 32+. Could you employ gradient accumulation to bump your
          effective batch size? '
        updatedAt: '2022-07-28T11:05:18.993Z'
      numEdits: 0
      reactions: []
      relatedEventId: 62e26d6e3715ed86cbc0ff98
    id: 62e26d6e3715ed86cbc0ff97
    type: comment
  author: sanchit-gandhi
  content: 'Great, that''s good to hear!


    In Flax, I struggled to get training to work with a batch size of 16. It fared
    better with 32+. Could you employ gradient accumulation to bump your effective
    batch size? '
  created_at: 2022-07-28 10:05:18+00:00
  edited: false
  hidden: false
  id: 62e26d6e3715ed86cbc0ff97
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-07-28T11:05:18.000Z'
    data:
      status: open
    id: 62e26d6e3715ed86cbc0ff98
    type: status-change
  author: sanchit-gandhi
  created_at: 2022-07-28 10:05:18+00:00
  id: 62e26d6e3715ed86cbc0ff98
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-07-29T08:23:36.000Z'
    data:
      edited: true
      editors:
      - mutiann
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
          fullname: Mutian He
          isHf: false
          isPro: false
          name: mutiann
          type: user
        html: '<p>I tried that and with a ~72 effective batch size, WER reduced to
          ~14%...<br>I guess I should further check your settings and see if I miss
          anything...</p>

          '
        raw: 'I tried that and with a ~72 effective batch size, WER reduced to ~14%...

          I guess I should further check your settings and see if I miss anything...'
        updatedAt: '2022-07-29T08:23:45.843Z'
      numEdits: 1
      reactions: []
    id: 62e39908697ead3a4f1a1f78
    type: comment
  author: mutiann
  content: 'I tried that and with a ~72 effective batch size, WER reduced to ~14%...

    I guess I should further check your settings and see if I miss anything...'
  created_at: 2022-07-29 07:23:36+00:00
  edited: true
  hidden: false
  id: 62e39908697ead3a4f1a1f78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-07-29T09:52:56.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>Feel free to send though your code/repo and I can take a quick look
          over to see if anything jumps out!</p>

          '
        raw: Feel free to send though your code/repo and I can take a quick look over
          to see if anything jumps out!
        updatedAt: '2022-07-29T09:52:56.358Z'
      numEdits: 0
      reactions: []
    id: 62e3adf8e538a2a624f8b2c6
    type: comment
  author: sanchit-gandhi
  content: Feel free to send though your code/repo and I can take a quick look over
    to see if anything jumps out!
  created_at: 2022-07-29 08:52:56+00:00
  edited: false
  hidden: false
  id: 62e3adf8e538a2a624f8b2c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-07-29T09:56:05.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>Note that number of train steps is extremely important too. I trained
          for 50k train steps with an effective batch size of 64, equivalent to 3200k
          training examples. After this time the eval loss and eval WER were still
          decreasing, so training for longer is also an option</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1659088563472-61f91cf54a8e5a275b2b3e7c.png"><img
          alt="Screenshot 2022-07-29 at 10.53.27.png" src="https://cdn-uploads.huggingface.co/production/uploads/1659088563472-61f91cf54a8e5a275b2b3e7c.png"></a></p>

          '
        raw: 'Note that number of train steps is extremely important too. I trained
          for 50k train steps with an effective batch size of 64, equivalent to 3200k
          training examples. After this time the eval loss and eval WER were still
          decreasing, so training for longer is also an option



          ![Screenshot 2022-07-29 at 10.53.27.png](https://cdn-uploads.huggingface.co/production/uploads/1659088563472-61f91cf54a8e5a275b2b3e7c.png)'
        updatedAt: '2022-07-29T09:56:05.684Z'
      numEdits: 0
      reactions: []
    id: 62e3aeb59c5e7bfb93f8a5c3
    type: comment
  author: sanchit-gandhi
  content: 'Note that number of train steps is extremely important too. I trained
    for 50k train steps with an effective batch size of 64, equivalent to 3200k training
    examples. After this time the eval loss and eval WER were still decreasing, so
    training for longer is also an option



    ![Screenshot 2022-07-29 at 10.53.27.png](https://cdn-uploads.huggingface.co/production/uploads/1659088563472-61f91cf54a8e5a275b2b3e7c.png)'
  created_at: 2022-07-29 08:56:05+00:00
  edited: false
  hidden: false
  id: 62e3aeb59c5e7bfb93f8a5c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-07-29T10:07:00.000Z'
    data:
      edited: false
      editors:
      - mutiann
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
          fullname: Mutian He
          isHf: false
          isPro: false
          name: mutiann
          type: user
        html: '<p>Yes I see that and I trained like 100K steps. I also checked your
          logs and my WER is much higher since the first evaluation at 10K steps (17%
          vs 14%)...</p>

          '
        raw: Yes I see that and I trained like 100K steps. I also checked your logs
          and my WER is much higher since the first evaluation at 10K steps (17% vs
          14%)...
        updatedAt: '2022-07-29T10:07:00.884Z'
      numEdits: 0
      reactions: []
    id: 62e3b1449f3719249c221a6f
    type: comment
  author: mutiann
  content: Yes I see that and I trained like 100K steps. I also checked your logs
    and my WER is much higher since the first evaluation at 10K steps (17% vs 14%)...
  created_at: 2022-07-29 09:07:00+00:00
  edited: false
  hidden: false
  id: 62e3b1449f3719249c221a6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-07-29T10:11:29.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>Sure, if you send your code/repo over I can take a quick look!</p>

          '
        raw: Sure, if you send your code/repo over I can take a quick look!
        updatedAt: '2022-07-29T10:11:29.384Z'
      numEdits: 0
      reactions: []
    id: 62e3b2512c8797d3196b69c4
    type: comment
  author: sanchit-gandhi
  content: Sure, if you send your code/repo over I can take a quick look!
  created_at: 2022-07-29 09:11:29+00:00
  edited: false
  hidden: false
  id: 62e3b2512c8797d3196b69c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-08-10T08:47:37.000Z'
    data:
      edited: false
      editors:
      - mutiann
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
          fullname: Mutian He
          isHf: false
          isPro: false
          name: mutiann
          type: user
        html: '<p>Hi I''m coming back again :(<br>I''m afraid that it will bother
          you too much time to read the code...The core part is available here: <a
          rel="nofollow" href="https://1drv.ms/u/s!AukS30WbNwYfgaKJBPDSvIJ3Gx9rjzA?e=SvSRSG">https://1drv.ms/u/s!AukS30WbNwYfgaKJBPDSvIJ3Gx9rjzA?e=SvSRSG</a><br>It
          might be a bit messed up because for debugging purposes I just plugged part
          of your data processing code into mine, to ensure that in your code and
          my code the model gets the exact same input in each step, but code other
          than dataloader.py should be readable. However my result doesn''t change
          much and is still much lower than yours, so the problem shouldn''t come
          from differences in input data but inside the model or optimization.<br>
          I''m still trying to determine where is the difference. Is it possible to
          extract intermediate values, like outputs of each layer? I read some docs
          of flax (like <a rel="nofollow" href="https://flax.readthedocs.io/en/latest/guides/extracting_intermediates.html">https://flax.readthedocs.io/en/latest/guides/extracting_intermediates.html</a>)
          but using their methods only gives some JVPTracer objects but not true intermediate
          values. Sorry I''ve never used flax before :(</p>

          '
        raw: "Hi I'm coming back again :(\nI'm afraid that it will bother you too\
          \ much time to read the code...The core part is available here: https://1drv.ms/u/s!AukS30WbNwYfgaKJBPDSvIJ3Gx9rjzA?e=SvSRSG\n\
          It might be a bit messed up because for debugging purposes I just plugged\
          \ part of your data processing code into mine, to ensure that in your code\
          \ and my code the model gets the exact same input in each step, but code\
          \ other than dataloader.py should be readable. However my result doesn't\
          \ change much and is still much lower than yours, so the problem shouldn't\
          \ come from differences in input data but inside the model or optimization.\n\
          \ I'm still trying to determine where is the difference. Is it possible\
          \ to extract intermediate values, like outputs of each layer? I read some\
          \ docs of flax (like https://flax.readthedocs.io/en/latest/guides/extracting_intermediates.html)\
          \ but using their methods only gives some JVPTracer objects but not true\
          \ intermediate values. Sorry I've never used flax before :("
        updatedAt: '2022-08-10T08:47:37.671Z'
      numEdits: 0
      reactions: []
    id: 62f370a9fb659e41329bcf7f
    type: comment
  author: mutiann
  content: "Hi I'm coming back again :(\nI'm afraid that it will bother you too much\
    \ time to read the code...The core part is available here: https://1drv.ms/u/s!AukS30WbNwYfgaKJBPDSvIJ3Gx9rjzA?e=SvSRSG\n\
    It might be a bit messed up because for debugging purposes I just plugged part\
    \ of your data processing code into mine, to ensure that in your code and my code\
    \ the model gets the exact same input in each step, but code other than dataloader.py\
    \ should be readable. However my result doesn't change much and is still much\
    \ lower than yours, so the problem shouldn't come from differences in input data\
    \ but inside the model or optimization.\n I'm still trying to determine where\
    \ is the difference. Is it possible to extract intermediate values, like outputs\
    \ of each layer? I read some docs of flax (like https://flax.readthedocs.io/en/latest/guides/extracting_intermediates.html)\
    \ but using their methods only gives some JVPTracer objects but not true intermediate\
    \ values. Sorry I've never used flax before :("
  created_at: 2022-08-10 07:47:37+00:00
  edited: false
  hidden: false
  id: 62f370a9fb659e41329bcf7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-08-10T12:56:04.000Z'
    data:
      edited: false
      editors:
      - mutiann
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
          fullname: Mutian He
          isHf: false
          isPro: false
          name: mutiann
          type: user
        html: '<p>I''ve just fixed a bug in my implementation and it works now :)<br>Thank
          you so much for all your aid!</p>

          '
        raw: 'I''ve just fixed a bug in my implementation and it works now :)

          Thank you so much for all your aid!'
        updatedAt: '2022-08-10T12:56:04.909Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - sanchit-gandhi
      relatedEventId: 62f3aae4fb659e41329d7d2d
    id: 62f3aae4fb659e41329d7d2c
    type: comment
  author: mutiann
  content: 'I''ve just fixed a bug in my implementation and it works now :)

    Thank you so much for all your aid!'
  created_at: 2022-08-10 11:56:04+00:00
  edited: false
  hidden: false
  id: 62f3aae4fb659e41329d7d2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ac723d73dbae0934f84ac01aa62ff493.svg
      fullname: Mutian He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mutiann
      type: user
    createdAt: '2022-08-10T12:56:04.000Z'
    data:
      status: closed
    id: 62f3aae4fb659e41329d7d2d
    type: status-change
  author: mutiann
  created_at: 2022-08-10 11:56:04+00:00
  id: 62f3aae4fb659e41329d7d2d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-08-12T17:09:15.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Amazing, very happy to hear that <span data-props=\"{&quot;user&quot;:&quot;mutiann&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mutiann\"\
          >@<span class=\"underline\">mutiann</span></a></span>\n\n\t</span></span>!\
          \ Best of luck!</p>\n"
        raw: Amazing, very happy to hear that @mutiann! Best of luck!
        updatedAt: '2022-08-12T17:09:15.557Z'
      numEdits: 0
      reactions: []
    id: 62f6893bffd6a0853ee44835
    type: comment
  author: sanchit-gandhi
  content: Amazing, very happy to hear that @mutiann! Best of luck!
  created_at: 2022-08-12 16:09:15+00:00
  edited: false
  hidden: false
  id: 62f6893bffd6a0853ee44835
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: sanchit-gandhi/wav2vec2-large-tedlium
repo_type: model
status: closed
target_branch: null
title: More training details?
