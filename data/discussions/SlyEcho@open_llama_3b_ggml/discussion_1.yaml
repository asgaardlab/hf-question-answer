!!python/object:huggingface_hub.community.DiscussionWithDetails
author: flyingkiwiguy
conflicting_files: null
created_at: 2023-06-02 10:19:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8665db99de3c4a4e62bc99c0f5d3b19.svg
      fullname: Gary Mulder
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flyingkiwiguy
      type: user
    createdAt: '2023-06-02T11:19:02.000Z'
    data:
      edited: false
      editors:
      - flyingkiwiguy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8665db99de3c4a4e62bc99c0f5d3b19.svg
          fullname: Gary Mulder
          isHf: false
          isPro: false
          name: flyingkiwiguy
          type: user
        html: '<p>I''ve integrated it into a Docker build for "Open Llama-in-a-box"
          REST API server. Please keep the open-llama-3b-q5_1.bin file available for
          download. Full details here:</p>

          <p><a rel="nofollow" href="https://github.com/abetlen/llama-cpp-python/pull/310">https://github.com/abetlen/llama-cpp-python/pull/310</a></p>

          '
        raw: "I've integrated it into a Docker build for \"Open Llama-in-a-box\" REST\
          \ API server. Please keep the open-llama-3b-q5_1.bin file available for\
          \ download. Full details here:\r\n\r\nhttps://github.com/abetlen/llama-cpp-python/pull/310\r\
          \n"
        updatedAt: '2023-06-02T11:19:02.253Z'
      numEdits: 0
      reactions: []
    id: 6479d026ed75e95d3e96d628
    type: comment
  author: flyingkiwiguy
  content: "I've integrated it into a Docker build for \"Open Llama-in-a-box\" REST\
    \ API server. Please keep the open-llama-3b-q5_1.bin file available for download.\
    \ Full details here:\r\n\r\nhttps://github.com/abetlen/llama-cpp-python/pull/310\r\
    \n"
  created_at: 2023-06-02 10:19:02+00:00
  edited: false
  hidden: false
  id: 6479d026ed75e95d3e96d628
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6461e6d29ca00a1b9af42a8f4549cd63.svg
      fullname: Henri Vasserman
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: SlyEcho
      type: user
    createdAt: '2023-06-05T09:52:47.000Z'
    data:
      edited: false
      editors:
      - SlyEcho
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.976630449295044
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6461e6d29ca00a1b9af42a8f4549cd63.svg
          fullname: Henri Vasserman
          isHf: false
          isPro: false
          name: SlyEcho
          type: user
        html: '<p>I will probably update the same repo when newer checkpoints are
          released, would that be alright?</p>

          <p>The conversion itself is not anything complex. I will add the Makefile
          here someday.</p>

          <p>I could maybe leave a branch for this specific one.</p>

          '
        raw: 'I will probably update the same repo when newer checkpoints are released,
          would that be alright?


          The conversion itself is not anything complex. I will add the Makefile here
          someday.


          I could maybe leave a branch for this specific one.'
        updatedAt: '2023-06-05T09:52:47.087Z'
      numEdits: 0
      reactions: []
    id: 647db06f5214d172cbb17a4e
    type: comment
  author: SlyEcho
  content: 'I will probably update the same repo when newer checkpoints are released,
    would that be alright?


    The conversion itself is not anything complex. I will add the Makefile here someday.


    I could maybe leave a branch for this specific one.'
  created_at: 2023-06-05 08:52:47+00:00
  edited: false
  hidden: false
  id: 647db06f5214d172cbb17a4e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8665db99de3c4a4e62bc99c0f5d3b19.svg
      fullname: Gary Mulder
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flyingkiwiguy
      type: user
    createdAt: '2023-06-07T15:21:21.000Z'
    data:
      edited: false
      editors:
      - flyingkiwiguy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8667523860931396
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8665db99de3c4a4e62bc99c0f5d3b19.svg
          fullname: Gary Mulder
          isHf: false
          isPro: false
          name: flyingkiwiguy
          type: user
        html: "<p>Sorry, didn't see your message.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;abetlen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/abetlen\"\
          >@<span class=\"underline\">abetlen</span></a></span>\n\n\t</span></span>\
          \ accepted my PR, so you can find the latest stable \"open llama in-a-box\"\
          \ here:</p>\n<p><a rel=\"nofollow\" href=\"https://github.com/abetlen/llama-cpp-python/tree/main/docker/open_llama\"\
          >https://github.com/abetlen/llama-cpp-python/tree/main/docker/open_llama</a></p>\n\
          <p> I just smoke tested your updated 3B repo and it looks to work without\
          \ any changes with the above code.  Once you have the 7B model converted,\
          \ it a single line change in build.sh to d/l and package that, instead.</p>\n"
        raw: "Sorry, didn't see your message.\n\n@abetlen accepted my PR, so you can\
          \ find the latest stable \"open llama in-a-box\" here:\n\nhttps://github.com/abetlen/llama-cpp-python/tree/main/docker/open_llama\n\
          \n I just smoke tested your updated 3B repo and it looks to work without\
          \ any changes with the above code.  Once you have the 7B model converted,\
          \ it a single line change in build.sh to d/l and package that, instead."
        updatedAt: '2023-06-07T15:21:21.646Z'
      numEdits: 0
      reactions: []
    id: 6480a071bb25a636c9dd2e0d
    type: comment
  author: flyingkiwiguy
  content: "Sorry, didn't see your message.\n\n@abetlen accepted my PR, so you can\
    \ find the latest stable \"open llama in-a-box\" here:\n\nhttps://github.com/abetlen/llama-cpp-python/tree/main/docker/open_llama\n\
    \n I just smoke tested your updated 3B repo and it looks to work without any changes\
    \ with the above code.  Once you have the 7B model converted, it a single line\
    \ change in build.sh to d/l and package that, instead."
  created_at: 2023-06-07 14:21:21+00:00
  edited: false
  hidden: false
  id: 6480a071bb25a636c9dd2e0d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: SlyEcho/open_llama_3b_ggml
repo_type: model
status: open
target_branch: null
title: Thanks for the ggml model - Docker integration
