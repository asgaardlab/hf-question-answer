!!python/object:huggingface_hub.community.DiscussionWithDetails
author: taohoang
conflicting_files: null
created_at: 2023-05-31 00:54:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b4c720e832ada569f8abf3751d40abb.svg
      fullname: Tao Hoang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: taohoang
      type: user
    createdAt: '2023-05-31T01:54:08.000Z'
    data:
      edited: false
      editors:
      - taohoang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b4c720e832ada569f8abf3751d40abb.svg
          fullname: Tao Hoang
          isHf: false
          isPro: false
          name: taohoang
          type: user
        html: "<p>Hi,</p>\n<p>In the definition of DataCollatorSpeechSeq2SeqWithPadding\
          \ in <a href=\"https://huggingface.co/blog/fine-tune-whisper\">https://huggingface.co/blog/fine-tune-whisper</a>,\
          \  I am trying to understand the following part:</p>\n<pre><code>\n# if\
          \ bos token is appended in previous tokenization step,\n# cut bos token\
          \ here as it's append later anyways\nif (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n\
          \       labels = labels[:, 1:]\n</code>\n</pre>\n\n<p>Where will bos token\
          \ be appended later in training?</p>\n<p>After loading the tokenizer, it\
          \ seems bos_token is &lt;|endoftext|&gt; instead of &lt;|startoftranscript|&gt;:</p>\n\
          <pre><code>\ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\"\
          , language=\"Hindi\", task=\"transcribe\")\n</code>\n</pre>\n\n<p>Will this\
          \ affect the checking for bos_token above?</p>\n"
        raw: "Hi,\r\n\r\nIn the definition of DataCollatorSpeechSeq2SeqWithPadding\
          \ in https://huggingface.co/blog/fine-tune-whisper,  I am trying to understand\
          \ the following part:\r\n<pre>\r\n<code>\r\n# if bos token is appended in\
          \ previous tokenization step,\r\n# cut bos token here as it's append later\
          \ anyways\r\nif (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\r\
          \n       labels = labels[:, 1:]\r\n</code>\r\n</pre>\r\n\r\nWhere will bos\
          \ token be appended later in training?\r\n\r\nAfter loading the tokenizer,\
          \ it seems bos_token is <|endoftext|> instead of <|startoftranscript|>:\r\
          \n<pre>\r\n<code>\r\ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\"\
          , language=\"Hindi\", task=\"transcribe\")\r\n</code>\r\n</pre>\r\n\r\n\
          Will this affect the checking for bos_token above?\r\n"
        updatedAt: '2023-05-31T01:54:08.388Z'
      numEdits: 0
      reactions: []
    id: 6476a8c0a0bde42d3f93ffc7
    type: comment
  author: taohoang
  content: "Hi,\r\n\r\nIn the definition of DataCollatorSpeechSeq2SeqWithPadding in\
    \ https://huggingface.co/blog/fine-tune-whisper,  I am trying to understand the\
    \ following part:\r\n<pre>\r\n<code>\r\n# if bos token is appended in previous\
    \ tokenization step,\r\n# cut bos token here as it's append later anyways\r\n\
    if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\r\
    \n       labels = labels[:, 1:]\r\n</code>\r\n</pre>\r\n\r\nWhere will bos token\
    \ be appended later in training?\r\n\r\nAfter loading the tokenizer, it seems\
    \ bos_token is <|endoftext|> instead of <|startoftranscript|>:\r\n<pre>\r\n<code>\r\
    \ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"\
    Hindi\", task=\"transcribe\")\r\n</code>\r\n</pre>\r\n\r\nWill this affect the\
    \ checking for bos_token above?\r\n"
  created_at: 2023-05-31 00:54:08+00:00
  edited: false
  hidden: false
  id: 6476a8c0a0bde42d3f93ffc7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: openai/whisper-small
repo_type: model
status: open
target_branch: null
title: Clarifying bos_token
