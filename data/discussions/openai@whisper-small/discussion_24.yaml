!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MLLife
conflicting_files: null
created_at: 2023-04-18 11:22:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a23ded6b84f35cb8803dace63f1efb3f.svg
      fullname: ml life
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MLLife
      type: user
    createdAt: '2023-04-18T12:22:17.000Z'
    data:
      edited: true
      editors:
      - MLLife
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a23ded6b84f35cb8803dace63f1efb3f.svg
          fullname: ml life
          isHf: false
          isPro: false
          name: MLLife
          type: user
        html: '<p>please refer to the issue detailed here; <a rel="nofollow" href="https://discuss.huggingface.co/t/deploying-open-ais-whisper-on-sagemaker/24761/54?u=mllife">https://discuss.huggingface.co/t/deploying-open-ais-whisper-on-sagemaker/24761/54?u=mllife</a></p>

          <p>basically, the currently sagemaker have a max payload set to 5 MB, and
          there is no way around it on how the current code for whisper is streaming
          the file to the end-point using just audio_path as input; which makes this
          model nearly useless for sagemaker deployment.</p>

          <p>if someone has done custom inference.py which loads file from s3_path
          at the endpoint itself and later processes it, please share</p>

          '
        raw: 'please refer to the issue detailed here; https://discuss.huggingface.co/t/deploying-open-ais-whisper-on-sagemaker/24761/54?u=mllife


          basically, the currently sagemaker have a max payload set to 5 MB, and there
          is no way around it on how the current code for whisper is streaming the
          file to the end-point using just audio_path as input; which makes this model
          nearly useless for sagemaker deployment.


          if someone has done custom inference.py which loads file from s3_path at
          the endpoint itself and later processes it, please share'
        updatedAt: '2023-04-18T12:25:52.787Z'
      numEdits: 1
      reactions: []
    id: 643e8b79b56b683f744b33a8
    type: comment
  author: MLLife
  content: 'please refer to the issue detailed here; https://discuss.huggingface.co/t/deploying-open-ais-whisper-on-sagemaker/24761/54?u=mllife


    basically, the currently sagemaker have a max payload set to 5 MB, and there is
    no way around it on how the current code for whisper is streaming the file to
    the end-point using just audio_path as input; which makes this model nearly useless
    for sagemaker deployment.


    if someone has done custom inference.py which loads file from s3_path at the endpoint
    itself and later processes it, please share'
  created_at: 2023-04-18 11:22:17+00:00
  edited: true
  hidden: false
  id: 643e8b79b56b683f744b33a8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: openai/whisper-small
repo_type: model
status: open
target_branch: null
title: Sagemaker Payload limit issue (413)
