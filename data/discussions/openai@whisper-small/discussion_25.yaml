!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MLLife
conflicting_files: null
created_at: 2023-04-19 06:15:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a23ded6b84f35cb8803dace63f1efb3f.svg
      fullname: ml life
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MLLife
      type: user
    createdAt: '2023-04-19T07:15:36.000Z'
    data:
      edited: true
      editors:
      - MLLife
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a23ded6b84f35cb8803dace63f1efb3f.svg
          fullname: ml life
          isHf: false
          isPro: false
          name: MLLife
          type: user
        html: '<h1 id="used-the-following-code-to-save-the-model-to-model-directory">used
          the following code to save the model to "model-directory"</h1>

          <p>!sudo yum update -y<br>!curl -s <a rel="nofollow" href="https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh">https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh</a>
          | sudo bash<br>!sudo yum install git-lfs git -y</p>

          <p>repository = "openai/whisper-small"<br>model_id=repository.split("/")[-1]</p>

          <p>!git lfs install<br>!git clone <a href="https://huggingface.co/$repository">https://huggingface.co/$repository</a></p>

          <h1 id="code-to-setup-pipeline">code to setup pipeline</h1>

          <p>from transformers import WhisperTokenizer, WhisperForConditionalGeneration,
          WhisperFeatureExtractor, WhisperProcessor</p>

          <p>model_dir = "./whisper-small"<br>t = WhisperTokenizer.from_pretrained(model_dir)<br>m
          = WhisperForConditionalGeneration.from_pretrained(model_dir)<br>fe = WhisperProcessor.from_pretrained(model_dir)</p>

          <p>from transformers import pipeline</p>

          <p>classifier = pipeline(<br>    task= ''automatic-speech-recognition'',<br>    model=
          m,<br>    feature_extrator = fe_hf,<br>    tokenizer=t,<br>    config="whisper-small/config.json"<br>)</p>

          <h1 id="getting-error----">getting error ---</h1>

          <p>Exception: Impossible to guess which feature extractor to use. Please
          provide a PreTrainedFeatureExtractor class or a path/identifier to a pretrained
          feature extractor.</p>

          <p>please help</p>

          '
        raw: "# used the following code to save the model to \"model-directory\"\n\
          \n!sudo yum update -y \n!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh\
          \ | sudo bash\n!sudo yum install git-lfs git -y\n\nrepository = \"openai/whisper-small\"\
          \nmodel_id=repository.split(\"/\")[-1]\n\n!git lfs install\n!git clone https://huggingface.co/$repository\n\
          \n# code to setup pipeline\n\nfrom transformers import WhisperTokenizer,\
          \ WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperProcessor\n\
          \nmodel_dir = \"./whisper-small\"\nt = WhisperTokenizer.from_pretrained(model_dir)\n\
          m = WhisperForConditionalGeneration.from_pretrained(model_dir)\nfe = WhisperProcessor.from_pretrained(model_dir)\n\
          \nfrom transformers import pipeline\n\nclassifier = pipeline(\n    task=\
          \ 'automatic-speech-recognition', \n    model= m,\n    feature_extrator\
          \ = fe_hf,\n    tokenizer=t,\n    config=\"whisper-small/config.json\"\n\
          )\n\n# getting error ---\nException: Impossible to guess which feature extractor\
          \ to use. Please provide a PreTrainedFeatureExtractor class or a path/identifier\
          \ to a pretrained feature extractor.\n\nplease help"
        updatedAt: '2023-04-19T07:16:56.359Z'
      numEdits: 1
      reactions: []
    id: 643f9518e3c86d5abeae35ef
    type: comment
  author: MLLife
  content: "# used the following code to save the model to \"model-directory\"\n\n\
    !sudo yum update -y \n!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh\
    \ | sudo bash\n!sudo yum install git-lfs git -y\n\nrepository = \"openai/whisper-small\"\
    \nmodel_id=repository.split(\"/\")[-1]\n\n!git lfs install\n!git clone https://huggingface.co/$repository\n\
    \n# code to setup pipeline\n\nfrom transformers import WhisperTokenizer, WhisperForConditionalGeneration,\
    \ WhisperFeatureExtractor, WhisperProcessor\n\nmodel_dir = \"./whisper-small\"\
    \nt = WhisperTokenizer.from_pretrained(model_dir)\nm = WhisperForConditionalGeneration.from_pretrained(model_dir)\n\
    fe = WhisperProcessor.from_pretrained(model_dir)\n\nfrom transformers import pipeline\n\
    \nclassifier = pipeline(\n    task= 'automatic-speech-recognition', \n    model=\
    \ m,\n    feature_extrator = fe_hf,\n    tokenizer=t,\n    config=\"whisper-small/config.json\"\
    \n)\n\n# getting error ---\nException: Impossible to guess which feature extractor\
    \ to use. Please provide a PreTrainedFeatureExtractor class or a path/identifier\
    \ to a pretrained feature extractor.\n\nplease help"
  created_at: 2023-04-19 06:15:36+00:00
  edited: true
  hidden: false
  id: 643f9518e3c86d5abeae35ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a23ded6b84f35cb8803dace63f1efb3f.svg
      fullname: ml life
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MLLife
      type: user
    createdAt: '2023-04-19T11:23:34.000Z'
    data:
      edited: false
      editors:
      - MLLife
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a23ded6b84f35cb8803dace63f1efb3f.svg
          fullname: ml life
          isHf: false
          isPro: false
          name: MLLife
          type: user
        html: '<p>anyone?? seeing this?</p>

          '
        raw: anyone?? seeing this?
        updatedAt: '2023-04-19T11:23:34.730Z'
      numEdits: 0
      reactions: []
    id: 643fcf364164a65ca1220fd4
    type: comment
  author: MLLife
  content: anyone?? seeing this?
  created_at: 2023-04-19 10:23:34+00:00
  edited: false
  hidden: false
  id: 643fcf364164a65ca1220fd4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-04-26T16:11:40.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>You can do it directly from a pre-trained identifier on the Hub:</p>

          <pre><code class="language-python"><span class="hljs-meta">&gt;&gt;&gt;
          </span><span class="hljs-keyword">import</span> torch

          <span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> pipeline

          <span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span>
          datasets <span class="hljs-keyword">import</span> load_dataset


          <span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">"cuda:0"</span>
          <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span>
          <span class="hljs-string">"cpu"</span>


          <span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(

          <span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">"automatic-speech-recognition"</span>,

          <span class="hljs-meta">&gt;&gt;&gt; </span>  model=<span class="hljs-string">"openai/whisper-small"</span>,

          <span class="hljs-meta">&gt;&gt;&gt; </span>  chunk_length_s=<span class="hljs-number">30</span>,

          <span class="hljs-meta">&gt;&gt;&gt; </span>  device=device,

          <span class="hljs-meta">&gt;&gt;&gt; </span>)

          </code></pre>

          <p>Or through a local model path, e.g. if you''ve cloned the model into
          <code>whisper-small</code>:</p>

          <pre><code class="language-python"><span class="hljs-meta">&gt;&gt;&gt;
          </span>pipe = pipeline(

          <span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">"automatic-speech-recognition"</span>,

          <span class="hljs-meta">&gt;&gt;&gt; </span>  model=<span class="hljs-string">"./whisper-small"</span>,

          <span class="hljs-meta">&gt;&gt;&gt; </span>  chunk_length_s=<span class="hljs-number">30</span>,

          <span class="hljs-meta">&gt;&gt;&gt; </span>  device=device,

          <span class="hljs-meta">&gt;&gt;&gt; </span>)

          </code></pre>

          '
        raw: 'You can do it directly from a pre-trained identifier on the Hub:

          ```python

          >>> import torch

          >>> from transformers import pipeline

          >>> from datasets import load_dataset


          >>> device = "cuda:0" if torch.cuda.is_available() else "cpu"


          >>> pipe = pipeline(

          >>>   "automatic-speech-recognition",

          >>>   model="openai/whisper-small",

          >>>   chunk_length_s=30,

          >>>   device=device,

          >>> )

          ```


          Or through a local model path, e.g. if you''ve cloned the model into `whisper-small`:

          ```python

          >>> pipe = pipeline(

          >>>   "automatic-speech-recognition",

          >>>   model="./whisper-small",

          >>>   chunk_length_s=30,

          >>>   device=device,

          >>> )

          ```'
        updatedAt: '2023-04-26T16:11:55.891Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MLLife
      relatedEventId: 64494d3c1af713976c2d89ea
    id: 64494d3c1af713976c2d89e9
    type: comment
  author: sanchit-gandhi
  content: 'You can do it directly from a pre-trained identifier on the Hub:

    ```python

    >>> import torch

    >>> from transformers import pipeline

    >>> from datasets import load_dataset


    >>> device = "cuda:0" if torch.cuda.is_available() else "cpu"


    >>> pipe = pipeline(

    >>>   "automatic-speech-recognition",

    >>>   model="openai/whisper-small",

    >>>   chunk_length_s=30,

    >>>   device=device,

    >>> )

    ```


    Or through a local model path, e.g. if you''ve cloned the model into `whisper-small`:

    ```python

    >>> pipe = pipeline(

    >>>   "automatic-speech-recognition",

    >>>   model="./whisper-small",

    >>>   chunk_length_s=30,

    >>>   device=device,

    >>> )

    ```'
  created_at: 2023-04-26 15:11:40+00:00
  edited: true
  hidden: false
  id: 64494d3c1af713976c2d89e9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-04-26T16:11:40.000Z'
    data:
      status: closed
    id: 64494d3c1af713976c2d89ea
    type: status-change
  author: sanchit-gandhi
  created_at: 2023-04-26 15:11:40+00:00
  id: 64494d3c1af713976c2d89ea
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a23ded6b84f35cb8803dace63f1efb3f.svg
      fullname: ml life
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MLLife
      type: user
    createdAt: '2023-04-27T05:02:17.000Z'
    data:
      edited: true
      editors:
      - MLLife
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a23ded6b84f35cb8803dace63f1efb3f.svg
          fullname: ml life
          isHf: false
          isPro: false
          name: MLLife
          type: user
        html: '<p>thanks, it worked</p>

          '
        raw: thanks, it worked
        updatedAt: '2023-04-27T05:03:22.719Z'
      numEdits: 1
      reactions: []
    id: 644a01d9111b3bf687896cd5
    type: comment
  author: MLLife
  content: thanks, it worked
  created_at: 2023-04-27 04:02:17+00:00
  edited: true
  hidden: false
  id: 644a01d9111b3bf687896cd5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: openai/whisper-small
repo_type: model
status: closed
target_branch: null
title: Unable to load saved model from "model-directory" using pipeline
