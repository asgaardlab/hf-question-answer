!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jcardenes
conflicting_files: null
created_at: 2023-04-28 00:50:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8cc4a07e993a6cd63ca129545890f511.svg
      fullname: Jesus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jcardenes
      type: user
    createdAt: '2023-04-28T01:50:07.000Z'
    data:
      edited: true
      editors:
      - jcardenes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8cc4a07e993a6cd63ca129545890f511.svg
          fullname: Jesus
          isHf: false
          isPro: false
          name: jcardenes
          type: user
        html: "<p>I followed the sagemaker deployment steps but it results in prediction\
          \ 400 error with gpt neox. </p>\n<p>I got the same results for 3b and 7b.</p>\n\
          <p>Default steps:</p>\n<pre><code>from sagemaker.huggingface import HuggingFaceModel\n\
          import boto3\n\niam_client = boto3.client('iam')\nrole = iam_client.get_role(RoleName='YourRoleName')['Role']['Arn']\n\
          \nhub = {\n    'HF_MODEL_ID':'stabilityai/stablelm-base-alpha-7b',\n   \
          \ 'HF_TASK':'conversational'\n}\n\n# create Hugging Face Model Class\nhuggingface_model\
          \ = HuggingFaceModel(\n    transformers_version='4.17.0',\n    pytorch_version='1.10.2',\n\
          \    py_version='py38',\n    env=hub,\n    role=role, \n)\n\n# deploy model\
          \ to SageMaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\
          \ # number of instances\n    instance_type='ml.m5.xlarge' # ec2 instance\
          \ type\n)\n\npredictor.predict({\n    'inputs': \"Can you please let us\
          \ know more details about your \"\n})\n</code></pre>\n<p>Error:</p>\n<pre><code>ModelError:\
          \ An error occurred (ModelError) when calling the InvokeEndpoint operation:\
          \ Received client error (400) from primary with message \"{\n  \"code\"\
          : 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"\\u0027gpt_neox\\\
          u0027\"\n}\n\n\n</code></pre>\n"
        raw: "I followed the sagemaker deployment steps but it results in prediction\
          \ 400 error with gpt neox. \n\nI got the same results for 3b and 7b.\n\n\
          Default steps:\n```\nfrom sagemaker.huggingface import HuggingFaceModel\n\
          import boto3\n\niam_client = boto3.client('iam')\nrole = iam_client.get_role(RoleName='YourRoleName')['Role']['Arn']\n\
          \nhub = {\n\t'HF_MODEL_ID':'stabilityai/stablelm-base-alpha-7b',\n\t'HF_TASK':'conversational'\n\
          }\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\
          \ttransformers_version='4.17.0',\n\tpytorch_version='1.10.2',\n\tpy_version='py38',\n\
          \tenv=hub,\n\trole=role, \n)\n\n# deploy model to SageMaker Inference\n\
          predictor = huggingface_model.deploy(\n\tinitial_instance_count=1, # number\
          \ of instances\n\tinstance_type='ml.m5.xlarge' # ec2 instance type\n)\n\n\
          predictor.predict({\n\t'inputs': \"Can you please let us know more details\
          \ about your \"\n})\n```\nError:\n\n```\nModelError: An error occurred (ModelError)\
          \ when calling the InvokeEndpoint operation: Received client error (400)\
          \ from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\"\
          ,\n  \"message\": \"\\u0027gpt_neox\\u0027\"\n}\n\n\n\n```"
        updatedAt: '2023-04-28T01:54:02.153Z'
      numEdits: 2
      reactions: []
    id: 644b264ff434a6a63b1aa635
    type: comment
  author: jcardenes
  content: "I followed the sagemaker deployment steps but it results in prediction\
    \ 400 error with gpt neox. \n\nI got the same results for 3b and 7b.\n\nDefault\
    \ steps:\n```\nfrom sagemaker.huggingface import HuggingFaceModel\nimport boto3\n\
    \niam_client = boto3.client('iam')\nrole = iam_client.get_role(RoleName='YourRoleName')['Role']['Arn']\n\
    \nhub = {\n\t'HF_MODEL_ID':'stabilityai/stablelm-base-alpha-7b',\n\t'HF_TASK':'conversational'\n\
    }\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\
    \ttransformers_version='4.17.0',\n\tpytorch_version='1.10.2',\n\tpy_version='py38',\n\
    \tenv=hub,\n\trole=role, \n)\n\n# deploy model to SageMaker Inference\npredictor\
    \ = huggingface_model.deploy(\n\tinitial_instance_count=1, # number of instances\n\
    \tinstance_type='ml.m5.xlarge' # ec2 instance type\n)\n\npredictor.predict({\n\
    \t'inputs': \"Can you please let us know more details about your \"\n})\n```\n\
    Error:\n\n```\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint\
    \ operation: Received client error (400) from primary with message \"{\n  \"code\"\
    : 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"\\u0027gpt_neox\\\
    u0027\"\n}\n\n\n\n```"
  created_at: 2023-04-28 00:50:07+00:00
  edited: true
  hidden: false
  id: 644b264ff434a6a63b1aa635
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: stabilityai/stablelm-base-alpha-3b
repo_type: model
status: open
target_branch: null
title: Sagemaker deployment not functional on default example
