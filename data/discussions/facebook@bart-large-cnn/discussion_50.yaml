!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cooper521
conflicting_files: null
created_at: 2023-10-04 11:57:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5b30f1c14cb2b2c633076719f824ec0.svg
      fullname: cooper x
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cooper521
      type: user
    createdAt: '2023-10-04T12:57:47.000Z'
    data:
      edited: false
      editors:
      - cooper521
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.83570796251297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5b30f1c14cb2b2c633076719f824ec0.svg
          fullname: cooper x
          isHf: false
          isPro: false
          name: cooper521
          type: user
        html: "<p>In the generation_config.json file, I am quite puzzled about \"\
          max_length\": 142, because the official documentation mentions:</p>\n<ul>\n\
          <li>max_length (int, optional, defaults to 20) \u2014 The maximum length\
          \ the generated tokens can have. Corresponds to the length of the input\
          \ prompt + max_new_tokens. Its effect is overridden by max_new_tokens, if\
          \ also set.</li>\n<li>max_new_tokens (int, optional) \u2014 The maximum\
          \ numbers of tokens to generate, ignoring the number of tokens in the prompt.</li>\n\
          </ul>\n<p>If configured this way, it means the total length of the article\
          \ + summary would only be 142 tokens. This is obviously inappropriate for\
          \ cnn/dm as the articles usually have hundreds of tokens. So what would\
          \ be a good setting for this parameter?</p>\n"
        raw: "In the generation_config.json file, I am quite puzzled about \"max_length\"\
          : 142, because the official documentation mentions:\r\n- max_length (int,\
          \ optional, defaults to 20) \u2014 The maximum length the generated tokens\
          \ can have. Corresponds to the length of the input prompt + max_new_tokens.\
          \ Its effect is overridden by max_new_tokens, if also set.\r\n- max_new_tokens\
          \ (int, optional) \u2014 The maximum numbers of tokens to generate, ignoring\
          \ the number of tokens in the prompt.\r\n\r\nIf configured this way, it\
          \ means the total length of the article + summary would only be 142 tokens.\
          \ This is obviously inappropriate for cnn/dm as the articles usually have\
          \ hundreds of tokens. So what would be a good setting for this parameter?"
        updatedAt: '2023-10-04T12:57:47.958Z'
      numEdits: 0
      reactions: []
    id: 651d614bc1530550864f2f4a
    type: comment
  author: cooper521
  content: "In the generation_config.json file, I am quite puzzled about \"max_length\"\
    : 142, because the official documentation mentions:\r\n- max_length (int, optional,\
    \ defaults to 20) \u2014 The maximum length the generated tokens can have. Corresponds\
    \ to the length of the input prompt + max_new_tokens. Its effect is overridden\
    \ by max_new_tokens, if also set.\r\n- max_new_tokens (int, optional) \u2014 The\
    \ maximum numbers of tokens to generate, ignoring the number of tokens in the\
    \ prompt.\r\n\r\nIf configured this way, it means the total length of the article\
    \ + summary would only be 142 tokens. This is obviously inappropriate for cnn/dm\
    \ as the articles usually have hundreds of tokens. So what would be a good setting\
    \ for this parameter?"
  created_at: 2023-10-04 11:57:47+00:00
  edited: false
  hidden: false
  id: 651d614bc1530550864f2f4a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 50
repo_id: facebook/bart-large-cnn
repo_type: model
status: open
target_branch: null
title: Question about Setting max_length and max_new_tokens in Generation Configuration
  for CNN/DM Dataset
