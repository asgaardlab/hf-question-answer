!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil337
conflicting_files: null
created_at: 2023-12-23 06:14:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-23T06:14:02.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9734868407249451
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>I used Solar instruct for hours across days, and while it scored
          slightly higher than Mistral 7bs in my testing, it didn''t score near as
          high as Mixtrals, Llama 2 70bs or GPT3.5.</p>

          <p>Usually the results of my testing roughly align with HF average scores,
          but since they were way off I looked into it.</p>

          <p>It appears the discrepancy is primary due to 2 things.</p>

          <p>(1) Solar instruct obsessively denies things are true, including countless
          millions of things which are in fact true, resulting in an absurdly high
          71.5 TruthfulQA score (much higher than even GPT4). When I removed TruthfulQA
          from the HF average score it was a much better representation of Solar Instruct''s
          true performance.</p>

          <p>(2) Solar instruct gives unusually brief responses, even when contraindicated
          by the circumstances or the user''s instruction. And because of automated
          eval limitations longer and more complex answers result in lower scores
          on numerous tests (more true answers falsely identified as false).</p>

          <p>All things considered, the true HF score of Solar Instruct is no higher
          than 68, and certainly nowhere near 74. 74 would put it above GPT3.5, yet
          it isn''t near as good (not my opinion). It''s not even near as good as
          Llama 2 70b or Mixtral. It has far less knowledge and gets tripped up by
          much simpler questions than all three, yet has a higher score.</p>

          '
        raw: "I used Solar instruct for hours across days, and while it scored slightly\
          \ higher than Mistral 7bs in my testing, it didn't score near as high as\
          \ Mixtrals, Llama 2 70bs or GPT3.5.\r\n\r\nUsually the results of my testing\
          \ roughly align with HF average scores, but since they were way off I looked\
          \ into it.\r\n\r\nIt appears the discrepancy is primary due to 2 things.\r\
          \n\r\n(1) Solar instruct obsessively denies things are true, including countless\
          \ millions of things which are in fact true, resulting in an absurdly high\
          \ 71.5 TruthfulQA score (much higher than even GPT4). When I removed TruthfulQA\
          \ from the HF average score it was a much better representation of Solar\
          \ Instruct's true performance.\r\n\r\n(2) Solar instruct gives unusually\
          \ brief responses, even when contraindicated by the circumstances or the\
          \ user's instruction. And because of automated eval limitations longer and\
          \ more complex answers result in lower scores on numerous tests (more true\
          \ answers falsely identified as false).\r\n\r\nAll things considered, the\
          \ true HF score of Solar Instruct is no higher than 68, and certainly nowhere\
          \ near 74. 74 would put it above GPT3.5, yet it isn't near as good (not\
          \ my opinion). It's not even near as good as Llama 2 70b or Mixtral. It\
          \ has far less knowledge and gets tripped up by much simpler questions than\
          \ all three, yet has a higher score."
        updatedAt: '2023-12-23T06:14:02.556Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PrimeD
    id: 65867aaa99ed106ac842242f
    type: comment
  author: Phil337
  content: "I used Solar instruct for hours across days, and while it scored slightly\
    \ higher than Mistral 7bs in my testing, it didn't score near as high as Mixtrals,\
    \ Llama 2 70bs or GPT3.5.\r\n\r\nUsually the results of my testing roughly align\
    \ with HF average scores, but since they were way off I looked into it.\r\n\r\n\
    It appears the discrepancy is primary due to 2 things.\r\n\r\n(1) Solar instruct\
    \ obsessively denies things are true, including countless millions of things which\
    \ are in fact true, resulting in an absurdly high 71.5 TruthfulQA score (much\
    \ higher than even GPT4). When I removed TruthfulQA from the HF average score\
    \ it was a much better representation of Solar Instruct's true performance.\r\n\
    \r\n(2) Solar instruct gives unusually brief responses, even when contraindicated\
    \ by the circumstances or the user's instruction. And because of automated eval\
    \ limitations longer and more complex answers result in lower scores on numerous\
    \ tests (more true answers falsely identified as false).\r\n\r\nAll things considered,\
    \ the true HF score of Solar Instruct is no higher than 68, and certainly nowhere\
    \ near 74. 74 would put it above GPT3.5, yet it isn't near as good (not my opinion).\
    \ It's not even near as good as Llama 2 70b or Mixtral. It has far less knowledge\
    \ and gets tripped up by much simpler questions than all three, yet has a higher\
    \ score."
  created_at: 2023-12-23 06:14:02+00:00
  edited: false
  hidden: false
  id: 65867aaa99ed106ac842242f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-23T06:31:59.000Z'
    data:
      status: closed
    id: 65867edf16a6a00820dbfd07
    type: status-change
  author: Phil337
  created_at: 2023-12-23 06:31:59+00:00
  id: 65867edf16a6a00820dbfd07
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7aae73caca1473788b82308a55e332d8.svg
      fullname: Samuel Azran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SamuelAzran
      type: user
    createdAt: '2023-12-23T11:09:03.000Z'
    data:
      edited: false
      editors:
      - SamuelAzran
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9555176496505737
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7aae73caca1473788b82308a55e332d8.svg
          fullname: Samuel Azran
          isHf: false
          isPro: false
          name: SamuelAzran
          type: user
        html: '<p>Thank you for the much needed analysis. Especially the point about
          TruthfulQA score was illuminating </p>

          '
        raw: 'Thank you for the much needed analysis. Especially the point about TruthfulQA
          score was illuminating '
        updatedAt: '2023-12-23T11:09:03.223Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - iphann
    id: 6586bfcf367c76b8eedbc8ac
    type: comment
  author: SamuelAzran
  content: 'Thank you for the much needed analysis. Especially the point about TruthfulQA
    score was illuminating '
  created_at: 2023-12-23 11:09:03+00:00
  edited: false
  hidden: false
  id: 6586bfcf367c76b8eedbc8ac
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: upstage/SOLAR-10.7B-Instruct-v1.0
repo_type: model
status: closed
target_branch: null
title: You know Mixtral, Llama 2 70b, GPT3.5... Are All Much Better
