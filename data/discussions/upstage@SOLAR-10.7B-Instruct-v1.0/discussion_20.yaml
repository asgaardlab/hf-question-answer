!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ianuvrat
conflicting_files: null
created_at: 2024-01-03 07:44:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
      fullname: Anuvrat Shukla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ianuvrat
      type: user
    createdAt: '2024-01-03T07:44:27.000Z'
    data:
      edited: false
      editors:
      - ianuvrat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.920646607875824
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
          fullname: Anuvrat Shukla
          isHf: false
          isPro: false
          name: ianuvrat
          type: user
        html: '<p>Cn anyone share the syntax how to load this model with CTransformers?</p>

          '
        raw: Cn anyone share the syntax how to load this model with CTransformers?
        updatedAt: '2024-01-03T07:44:27.330Z'
      numEdits: 0
      reactions: []
    id: 6595105bc7200df8bdb20fe7
    type: comment
  author: ianuvrat
  content: Cn anyone share the syntax how to load this model with CTransformers?
  created_at: 2024-01-03 07:44:27+00:00
  edited: false
  hidden: false
  id: 6595105bc7200df8bdb20fe7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2024-01-04T11:30:41.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.948245108127594
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: '<p>Hi,<br>Unfortunately ctransformers is running a bit late to the
          game in regards of keeping up with the recent new and exciting large language
          model architectural changes this winter.<br>From what I can tell, due to
          how ctransformers works, some of the new unique models with architectural
          changes may not be supported yet. It doesn''t seem very flexible with loading
          models outside of the model families that it was built for already. ctransformers
          has not been updated since November. Which was just before all the fun new
          model excitement this winter.<br>I don''t think they are out of the game
          yet. You may have to wait. Or consider using another backend/system interim
          if you don''t want to wait. I''ve been using ctransformers this year and
          considering moving to something new due to the delays. (I hope that ctransformers
          is ok and catches up soon. I liked using it.)<br>ctransformers is built
          around running quantized models. So you might have better luck looking around
          areas where that is the main theme, if you were not already.<br><a href="https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF#about-gguf">https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF#about-gguf</a>
          &lt;- For example, TheBloke points out other tools/clients/libraries that
          could be used to load GGUF quants of this and some newer model(s).<br>The
          llama-cpp family (llama-cpp and llama-cpp-python) might be a good choice.
          They seem to be able to move fast to build support for new model changes.
          And I''ve personally seen a successful SOLAR load and reasonable inference
          by llama-cpp-python. They have good documentation as well. ctransformers
          and llama-cpp-python felt alike. So your experience/understanding may transfer
          decently.<br>If your ok with loading models on your CPU, it''s pretty easy
          to setup for CPU. And it''s OS agnostic. Though things could be really slow
          if your not running a recent CPU with AVX support or something similar and
          recent. And it has bindings that likely will bridge things decently between
          it and LangChain. (<a rel="nofollow" href="https://python.langchain.com/docs/integrations/providers/llamacpp">https://python.langchain.com/docs/integrations/providers/llamacpp</a>  Though
          I think LangChain might be a bit dated with it''s info. And I''m not a LangChain
          user. Still might be useful.) (Llama-cpp caveat. Running on something other
          than CPU will likely require a couple year seasoned Linux/Unix/VM OS management
          experience and likely some decent experience with software development to
          understand what it needed. Should be do-able if your familiar with that.
          If not, that is asking a lot of people just to guide someone blindly through.)<br>That
          being said, likely something could cross between providers that LangChain
          supports and the systems that can load GGUF''s of SOLAR that may work for
          you.<br>Cross checking between ''<a rel="nofollow" href="https://python.langchain.com/docs/integrations/providers">https://python.langchain.com/docs/integrations/providers</a>
          -&gt; More'' and ''<a href="https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF#about-gguf">https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF#about-gguf</a>
          '' might yield some reasonable results in search of a new loading and inference
          system that works for different constraints.</p>

          '
        raw: "Hi, \nUnfortunately ctransformers is running a bit late to the game\
          \ in regards of keeping up with the recent new and exciting large language\
          \ model architectural changes this winter.\nFrom what I can tell, due to\
          \ how ctransformers works, some of the new unique models with architectural\
          \ changes may not be supported yet. It doesn't seem very flexible with loading\
          \ models outside of the model families that it was built for already. ctransformers\
          \ has not been updated since November. Which was just before all the fun\
          \ new model excitement this winter.\nI don't think they are out of the game\
          \ yet. You may have to wait. Or consider using another backend/system interim\
          \ if you don't want to wait. I've been using ctransformers this year and\
          \ considering moving to something new due to the delays. (I hope that ctransformers\
          \ is ok and catches up soon. I liked using it.)\nctransformers is built\
          \ around running quantized models. So you might have better luck looking\
          \ around areas where that is the main theme, if you were not already.\n\
          https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF#about-gguf\
          \ <- For example, TheBloke points out other tools/clients/libraries that\
          \ could be used to load GGUF quants of this and some newer model(s).\nThe\
          \ llama-cpp family (llama-cpp and llama-cpp-python) might be a good choice.\
          \ They seem to be able to move fast to build support for new model changes.\
          \ And I've personally seen a successful SOLAR load and reasonable inference\
          \ by llama-cpp-python. They have good documentation as well. ctransformers\
          \ and llama-cpp-python felt alike. So your experience/understanding may\
          \ transfer decently.\nIf your ok with loading models on your CPU, it's pretty\
          \ easy to setup for CPU. And it's OS agnostic. Though things could be really\
          \ slow if your not running a recent CPU with AVX support or something similar\
          \ and recent. And it has bindings that likely will bridge things decently\
          \ between it and LangChain. (https://python.langchain.com/docs/integrations/providers/llamacpp\
          \  Though I think LangChain might be a bit dated with it's info. And I'm\
          \ not a LangChain user. Still might be useful.) (Llama-cpp caveat. Running\
          \ on something other than CPU will likely require a couple year seasoned\
          \ Linux/Unix/VM OS management experience and likely some decent experience\
          \ with software development to understand what it needed. Should be do-able\
          \ if your familiar with that. If not, that is asking a lot of people just\
          \ to guide someone blindly through.)\nThat being said, likely something\
          \ could cross between providers that LangChain supports and the systems\
          \ that can load GGUF's of SOLAR that may work for you.\nCross checking between\
          \ 'https://python.langchain.com/docs/integrations/providers -> More' and\
          \ 'https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF#about-gguf\
          \ ' might yield some reasonable results in search of a new loading and inference\
          \ system that works for different constraints."
        updatedAt: '2024-01-04T11:30:41.857Z'
      numEdits: 0
      reactions: []
    id: 659696e1573369a3e6ff07b8
    type: comment
  author: dyoung
  content: "Hi, \nUnfortunately ctransformers is running a bit late to the game in\
    \ regards of keeping up with the recent new and exciting large language model\
    \ architectural changes this winter.\nFrom what I can tell, due to how ctransformers\
    \ works, some of the new unique models with architectural changes may not be supported\
    \ yet. It doesn't seem very flexible with loading models outside of the model\
    \ families that it was built for already. ctransformers has not been updated since\
    \ November. Which was just before all the fun new model excitement this winter.\n\
    I don't think they are out of the game yet. You may have to wait. Or consider\
    \ using another backend/system interim if you don't want to wait. I've been using\
    \ ctransformers this year and considering moving to something new due to the delays.\
    \ (I hope that ctransformers is ok and catches up soon. I liked using it.)\nctransformers\
    \ is built around running quantized models. So you might have better luck looking\
    \ around areas where that is the main theme, if you were not already.\nhttps://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF#about-gguf\
    \ <- For example, TheBloke points out other tools/clients/libraries that could\
    \ be used to load GGUF quants of this and some newer model(s).\nThe llama-cpp\
    \ family (llama-cpp and llama-cpp-python) might be a good choice. They seem to\
    \ be able to move fast to build support for new model changes. And I've personally\
    \ seen a successful SOLAR load and reasonable inference by llama-cpp-python. They\
    \ have good documentation as well. ctransformers and llama-cpp-python felt alike.\
    \ So your experience/understanding may transfer decently.\nIf your ok with loading\
    \ models on your CPU, it's pretty easy to setup for CPU. And it's OS agnostic.\
    \ Though things could be really slow if your not running a recent CPU with AVX\
    \ support or something similar and recent. And it has bindings that likely will\
    \ bridge things decently between it and LangChain. (https://python.langchain.com/docs/integrations/providers/llamacpp\
    \  Though I think LangChain might be a bit dated with it's info. And I'm not a\
    \ LangChain user. Still might be useful.) (Llama-cpp caveat. Running on something\
    \ other than CPU will likely require a couple year seasoned Linux/Unix/VM OS management\
    \ experience and likely some decent experience with software development to understand\
    \ what it needed. Should be do-able if your familiar with that. If not, that is\
    \ asking a lot of people just to guide someone blindly through.)\nThat being said,\
    \ likely something could cross between providers that LangChain supports and the\
    \ systems that can load GGUF's of SOLAR that may work for you.\nCross checking\
    \ between 'https://python.langchain.com/docs/integrations/providers -> More' and\
    \ 'https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF#about-gguf '\
    \ might yield some reasonable results in search of a new loading and inference\
    \ system that works for different constraints."
  created_at: 2024-01-04 11:30:41+00:00
  edited: false
  hidden: false
  id: 659696e1573369a3e6ff07b8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: upstage/SOLAR-10.7B-Instruct-v1.0
repo_type: model
status: open
target_branch: null
title: How to load with Langchain using Ctransformers?
