!!python/object:huggingface_hub.community.DiscussionWithDetails
author: agershun
conflicting_files: null
created_at: 2023-12-16 17:56:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64e057dd8e2084e1d7c60e45/3hvcSM0nZfQ8LeMHfC63W.jpeg?w=200&h=200&f=face
      fullname: Andrey Gershun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: agershun
      type: user
    createdAt: '2023-12-16T17:56:36.000Z'
    data:
      edited: false
      editors:
      - agershun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9058772921562195
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64e057dd8e2084e1d7c60e45/3hvcSM0nZfQ8LeMHfC63W.jpeg?w=200&h=200&f=face
          fullname: Andrey Gershun
          isHf: false
          isPro: false
          name: agershun
          type: user
        html: '<p>I feel that this 11B model is smarter and less hallucinate then
          current leaders in the leaderboard (on December 16, 2023).</p>

          <p>I have tried these models with better current scores then upstage/SOLAR-10.7B-Instruct-v1.0:</p>

          <ul>

          <li>rwitz2/go-bruins-v2.1.1</li>

          <li>ignos/LeoScorpius-GreenNode-Alpaca-7B-v1</li>

          <li>Toten5/LeoScorpius-GreenNode-7B-v1</li>

          </ul>

          <p>and found that all of them hallucinate more often than the SOLAR does.
          </p>

          <p>Probably, the reason is:<br>a) in the size of the model and it is more
          perspective to improve 11B models than 7B.<br>b) they are overtrained on
          the test dataset data.</p>

          '
        raw: "I feel that this 11B model is smarter and less hallucinate then current\
          \ leaders in the leaderboard (on December 16, 2023).\r\n\r\nI have tried\
          \ these models with better current scores then upstage/SOLAR-10.7B-Instruct-v1.0:\r\
          \n- rwitz2/go-bruins-v2.1.1\r\n- ignos/LeoScorpius-GreenNode-Alpaca-7B-v1\r\
          \n- Toten5/LeoScorpius-GreenNode-7B-v1\r\n\r\nand found that all of them\
          \ hallucinate more often than the SOLAR does. \r\n\r\nProbably, the reason\
          \ is:\r\na) in the size of the model and it is more perspective to improve\
          \ 11B models than 7B. \r\nb) they are overtrained on the test dataset data.\r\
          \n"
        updatedAt: '2023-12-16T17:56:36.505Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - killawhale2
    id: 657de4d4416635415ff8e513
    type: comment
  author: agershun
  content: "I feel that this 11B model is smarter and less hallucinate then current\
    \ leaders in the leaderboard (on December 16, 2023).\r\n\r\nI have tried these\
    \ models with better current scores then upstage/SOLAR-10.7B-Instruct-v1.0:\r\n\
    - rwitz2/go-bruins-v2.1.1\r\n- ignos/LeoScorpius-GreenNode-Alpaca-7B-v1\r\n- Toten5/LeoScorpius-GreenNode-7B-v1\r\
    \n\r\nand found that all of them hallucinate more often than the SOLAR does. \r\
    \n\r\nProbably, the reason is:\r\na) in the size of the model and it is more perspective\
    \ to improve 11B models than 7B. \r\nb) they are overtrained on the test dataset\
    \ data.\r\n"
  created_at: 2023-12-16 17:56:36+00:00
  edited: false
  hidden: false
  id: 657de4d4416635415ff8e513
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64e057dd8e2084e1d7c60e45/3hvcSM0nZfQ8LeMHfC63W.jpeg?w=200&h=200&f=face
      fullname: Andrey Gershun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: agershun
      type: user
    createdAt: '2023-12-16T17:56:55.000Z'
    data:
      from: I feel that this 11B model is smarter and less hallucinate then
      to: I feel that this 11B model is smarter and less hallucinate than other leaders
    id: 657de4e769cfe9b4cd22f586
    type: title-change
  author: agershun
  created_at: 2023-12-16 17:56:55+00:00
  id: 657de4e769cfe9b4cd22f586
  new_title: I feel that this 11B model is smarter and less hallucinate than other
    leaders
  old_title: I feel that this 11B model is smarter and less hallucinate then
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64e057dd8e2084e1d7c60e45/3hvcSM0nZfQ8LeMHfC63W.jpeg?w=200&h=200&f=face
      fullname: Andrey Gershun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: agershun
      type: user
    createdAt: '2023-12-16T17:57:12.000Z'
    data:
      from: I feel that this 11B model is smarter and less hallucinate than other
        leaders
      to: I feel that this 11B model is smarter and hallucinates less than other leaders
    id: 657de4f8647c0211e78dc4b2
    type: title-change
  author: agershun
  created_at: 2023-12-16 17:57:12+00:00
  id: 657de4f8647c0211e78dc4b2
  new_title: I feel that this 11B model is smarter and hallucinates less than other
    leaders
  old_title: I feel that this 11B model is smarter and less hallucinate than other
    leaders
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-19T15:27:14.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9511408805847168
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;agershun&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/agershun\">@<span class=\"\
          underline\">agershun</span></a></span>\n\n\t</span></span> I agree that\
          \ it seems smarter, but have experienced it hallucinating more.</p>\n<p>The\
          \ reduction in hallucinations appears to be an illusion caused by things\
          \ like its propensity to shy away from long lists filled with details, hence\
          \ reducing the opportunity for hallucinations, and denying things are true,\
          \ even when they are (throwing the baby out with the bath water).</p>\n\
          <p>When I put it to the test with tricky fringe knowledge it performed worse\
          \ than all other leading 7b Mistrals. For example, when I ask about the\
          \ 2 ex-wives of Alan Harper from the show Two and a Half Men this LLM got\
          \ all 4 names wrong (screen and real names), while 7b Mistrals reliably\
          \ get 2 of 4 right (first wife Judith). And this wasn't the exception. It\
          \ reliable hallucinated more on fringe knowledge.</p>\n<p>So ironically,\
          \ despite its larger size, this LLM is far less knowledgeable than the original\
          \ 7b Mistrals. Hallucinated more at the fringes, provides less information\
          \ (to avoid hallucinations) and denying ~10x more things aren't true, that\
          \ actually are true, in an attempt to minimize the frequency of saying things\
          \ are true that aren't.</p>\n<p>In short, it actually hallucinates more,\
          \ which is why I suspect it's overly tight-lipped, brief, cynical...</p>\n"
        raw: '@agershun I agree that it seems smarter, but have experienced it hallucinating
          more.


          The reduction in hallucinations appears to be an illusion caused by things
          like its propensity to shy away from long lists filled with details, hence
          reducing the opportunity for hallucinations, and denying things are true,
          even when they are (throwing the baby out with the bath water).


          When I put it to the test with tricky fringe knowledge it performed worse
          than all other leading 7b Mistrals. For example, when I ask about the 2
          ex-wives of Alan Harper from the show Two and a Half Men this LLM got all
          4 names wrong (screen and real names), while 7b Mistrals reliably get 2
          of 4 right (first wife Judith). And this wasn''t the exception. It reliable
          hallucinated more on fringe knowledge.


          So ironically, despite its larger size, this LLM is far less knowledgeable
          than the original 7b Mistrals. Hallucinated more at the fringes, provides
          less information (to avoid hallucinations) and denying ~10x more things
          aren''t true, that actually are true, in an attempt to minimize the frequency
          of saying things are true that aren''t.


          In short, it actually hallucinates more, which is why I suspect it''s overly
          tight-lipped, brief, cynical...'
        updatedAt: '2023-12-19T15:27:14.629Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - agershun
    id: 6581b652a1291cc3c77a15c2
    type: comment
  author: Phil337
  content: '@agershun I agree that it seems smarter, but have experienced it hallucinating
    more.


    The reduction in hallucinations appears to be an illusion caused by things like
    its propensity to shy away from long lists filled with details, hence reducing
    the opportunity for hallucinations, and denying things are true, even when they
    are (throwing the baby out with the bath water).


    When I put it to the test with tricky fringe knowledge it performed worse than
    all other leading 7b Mistrals. For example, when I ask about the 2 ex-wives of
    Alan Harper from the show Two and a Half Men this LLM got all 4 names wrong (screen
    and real names), while 7b Mistrals reliably get 2 of 4 right (first wife Judith).
    And this wasn''t the exception. It reliable hallucinated more on fringe knowledge.


    So ironically, despite its larger size, this LLM is far less knowledgeable than
    the original 7b Mistrals. Hallucinated more at the fringes, provides less information
    (to avoid hallucinations) and denying ~10x more things aren''t true, that actually
    are true, in an attempt to minimize the frequency of saying things are true that
    aren''t.


    In short, it actually hallucinates more, which is why I suspect it''s overly tight-lipped,
    brief, cynical...'
  created_at: 2023-12-19 15:27:14+00:00
  edited: false
  hidden: false
  id: 6581b652a1291cc3c77a15c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcddb0134925f39a538724f2545931a2.svg
      fullname: Minh Duong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mino24she
      type: user
    createdAt: '2023-12-19T17:45:12.000Z'
    data:
      edited: false
      editors:
      - Mino24she
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9838594794273376
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcddb0134925f39a538724f2545931a2.svg
          fullname: Minh Duong
          isHf: false
          isPro: false
          name: Mino24she
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Phil337&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Phil337\">@<span class=\"\
          underline\">Phil337</span></a></span>\n\n\t</span></span> Could you try\
          \ out UNA-Solar by the UNA guy? Or better, the Frostwind tune from Sao10K?</p>\n\
          <p>I still have no idea what UNA is, but it seems he finally relented some\
          \ details in his model card. I have more hope for the Sao10K tune, however,\
          \ because it's actually trained on the base model.</p>\n<p>I'm traveling\
          \ so can't quite test at the moment. I'm rather invested in this model,\
          \ due to how transparent the team has been in their testing. Seems like\
          \ a rare breed in recent climate.</p>\n"
        raw: '@Phil337 Could you try out UNA-Solar by the UNA guy? Or better, the
          Frostwind tune from Sao10K?


          I still have no idea what UNA is, but it seems he finally relented some
          details in his model card. I have more hope for the Sao10K tune, however,
          because it''s actually trained on the base model.


          I''m traveling so can''t quite test at the moment. I''m rather invested
          in this model, due to how transparent the team has been in their testing.
          Seems like a rare breed in recent climate.'
        updatedAt: '2023-12-19T17:45:12.694Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - agershun
    id: 6581d6a8ddeea5eb5f6d02c2
    type: comment
  author: Mino24she
  content: '@Phil337 Could you try out UNA-Solar by the UNA guy? Or better, the Frostwind
    tune from Sao10K?


    I still have no idea what UNA is, but it seems he finally relented some details
    in his model card. I have more hope for the Sao10K tune, however, because it''s
    actually trained on the base model.


    I''m traveling so can''t quite test at the moment. I''m rather invested in this
    model, due to how transparent the team has been in their testing. Seems like a
    rare breed in recent climate.'
  created_at: 2023-12-19 17:45:12+00:00
  edited: false
  hidden: false
  id: 6581d6a8ddeea5eb5f6d02c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-19T20:03:09.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9761138558387756
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Mino24she&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Mino24she\">@<span class=\"\
          underline\">Mino24she</span></a></span>\n\n\t</span></span> I haven't tried\
          \ Sao10K yet, but I did try the UNA-Solar version of instruct and it performed\
          \ slightly better on my test (e.g. got 3 of 4 names right in my aforementioned\
          \ question about Alan's ex-wives from 2.5 Men). However, it's plagued by\
          \ the exact same stubborn denials of facts, as well as excessive censorship\
          \ and moralizing. But I guess this is expected if UNA is more about the\
          \ transformers than weights.</p>\n<p>I also tried the Uncensored version\
          \ of Solaris and it performed better (less censorship and moralizing, plus\
          \ longer responses), but it still performed poorly on fridge knowledge questions\
          \ like the Alan question above.</p>\n"
        raw: '@Mino24she I haven''t tried Sao10K yet, but I did try the UNA-Solar
          version of instruct and it performed slightly better on my test (e.g. got
          3 of 4 names right in my aforementioned question about Alan''s ex-wives
          from 2.5 Men). However, it''s plagued by the exact same stubborn denials
          of facts, as well as excessive censorship and moralizing. But I guess this
          is expected if UNA is more about the transformers than weights.


          I also tried the Uncensored version of Solaris and it performed better (less
          censorship and moralizing, plus longer responses), but it still performed
          poorly on fridge knowledge questions like the Alan question above.'
        updatedAt: '2023-12-19T20:03:09.322Z'
      numEdits: 0
      reactions: []
    id: 6581f6fdb0c93d553908a3da
    type: comment
  author: Phil337
  content: '@Mino24she I haven''t tried Sao10K yet, but I did try the UNA-Solar version
    of instruct and it performed slightly better on my test (e.g. got 3 of 4 names
    right in my aforementioned question about Alan''s ex-wives from 2.5 Men). However,
    it''s plagued by the exact same stubborn denials of facts, as well as excessive
    censorship and moralizing. But I guess this is expected if UNA is more about the
    transformers than weights.


    I also tried the Uncensored version of Solaris and it performed better (less censorship
    and moralizing, plus longer responses), but it still performed poorly on fridge
    knowledge questions like the Alan question above.'
  created_at: 2023-12-19 20:03:09+00:00
  edited: false
  hidden: false
  id: 6581f6fdb0c93d553908a3da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcddb0134925f39a538724f2545931a2.svg
      fullname: Minh Duong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mino24she
      type: user
    createdAt: '2023-12-19T20:40:28.000Z'
    data:
      edited: false
      editors:
      - Mino24she
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9754419922828674
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcddb0134925f39a538724f2545931a2.svg
          fullname: Minh Duong
          isHf: false
          isPro: false
          name: Mino24she
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Phil337&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Phil337\">@<span class=\"\
          underline\">Phil337</span></a></span>\n\n\t</span></span> The UNA one is\
          \ based on the instruct tune, which obviously, won't be as practical. The\
          \ base model looks more organic to me. The Sao10K (of Euryale fame) one\
          \ (Frostwind) otherwise looks pretty good.</p>\n"
        raw: '@Phil337 The UNA one is based on the instruct tune, which obviously,
          won''t be as practical. The base model looks more organic to me. The Sao10K
          (of Euryale fame) one (Frostwind) otherwise looks pretty good.'
        updatedAt: '2023-12-19T20:40:28.394Z'
      numEdits: 0
      reactions: []
    id: 6581ffbc117b524ef3e130af
    type: comment
  author: Mino24she
  content: '@Phil337 The UNA one is based on the instruct tune, which obviously, won''t
    be as practical. The base model looks more organic to me. The Sao10K (of Euryale
    fame) one (Frostwind) otherwise looks pretty good.'
  created_at: 2023-12-19 20:40:28+00:00
  edited: false
  hidden: false
  id: 6581ffbc117b524ef3e130af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-19T21:26:08.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.972466230392456
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Mino24she&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Mino24she\">@<span class=\"\
          underline\">Mino24she</span></a></span>\n\n\t</span></span> I tested Frostwind\
          \ and it still has some censorship, but less than Solar instruct. It also\
          \ hallucinates more than leading Mistrals at the fringes of knowledge.</p>\n\
          <p>So far all the Solar LLMs have been smarter than the top Mistrals, but\
          \ none have been more knowledgeable, or even as knowledgeable. I don't know\
          \ what up-scaling is in the context of LLMs, but it appears to only improve\
          \ the transformers, not the weights.</p>\n"
        raw: '@Mino24she I tested Frostwind and it still has some censorship, but
          less than Solar instruct. It also hallucinates more than leading Mistrals
          at the fringes of knowledge.


          So far all the Solar LLMs have been smarter than the top Mistrals, but none
          have been more knowledgeable, or even as knowledgeable. I don''t know what
          up-scaling is in the context of LLMs, but it appears to only improve the
          transformers, not the weights.'
        updatedAt: '2023-12-19T21:26:08.056Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - agershun
    id: 65820a70f3006507eaf4cd5c
    type: comment
  author: Phil337
  content: '@Mino24she I tested Frostwind and it still has some censorship, but less
    than Solar instruct. It also hallucinates more than leading Mistrals at the fringes
    of knowledge.


    So far all the Solar LLMs have been smarter than the top Mistrals, but none have
    been more knowledgeable, or even as knowledgeable. I don''t know what up-scaling
    is in the context of LLMs, but it appears to only improve the transformers, not
    the weights.'
  created_at: 2023-12-19 21:26:08+00:00
  edited: false
  hidden: false
  id: 65820a70f3006507eaf4cd5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0bd616c59b8b51b22a2361f08a70a289.svg
      fullname: Marcelo Teixeira
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maxterm2000
      type: user
    createdAt: '2023-12-19T22:47:41.000Z'
    data:
      edited: true
      editors:
      - maxterm2000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9414025545120239
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0bd616c59b8b51b22a2361f08a70a289.svg
          fullname: Marcelo Teixeira
          isHf: false
          isPro: false
          name: maxterm2000
          type: user
        html: '<p>Good model, have some problems for common sense reasoning and counterfactual
          reasoning, but isn''t impossible to adjust. It''s the wright direction,
          with more accuracy on MMLU benchmark I think the upstage can surpass Mistrals
          in the future. Have something marvelous in this model.</p>

          '
        raw: 'Good model, have some problems for common sense reasoning and counterfactual
          reasoning, but isn''t impossible to adjust. It''s the wright direction,
          with more accuracy on MMLU benchmark I think the upstage can surpass Mistrals
          in the future. Have something marvelous in this model.

          '
        updatedAt: '2023-12-19T22:48:30.670Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - agershun
    id: 65821d8d253cc8df69502c83
    type: comment
  author: maxterm2000
  content: 'Good model, have some problems for common sense reasoning and counterfactual
    reasoning, but isn''t impossible to adjust. It''s the wright direction, with more
    accuracy on MMLU benchmark I think the upstage can surpass Mistrals in the future.
    Have something marvelous in this model.

    '
  created_at: 2023-12-19 22:47:41+00:00
  edited: true
  hidden: false
  id: 65821d8d253cc8df69502c83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64e057dd8e2084e1d7c60e45/3hvcSM0nZfQ8LeMHfC63W.jpeg?w=200&h=200&f=face
      fullname: Andrey Gershun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: agershun
      type: user
    createdAt: '2023-12-20T14:38:22.000Z'
    data:
      edited: true
      editors:
      - agershun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9665752053260803
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64e057dd8e2084e1d7c60e45/3hvcSM0nZfQ8LeMHfC63W.jpeg?w=200&h=200&f=face
          fullname: Andrey Gershun
          isHf: false
          isPro: false
          name: agershun
          type: user
        html: '<p>Agree that reducing the size of the response is an excellent way
          to decrease hallucinations. However, for solving specific tasks, the length
          of the response is not a critical factor.</p>

          <p>This SOLAR neural network doesn''t know a lot, that''s true. I usually
          use the story "Mumu" by Ivan Turgenev for tests, and I have heard so many
          interesting and diverse stories from different neural networks. But for
          my tasks, this is not a problem, I want to further train it for my actual
          material (figuratively speaking, to instill in it the correct version of
          Mumu). For me, it''s more important that it is still capable of making inferences
          and doesn''t give random characteers like ===~=#$== as some other networks
          do."</p>

          '
        raw: 'Agree that reducing the size of the response is an excellent way to
          decrease hallucinations. However, for solving specific tasks, the length
          of the response is not a critical factor.


          This SOLAR neural network doesn''t know a lot, that''s true. I usually use
          the story "Mumu" by Ivan Turgenev for tests, and I have heard so many interesting
          and diverse stories from different neural networks. But for my tasks, this
          is not a problem, I want to further train it for my actual material (figuratively
          speaking, to instill in it the correct version of Mumu). For me, it''s more
          important that it is still capable of making inferences and doesn''t give
          random characteers like ===~=#$== as some other networks do."'
        updatedAt: '2023-12-20T14:38:55.005Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Phil337
        - Mino24she
    id: 6582fc5e8166a8b73644308d
    type: comment
  author: agershun
  content: 'Agree that reducing the size of the response is an excellent way to decrease
    hallucinations. However, for solving specific tasks, the length of the response
    is not a critical factor.


    This SOLAR neural network doesn''t know a lot, that''s true. I usually use the
    story "Mumu" by Ivan Turgenev for tests, and I have heard so many interesting
    and diverse stories from different neural networks. But for my tasks, this is
    not a problem, I want to further train it for my actual material (figuratively
    speaking, to instill in it the correct version of Mumu). For me, it''s more important
    that it is still capable of making inferences and doesn''t give random characteers
    like ===~=#$== as some other networks do."'
  created_at: 2023-12-20 14:38:22+00:00
  edited: true
  hidden: false
  id: 6582fc5e8166a8b73644308d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a813dedbb9e28866a91b27/i2pGYzY1htiY-L3WFPSgl.jpeg?w=200&h=200&f=face
      fullname: appvoid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appvoid
      type: user
    createdAt: '2023-12-21T07:17:10.000Z'
    data:
      edited: false
      editors:
      - appvoid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9113978147506714
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a813dedbb9e28866a91b27/i2pGYzY1htiY-L3WFPSgl.jpeg?w=200&h=200&f=face
          fullname: appvoid
          isHf: false
          isPro: false
          name: appvoid
          type: user
        html: '<p>This model seems to be better when it comes to RAG, it hallucinates
          a lot less, this is the most useful model i''ve loaded on my 8gb vram laptop.
          You can really rely on it on common, easy language tasks just like chatgpt
          3.5. Of course, GPT-4 is better at everything but hey, this is a free, fast
          decent language model!</p>

          '
        raw: This model seems to be better when it comes to RAG, it hallucinates a
          lot less, this is the most useful model i've loaded on my 8gb vram laptop.
          You can really rely on it on common, easy language tasks just like chatgpt
          3.5. Of course, GPT-4 is better at everything but hey, this is a free, fast
          decent language model!
        updatedAt: '2023-12-21T07:17:10.728Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - agershun
        - hunkim
        - vincent88
    id: 6583e676cce29a062153a669
    type: comment
  author: appvoid
  content: This model seems to be better when it comes to RAG, it hallucinates a lot
    less, this is the most useful model i've loaded on my 8gb vram laptop. You can
    really rely on it on common, easy language tasks just like chatgpt 3.5. Of course,
    GPT-4 is better at everything but hey, this is a free, fast decent language model!
  created_at: 2023-12-21 07:17:10+00:00
  edited: false
  hidden: false
  id: 6583e676cce29a062153a669
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2672e3ce2cab00802700bdbaaeb98bd9.svg
      fullname: Sascha Wageringel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: warlock76
      type: user
    createdAt: '2023-12-22T01:44:34.000Z'
    data:
      edited: false
      editors:
      - warlock76
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9105537533760071
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2672e3ce2cab00802700bdbaaeb98bd9.svg
          fullname: Sascha Wageringel
          isHf: false
          isPro: false
          name: warlock76
          type: user
        html: '<p>For some reason at 16 bit attention it increases in VRam use with
          each response. Its stable in 8-bit attention but that is sad because you
          can load it at 16-bit into 24GB VRam and its really fast. The problem is
          once it hits the ceiling it crashes. I use fastchat with "python -m fastchat.serve.cli
          --model-path I:\misc\downloaded\Ai_models\models_for_fastchat\upstageSOLAR-10.7B-Instruct-v1.0
          --style rich" on a RTX Quadro 6000 Passive. Its stable with --load-8-bit.
          I will try the other solar models. So far it seems very fast with solid
          replies.</p>

          '
        raw: For some reason at 16 bit attention it increases in VRam use with each
          response. Its stable in 8-bit attention but that is sad because you can
          load it at 16-bit into 24GB VRam and its really fast. The problem is once
          it hits the ceiling it crashes. I use fastchat with "python -m fastchat.serve.cli
          --model-path I:\misc\downloaded\Ai_models\models_for_fastchat\upstageSOLAR-10.7B-Instruct-v1.0
          --style rich" on a RTX Quadro 6000 Passive. Its stable with --load-8-bit.
          I will try the other solar models. So far it seems very fast with solid
          replies.
        updatedAt: '2023-12-22T01:44:34.462Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - agershun
    id: 6584ea02b48986be10ccb285
    type: comment
  author: warlock76
  content: For some reason at 16 bit attention it increases in VRam use with each
    response. Its stable in 8-bit attention but that is sad because you can load it
    at 16-bit into 24GB VRam and its really fast. The problem is once it hits the
    ceiling it crashes. I use fastchat with "python -m fastchat.serve.cli --model-path
    I:\misc\downloaded\Ai_models\models_for_fastchat\upstageSOLAR-10.7B-Instruct-v1.0
    --style rich" on a RTX Quadro 6000 Passive. Its stable with --load-8-bit. I will
    try the other solar models. So far it seems very fast with solid replies.
  created_at: 2023-12-22 01:44:34+00:00
  edited: false
  hidden: false
  id: 6584ea02b48986be10ccb285
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/603c29094a944b99e81476fd/LaSNcrKmCEUBEBZZCce3k.png?w=200&h=200&f=face
      fullname: Sung Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hunkim
      type: user
    createdAt: '2023-12-22T10:48:18.000Z'
    data:
      edited: false
      editors:
      - hunkim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9745250940322876
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/603c29094a944b99e81476fd/LaSNcrKmCEUBEBZZCce3k.png?w=200&h=200&f=face
          fullname: Sung Kim
          isHf: false
          isPro: false
          name: hunkim
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;appvoid&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/appvoid\">@<span class=\"\
          underline\">appvoid</span></a></span>\n\n\t</span></span> You are absolutely\
          \ right. We designed this model to follow instructions well, including the\
          \ RAG model. Thank you very much for your comment! :-)</p>\n"
        raw: '@appvoid You are absolutely right. We designed this model to follow
          instructions well, including the RAG model. Thank you very much for your
          comment! :-)'
        updatedAt: '2023-12-22T10:48:18.233Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - agershun
        - jaeholee2212
        - appvoid
        - killawhale2
    id: 65856972c8b8bd66f7be4a03
    type: comment
  author: hunkim
  content: '@appvoid You are absolutely right. We designed this model to follow instructions
    well, including the RAG model. Thank you very much for your comment! :-)'
  created_at: 2023-12-22 10:48:18+00:00
  edited: false
  hidden: false
  id: 65856972c8b8bd66f7be4a03
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0fa2148c818aa15435d95fa3c424c98.svg
      fullname: Angelos Papageorgiou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: frenzygr
      type: user
    createdAt: '2023-12-23T18:15:01.000Z'
    data:
      edited: false
      editors:
      - frenzygr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8787658214569092
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0fa2148c818aa15435d95fa3c424c98.svg
          fullname: Angelos Papageorgiou
          isHf: false
          isPro: false
          name: frenzygr
          type: user
        html: '<blockquote>

          <p>This model seems to be better when it comes to RAG, it hallucinates a
          lot less, this is the most useful model i''ve loaded on my 8gb vram laptop.
          You can really rely on it on common, easy language tasks just like chatgpt
          3.5. Of course, GPT-4 is better at everything but hey, this is a free, fast
          decent language model!</p>

          </blockquote>

          <p>hi mate kind of irrelevant question but how do you run the .safetensor
          files of this model? Do you convert it to .gguf using <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/discussions/2948">https://github.com/ggerganov/llama.cpp/discussions/2948</a>
          first or do you have some other method?</p>

          '
        raw: '> This model seems to be better when it comes to RAG, it hallucinates
          a lot less, this is the most useful model i''ve loaded on my 8gb vram laptop.
          You can really rely on it on common, easy language tasks just like chatgpt
          3.5. Of course, GPT-4 is better at everything but hey, this is a free, fast
          decent language model!


          hi mate kind of irrelevant question but how do you run the .safetensor files
          of this model? Do you convert it to .gguf using https://github.com/ggerganov/llama.cpp/discussions/2948
          first or do you have some other method?'
        updatedAt: '2023-12-23T18:15:01.698Z'
      numEdits: 0
      reactions: []
    id: 658723a57959448ef5e31320
    type: comment
  author: frenzygr
  content: '> This model seems to be better when it comes to RAG, it hallucinates
    a lot less, this is the most useful model i''ve loaded on my 8gb vram laptop.
    You can really rely on it on common, easy language tasks just like chatgpt 3.5.
    Of course, GPT-4 is better at everything but hey, this is a free, fast decent
    language model!


    hi mate kind of irrelevant question but how do you run the .safetensor files of
    this model? Do you convert it to .gguf using https://github.com/ggerganov/llama.cpp/discussions/2948
    first or do you have some other method?'
  created_at: 2023-12-23 18:15:01+00:00
  edited: false
  hidden: false
  id: 658723a57959448ef5e31320
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a813dedbb9e28866a91b27/i2pGYzY1htiY-L3WFPSgl.jpeg?w=200&h=200&f=face
      fullname: appvoid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appvoid
      type: user
    createdAt: '2023-12-23T18:48:21.000Z'
    data:
      edited: false
      editors:
      - appvoid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8842668533325195
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a813dedbb9e28866a91b27/i2pGYzY1htiY-L3WFPSgl.jpeg?w=200&h=200&f=face
          fullname: appvoid
          isHf: false
          isPro: false
          name: appvoid
          type: user
        html: '<blockquote>

          <blockquote>

          <p>This model seems to be better when it comes to RAG, it hallucinates a
          lot less, this is the most useful model i''ve loaded on my 8gb vram laptop.
          You can really rely on it on common, easy language tasks just like chatgpt
          3.5. Of course, GPT-4 is better at everything but hey, this is a free, fast
          decent language model!</p>

          </blockquote>

          <p>hi mate kind of irrelevant question but how do you run the .safetensor
          files of this model? Do you convert it to .gguf using <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/discussions/2948">https://github.com/ggerganov/llama.cpp/discussions/2948</a>
          first or do you have some other method?</p>

          </blockquote>

          <p>Hi, i just used gguf version from "the bloke" on lmstudio. What really
          suprises me though is that i''m using a q5km quantized version and still
          manages to be good.</p>

          '
        raw: "> > This model seems to be better when it comes to RAG, it hallucinates\
          \ a lot less, this is the most useful model i've loaded on my 8gb vram laptop.\
          \ You can really rely on it on common, easy language tasks just like chatgpt\
          \ 3.5. Of course, GPT-4 is better at everything but hey, this is a free,\
          \ fast decent language model!\n> \n> hi mate kind of irrelevant question\
          \ but how do you run the .safetensor files of this model? Do you convert\
          \ it to .gguf using https://github.com/ggerganov/llama.cpp/discussions/2948\
          \ first or do you have some other method?\n\nHi, i just used gguf version\
          \ from \"the bloke\" on lmstudio. What really suprises me though is that\
          \ i'm using a q5km quantized version and still manages to be good."
        updatedAt: '2023-12-23T18:48:21.014Z'
      numEdits: 0
      reactions: []
    id: 65872b75d1331d552bbcb90b
    type: comment
  author: appvoid
  content: "> > This model seems to be better when it comes to RAG, it hallucinates\
    \ a lot less, this is the most useful model i've loaded on my 8gb vram laptop.\
    \ You can really rely on it on common, easy language tasks just like chatgpt 3.5.\
    \ Of course, GPT-4 is better at everything but hey, this is a free, fast decent\
    \ language model!\n> \n> hi mate kind of irrelevant question but how do you run\
    \ the .safetensor files of this model? Do you convert it to .gguf using https://github.com/ggerganov/llama.cpp/discussions/2948\
    \ first or do you have some other method?\n\nHi, i just used gguf version from\
    \ \"the bloke\" on lmstudio. What really suprises me though is that i'm using\
    \ a q5km quantized version and still manages to be good."
  created_at: 2023-12-23 18:48:21+00:00
  edited: false
  hidden: false
  id: 65872b75d1331d552bbcb90b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64e057dd8e2084e1d7c60e45/3hvcSM0nZfQ8LeMHfC63W.jpeg?w=200&h=200&f=face
      fullname: Andrey Gershun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: agershun
      type: user
    createdAt: '2023-12-23T19:28:08.000Z'
    data:
      edited: true
      editors:
      - agershun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4675612151622772
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64e057dd8e2084e1d7c60e45/3hvcSM0nZfQ8LeMHfC63W.jpeg?w=200&h=200&f=face
          fullname: Andrey Gershun
          isHf: false
          isPro: false
          name: agershun
          type: user
        html: "<p>I used SOLAR with the following methods:</p>\n<ol>\n<li>ollama supports\
          \ it out of the box</li>\n<li>vllm as well</li>\n<li>For A100/40 + Jupyter\
          \ I used this code:</li>\n</ol>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >from</span> datasets <span class=\"hljs-keyword\">import</span> Dataset,\
          \ load_dataset\n<span class=\"hljs-keyword\">from</span> transformers <span\
          \ class=\"hljs-keyword\">import</span> (\n    AutoModelForCausalLM,\n  \
          \  AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n   \
          \ pipeline,\n    logging,\n)\n<span class=\"hljs-keyword\">from</span> trl\
          \ <span class=\"hljs-keyword\">import</span> SFTTrainer\nmodel_name = <span\
          \ class=\"hljs-string\">\"upstage/SOLAR-10.7B-Instruct-v1.0\"</span>\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side =\
          \ <span class=\"hljs-string\">'right'</span>\nuse_4bit = <span class=\"\
          hljs-literal\">True</span>\nbnb_4bit_compute_dtype = <span class=\"hljs-string\"\
          >\"float16\"</span>\nbnb_4bit_quant_type = <span class=\"hljs-string\">\"\
          nf4\"</span>\nuse_nested_quant = <span class=\"hljs-literal\">False</span>\n\
          compute_dtype = <span class=\"hljs-built_in\">getattr</span>(torch, bnb_4bit_compute_dtype)\n\
          \nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n\
          \    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n\
          )\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n\
          \    device_map=<span class=\"hljs-string\">\"auto\"</span>,\n    quantization_config=bnb_config,\n\
          )\n\nquery = <span class=\"hljs-string\">\"\"\"What do I want to ask?\"\"\
          \"</span>\nconversation = [ {<span class=\"hljs-string\">'role'</span>:\
          \ <span class=\"hljs-string\">'user'</span>, <span class=\"hljs-string\"\
          >'content'</span>: query} ] \nprompt = tokenizer.apply_chat_template(conversation,\
          \ tokenize=<span class=\"hljs-literal\">False</span>, add_generation_prompt=<span\
          \ class=\"hljs-literal\">True</span>)\n\ninputs = tokenizer(prompt, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>).to(base_model.device) \noutputs =\
          \ base_model.generate(**inputs, use_cache=<span class=\"hljs-literal\">True</span>,\
          \ max_length=<span class=\"hljs-number\">4096</span>)\noutput_text = tokenizer.decode(outputs[<span\
          \ class=\"hljs-number\">0</span>]) \noutput_text = output_text.split(<span\
          \ class=\"hljs-string\">\"\\n### Assistant:\\n\"</span>, <span class=\"\
          hljs-number\">1</span>)[-<span class=\"hljs-number\">1</span>].replace(<span\
          \ class=\"hljs-string\">\"&lt;s&gt;\"</span>, <span class=\"hljs-string\"\
          >\"\"</span>).replace(<span class=\"hljs-string\">\"&lt;/s&gt;\"</span>,\
          \ <span class=\"hljs-string\">\"\"</span>).strip()\n<span class=\"hljs-built_in\"\
          >print</span>(output_text)\n</code></pre>\n"
        raw: "I used SOLAR with the following methods:\n1) ollama supports it out\
          \ of the box\n2) vllm as well\n3) For A100/40 + Jupyter I used this code:\n\
          \n```python\nimport torch\nfrom datasets import Dataset, load_dataset\n\
          from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n\
          \    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n    logging,\n\
          )\nfrom trl import SFTTrainer\nmodel_name = \"upstage/SOLAR-10.7B-Instruct-v1.0\"\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side\
          \ = 'right'\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type\
          \ = \"nf4\"\nuse_nested_quant = False\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\
          \nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n\
          \    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n\
          )\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n\
          \    device_map=\"auto\",\n    quantization_config=bnb_config,\n)\n\nquery\
          \ = \"\"\"What do I want to ask?\"\"\"\nconversation = [ {'role': 'user',\
          \ 'content': query} ] \nprompt = tokenizer.apply_chat_template(conversation,\
          \ tokenize=False, add_generation_prompt=True)\n\ninputs = tokenizer(prompt,\
          \ return_tensors=\"pt\").to(base_model.device) \noutputs = base_model.generate(**inputs,\
          \ use_cache=True, max_length=4096)\noutput_text = tokenizer.decode(outputs[0])\
          \ \noutput_text = output_text.split(\"\\n### Assistant:\\n\", 1)[-1].replace(\"\
          <s>\", \"\").replace(\"</s>\", \"\").strip()\nprint(output_text)\n```"
        updatedAt: '2023-12-23T19:30:37.867Z'
      numEdits: 1
      reactions: []
    id: 658734c8d2ea3f32940b8bb0
    type: comment
  author: agershun
  content: "I used SOLAR with the following methods:\n1) ollama supports it out of\
    \ the box\n2) vllm as well\n3) For A100/40 + Jupyter I used this code:\n\n```python\n\
    import torch\nfrom datasets import Dataset, load_dataset\nfrom transformers import\
    \ (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n\
    \    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom trl import SFTTrainer\n\
    model_name = \"upstage/SOLAR-10.7B-Instruct-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
    tokenizer.padding_side = 'right'\nuse_4bit = True\nbnb_4bit_compute_dtype = \"\
    float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\ncompute_dtype\
    \ = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n\
    \    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n  \
    \  bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n\
    )\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"\
    auto\",\n    quantization_config=bnb_config,\n)\n\nquery = \"\"\"What do I want\
    \ to ask?\"\"\"\nconversation = [ {'role': 'user', 'content': query} ] \nprompt\
    \ = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n\
    \ninputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device) \noutputs\
    \ = base_model.generate(**inputs, use_cache=True, max_length=4096)\noutput_text\
    \ = tokenizer.decode(outputs[0]) \noutput_text = output_text.split(\"\\n### Assistant:\\\
    n\", 1)[-1].replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\nprint(output_text)\n\
    ```"
  created_at: 2023-12-23 19:28:08+00:00
  edited: true
  hidden: false
  id: 658734c8d2ea3f32940b8bb0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0fa2148c818aa15435d95fa3c424c98.svg
      fullname: Angelos Papageorgiou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: frenzygr
      type: user
    createdAt: '2023-12-23T23:19:47.000Z'
    data:
      edited: false
      editors:
      - frenzygr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8707740902900696
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0fa2148c818aa15435d95fa3c424c98.svg
          fullname: Angelos Papageorgiou
          isHf: false
          isPro: false
          name: frenzygr
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>This model seems to be better when it comes to RAG, it hallucinates a
          lot less, this is the most useful model i''ve loaded on my 8gb vram laptop.
          You can really rely on it on common, easy language tasks just like chatgpt
          3.5. Of course, GPT-4 is better at everything but hey, this is a free, fast
          decent language model!</p>

          </blockquote>

          <p>hi mate kind of irrelevant question but how do you run the .safetensor
          files of this model? Do you convert it to .gguf using <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/discussions/2948">https://github.com/ggerganov/llama.cpp/discussions/2948</a>
          first or do you have some other method?</p>

          </blockquote>

          <p>Hi, i just used gguf version from "the bloke" on lmstudio. What really
          suprises me though is that i''m using a q5km quantized version and still
          manages to be good.</p>

          </blockquote>

          <p>thanks the reply mate, is it this one? <a href="https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF">https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF</a></p>

          '
        raw: "> > > This model seems to be better when it comes to RAG, it hallucinates\
          \ a lot less, this is the most useful model i've loaded on my 8gb vram laptop.\
          \ You can really rely on it on common, easy language tasks just like chatgpt\
          \ 3.5. Of course, GPT-4 is better at everything but hey, this is a free,\
          \ fast decent language model!\n> > \n> > hi mate kind of irrelevant question\
          \ but how do you run the .safetensor files of this model? Do you convert\
          \ it to .gguf using https://github.com/ggerganov/llama.cpp/discussions/2948\
          \ first or do you have some other method?\n> \n> Hi, i just used gguf version\
          \ from \"the bloke\" on lmstudio. What really suprises me though is that\
          \ i'm using a q5km quantized version and still manages to be good.\n\nthanks\
          \ the reply mate, is it this one? https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF"
        updatedAt: '2023-12-23T23:19:47.663Z'
      numEdits: 0
      reactions: []
    id: 65876b13bbb04840e36d3d6a
    type: comment
  author: frenzygr
  content: "> > > This model seems to be better when it comes to RAG, it hallucinates\
    \ a lot less, this is the most useful model i've loaded on my 8gb vram laptop.\
    \ You can really rely on it on common, easy language tasks just like chatgpt 3.5.\
    \ Of course, GPT-4 is better at everything but hey, this is a free, fast decent\
    \ language model!\n> > \n> > hi mate kind of irrelevant question but how do you\
    \ run the .safetensor files of this model? Do you convert it to .gguf using https://github.com/ggerganov/llama.cpp/discussions/2948\
    \ first or do you have some other method?\n> \n> Hi, i just used gguf version\
    \ from \"the bloke\" on lmstudio. What really suprises me though is that i'm using\
    \ a q5km quantized version and still manages to be good.\n\nthanks the reply mate,\
    \ is it this one? https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF"
  created_at: 2023-12-23 23:19:47+00:00
  edited: false
  hidden: false
  id: 65876b13bbb04840e36d3d6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a813dedbb9e28866a91b27/i2pGYzY1htiY-L3WFPSgl.jpeg?w=200&h=200&f=face
      fullname: appvoid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appvoid
      type: user
    createdAt: '2023-12-24T04:27:19.000Z'
    data:
      edited: false
      editors:
      - appvoid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8460577130317688
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a813dedbb9e28866a91b27/i2pGYzY1htiY-L3WFPSgl.jpeg?w=200&h=200&f=face
          fullname: appvoid
          isHf: false
          isPro: false
          name: appvoid
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <blockquote>

          <p>This model seems to be better when it comes to RAG, it hallucinates a
          lot less, this is the most useful model i''ve loaded on my 8gb vram laptop.
          You can really rely on it on common, easy language tasks just like chatgpt
          3.5. Of course, GPT-4 is better at everything but hey, this is a free, fast
          decent language model!</p>

          </blockquote>

          <p>hi mate kind of irrelevant question but how do you run the .safetensor
          files of this model? Do you convert it to .gguf using <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/discussions/2948">https://github.com/ggerganov/llama.cpp/discussions/2948</a>
          first or do you have some other method?</p>

          </blockquote>

          <p>Hi, i just used gguf version from "the bloke" on lmstudio. What really
          suprises me though is that i''m using a q5km quantized version and still
          manages to be good.</p>

          </blockquote>

          <p>thanks the reply mate, is it this one? <a href="https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF">https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF</a></p>

          </blockquote>

          <p>This one:<br><a href="https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF">https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF</a></p>

          '
        raw: "> > > > This model seems to be better when it comes to RAG, it hallucinates\
          \ a lot less, this is the most useful model i've loaded on my 8gb vram laptop.\
          \ You can really rely on it on common, easy language tasks just like chatgpt\
          \ 3.5. Of course, GPT-4 is better at everything but hey, this is a free,\
          \ fast decent language model!\n> > > \n> > > hi mate kind of irrelevant\
          \ question but how do you run the .safetensor files of this model? Do you\
          \ convert it to .gguf using https://github.com/ggerganov/llama.cpp/discussions/2948\
          \ first or do you have some other method?\n> > \n> > Hi, i just used gguf\
          \ version from \"the bloke\" on lmstudio. What really suprises me though\
          \ is that i'm using a q5km quantized version and still manages to be good.\n\
          > \n> thanks the reply mate, is it this one? https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF\n\
          \nThis one:\nhttps://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF"
        updatedAt: '2023-12-24T04:27:19.175Z'
      numEdits: 0
      reactions: []
    id: 6587b3277959448ef5f5f382
    type: comment
  author: appvoid
  content: "> > > > This model seems to be better when it comes to RAG, it hallucinates\
    \ a lot less, this is the most useful model i've loaded on my 8gb vram laptop.\
    \ You can really rely on it on common, easy language tasks just like chatgpt 3.5.\
    \ Of course, GPT-4 is better at everything but hey, this is a free, fast decent\
    \ language model!\n> > > \n> > > hi mate kind of irrelevant question but how do\
    \ you run the .safetensor files of this model? Do you convert it to .gguf using\
    \ https://github.com/ggerganov/llama.cpp/discussions/2948 first or do you have\
    \ some other method?\n> > \n> > Hi, i just used gguf version from \"the bloke\"\
    \ on lmstudio. What really suprises me though is that i'm using a q5km quantized\
    \ version and still manages to be good.\n> \n> thanks the reply mate, is it this\
    \ one? https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF\n\
    \nThis one:\nhttps://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF"
  created_at: 2023-12-24 04:27:19+00:00
  edited: false
  hidden: false
  id: 6587b3277959448ef5f5f382
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-27T20:16:54.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.88381427526474
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Mino24she&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Mino24she\">@<span class=\"\
          underline\">Mino24she</span></a></span>\n\n\t</span></span> I remembered\
          \ you thought Solar has a lot of promise and wanted to find a good Solar\
          \ fine-tune. Check out the uncensored version linked below. Even when it\
          \ comes to uncensored prompts it's more verbose and hallucinates less on\
          \ the fringes of knowledge.</p>\n<p><a href=\"https://huggingface.co/w4r10ck/SOLAR-10.7B-Instruct-v1.0-uncensored\"\
          >https://huggingface.co/w4r10ck/SOLAR-10.7B-Instruct-v1.0-uncensored</a></p>\n"
        raw: '@Mino24she I remembered you thought Solar has a lot of promise and wanted
          to find a good Solar fine-tune. Check out the uncensored version linked
          below. Even when it comes to uncensored prompts it''s more verbose and hallucinates
          less on the fringes of knowledge.


          https://huggingface.co/w4r10ck/SOLAR-10.7B-Instruct-v1.0-uncensored'
        updatedAt: '2023-12-27T20:16:54.166Z'
      numEdits: 0
      reactions: []
    id: 658c8636bc95ccc56250b402
    type: comment
  author: Phil337
  content: '@Mino24she I remembered you thought Solar has a lot of promise and wanted
    to find a good Solar fine-tune. Check out the uncensored version linked below.
    Even when it comes to uncensored prompts it''s more verbose and hallucinates less
    on the fringes of knowledge.


    https://huggingface.co/w4r10ck/SOLAR-10.7B-Instruct-v1.0-uncensored'
  created_at: 2023-12-27 20:16:54+00:00
  edited: false
  hidden: false
  id: 658c8636bc95ccc56250b402
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c58ab35bd375105c497e2818881c130.svg
      fullname: Ed Moman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirix
      type: user
    createdAt: '2023-12-29T12:45:49.000Z'
    data:
      edited: false
      editors:
      - mirix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9844470024108887
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c58ab35bd375105c497e2818881c130.svg
          fullname: Ed Moman
          isHf: false
          isPro: false
          name: mirix
          type: user
        html: '<p>I have tried this model with RAG and sometimes the answers are unrelated
          both to the question and to the context provided. I have switched to Tulu
          2 DPO, which provides less elegant answers, but I find it a lot more reliable
          for RAG. </p>

          <p>Here is the code (with chatbot interface) if anyone is interested:</p>

          <p><a rel="nofollow" href="https://github.com/mirix/retrieval-augmented-generation">https://github.com/mirix/retrieval-augmented-generation</a></p>

          <p>For instance, I had it read The Little Prince and asked a question that
          was within the context and the answer was about The Shawshank Redemption
          and had nothing whatsoever to do with the question. </p>

          <p>I have tried several models with the same script and none of them hallucinated
          like that. </p>

          <p>I thought it could be the template. but SOLAR seem to get most answers
          right.</p>

          <p>Any ideas?</p>

          '
        raw: "I have tried this model with RAG and sometimes the answers are unrelated\
          \ both to the question and to the context provided. I have switched to Tulu\
          \ 2 DPO, which provides less elegant answers, but I find it a lot more reliable\
          \ for RAG. \n\nHere is the code (with chatbot interface) if anyone is interested:\n\
          \nhttps://github.com/mirix/retrieval-augmented-generation\n\nFor instance,\
          \ I had it read The Little Prince and asked a question that was within the\
          \ context and the answer was about The Shawshank Redemption and had nothing\
          \ whatsoever to do with the question. \n\nI have tried several models with\
          \ the same script and none of them hallucinated like that. \n\nI thought\
          \ it could be the template. but SOLAR seem to get most answers right.\n\n\
          Any ideas?\n"
        updatedAt: '2023-12-29T12:45:49.512Z'
      numEdits: 0
      reactions: []
    id: 658ebf7d35c41262d6396d0a
    type: comment
  author: mirix
  content: "I have tried this model with RAG and sometimes the answers are unrelated\
    \ both to the question and to the context provided. I have switched to Tulu 2\
    \ DPO, which provides less elegant answers, but I find it a lot more reliable\
    \ for RAG. \n\nHere is the code (with chatbot interface) if anyone is interested:\n\
    \nhttps://github.com/mirix/retrieval-augmented-generation\n\nFor instance, I had\
    \ it read The Little Prince and asked a question that was within the context and\
    \ the answer was about The Shawshank Redemption and had nothing whatsoever to\
    \ do with the question. \n\nI have tried several models with the same script and\
    \ none of them hallucinated like that. \n\nI thought it could be the template.\
    \ but SOLAR seem to get most answers right.\n\nAny ideas?\n"
  created_at: 2023-12-29 12:45:49+00:00
  edited: false
  hidden: false
  id: 658ebf7d35c41262d6396d0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c58ab35bd375105c497e2818881c130.svg
      fullname: Ed Moman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirix
      type: user
    createdAt: '2023-12-29T13:29:41.000Z'
    data:
      edited: true
      editors:
      - mirix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8625795841217041
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c58ab35bd375105c497e2818881c130.svg
          fullname: Ed Moman
          isHf: false
          isPro: false
          name: mirix
          type: user
        html: '<p>Please, disregard my previous comment. The issue seems indeed to
          be related to the prompt template and it is solved by using the following
          wrapper:</p>

          <pre><code>query_wrapper_prompt = PromptTemplate(

          "### System:"

          "Please, check if the anwser can be inferred from the pieces of context
          provided. If the answer cannot be inferred from the context, just state
          that the question is out of scope and do not provide any answer.\n"

          "### User:"

          "{query_str}\n"

          "### Assistant:\n"

          )

          </code></pre>

          '
        raw: 'Please, disregard my previous comment. The issue seems indeed to be
          related to the prompt template and it is solved by using the following wrapper:


          ```

          query_wrapper_prompt = PromptTemplate(

          "### System:"

          "Please, check if the anwser can be inferred from the pieces of context
          provided. If the answer cannot be inferred from the context, just state
          that the question is out of scope and do not provide any answer.\n"

          "### User:"

          "{query_str}\n"

          "### Assistant:\n"

          )

          ```'
        updatedAt: '2023-12-29T13:29:55.235Z'
      numEdits: 1
      reactions: []
    id: 658ec9c57fe0235473ef38fd
    type: comment
  author: mirix
  content: 'Please, disregard my previous comment. The issue seems indeed to be related
    to the prompt template and it is solved by using the following wrapper:


    ```

    query_wrapper_prompt = PromptTemplate(

    "### System:"

    "Please, check if the anwser can be inferred from the pieces of context provided.
    If the answer cannot be inferred from the context, just state that the question
    is out of scope and do not provide any answer.\n"

    "### User:"

    "{query_str}\n"

    "### Assistant:\n"

    )

    ```'
  created_at: 2023-12-29 13:29:41+00:00
  edited: true
  hidden: false
  id: 658ec9c57fe0235473ef38fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a41a1074f417046295aea9b658b83d78.svg
      fullname: Elliott Dyson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ElliottDyson
      type: user
    createdAt: '2023-12-29T14:28:58.000Z'
    data:
      edited: false
      editors:
      - ElliottDyson
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9895440936088562
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a41a1074f417046295aea9b658b83d78.svg
          fullname: Elliott Dyson
          isHf: false
          isPro: false
          name: ElliottDyson
          type: user
        html: '<p>It would be great to see a long-context version of this model, it
          may also help with the multi-turn conversations that many have mentioned
          it seems to struggle with.</p>

          '
        raw: It would be great to see a long-context version of this model, it may
          also help with the multi-turn conversations that many have mentioned it
          seems to struggle with.
        updatedAt: '2023-12-29T14:28:58.690Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - agershun
        - Shatters
    id: 658ed7aa43edad2169e9f348
    type: comment
  author: ElliottDyson
  content: It would be great to see a long-context version of this model, it may also
    help with the multi-turn conversations that many have mentioned it seems to struggle
    with.
  created_at: 2023-12-29 14:28:58+00:00
  edited: false
  hidden: false
  id: 658ed7aa43edad2169e9f348
  type: comment
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/603c29094a944b99e81476fd/LaSNcrKmCEUBEBZZCce3k.png?w=200&h=200&f=face
      fullname: Sung Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hunkim
      type: user
    createdAt: '2023-12-30T04:09:20.000Z'
    data:
      pinned: true
    id: 658f97f0a6567cb93c46305b
    type: pinning-change
  author: hunkim
  created_at: 2023-12-30 04:09:20+00:00
  id: 658f97f0a6567cb93c46305b
  type: pinning-change
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: upstage/SOLAR-10.7B-Instruct-v1.0
repo_type: model
status: open
target_branch: null
title: I feel that this 11B model is smarter and hallucinates less than other leaders
