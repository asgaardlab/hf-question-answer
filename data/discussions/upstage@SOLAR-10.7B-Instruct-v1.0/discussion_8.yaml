!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andreaKIM
conflicting_files: null
created_at: 2023-12-15 09:36:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1f763ca61fb1f281f7ac24bdee8722f.svg
      fullname: DAEHEEKIM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreaKIM
      type: user
    createdAt: '2023-12-15T09:36:03.000Z'
    data:
      edited: false
      editors:
      - andreaKIM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9550369381904602
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1f763ca61fb1f281f7ac24bdee8722f.svg
          fullname: DAEHEEKIM
          isHf: false
          isPro: false
          name: andreaKIM
          type: user
        html: '<p>Hello, Thanks for giving such a great model with instruction tuning.<br>This
          model showed highest benchmark in the open llm leaderboard however when
          i try fine tune with SFTtrainer, this models shows lower performance with
          comparison to just llama2-13b-chat model.<br>I did even try to tune hyperparameters
          for optimization several times, it didn''t work.<br>Is there any specific
          reason for this reason?</p>

          '
        raw: "Hello, Thanks for giving such a great model with instruction tuning.\r\
          \nThis model showed highest benchmark in the open llm leaderboard however\
          \ when i try fine tune with SFTtrainer, this models shows lower performance\
          \ with comparison to just llama2-13b-chat model.\r\nI did even try to tune\
          \ hyperparameters for optimization several times, it didn't work.\r\nIs\
          \ there any specific reason for this reason?"
        updatedAt: '2023-12-15T09:36:03.835Z'
      numEdits: 0
      reactions: []
    id: 657c1e03688f1a0f7ecf9072
    type: comment
  author: andreaKIM
  content: "Hello, Thanks for giving such a great model with instruction tuning.\r\
    \nThis model showed highest benchmark in the open llm leaderboard however when\
    \ i try fine tune with SFTtrainer, this models shows lower performance with comparison\
    \ to just llama2-13b-chat model.\r\nI did even try to tune hyperparameters for\
    \ optimization several times, it didn't work.\r\nIs there any specific reason\
    \ for this reason?"
  created_at: 2023-12-15 09:36:03+00:00
  edited: false
  hidden: false
  id: 657c1e03688f1a0f7ecf9072
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64823fc42eeb6920563c7b07/7o2pArIZkW8Gu0rF2X96F.jpeg?w=200&h=200&f=face
      fullname: Sanghoon Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Limerobot
      type: user
    createdAt: '2023-12-15T12:45:32.000Z'
    data:
      edited: true
      editors:
      - Limerobot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7734091877937317
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64823fc42eeb6920563c7b07/7o2pArIZkW8Gu0rF2X96F.jpeg?w=200&h=200&f=face
          fullname: Sanghoon Kim
          isHf: false
          isPro: false
          name: Limerobot
          type: user
        html: '<p>Hello,</p>

          <p>Are you using this model (upstage/SOLAR-10.7B-Instruct-v1.0) for finetuning?
          If so, you might not achieve the highest score.<br>You should instead use
          our pretrained model (upstage/SOLAR-10.7B-v1.0) for finetuning.</p>

          <p>(If you have already been using our pretrained model for your finetuning
          process, please disregard the above.)</p>

          <p>I recommend the following settings:</p>

          <ul>

          <li>learning rate: 1e-06 ~ 2e-06</li>

          <li>at least 5 epochs</li>

          </ul>

          '
        raw: 'Hello,


          Are you using this model (upstage/SOLAR-10.7B-Instruct-v1.0) for finetuning?
          If so, you might not achieve the highest score.

          You should instead use our pretrained model (upstage/SOLAR-10.7B-v1.0) for
          finetuning.


          (If you have already been using our pretrained model for your finetuning
          process, please disregard the above.)


          I recommend the following settings:

          - learning rate: 1e-06 ~ 2e-06

          - at least 5 epochs'
        updatedAt: '2023-12-15T12:49:47.461Z'
      numEdits: 1
      reactions: []
      relatedEventId: 657c4a6ce5e3a1934a41fd73
    id: 657c4a6ce5e3a1934a41fd72
    type: comment
  author: Limerobot
  content: 'Hello,


    Are you using this model (upstage/SOLAR-10.7B-Instruct-v1.0) for finetuning? If
    so, you might not achieve the highest score.

    You should instead use our pretrained model (upstage/SOLAR-10.7B-v1.0) for finetuning.


    (If you have already been using our pretrained model for your finetuning process,
    please disregard the above.)


    I recommend the following settings:

    - learning rate: 1e-06 ~ 2e-06

    - at least 5 epochs'
  created_at: 2023-12-15 12:45:32+00:00
  edited: true
  hidden: false
  id: 657c4a6ce5e3a1934a41fd72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64823fc42eeb6920563c7b07/7o2pArIZkW8Gu0rF2X96F.jpeg?w=200&h=200&f=face
      fullname: Sanghoon Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Limerobot
      type: user
    createdAt: '2023-12-15T12:45:32.000Z'
    data:
      status: closed
    id: 657c4a6ce5e3a1934a41fd73
    type: status-change
  author: Limerobot
  created_at: 2023-12-15 12:45:32+00:00
  id: 657c4a6ce5e3a1934a41fd73
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1f763ca61fb1f281f7ac24bdee8722f.svg
      fullname: DAEHEEKIM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreaKIM
      type: user
    createdAt: '2023-12-15T13:20:03.000Z'
    data:
      edited: false
      editors:
      - andreaKIM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.915384829044342
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1f763ca61fb1f281f7ac24bdee8722f.svg
          fullname: DAEHEEKIM
          isHf: false
          isPro: false
          name: andreaKIM
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Limerobot&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Limerobot\">@<span class=\"\
          underline\">Limerobot</span></a></span>\n\n\t</span></span> Thanks for recipes\
          \ for fine tuning<br>Is there any reason fine tuned model get poor performance\
          \ when they fine tuned once again?<br>I am just curious if there exist some\
          \ specific reason for this situation.</p>\n"
        raw: '@Limerobot Thanks for recipes for fine tuning

          Is there any reason fine tuned model get poor performance when they fine
          tuned once again?

          I am just curious if there exist some specific reason for this situation.'
        updatedAt: '2023-12-15T13:20:03.952Z'
      numEdits: 0
      reactions: []
    id: 657c528375b953854f27e433
    type: comment
  author: andreaKIM
  content: '@Limerobot Thanks for recipes for fine tuning

    Is there any reason fine tuned model get poor performance when they fine tuned
    once again?

    I am just curious if there exist some specific reason for this situation.'
  created_at: 2023-12-15 13:20:03+00:00
  edited: false
  hidden: false
  id: 657c528375b953854f27e433
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: upstage/SOLAR-10.7B-Instruct-v1.0
repo_type: model
status: closed
target_branch: null
title: This model ranked 1st place in open llm leader board, However this model has
  lower performance in supervised fine tuning.
