!!python/object:huggingface_hub.community.DiscussionWithDetails
author: thlw
conflicting_files: null
created_at: 2023-08-19 07:30:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1e0a30cc73cd6860f9f3b2c42180ebd.svg
      fullname: Jiang Zixu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thlw
      type: user
    createdAt: '2023-08-19T08:30:14.000Z'
    data:
      edited: false
      editors:
      - thlw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2932916283607483
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1e0a30cc73cd6860f9f3b2c42180ebd.svg
          fullname: Jiang Zixu
          isHf: false
          isPro: false
          name: thlw
          type: user
        html: "<p>from peft import PeftModel,PeftConfig<br>finetune_model_path=\"\
          /data/work/Llama2-Chinese/Llama2-Chinese-7b-Chat-LoRA\"<br>config = PeftConfig.from_pretrained(finetune_model_path)<br>tokenizer\
          \ = AutoTokenizer.from_pretrained(config.base_model_name_or_path,use_fast=False)<br>tokenizer.pad_token\
          \ = tokenizer.eos_token<br>model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map='auto',torch_dtype=torch.float16,load_in_8bit=True)<br>model\
          \ = PeftModel.from_pretrained(model, finetune_model_path, device_map={\"\
          \": 0})<br>model =model.eval()<br>input_ids = tokenizer(['<s>Human: \u4ECB\
          \u7ECD\u4E00\u4E0B\u5317\u4EAC\\n</s><s>Assistant: '], return_tensors=\"\
          pt\",add_special_tokens=False).input_ids.to('cuda')<br>generate_input =\
          \ {<br>    \"input_ids\":input_ids,<br>    \"max_new_tokens\":512,<br> \
          \   \"do_sample\":True,<br>    \"top_k\":50,<br>    \"top_p\":0.95,<br>\
          \    \"temperature\":0.3,<br>    \"repetition_penalty\":1.3,<br>    \"eos_token_id\"\
          :tokenizer.eos_token_id,<br>    \"bos_token_id\":tokenizer.bos_token_id,<br>\
          \    \"pad_token_id\":tokenizer.pad_token_id<br>}<br>generate_ids  = model.generate(**generate_input)<br>text\
          \ = tokenizer.decode(generate_ids[0])<br>print(text)</s></p><s>\n<p>i just\
          \ load the lora as description , but </p>\n<p><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/64bd0c779f94ea2554f4c0e7/IaWE87H9VgomeYtbuqMad.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64bd0c779f94ea2554f4c0e7/IaWE87H9VgomeYtbuqMad.png\"\
          ></a><br>i wander whether my llama2 is expired or other questions</p>\n\
          </s>"
        raw: "from peft import PeftModel,PeftConfig\r\nfinetune_model_path=\"/data/work/Llama2-Chinese/Llama2-Chinese-7b-Chat-LoRA\"\
          \r\nconfig = PeftConfig.from_pretrained(finetune_model_path)\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(config.base_model_name_or_path,use_fast=False)\r\
          \ntokenizer.pad_token = tokenizer.eos_token\r\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map='auto',torch_dtype=torch.float16,load_in_8bit=True)\r\
          \nmodel = PeftModel.from_pretrained(model, finetune_model_path, device_map={\"\
          \": 0})\r\nmodel =model.eval()\r\ninput_ids = tokenizer(['<s>Human: \u4ECB\
          \u7ECD\u4E00\u4E0B\u5317\u4EAC\\n</s><s>Assistant: '], return_tensors=\"\
          pt\",add_special_tokens=False).input_ids.to('cuda')        \r\ngenerate_input\
          \ = {\r\n    \"input_ids\":input_ids,\r\n    \"max_new_tokens\":512,\r\n\
          \    \"do_sample\":True,\r\n    \"top_k\":50,\r\n    \"top_p\":0.95,\r\n\
          \    \"temperature\":0.3,\r\n    \"repetition_penalty\":1.3,\r\n    \"eos_token_id\"\
          :tokenizer.eos_token_id,\r\n    \"bos_token_id\":tokenizer.bos_token_id,\r\
          \n    \"pad_token_id\":tokenizer.pad_token_id\r\n}\r\ngenerate_ids  = model.generate(**generate_input)\r\
          \ntext = tokenizer.decode(generate_ids[0])\r\nprint(text)\r\n\r\n\r\n\r\n\
          i just load the lora as description , but \r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64bd0c779f94ea2554f4c0e7/IaWE87H9VgomeYtbuqMad.png)\r\
          \ni wander whether my llama2 is expired or other questions"
        updatedAt: '2023-08-19T08:30:14.212Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - thlw
    id: 64e07d9618af51be8e039276
    type: comment
  author: thlw
  content: "from peft import PeftModel,PeftConfig\r\nfinetune_model_path=\"/data/work/Llama2-Chinese/Llama2-Chinese-7b-Chat-LoRA\"\
    \r\nconfig = PeftConfig.from_pretrained(finetune_model_path)\r\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,use_fast=False)\r\
    \ntokenizer.pad_token = tokenizer.eos_token\r\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map='auto',torch_dtype=torch.float16,load_in_8bit=True)\r\
    \nmodel = PeftModel.from_pretrained(model, finetune_model_path, device_map={\"\
    \": 0})\r\nmodel =model.eval()\r\ninput_ids = tokenizer(['<s>Human: \u4ECB\u7ECD\
    \u4E00\u4E0B\u5317\u4EAC\\n</s><s>Assistant: '], return_tensors=\"pt\",add_special_tokens=False).input_ids.to('cuda')\
    \        \r\ngenerate_input = {\r\n    \"input_ids\":input_ids,\r\n    \"max_new_tokens\"\
    :512,\r\n    \"do_sample\":True,\r\n    \"top_k\":50,\r\n    \"top_p\":0.95,\r\
    \n    \"temperature\":0.3,\r\n    \"repetition_penalty\":1.3,\r\n    \"eos_token_id\"\
    :tokenizer.eos_token_id,\r\n    \"bos_token_id\":tokenizer.bos_token_id,\r\n \
    \   \"pad_token_id\":tokenizer.pad_token_id\r\n}\r\ngenerate_ids  = model.generate(**generate_input)\r\
    \ntext = tokenizer.decode(generate_ids[0])\r\nprint(text)\r\n\r\n\r\n\r\ni just\
    \ load the lora as description , but \r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64bd0c779f94ea2554f4c0e7/IaWE87H9VgomeYtbuqMad.png)\r\
    \ni wander whether my llama2 is expired or other questions"
  created_at: 2023-08-19 07:30:14+00:00
  edited: false
  hidden: false
  id: 64e07d9618af51be8e039276
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61cc7b5c43b3d9d55e49b7e8/iExm6DRBmwOKUOuld2wuf.jpeg?w=200&h=200&f=face
      fullname: zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: EricZHangZHeng
      type: user
    createdAt: '2023-08-29T07:06:41.000Z'
    data:
      edited: false
      editors:
      - EricZHangZHeng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.363469660282135
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61cc7b5c43b3d9d55e49b7e8/iExm6DRBmwOKUOuld2wuf.jpeg?w=200&h=200&f=face
          fullname: zhang
          isHf: false
          isPro: false
          name: EricZHangZHeng
          type: user
        html: '<p>use peft to load</p>

          '
        raw: use peft to load
        updatedAt: '2023-08-29T07:06:41.592Z'
      numEdits: 0
      reactions: []
    id: 64ed990194cee41a759c17bb
    type: comment
  author: EricZHangZHeng
  content: use peft to load
  created_at: 2023-08-29 06:06:41+00:00
  edited: false
  hidden: false
  id: 64ed990194cee41a759c17bb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: FlagAlpha/Llama2-Chinese-7b-Chat-LoRA
repo_type: model
status: open
target_branch: null
title: hello,i am honored to use your model but i met some problem when i load lora
