!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rdo4920
conflicting_files: null
created_at: 2023-04-28 07:42:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76a7ae30865322359236bd89b0f74dcc.svg
      fullname: RDO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rdo4920
      type: user
    createdAt: '2023-04-28T08:42:17.000Z'
    data:
      edited: false
      editors:
      - rdo4920
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76a7ae30865322359236bd89b0f74dcc.svg
          fullname: RDO
          isHf: false
          isPro: false
          name: rdo4920
          type: user
        html: "<p>Thank you for providing this model for low-GPU-memory users.</p>\n\
          <p>There is a potential for improvement, as I encountered several issues\
          \ while setting up the environment on a Windows 10 machine. However, this\
          \ model can be used in a normal Win10 environment without requiring a gcc\
          \ compiler or WSL support. You just need to avoid the CPU kernel loading\
          \ process.</p>\n<p>To achieve this, modify the 'load_cpu_kernel' method\
          \ in the 'quantization.py' file located in the model folder 'chatglm-6b-int4'.\
          \ Ensure that the actual loading is not triggered if the GPU device is available\
          \ by changing it to the following code:</p>\n<pre><code>def load_cpu_kernel(**kwargs):\n\
          \    if not torch.cuda.is_available(): # check before load CPU kernel\n\
          \        global cpu_kernels\n        cpu_kernels = CPUKernel(**kwargs)\n\
          \        assert cpu_kernels.load\n</code></pre>\n<p>If the user does not\
          \ have a GPU, the normal CPU kernel loading process will be triggered, which\
          \ requires a 'gcc' and 'WSL' environment on a Windows machine. </p>\n<p>After\
          \ making the above modification, users can easily load the model from any\
          \ front-end code without worrying about the 'assert cpu_kernels.load' error.\
          \ For example, in the chatglm-webui project, simply download the model folder\
          \ and name it 'chatglm-6b-int4'. Use the following command to load it on\
          \ a Windows 10 machine (assuming that the cuda and required python libraries\
          \ are already installed):</p>\n<pre><code>python webui.py --model-path chatglm-6b-int4\
          \ --precision int4\n</code></pre>\n<p>I successfully loaded the model on\
          \ a Win10 + 2080 (8GB) machine without gcc or WSL installed.<br>Thanks again\
          \ for this awesome model!</p>\n"
        raw: "Thank you for providing this model for low-GPU-memory users.\r\n\r\n\
          There is a potential for improvement, as I encountered several issues while\
          \ setting up the environment on a Windows 10 machine. However, this model\
          \ can be used in a normal Win10 environment without requiring a gcc compiler\
          \ or WSL support. You just need to avoid the CPU kernel loading process.\r\
          \n\r\nTo achieve this, modify the 'load_cpu_kernel' method in the 'quantization.py'\
          \ file located in the model folder 'chatglm-6b-int4'. Ensure that the actual\
          \ loading is not triggered if the GPU device is available by changing it\
          \ to the following code:\r\n\r\n```\r\ndef load_cpu_kernel(**kwargs):\r\n\
          \    if not torch.cuda.is_available(): # check before load CPU kernel\r\n\
          \        global cpu_kernels\r\n        cpu_kernels = CPUKernel(**kwargs)\r\
          \n        assert cpu_kernels.load\r\n```\r\nIf the user does not have a\
          \ GPU, the normal CPU kernel loading process will be triggered, which requires\
          \ a 'gcc' and 'WSL' environment on a Windows machine. \r\n\r\nAfter making\
          \ the above modification, users can easily load the model from any front-end\
          \ code without worrying about the 'assert cpu_kernels.load' error. For example,\
          \ in the chatglm-webui project, simply download the model folder and name\
          \ it 'chatglm-6b-int4'. Use the following command to load it on a Windows\
          \ 10 machine (assuming that the cuda and required python libraries are already\
          \ installed):\r\n\r\n```\r\npython webui.py --model-path chatglm-6b-int4\
          \ --precision int4\r\n```\r\n\r\nI successfully loaded the model on a Win10\
          \ + 2080 (8GB) machine without gcc or WSL installed.\r\nThanks again for\
          \ this awesome model!"
        updatedAt: '2023-04-28T08:42:17.210Z'
      numEdits: 0
      reactions: []
    id: 644b86e96ebb3ebf72642de0
    type: comment
  author: rdo4920
  content: "Thank you for providing this model for low-GPU-memory users.\r\n\r\nThere\
    \ is a potential for improvement, as I encountered several issues while setting\
    \ up the environment on a Windows 10 machine. However, this model can be used\
    \ in a normal Win10 environment without requiring a gcc compiler or WSL support.\
    \ You just need to avoid the CPU kernel loading process.\r\n\r\nTo achieve this,\
    \ modify the 'load_cpu_kernel' method in the 'quantization.py' file located in\
    \ the model folder 'chatglm-6b-int4'. Ensure that the actual loading is not triggered\
    \ if the GPU device is available by changing it to the following code:\r\n\r\n\
    ```\r\ndef load_cpu_kernel(**kwargs):\r\n    if not torch.cuda.is_available():\
    \ # check before load CPU kernel\r\n        global cpu_kernels\r\n        cpu_kernels\
    \ = CPUKernel(**kwargs)\r\n        assert cpu_kernels.load\r\n```\r\nIf the user\
    \ does not have a GPU, the normal CPU kernel loading process will be triggered,\
    \ which requires a 'gcc' and 'WSL' environment on a Windows machine. \r\n\r\n\
    After making the above modification, users can easily load the model from any\
    \ front-end code without worrying about the 'assert cpu_kernels.load' error. For\
    \ example, in the chatglm-webui project, simply download the model folder and\
    \ name it 'chatglm-6b-int4'. Use the following command to load it on a Windows\
    \ 10 machine (assuming that the cuda and required python libraries are already\
    \ installed):\r\n\r\n```\r\npython webui.py --model-path chatglm-6b-int4 --precision\
    \ int4\r\n```\r\n\r\nI successfully loaded the model on a Win10 + 2080 (8GB) machine\
    \ without gcc or WSL installed.\r\nThanks again for this awesome model!"
  created_at: 2023-04-28 07:42:17+00:00
  edited: false
  hidden: false
  id: 644b86e96ebb3ebf72642de0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg?w=200&h=200&f=face
      fullname: Zhengxiao Du
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zxdu20
      type: user
    createdAt: '2023-04-28T11:46:39.000Z'
    data:
      edited: false
      editors:
      - zxdu20
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg?w=200&h=200&f=face
          fullname: Zhengxiao Du
          isHf: false
          isPro: false
          name: zxdu20
          type: user
        html: '<p>It is possible that someone has a GPU but he/she wants to use CPU
          inference (for example the GPU memory is not enough to load the model).<br>Currently,
          if the <code>load_cpu_kernel</code> method fails, the exception is caught
          and the program only prints a warning. The program fails only if  both the
          CPU kernel loading and GPU kernel loading fail. Therefore I don''t think
          it is necessary to avoid the CPU kernel loading process according to CUDA
          availability.</p>

          '
        raw: 'It is possible that someone has a GPU but he/she wants to use CPU inference
          (for example the GPU memory is not enough to load the model).

          Currently, if the `load_cpu_kernel` method fails, the exception is caught
          and the program only prints a warning. The program fails only if  both the
          CPU kernel loading and GPU kernel loading fail. Therefore I don''t think
          it is necessary to avoid the CPU kernel loading process according to CUDA
          availability.'
        updatedAt: '2023-04-28T11:46:39.324Z'
      numEdits: 0
      reactions: []
      relatedEventId: 644bb21fb5da3e194a7006ab
    id: 644bb21fb5da3e194a7006aa
    type: comment
  author: zxdu20
  content: 'It is possible that someone has a GPU but he/she wants to use CPU inference
    (for example the GPU memory is not enough to load the model).

    Currently, if the `load_cpu_kernel` method fails, the exception is caught and
    the program only prints a warning. The program fails only if  both the CPU kernel
    loading and GPU kernel loading fail. Therefore I don''t think it is necessary
    to avoid the CPU kernel loading process according to CUDA availability.'
  created_at: 2023-04-28 10:46:39+00:00
  edited: false
  hidden: false
  id: 644bb21fb5da3e194a7006aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg?w=200&h=200&f=face
      fullname: Zhengxiao Du
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zxdu20
      type: user
    createdAt: '2023-04-28T11:46:39.000Z'
    data:
      status: closed
    id: 644bb21fb5da3e194a7006ab
    type: status-change
  author: zxdu20
  created_at: 2023-04-28 10:46:39+00:00
  id: 644bb21fb5da3e194a7006ab
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76a7ae30865322359236bd89b0f74dcc.svg
      fullname: RDO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rdo4920
      type: user
    createdAt: '2023-04-28T13:30:17.000Z'
    data:
      edited: false
      editors:
      - rdo4920
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76a7ae30865322359236bd89b0f74dcc.svg
          fullname: RDO
          isHf: false
          isPro: false
          name: rdo4920
          type: user
        html: '<p>Not all load_cpu_kernel calls are wrapped in try/catch section.</p>

          <p>For example, in modeling_chatglm.py on line 1430, the loading is triggered,
          which will cause an AssertionError: ''assert cpu_kernels.load''.<br>An issue
          has already been reported about the same error: <a rel="nofollow" href="https://github.com/THUDM/ChatGLM-6B/issues/676">https://github.com/THUDM/ChatGLM-6B/issues/676</a></p>

          <p>A possible fix could be placing the try/catch section within the ''load_cpu_kernel''
          function.</p>

          <p>It''s not a major issue but quite confusing, as the error appears to
          be related to the CPU, yet users might not want to use the CPU at all.<br>Just
          my two cents.</p>

          '
        raw: 'Not all load_cpu_kernel calls are wrapped in try/catch section.


          For example, in modeling_chatglm.py on line 1430, the loading is triggered,
          which will cause an AssertionError: ''assert cpu_kernels.load''.

          An issue has already been reported about the same error: https://github.com/THUDM/ChatGLM-6B/issues/676


          A possible fix could be placing the try/catch section within the ''load_cpu_kernel''
          function.


          It''s not a major issue but quite confusing, as the error appears to be
          related to the CPU, yet users might not want to use the CPU at all.

          Just my two cents.'
        updatedAt: '2023-04-28T13:30:17.805Z'
      numEdits: 0
      reactions: []
      relatedEventId: 644bca69cb0886c51c242b3c
    id: 644bca69cb0886c51c242b3b
    type: comment
  author: rdo4920
  content: 'Not all load_cpu_kernel calls are wrapped in try/catch section.


    For example, in modeling_chatglm.py on line 1430, the loading is triggered, which
    will cause an AssertionError: ''assert cpu_kernels.load''.

    An issue has already been reported about the same error: https://github.com/THUDM/ChatGLM-6B/issues/676


    A possible fix could be placing the try/catch section within the ''load_cpu_kernel''
    function.


    It''s not a major issue but quite confusing, as the error appears to be related
    to the CPU, yet users might not want to use the CPU at all.

    Just my two cents.'
  created_at: 2023-04-28 12:30:17+00:00
  edited: false
  hidden: false
  id: 644bca69cb0886c51c242b3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/76a7ae30865322359236bd89b0f74dcc.svg
      fullname: RDO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rdo4920
      type: user
    createdAt: '2023-04-28T13:30:17.000Z'
    data:
      status: open
    id: 644bca69cb0886c51c242b3c
    type: status-change
  author: rdo4920
  created_at: 2023-04-28 12:30:17+00:00
  id: 644bca69cb0886c51c242b3c
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg?w=200&h=200&f=face
      fullname: Zhengxiao Du
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zxdu20
      type: user
    createdAt: '2023-04-29T02:36:49.000Z'
    data:
      edited: false
      editors:
      - zxdu20
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg?w=200&h=200&f=face
          fullname: Zhengxiao Du
          isHf: false
          isPro: false
          name: zxdu20
          type: user
        html: '<blockquote>

          <p>Not all load_cpu_kernel calls are wrapped in try/catch section.</p>

          <p>For example, in modeling_chatglm.py on line 1430, the loading is triggered,
          which will cause an AssertionError: ''assert cpu_kernels.load''.<br>An issue
          has already been reported about the same error: <a rel="nofollow" href="https://github.com/THUDM/ChatGLM-6B/issues/676">https://github.com/THUDM/ChatGLM-6B/issues/676</a></p>

          <p>A possible fix could be placing the try/catch section within the ''load_cpu_kernel''
          function.</p>

          <p>It''s not a major issue but quite confusing, as the error appears to
          be related to the CPU, yet users might not want to use the CPU at all.<br>Just
          my two cents.</p>

          </blockquote>

          <p>Thank you for your advice. Removed the assert in load_cpu_kernel</p>

          '
        raw: "> Not all load_cpu_kernel calls are wrapped in try/catch section.\n\
          > \n> For example, in modeling_chatglm.py on line 1430, the loading is triggered,\
          \ which will cause an AssertionError: 'assert cpu_kernels.load'.\n> An issue\
          \ has already been reported about the same error: https://github.com/THUDM/ChatGLM-6B/issues/676\n\
          > \n> A possible fix could be placing the try/catch section within the 'load_cpu_kernel'\
          \ function.\n> \n> It's not a major issue but quite confusing, as the error\
          \ appears to be related to the CPU, yet users might not want to use the\
          \ CPU at all.\n> Just my two cents.\n\nThank you for your advice. Removed\
          \ the assert in load_cpu_kernel"
        updatedAt: '2023-04-29T02:36:49.806Z'
      numEdits: 0
      reactions: []
    id: 644c82c1ed08a4fdf4ecbd79
    type: comment
  author: zxdu20
  content: "> Not all load_cpu_kernel calls are wrapped in try/catch section.\n> \n\
    > For example, in modeling_chatglm.py on line 1430, the loading is triggered,\
    \ which will cause an AssertionError: 'assert cpu_kernels.load'.\n> An issue has\
    \ already been reported about the same error: https://github.com/THUDM/ChatGLM-6B/issues/676\n\
    > \n> A possible fix could be placing the try/catch section within the 'load_cpu_kernel'\
    \ function.\n> \n> It's not a major issue but quite confusing, as the error appears\
    \ to be related to the CPU, yet users might not want to use the CPU at all.\n\
    > Just my two cents.\n\nThank you for your advice. Removed the assert in load_cpu_kernel"
  created_at: 2023-04-29 01:36:49+00:00
  edited: false
  hidden: false
  id: 644c82c1ed08a4fdf4ecbd79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/76a7ae30865322359236bd89b0f74dcc.svg
      fullname: RDO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rdo4920
      type: user
    createdAt: '2023-04-30T12:08:07.000Z'
    data:
      status: closed
    id: 644e5a27a00f4b11d38f7307
    type: status-change
  author: rdo4920
  created_at: 2023-04-30 11:08:07+00:00
  id: 644e5a27a00f4b11d38f7307
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: THUDM/chatglm-6b-int4
repo_type: model
status: closed
target_branch: null
title: Avoid Loading CPU kernel if User Have a GPU and Cuda Environment
