!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dzupin
conflicting_files: null
created_at: 2023-07-31 14:57:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
      fullname: Robert Dzupin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dzupin
      type: user
    createdAt: '2023-07-31T15:57:22.000Z'
    data:
      edited: true
      editors:
      - dzupin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4905204474925995
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
          fullname: Robert Dzupin
          isHf: false
          isPro: false
          name: dzupin
          type: user
        html: '<p>Anybody else experiencing GGML_ASSERT error when trying to run this
          model with cblast llama.cpp ? </p>

          <p>I am using latest version of Cblast  llama.cpp (from 31/July2023) but
          experiencing  the same issue with several older versions as well<br>I am
          using Model card example of prompt. </p>

          <p>Model seems to be loading fine, but then for response I only get 1-liner:    "GGML_ASSERT:
          D:\a\llama.cpp\llama.cpp\ggml.c:10463: ne02 == ne12"</p>

          <p>Any suggestion on how to fix it, would be appreciated. </p>

          <p>Thanks  </p>

          <p>OUTPUT I am getting is as follows: </p>

          <p>main.exe  -t 16 -ngl 21 -gqa 8 -m C:\Users\robo\Downloads\upstage-llama-2-70b-instruct-v2.ggmlv3.q4_K_M.bin
          --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "### System:\nThis
          is a system prompt, please behave and help the user.\n\n### User:\nWrite
          a story about llamas\n\n### Assistant:"<br>main: warning: base model only
          supports context sizes no greater than 2048 tokens (4096 specified)<br>main:
          build = 930 (9d2382b)<br>main: seed  = 1690818332<br>ggml_opencl: selecting
          platform: ''NVIDIA CUDA''<br>ggml_opencl: selecting device: ''NVIDIA GeForce
          RTX 4090 Laptop GPU''<br>ggml_opencl: device FP16 support: false<br>llama.cpp:
          loading model from C:\Users\robo\Downloads\upstage-llama-2-70b-instruct-v2.ggmlv3.q4_K_M.bin<br>llama_model_load_internal:
          warning: assuming 70B model based on GQA == 8<br>llama_model_load_internal:
          format     = ggjt v3 (latest)<br>llama_model_load_internal: n_vocab    =
          32000<br>llama_model_load_internal: n_ctx      = 4096<br>llama_model_load_internal:
          n_embd     = 8192<br>llama_model_load_internal: n_mult     = 7168<br>llama_model_load_internal:
          n_head     = 64<br>llama_model_load_internal: n_head_kv  = 8<br>llama_model_load_internal:
          n_layer    = 80<br>llama_model_load_internal: n_rot      = 128<br>llama_model_load_internal:
          n_gqa      = 8<br>llama_model_load_internal: rnorm_eps  = 5.0e-06<br>llama_model_load_internal:
          n_ff       = 28672<br>llama_model_load_internal: freq_base  = 10000.0<br>llama_model_load_internal:
          freq_scale = 1<br>llama_model_load_internal: ftype      = 15 (mostly Q4_K
          - Medium)<br>llama_model_load_internal: model size = 70B<br>llama_model_load_internal:
          ggml ctx size =    0.21 MB<br>llama_model_load_internal: using OpenCL for
          GPU acceleration<br>llama_model_load_internal: mem required  = 28985.77
          MB (+ 1280.00 MB per state)<br>llama_model_load_internal: offloading 21
          repeating layers to GPU<br>llama_model_load_internal: offloaded 21/81 layers
          to GPU<br>llama_model_load_internal: total VRAM used: 10478 MB<br>llama_new_context_with_model:
          kv self size  = 1280.00 MB<br>llama_new_context_with_model: compute buffer
          total size =  561.35 MB</p>

          <p>system_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 |
          AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C
          = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |<br>sampling:
          repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000,
          frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000,
          typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000,
          mirostat_ent = 5.000000<br>generate: n_ctx = 4096, n_batch = 512, n_predict
          = -1, n_keep = 0</p>

          <h3 id="systemnthis-is-a-system-prompt-please-behave-and-help-the-usernn-usernwrite-a-story-about-llamasnn-assistantggml_assert-dallamacppllamacppggmlc10463-ne02--ne12">System:\nThis
          is a system prompt, please behave and help the user.\n\n### User:\nWrite
          a story about llamas\n\n### Assistant:GGML_ASSERT: D:\a\llama.cpp\llama.cpp\ggml.c:10463:
          ne02 == ne12</h3>

          '
        raw: "Anybody else experiencing GGML_ASSERT error when trying to run this\
          \ model with cblast llama.cpp ? \n\nI am using latest version of Cblast\
          \  llama.cpp (from 31/July2023) but experiencing  the same issue with several\
          \ older versions as well\nI am using Model card example of prompt. \n\n\
          Model seems to be loading fine, but then for response I only get 1-liner:\
          \    \"GGML_ASSERT: D:\\a\\llama.cpp\\llama.cpp\\ggml.c:10463: ne02 == ne12\"\
          \n\nAny suggestion on how to fix it, would be appreciated. \n\nThanks  \n\
          \n\nOUTPUT I am getting is as follows: \n\nmain.exe  -t 16 -ngl 21 -gqa\
          \ 8 -m C:\\Users\\robo\\Downloads\\upstage-llama-2-70b-instruct-v2.ggmlv3.q4_K_M.bin\
          \ --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"### System:\\\
          nThis is a system prompt, please behave and help the user.\\n\\n### User:\\\
          nWrite a story about llamas\\n\\n### Assistant:\"\nmain: warning: base model\
          \ only supports context sizes no greater than 2048 tokens (4096 specified)\n\
          main: build = 930 (9d2382b)\nmain: seed  = 1690818332\nggml_opencl: selecting\
          \ platform: 'NVIDIA CUDA'\nggml_opencl: selecting device: 'NVIDIA GeForce\
          \ RTX 4090 Laptop GPU'\nggml_opencl: device FP16 support: false\nllama.cpp:\
          \ loading model from C:\\Users\\robo\\Downloads\\upstage-llama-2-70b-instruct-v2.ggmlv3.q4_K_M.bin\n\
          llama_model_load_internal: warning: assuming 70B model based on GQA == 8\n\
          llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
          \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 4096\nllama_model_load_internal:\
          \ n_embd     = 8192\nllama_model_load_internal: n_mult     = 7168\nllama_model_load_internal:\
          \ n_head     = 64\nllama_model_load_internal: n_head_kv  = 8\nllama_model_load_internal:\
          \ n_layer    = 80\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal:\
          \ n_gqa      = 8\nllama_model_load_internal: rnorm_eps  = 5.0e-06\nllama_model_load_internal:\
          \ n_ff       = 28672\nllama_model_load_internal: freq_base  = 10000.0\n\
          llama_model_load_internal: freq_scale = 1\nllama_model_load_internal: ftype\
          \      = 15 (mostly Q4_K - Medium)\nllama_model_load_internal: model size\
          \ = 70B\nllama_model_load_internal: ggml ctx size =    0.21 MB\nllama_model_load_internal:\
          \ using OpenCL for GPU acceleration\nllama_model_load_internal: mem required\
          \  = 28985.77 MB (+ 1280.00 MB per state)\nllama_model_load_internal: offloading\
          \ 21 repeating layers to GPU\nllama_model_load_internal: offloaded 21/81\
          \ layers to GPU\nllama_model_load_internal: total VRAM used: 10478 MB\n\
          llama_new_context_with_model: kv self size  = 1280.00 MB\nllama_new_context_with_model:\
          \ compute buffer total size =  561.35 MB\n\nsystem_info: n_threads = 16\
          \ / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
          \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
          \ = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\nsampling: repeat_last_n = 64, repeat_penalty\
          \ = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000,\
          \ top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000,\
          \ temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent =\
          \ 5.000000\ngenerate: n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep\
          \ = 0\n\n\n ### System:\\nThis is a system prompt, please behave and help\
          \ the user.\\n\\n### User:\\nWrite a story about llamas\\n\\n### Assistant:GGML_ASSERT:\
          \ D:\\a\\llama.cpp\\llama.cpp\\ggml.c:10463: ne02 == ne12\n\n\n\n  "
        updatedAt: '2023-07-31T15:58:19.357Z'
      numEdits: 1
      reactions: []
    id: 64c7d9e2ca9e7f2fb66d056f
    type: comment
  author: dzupin
  content: "Anybody else experiencing GGML_ASSERT error when trying to run this model\
    \ with cblast llama.cpp ? \n\nI am using latest version of Cblast  llama.cpp (from\
    \ 31/July2023) but experiencing  the same issue with several older versions as\
    \ well\nI am using Model card example of prompt. \n\nModel seems to be loading\
    \ fine, but then for response I only get 1-liner:    \"GGML_ASSERT: D:\\a\\llama.cpp\\\
    llama.cpp\\ggml.c:10463: ne02 == ne12\"\n\nAny suggestion on how to fix it, would\
    \ be appreciated. \n\nThanks  \n\n\nOUTPUT I am getting is as follows: \n\nmain.exe\
    \  -t 16 -ngl 21 -gqa 8 -m C:\\Users\\robo\\Downloads\\upstage-llama-2-70b-instruct-v2.ggmlv3.q4_K_M.bin\
    \ --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"### System:\\nThis\
    \ is a system prompt, please behave and help the user.\\n\\n### User:\\nWrite\
    \ a story about llamas\\n\\n### Assistant:\"\nmain: warning: base model only supports\
    \ context sizes no greater than 2048 tokens (4096 specified)\nmain: build = 930\
    \ (9d2382b)\nmain: seed  = 1690818332\nggml_opencl: selecting platform: 'NVIDIA\
    \ CUDA'\nggml_opencl: selecting device: 'NVIDIA GeForce RTX 4090 Laptop GPU'\n\
    ggml_opencl: device FP16 support: false\nllama.cpp: loading model from C:\\Users\\\
    robo\\Downloads\\upstage-llama-2-70b-instruct-v2.ggmlv3.q4_K_M.bin\nllama_model_load_internal:\
    \ warning: assuming 70B model based on GQA == 8\nllama_model_load_internal: format\
    \     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal:\
    \ n_ctx      = 4096\nllama_model_load_internal: n_embd     = 8192\nllama_model_load_internal:\
    \ n_mult     = 7168\nllama_model_load_internal: n_head     = 64\nllama_model_load_internal:\
    \ n_head_kv  = 8\nllama_model_load_internal: n_layer    = 80\nllama_model_load_internal:\
    \ n_rot      = 128\nllama_model_load_internal: n_gqa      = 8\nllama_model_load_internal:\
    \ rnorm_eps  = 5.0e-06\nllama_model_load_internal: n_ff       = 28672\nllama_model_load_internal:\
    \ freq_base  = 10000.0\nllama_model_load_internal: freq_scale = 1\nllama_model_load_internal:\
    \ ftype      = 15 (mostly Q4_K - Medium)\nllama_model_load_internal: model size\
    \ = 70B\nllama_model_load_internal: ggml ctx size =    0.21 MB\nllama_model_load_internal:\
    \ using OpenCL for GPU acceleration\nllama_model_load_internal: mem required \
    \ = 28985.77 MB (+ 1280.00 MB per state)\nllama_model_load_internal: offloading\
    \ 21 repeating layers to GPU\nllama_model_load_internal: offloaded 21/81 layers\
    \ to GPU\nllama_model_load_internal: total VRAM used: 10478 MB\nllama_new_context_with_model:\
    \ kv self size  = 1280.00 MB\nllama_new_context_with_model: compute buffer total\
    \ size =  561.35 MB\n\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 |\
    \ AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA\
    \ = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0\
    \ |\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty\
    \ = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p\
    \ = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr\
    \ = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 4096, n_batch = 512,\
    \ n_predict = -1, n_keep = 0\n\n\n ### System:\\nThis is a system prompt, please\
    \ behave and help the user.\\n\\n### User:\\nWrite a story about llamas\\n\\n###\
    \ Assistant:GGML_ASSERT: D:\\a\\llama.cpp\\llama.cpp\\ggml.c:10463: ne02 == ne12\n\
    \n\n\n  "
  created_at: 2023-07-31 14:57:22+00:00
  edited: true
  hidden: false
  id: 64c7d9e2ca9e7f2fb66d056f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2023-08-01T07:26:54.000Z'
    data:
      edited: false
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8414150476455688
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: '<p>I am also getting an error while trying to run cublas. But my error
          says:</p>

          <p><code>CUDA error 222 at D:\a\llama.cpp\llama.cpp\ggml-cuda.cu:4816: the
          provided PTX was compiled with an unsupported toolchain.</code></p>

          '
        raw: 'I am also getting an error while trying to run cublas. But my error
          says:


          `CUDA error 222 at D:\a\llama.cpp\llama.cpp\ggml-cuda.cu:4816: the provided
          PTX was compiled with an unsupported toolchain.`'
        updatedAt: '2023-08-01T07:26:54.365Z'
      numEdits: 0
      reactions: []
    id: 64c8b3beb8685df8003947b4
    type: comment
  author: dillfrescott
  content: 'I am also getting an error while trying to run cublas. But my error says:


    `CUDA error 222 at D:\a\llama.cpp\llama.cpp\ggml-cuda.cu:4816: the provided PTX
    was compiled with an unsupported toolchain.`'
  created_at: 2023-08-01 06:26:54+00:00
  edited: false
  hidden: false
  id: 64c8b3beb8685df8003947b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2023-08-01T07:37:29.000Z'
    data:
      edited: false
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9952675104141235
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: '<p>Ok my error was related to having out of date video drivers. I have
          fixed it.</p>

          '
        raw: Ok my error was related to having out of date video drivers. I have fixed
          it.
        updatedAt: '2023-08-01T07:37:29.388Z'
      numEdits: 0
      reactions: []
    id: 64c8b639f723215b6dc79b1f
    type: comment
  author: dillfrescott
  content: Ok my error was related to having out of date video drivers. I have fixed
    it.
  created_at: 2023-08-01 06:37:29+00:00
  edited: false
  hidden: false
  id: 64c8b639f723215b6dc79b1f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
      fullname: Robert Dzupin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dzupin
      type: user
    createdAt: '2023-08-01T16:26:53.000Z'
    data:
      edited: true
      editors:
      - dzupin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9689139723777771
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
          fullname: Robert Dzupin
          isHf: false
          isPro: false
          name: dzupin
          type: user
        html: '<p>Hi dillfrescott</p>

          <p>Thanks for note about cublas working for you.<br>I tried it and cublas
          is working for me as well.<br>Model GGML conversion  therefore seems to
          be working fine and issue is in cblast version of llama.cpp implementation</p>

          <p>I am upgrading to cublas and ditching cblast mess.<br>Issue solved </p>

          <p>Thanks</p>

          '
        raw: "Hi dillfrescott\n\nThanks for note about cublas working for you.\nI\
          \ tried it and cublas is working for me as well. \nModel GGML conversion\
          \  therefore seems to be working fine and issue is in cblast version of\
          \ llama.cpp implementation\n\nI am upgrading to cublas and ditching cblast\
          \ mess. \nIssue solved \n\nThanks\n\n\n"
        updatedAt: '2023-08-01T17:46:21.567Z'
      numEdits: 1
      reactions: []
    id: 64c9324d7b4d0d947ceffdc4
    type: comment
  author: dzupin
  content: "Hi dillfrescott\n\nThanks for note about cublas working for you.\nI tried\
    \ it and cublas is working for me as well. \nModel GGML conversion  therefore\
    \ seems to be working fine and issue is in cblast version of llama.cpp implementation\n\
    \nI am upgrading to cublas and ditching cblast mess. \nIssue solved \n\nThanks\n\
    \n\n"
  created_at: 2023-08-01 15:26:53+00:00
  edited: true
  hidden: false
  id: 64c9324d7b4d0d947ceffdc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
      fullname: Robert Dzupin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dzupin
      type: user
    createdAt: '2023-08-01T16:27:10.000Z'
    data:
      status: closed
    id: 64c9325e6c2d4ce2c93453c4
    type: status-change
  author: dzupin
  created_at: 2023-08-01 15:27:10+00:00
  id: 64c9325e6c2d4ce2c93453c4
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2023-08-01T21:26:16.000Z'
    data:
      edited: false
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9948544502258301
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: '<p>No problem! Glad you were able to sort the issue out!</p>

          '
        raw: No problem! Glad you were able to sort the issue out!
        updatedAt: '2023-08-01T21:26:16.742Z'
      numEdits: 0
      reactions: []
    id: 64c978780986bd6fa24f7e69
    type: comment
  author: dillfrescott
  content: No problem! Glad you were able to sort the issue out!
  created_at: 2023-08-01 20:26:16+00:00
  edited: false
  hidden: false
  id: 64c978780986bd6fa24f7e69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/613ff5bf0be84915726fc23c/BHqZCO6bLSLL1ld5Ukr1L.jpeg?w=200&h=200&f=face
      fullname: Christian Stewart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: paralin
      type: user
    createdAt: '2023-09-01T21:46:38.000Z'
    data:
      edited: false
      editors:
      - paralin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9272669553756714
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/613ff5bf0be84915726fc23c/BHqZCO6bLSLL1ld5Ukr1L.jpeg?w=200&h=200&f=face
          fullname: Christian Stewart
          isHf: false
          isPro: false
          name: paralin
          type: user
        html: '<p>I''m still seeing this assertion error with clBLAST.</p>

          '
        raw: I'm still seeing this assertion error with clBLAST.
        updatedAt: '2023-09-01T21:46:38.283Z'
      numEdits: 0
      reactions: []
    id: 64f25bbea7f519393bba25ad
    type: comment
  author: paralin
  content: I'm still seeing this assertion error with clBLAST.
  created_at: 2023-09-01 20:46:38+00:00
  edited: false
  hidden: false
  id: 64f25bbea7f519393bba25ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/75883d3ff82a785a4b9bb0008e848012.svg
      fullname: Peter Ma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nyceane
      type: user
    createdAt: '2023-09-14T18:00:48.000Z'
    data:
      edited: false
      editors:
      - nyceane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6402202248573303
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/75883d3ff82a785a4b9bb0008e848012.svg
          fullname: Peter Ma
          isHf: false
          isPro: false
          name: nyceane
          type: user
        html: '<p>same here :)</p>

          '
        raw: same here :)
        updatedAt: '2023-09-14T18:00:48.441Z'
      numEdits: 0
      reactions: []
    id: 65034a5036bc3431217964a6
    type: comment
  author: nyceane
  content: same here :)
  created_at: 2023-09-14 17:00:48+00:00
  edited: false
  hidden: false
  id: 65034a5036bc3431217964a6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Upstage-Llama-2-70B-instruct-v2-GGML
repo_type: model
status: closed
target_branch: null
title: 'Receiving assert error when trying to do prediction with the model .  '
