!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alfredplpl
conflicting_files: null
created_at: 2023-09-28 01:39:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-09-28T02:39:33.000Z'
    data:
      edited: true
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.732918918132782
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: '<p>I ran the code. But, the code ran on lower speed because the code
          ran on CPU .</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          transformers

          pipeline = transformers.pipeline(<span class="hljs-string">"text-generation"</span>,
          model=<span class="hljs-string">"pfnet/plamo-13b"</span>, trust_remote_code=<span
          class="hljs-literal">True</span>)

          prompt = <span class="hljs-string">"""</span>

          <span class="hljs-string">PLaMo-13B is a LLaMA-based 13B model pre-trained
          on English and Japanese open datasets, developed by Preferred Networks,
          Inc. PLaMo-13B is released under Apache v2.0 license.</span>

          <span class="hljs-string">I translate the above mentioned sentences to Japanese:</span>

          <span class="hljs-string">"""</span>

          <span class="hljs-built_in">print</span>(pipeline(text_inputs=prompt, max_new_tokens=<span
          class="hljs-number">512</span>))

          </code></pre>

          <p>So, I recommend the code instead of the above mentioned code.</p>

          <pre><code class="language-python">pipeline = transformers.pipeline(<span
          class="hljs-string">"text-generation"</span>, model=<span class="hljs-string">"pfnet/plamo-13b"</span>,
          device_map=<span class="hljs-string">"auto"</span>,model_kwargs={<span class="hljs-string">"load_in_8bit"</span>:
          <span class="hljs-literal">True</span>},trust_remote_code=<span class="hljs-literal">True</span>)

          </code></pre>

          <p>The code needs <code>pip install accelerate bitsandbytes scipy</code>
          but the code runs on higher speed on GPU.</p>

          '
        raw: 'I ran the code. But, the code ran on lower speed because the code ran
          on CPU .

          ```python

          import transformers

          pipeline = transformers.pipeline("text-generation", model="pfnet/plamo-13b",
          trust_remote_code=True)

          prompt = """

          PLaMo-13B is a LLaMA-based 13B model pre-trained on English and Japanese
          open datasets, developed by Preferred Networks, Inc. PLaMo-13B is released
          under Apache v2.0 license.

          I translate the above mentioned sentences to Japanese:

          """

          print(pipeline(text_inputs=prompt, max_new_tokens=512))

          ```

          So, I recommend the code instead of the above mentioned code.

          ```python

          pipeline = transformers.pipeline("text-generation", model="pfnet/plamo-13b",
          device_map="auto",model_kwargs={"load_in_8bit": True},trust_remote_code=True)

          ```

          The code needs `pip install accelerate bitsandbytes scipy` but the code
          runs on higher speed on GPU.'
        updatedAt: '2023-09-28T02:43:37.061Z'
      numEdits: 1
      reactions: []
    id: 6514e765341103249d73b9fb
    type: comment
  author: alfredplpl
  content: 'I ran the code. But, the code ran on lower speed because the code ran
    on CPU .

    ```python

    import transformers

    pipeline = transformers.pipeline("text-generation", model="pfnet/plamo-13b", trust_remote_code=True)

    prompt = """

    PLaMo-13B is a LLaMA-based 13B model pre-trained on English and Japanese open
    datasets, developed by Preferred Networks, Inc. PLaMo-13B is released under Apache
    v2.0 license.

    I translate the above mentioned sentences to Japanese:

    """

    print(pipeline(text_inputs=prompt, max_new_tokens=512))

    ```

    So, I recommend the code instead of the above mentioned code.

    ```python

    pipeline = transformers.pipeline("text-generation", model="pfnet/plamo-13b", device_map="auto",model_kwargs={"load_in_8bit":
    True},trust_remote_code=True)

    ```

    The code needs `pip install accelerate bitsandbytes scipy` but the code runs on
    higher speed on GPU.'
  created_at: 2023-09-28 01:39:33+00:00
  edited: true
  hidden: false
  id: 6514e765341103249d73b9fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-09-28T02:55:21.000Z'
    data:
      edited: false
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: ja
        probability: 0.6834830045700073
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: "<p>In addition, I ran the code.</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> transformers\npipeline = transformers.pipeline(<span\
          \ class=\"hljs-string\">\"text-generation\"</span>, model=<span class=\"\
          hljs-string\">\"pfnet/plamo-13b\"</span>, device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>,model_kwargs={<span class=\"hljs-string\">\"load_in_8bit\"\
          </span>: <span class=\"hljs-literal\">True</span>},trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\nprompt = <span class=\"hljs-string\"\
          >\"\"\"\u6B21\u306E\u82F1\u6587\u306F\u79C1\u304C\u548C\u8A33\u3059\u308B\
          \u4E88\u5B9A\u306E\u6587\u7AE0\u3067\u3059\u3002</span>\n<span class=\"\
          hljs-string\">'PLaMo-13B is a LLaMA-based 13B model pre-trained on English\
          \ and Japanese open datasets, developed by Preferred Networks, Inc. PLaMo-13B\
          \ is released under Apache v2.0 license.' </span>\n<span class=\"hljs-string\"\
          >\u79C1\u306F\u4EE5\u4E0A\u306E\u82F1\u6587\u3092\u6B21\u306E\u3088\u3046\
          \u306A\u65E5\u672C\u8A9E\u306B\u7FFB\u8A33\u3057\u307E\u3059\u3002\"\"\"\
          </span>\n<span class=\"hljs-built_in\">print</span>(pipeline(text_inputs=prompt,\
          \ max_new_tokens=<span class=\"hljs-number\">128</span>))\n</code></pre>\n\
          <p>Then, I got the result.</p>\n<pre><code class=\"language-python\">Loading\
          \ checkpoint shards: <span class=\"hljs-number\">100</span>%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| <span class=\"hljs-number\"\
          >3</span>/<span class=\"hljs-number\">3</span> [<span class=\"hljs-number\"\
          >00</span>:06&lt;<span class=\"hljs-number\">00</span>:<span class=\"hljs-number\"\
          >00</span>,  <span class=\"hljs-number\">2.16</span>s/it]\n[{<span class=\"\
          hljs-string\">'generated_text'</span>: <span class=\"hljs-string\">\"\u6B21\
          \u306E\u82F1\u6587\u306F\u79C1\u304C\u548C\u8A33\u3059\u308B\u4E88\u5B9A\
          \u306E\u6587\u7AE0\u3067\u3059\u3002\\n'PLaMo-13B is a LLaMA-based 13B model\
          \ pre-trained on English and Japanese open datasets, developed by Preferred\
          \ Networks, Inc. PLaMo-13B is released under Apache v2.0 license.' \\n\u79C1\
          \u306F\u4EE5\u4E0A\u306E\u82F1\u6587\u3092\u6B21\u306E\u3088\u3046\u306A\
          \u65E5\u672C\u8A9E\u306B\u7FFB\u8A33\u3057\u307E\u3059\u3002\\n\u300CPLaMo-13B\u306F\
          \u3001\u82F1\u8A9E\u3068\u65E5\u672C\u8A9E\u306E\u30AA\u30FC\u30D7\u30F3\
          \u30C7\u30FC\u30BF\u30BB\u30C3\u30C8\u3067\u8A13\u7DF4\u3055\u308C\u305F\
          \u300113B\u30E2\u30C7\u30EB\u3067\u3059\u3002PLaMo-13B\u306FApache v2.0\u30E9\
          \u30A4\u30BB\u30F3\u30B9\u306E\u4E0B\u3067\u516C\u958B\u3055\u308C\u3066\
          \u3044\u307E\u3059\u3002\u300D\\n\u3053\u306E\u82F1\u6587\u3092\u548C\u8A33\
          \u3059\u308B\u306B\u3042\u305F\u3063\u3066\u3001\u4EE5\u4E0B\u306E\u70B9\
          \u306B\u6CE8\u610F\u3057\u307E\u3057\u305F\u3002\\n1. \u300C13B\u300D\u306F\
          \u300C13\u30D3\u30C3\u30C8\u300D\u306E\u7565\u3067\u3059\u3002\\n2. \u300C\
          13B\u300D\u306F\u300C13\u30D3\u30C3\u30C8\u300D\u306E\u7565\u3067\u3059\u3002\
          \\n3. \u300C13B\u300D\u306F\u300C13\u30D3\u30C3\u30C8\u300D\u306E\u7565\u3067\
          \u3059\u3002\\n4. \u300C13B\u300D\u306F\"</span>}]\n</code></pre>\n"
        raw: "In addition, I ran the code.\n```python\nimport transformers\npipeline\
          \ = transformers.pipeline(\"text-generation\", model=\"pfnet/plamo-13b\"\
          , device_map=\"auto\",model_kwargs={\"load_in_8bit\": True},trust_remote_code=True)\n\
          prompt = \"\"\"\u6B21\u306E\u82F1\u6587\u306F\u79C1\u304C\u548C\u8A33\u3059\
          \u308B\u4E88\u5B9A\u306E\u6587\u7AE0\u3067\u3059\u3002\n'PLaMo-13B is a\
          \ LLaMA-based 13B model pre-trained on English and Japanese open datasets,\
          \ developed by Preferred Networks, Inc. PLaMo-13B is released under Apache\
          \ v2.0 license.' \n\u79C1\u306F\u4EE5\u4E0A\u306E\u82F1\u6587\u3092\u6B21\
          \u306E\u3088\u3046\u306A\u65E5\u672C\u8A9E\u306B\u7FFB\u8A33\u3057\u307E\
          \u3059\u3002\"\"\"\nprint(pipeline(text_inputs=prompt, max_new_tokens=128))\n\
          \n```\nThen, I got the result.\n```python\nLoading checkpoint shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:06<00:00,\
          \  2.16s/it]\n[{'generated_text': \"\u6B21\u306E\u82F1\u6587\u306F\u79C1\
          \u304C\u548C\u8A33\u3059\u308B\u4E88\u5B9A\u306E\u6587\u7AE0\u3067\u3059\
          \u3002\\n'PLaMo-13B is a LLaMA-based 13B model pre-trained on English and\
          \ Japanese open datasets, developed by Preferred Networks, Inc. PLaMo-13B\
          \ is released under Apache v2.0 license.' \\n\u79C1\u306F\u4EE5\u4E0A\u306E\
          \u82F1\u6587\u3092\u6B21\u306E\u3088\u3046\u306A\u65E5\u672C\u8A9E\u306B\
          \u7FFB\u8A33\u3057\u307E\u3059\u3002\\n\u300CPLaMo-13B\u306F\u3001\u82F1\
          \u8A9E\u3068\u65E5\u672C\u8A9E\u306E\u30AA\u30FC\u30D7\u30F3\u30C7\u30FC\
          \u30BF\u30BB\u30C3\u30C8\u3067\u8A13\u7DF4\u3055\u308C\u305F\u300113B\u30E2\
          \u30C7\u30EB\u3067\u3059\u3002PLaMo-13B\u306FApache v2.0\u30E9\u30A4\u30BB\
          \u30F3\u30B9\u306E\u4E0B\u3067\u516C\u958B\u3055\u308C\u3066\u3044\u307E\
          \u3059\u3002\u300D\\n\u3053\u306E\u82F1\u6587\u3092\u548C\u8A33\u3059\u308B\
          \u306B\u3042\u305F\u3063\u3066\u3001\u4EE5\u4E0B\u306E\u70B9\u306B\u6CE8\
          \u610F\u3057\u307E\u3057\u305F\u3002\\n1. \u300C13B\u300D\u306F\u300C13\u30D3\
          \u30C3\u30C8\u300D\u306E\u7565\u3067\u3059\u3002\\n2. \u300C13B\u300D\u306F\
          \u300C13\u30D3\u30C3\u30C8\u300D\u306E\u7565\u3067\u3059\u3002\\n3. \u300C\
          13B\u300D\u306F\u300C13\u30D3\u30C3\u30C8\u300D\u306E\u7565\u3067\u3059\u3002\
          \\n4. \u300C13B\u300D\u306F\"}]\n```"
        updatedAt: '2023-09-28T02:55:21.080Z'
      numEdits: 0
      reactions: []
    id: 6514eb195aef9f39138cd0d4
    type: comment
  author: alfredplpl
  content: "In addition, I ran the code.\n```python\nimport transformers\npipeline\
    \ = transformers.pipeline(\"text-generation\", model=\"pfnet/plamo-13b\", device_map=\"\
    auto\",model_kwargs={\"load_in_8bit\": True},trust_remote_code=True)\nprompt =\
    \ \"\"\"\u6B21\u306E\u82F1\u6587\u306F\u79C1\u304C\u548C\u8A33\u3059\u308B\u4E88\
    \u5B9A\u306E\u6587\u7AE0\u3067\u3059\u3002\n'PLaMo-13B is a LLaMA-based 13B model\
    \ pre-trained on English and Japanese open datasets, developed by Preferred Networks,\
    \ Inc. PLaMo-13B is released under Apache v2.0 license.' \n\u79C1\u306F\u4EE5\u4E0A\
    \u306E\u82F1\u6587\u3092\u6B21\u306E\u3088\u3046\u306A\u65E5\u672C\u8A9E\u306B\
    \u7FFB\u8A33\u3057\u307E\u3059\u3002\"\"\"\nprint(pipeline(text_inputs=prompt,\
    \ max_new_tokens=128))\n\n```\nThen, I got the result.\n```python\nLoading checkpoint\
    \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3\
    \ [00:06<00:00,  2.16s/it]\n[{'generated_text': \"\u6B21\u306E\u82F1\u6587\u306F\
    \u79C1\u304C\u548C\u8A33\u3059\u308B\u4E88\u5B9A\u306E\u6587\u7AE0\u3067\u3059\
    \u3002\\n'PLaMo-13B is a LLaMA-based 13B model pre-trained on English and Japanese\
    \ open datasets, developed by Preferred Networks, Inc. PLaMo-13B is released under\
    \ Apache v2.0 license.' \\n\u79C1\u306F\u4EE5\u4E0A\u306E\u82F1\u6587\u3092\u6B21\
    \u306E\u3088\u3046\u306A\u65E5\u672C\u8A9E\u306B\u7FFB\u8A33\u3057\u307E\u3059\
    \u3002\\n\u300CPLaMo-13B\u306F\u3001\u82F1\u8A9E\u3068\u65E5\u672C\u8A9E\u306E\
    \u30AA\u30FC\u30D7\u30F3\u30C7\u30FC\u30BF\u30BB\u30C3\u30C8\u3067\u8A13\u7DF4\
    \u3055\u308C\u305F\u300113B\u30E2\u30C7\u30EB\u3067\u3059\u3002PLaMo-13B\u306F\
    Apache v2.0\u30E9\u30A4\u30BB\u30F3\u30B9\u306E\u4E0B\u3067\u516C\u958B\u3055\u308C\
    \u3066\u3044\u307E\u3059\u3002\u300D\\n\u3053\u306E\u82F1\u6587\u3092\u548C\u8A33\
    \u3059\u308B\u306B\u3042\u305F\u3063\u3066\u3001\u4EE5\u4E0B\u306E\u70B9\u306B\
    \u6CE8\u610F\u3057\u307E\u3057\u305F\u3002\\n1. \u300C13B\u300D\u306F\u300C13\u30D3\
    \u30C3\u30C8\u300D\u306E\u7565\u3067\u3059\u3002\\n2. \u300C13B\u300D\u306F\u300C\
    13\u30D3\u30C3\u30C8\u300D\u306E\u7565\u3067\u3059\u3002\\n3. \u300C13B\u300D\u306F\
    \u300C13\u30D3\u30C3\u30C8\u300D\u306E\u7565\u3067\u3059\u3002\\n4. \u300C13B\u300D\
    \u306F\"}]\n```"
  created_at: 2023-09-28 01:55:21+00:00
  edited: false
  hidden: false
  id: 6514eb195aef9f39138cd0d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ICTuyjzcut5mWWWPYXC_X.jpeg?w=200&h=200&f=face
      fullname: Daiki Higurashi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dhigurashi
      type: user
    createdAt: '2023-09-28T06:14:40.000Z'
    data:
      edited: false
      editors:
      - dhigurashi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9382019639015198
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ICTuyjzcut5mWWWPYXC_X.jpeg?w=200&h=200&f=face
          fullname: Daiki Higurashi
          isHf: false
          isPro: false
          name: dhigurashi
          type: user
        html: '<p>Thank you for your suggestion!</p>

          <p>You''re right, using such options could speed up the model''s inference.
          However, this requires additional libraries and GPU. Meanwhile, we''ve confirmed
          our example can run without GPU.</p>

          <p>There are indeed many other options to try, such as <code>torch_dtype="auto"</code>
          to reduce memory usage. However, adding too many options could potentially
          confuse users. Therefore, we''ve decided to stick with the simplest setup
          in our example.</p>

          <p>Thanks again for your input! It helps us continue to improve our examples.</p>

          '
        raw: 'Thank you for your suggestion!


          You''re right, using such options could speed up the model''s inference.
          However, this requires additional libraries and GPU. Meanwhile, we''ve confirmed
          our example can run without GPU.


          There are indeed many other options to try, such as `torch_dtype="auto"`
          to reduce memory usage. However, adding too many options could potentially
          confuse users. Therefore, we''ve decided to stick with the simplest setup
          in our example.


          Thanks again for your input! It helps us continue to improve our examples.'
        updatedAt: '2023-09-28T06:14:40.411Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - alfredplpl
    id: 651519d02a1aaddbb6662cdf
    type: comment
  author: dhigurashi
  content: 'Thank you for your suggestion!


    You''re right, using such options could speed up the model''s inference. However,
    this requires additional libraries and GPU. Meanwhile, we''ve confirmed our example
    can run without GPU.


    There are indeed many other options to try, such as `torch_dtype="auto"` to reduce
    memory usage. However, adding too many options could potentially confuse users.
    Therefore, we''ve decided to stick with the simplest setup in our example.


    Thanks again for your input! It helps us continue to improve our examples.'
  created_at: 2023-09-28 05:14:40+00:00
  edited: false
  hidden: false
  id: 651519d02a1aaddbb6662cdf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-09-28T06:29:20.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7547484040260315
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Hey! In order to put the pipeline on device you should use: </p>

          <pre><code class="language-python">pipeline = transformers.pipeline(<span
          class="hljs-string">"text-generation"</span>, model=<span class="hljs-string">"pfnet/plamo-13b"</span>,
          trust_remote_code=<span class="hljs-literal">True</span>, device = <span
          class="hljs-string">"cuda"</span>)

          </code></pre>

          '
        raw: "Hey! In order to put the pipeline on device you should use: \n```python\n\
          pipeline = transformers.pipeline(\"text-generation\", model=\"pfnet/plamo-13b\"\
          , trust_remote_code=True, device = \"cuda\")\n``` \n"
        updatedAt: '2023-09-28T06:29:20.164Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - alfredplpl
    id: 65151d4068414f3480e11831
    type: comment
  author: ArthurZ
  content: "Hey! In order to put the pipeline on device you should use: \n```python\n\
    pipeline = transformers.pipeline(\"text-generation\", model=\"pfnet/plamo-13b\"\
    , trust_remote_code=True, device = \"cuda\")\n``` \n"
  created_at: 2023-09-28 05:29:20+00:00
  edited: false
  hidden: false
  id: 65151d4068414f3480e11831
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ICTuyjzcut5mWWWPYXC_X.jpeg?w=200&h=200&f=face
      fullname: Daiki Higurashi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dhigurashi
      type: user
    createdAt: '2023-09-29T05:32:48.000Z'
    data:
      edited: false
      editors:
      - dhigurashi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9510473012924194
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ICTuyjzcut5mWWWPYXC_X.jpeg?w=200&h=200&f=face
          fullname: Daiki Higurashi
          isHf: false
          isPro: false
          name: dhigurashi
          type: user
        html: '<p>Thanks for your comment! While <code>device="cuda"</code> allows
          the model to run on NVIDIA GPUs, it''s not suitable for other hardware.
          For instance, device="cuda" isn''t compatible with MacBooks. Our goal is
          to make this example as straightforward as possible.</p>

          <p>However, we greatly appreciate your valuable suggestions!</p>

          '
        raw: 'Thanks for your comment! While `device="cuda"` allows the model to run
          on NVIDIA GPUs, it''s not suitable for other hardware. For instance, device="cuda"
          isn''t compatible with MacBooks. Our goal is to make this example as straightforward
          as possible.


          However, we greatly appreciate your valuable suggestions!'
        updatedAt: '2023-09-29T05:32:48.458Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65166180d467ffdc052ac6fc
    id: 65166180d467ffdc052ac6fa
    type: comment
  author: dhigurashi
  content: 'Thanks for your comment! While `device="cuda"` allows the model to run
    on NVIDIA GPUs, it''s not suitable for other hardware. For instance, device="cuda"
    isn''t compatible with MacBooks. Our goal is to make this example as straightforward
    as possible.


    However, we greatly appreciate your valuable suggestions!'
  created_at: 2023-09-29 04:32:48+00:00
  edited: false
  hidden: false
  id: 65166180d467ffdc052ac6fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ICTuyjzcut5mWWWPYXC_X.jpeg?w=200&h=200&f=face
      fullname: Daiki Higurashi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dhigurashi
      type: user
    createdAt: '2023-09-29T05:32:48.000Z'
    data:
      status: closed
    id: 65166180d467ffdc052ac6fc
    type: status-change
  author: dhigurashi
  created_at: 2023-09-29 04:32:48+00:00
  id: 65166180d467ffdc052ac6fc
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: pfnet/plamo-13b
repo_type: model
status: closed
target_branch: null
title: 'Option: Using GPU with quantized model'
