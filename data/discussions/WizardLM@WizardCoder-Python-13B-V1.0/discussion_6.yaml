!!python/object:huggingface_hub.community.DiscussionWithDetails
author: viktor-ferenczi
conflicting_files: null
created_at: 2023-09-10 00:37:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
      fullname: Viktor Ferenczi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viktor-ferenczi
      type: user
    createdAt: '2023-09-10T01:37:56.000Z'
    data:
      edited: false
      editors:
      - viktor-ferenczi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.883732259273529
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
          fullname: Viktor Ferenczi
          isHf: false
          isPro: false
          name: viktor-ferenczi
          type: user
        html: '<p>I had to fix <code>rope_theta</code> in the <code>config.json</code>
          inside my local model folder to be able to use this model with vLLM:</p>

          <pre><code>&lt;   "rope_theta": 1000000,

          ---

          &gt;   "rope_theta": 10000,

          </code></pre>

          <p>I''m not sure this is a vLLM only issue or a problem with the configuration
          of this model.</p>

          <p>Related PR and discussion at vLLM:<br><a rel="nofollow" href="https://github.com/vllm-project/vllm/pull/998">https://github.com/vllm-project/vllm/pull/998</a></p>

          <p>Could you please clarify?</p>

          '
        raw: "I had to fix `rope_theta` in the `config.json` inside my local model\
          \ folder to be able to use this model with vLLM:\r\n```\r\n<   \"rope_theta\"\
          : 1000000,\r\n---\r\n>   \"rope_theta\": 10000,\r\n```\r\n\r\nI'm not sure\
          \ this is a vLLM only issue or a problem with the configuration of this\
          \ model.\r\n\r\nRelated PR and discussion at vLLM:\r\nhttps://github.com/vllm-project/vllm/pull/998\r\
          \n\r\nCould you please clarify?"
        updatedAt: '2023-09-10T01:37:56.582Z'
      numEdits: 0
      reactions: []
    id: 64fd1df49a62bb2791db8e4d
    type: comment
  author: viktor-ferenczi
  content: "I had to fix `rope_theta` in the `config.json` inside my local model folder\
    \ to be able to use this model with vLLM:\r\n```\r\n<   \"rope_theta\": 1000000,\r\
    \n---\r\n>   \"rope_theta\": 10000,\r\n```\r\n\r\nI'm not sure this is a vLLM\
    \ only issue or a problem with the configuration of this model.\r\n\r\nRelated\
    \ PR and discussion at vLLM:\r\nhttps://github.com/vllm-project/vllm/pull/998\r\
    \n\r\nCould you please clarify?"
  created_at: 2023-09-10 00:37:56+00:00
  edited: false
  hidden: false
  id: 64fd1df49a62bb2791db8e4d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: WizardLM/WizardCoder-Python-13B-V1.0
repo_type: model
status: open
target_branch: null
title: rope_theta
