!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ZeyuZhang
conflicting_files: null
created_at: 2024-01-23 18:52:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65b00730403a23a2fd765110/Uw-obs-VymyU9iMVKLURZ.jpeg?w=200&h=200&f=face
      fullname: Zeyu Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZeyuZhang
      type: user
    createdAt: '2024-01-23T18:52:43.000Z'
    data:
      edited: false
      editors:
      - ZeyuZhang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8861110806465149
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65b00730403a23a2fd765110/Uw-obs-VymyU9iMVKLURZ.jpeg?w=200&h=200&f=face
          fullname: Zeyu Zhang
          isHf: false
          isPro: false
          name: ZeyuZhang
          type: user
        html: '<p>To whom it may concern,<br>Hi, this is Zeyu, a PhD working on a
          similar project for data wrangling tasks. We came across this Jellyfish
          work and realize it super cool. Therefore, we would like to use it as a
          baseline to compare with our designed approach. However, it seems I am not
          able to use the model directly as how I can do with a regular Llama2. For
          instance, here is some snippets I wrote to use your model for inferencing:</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("NECOUDBFM/Jellyfish")<br>model
          = AutoModelForCausalLM.from_pretrained("NECOUDBFM/Jellyfish")<br>input =
          ''''The input for an entity matching task''''''        #referred to the
          showcased prompt<br>inputs = tokenizer(input, return_tensors="pt")<br>generate_ids
          = model.generate(inputs.input_ids, max_length=30)<br>print(te_tokenizer.batch_decode(generate_ids,
          skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])</p>

          <p>However, the printed output makes no sense to me, while it just repeated
          what I forwarded. Moreover, it seems I am not able to feed the model input
          in batch style, while it continuously throws me dimensional incompatible
          error.</p>

          <p>Looking forward to hearing from you.<br>Best,<br>Zeyu</p>

          '
        raw: "To whom it may concern,\r\nHi, this is Zeyu, a PhD working on a similar\
          \ project for data wrangling tasks. We came across this Jellyfish work and\
          \ realize it super cool. Therefore, we would like to use it as a baseline\
          \ to compare with our designed approach. However, it seems I am not able\
          \ to use the model directly as how I can do with a regular Llama2. For instance,\
          \ here is some snippets I wrote to use your model for inferencing:\r\n\r\
          \ntokenizer = AutoTokenizer.from_pretrained(\"NECOUDBFM/Jellyfish\")\r\n\
          model = AutoModelForCausalLM.from_pretrained(\"NECOUDBFM/Jellyfish\")\r\n\
          input = ''The input for an entity matching task'''        #referred to the\
          \ showcased prompt\r\ninputs = tokenizer(input, return_tensors=\"pt\")\r\
          \ngenerate_ids = model.generate(inputs.input_ids, max_length=30)\r\nprint(te_tokenizer.batch_decode(generate_ids,\
          \ skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\r\n\r\
          \nHowever, the printed output makes no sense to me, while it just repeated\
          \ what I forwarded. Moreover, it seems I am not able to feed the model input\
          \ in batch style, while it continuously throws me dimensional incompatible\
          \ error.\r\n\r\nLooking forward to hearing from you.\r\nBest,\r\nZeyu"
        updatedAt: '2024-01-23T18:52:43.122Z'
      numEdits: 0
      reactions: []
    id: 65b00afba46e8751c4ee834c
    type: comment
  author: ZeyuZhang
  content: "To whom it may concern,\r\nHi, this is Zeyu, a PhD working on a similar\
    \ project for data wrangling tasks. We came across this Jellyfish work and realize\
    \ it super cool. Therefore, we would like to use it as a baseline to compare with\
    \ our designed approach. However, it seems I am not able to use the model directly\
    \ as how I can do with a regular Llama2. For instance, here is some snippets I\
    \ wrote to use your model for inferencing:\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    NECOUDBFM/Jellyfish\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"NECOUDBFM/Jellyfish\"\
    )\r\ninput = ''The input for an entity matching task'''        #referred to the\
    \ showcased prompt\r\ninputs = tokenizer(input, return_tensors=\"pt\")\r\ngenerate_ids\
    \ = model.generate(inputs.input_ids, max_length=30)\r\nprint(te_tokenizer.batch_decode(generate_ids,\
    \ skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\r\n\r\nHowever,\
    \ the printed output makes no sense to me, while it just repeated what I forwarded.\
    \ Moreover, it seems I am not able to feed the model input in batch style, while\
    \ it continuously throws me dimensional incompatible error.\r\n\r\nLooking forward\
    \ to hearing from you.\r\nBest,\r\nZeyu"
  created_at: 2024-01-23 18:52:43+00:00
  edited: false
  hidden: false
  id: 65b00afba46e8751c4ee834c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/070fb6d792ef5907dd62d10019cab50a.svg
      fullname: Haochen Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: HCZhang
      type: user
    createdAt: '2024-01-24T05:24:07.000Z'
    data:
      edited: false
      editors:
      - HCZhang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7138292193412781
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/070fb6d792ef5907dd62d10019cab50a.svg
          fullname: Haochen Zhang
          isHf: false
          isPro: false
          name: HCZhang
          type: user
        html: "<p>Hello Zeyu,</p>\n<p>Thank you for reaching out and expressing your\
          \ interest in our model. We apologize for the difficulties you've encountered.</p>\n\
          <p>From the code you've shared, it appears that you're attempting to load\
          \ Jellyfish directly via the Hugging Face API. Unfortunately, we don\u2019\
          t have direct control over how the model is handled during this process.\
          \ Therefore, we strongly recommend manually downloading the model files\
          \ before attempting to use Jellyfish for inference.</p>\n<p>To assist you\
          \ further, we are providing two scripts specifically designed for inference.\
          \ One is tailored for the vLLM module (<a rel=\"nofollow\" href=\"https://github.com/vllm-project\"\
          >https://github.com/vllm-project</a>) and the other for the Transformers\
          \ module (<a href=\"https://huggingface.co/docs/transformers/index\">https://huggingface.co/docs/transformers/index</a>).<br>Note\
          \ that vLLM has demonstrated significantly improved inference efficiency.</p>\n\
          <p>Code Snippet 1 for vLLM:</p>\n<p>from vllm import LLM, SamplingParams</p>\n\
          <p>model = LLM(model={path to Jellyfish files, e.g. \"/workspace/Jellyfish/\"\
          })<br>sampling_params = SamplingParams(<br>        temperature={temperature,\
          \ e.g. 0.35},<br>        top_p={top_p, e.g. 0.9},<br>        max_tokens={max\
          \ new token, e.g. 1024},<br>        stop=[\"### Instruction:\"],<br>   \
          \ )</p>\n<p>system_message = \"You are an AI assistant that follows instruction\
          \ extremely well. Help as much as you can.\"<br>user_message = \"\"\"You\
          \ are tasked with determining whether two records listed below are the same\
          \ based on the information provided.<br>Carefully compare the {attribute\
          \ 1}, {attribute 2}... for each record before making your decision.<br>Note:\
          \ Missing values (N/A or \"nan\") should not be used as a basis for your\
          \ decision.<br>Record A: [{attribute 1}: {attribute 1 value}, {attribute\
          \ 2}: {attribute 2 value}, ...]<br>Record B: [{attribute 1}: {attribute\
          \ 1 value}, {attribute 2}: {attribute 2 value}, ...]<br>Are record A and\
          \ record B the same entity? Choose your answer from: [Yes, No].<br>\"\"\"\
          </p>\n<p>prompt = f\"{system_message}\\n\\n### Instruction:\\n\\n{user_message}\\\
          n\\n### Response:\\n\\n\"<br>outputs = model.generate(prompt, sampling_params)<br>response\
          \ = outputs[0].outputs[0].text.strip()<br>print(response)</p>\n<p>Code Snippet\
          \ 2 for Transformers:</p>\n<p>from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer, GenerationConfig<br>import torch</p>\n<p>if torch.cuda.is_available():<br>\
          \    device = \"cuda\"<br>else:<br>    device = \"cpu\"<br>model = AutoModelForCausalLM.from_pretrained(<br>\
          \    {path_to_model},<br>    torch_dtype=torch.float16,<br>    device_map=\"\
          auto\",<br>)<br>tokenizer = AutoTokenizer.from_pretrained({path_to_model})</p>\n\
          <p>prompt = f\"{system_message}\\n\\n### Instruction:\\n\\n{user_message}\\\
          n\\n### Response:\\n\\n\"<br>inputs = tokenizer(prompt, return_tensors=\"\
          pt\")<br>input_ids = inputs[\"input_ids\"].to(device)</p>\n<p>generation_config\
          \ = GenerationConfig(<br>    do_samples=True,<br>    temperature=0.35,<br>\
          \    top_p=0.9,<br>)</p>\n<p>with torch.no_grad():<br>    generation_output\
          \ = model.generate(<br>        input_ids=input_ids,<br>        generation_config=generation_config,<br>\
          \        return_dict_in_generate=True,<br>        output_scores=True,<br>\
          \        max_new_tokens=1024,<br>        pad_token_id=tokenizer.eos_token_id,<br>\
          \        repetition_penalty=1.15,<br>    )<br>output = generation_output[0]<br>response\
          \ = tokenizer.decode(<br>    output[:, input_ids.shape[-1] :][0], skip_special_tokens=True<br>).strip()</p>\n\
          <p>print(response)</p>\n<p>We hope these scripts will resolve the issues\
          \ you're facing. Your participation in testing Jellyfish is immensely valuable\
          \ to us, and we encourage you to reach out again should you have any further\
          \ questions or need additional assistance.</p>\n<p>Best regards,</p>\n<p>Haochen,\
          \ NECOUDBFM</p>\n"
        raw: "Hello Zeyu,\n\nThank you for reaching out and expressing your interest\
          \ in our model. We apologize for the difficulties you've encountered.\n\n\
          From the code you've shared, it appears that you're attempting to load Jellyfish\
          \ directly via the Hugging Face API. Unfortunately, we don\u2019t have direct\
          \ control over how the model is handled during this process. Therefore,\
          \ we strongly recommend manually downloading the model files before attempting\
          \ to use Jellyfish for inference.\n\nTo assist you further, we are providing\
          \ two scripts specifically designed for inference. One is tailored for the\
          \ vLLM module (https://github.com/vllm-project) and the other for the Transformers\
          \ module (https://huggingface.co/docs/transformers/index).  \nNote that\
          \ vLLM has demonstrated significantly improved inference efficiency.\n\n\
          \n\nCode Snippet 1 for vLLM:\n\nfrom vllm import LLM, SamplingParams\n\n\
          model = LLM(model={path to Jellyfish files, e.g. \"/workspace/Jellyfish/\"\
          })\nsampling_params = SamplingParams(\n        temperature={temperature,\
          \ e.g. 0.35},\n        top_p={top_p, e.g. 0.9},\n        max_tokens={max\
          \ new token, e.g. 1024},\n        stop=[\"### Instruction:\"],\n    )\n\n\
          system_message = \"You are an AI assistant that follows instruction extremely\
          \ well. Help as much as you can.\"\nuser_message = \"\"\"You are tasked\
          \ with determining whether two records listed below are the same based on\
          \ the information provided.\nCarefully compare the {attribute 1}, {attribute\
          \ 2}... for each record before making your decision.  \nNote: Missing values\
          \ (N/A or \\\"nan\\\") should not be used as a basis for your decision.\
          \  \nRecord A: [{attribute 1}: {attribute 1 value}, {attribute 2}: {attribute\
          \ 2 value}, ...]\nRecord B: [{attribute 1}: {attribute 1 value}, {attribute\
          \ 2}: {attribute 2 value}, ...]  \nAre record A and record B the same entity?\
          \ Choose your answer from: [Yes, No].\n\"\"\"\n\nprompt = f\"{system_message}\\\
          n\\n### Instruction:\\n\\n{user_message}\\n\\n### Response:\\n\\n\"\noutputs\
          \ = model.generate(prompt, sampling_params)\nresponse = outputs[0].outputs[0].text.strip()\n\
          print(response)\n\n\nCode Snippet 2 for Transformers:\n\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\nimport torch\n\
          \nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device\
          \ = \"cpu\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    {path_to_model},\n\
          \    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\ntokenizer\
          \ = AutoTokenizer.from_pretrained({path_to_model})\n\nprompt = f\"{system_message}\\\
          n\\n### Instruction:\\n\\n{user_message}\\n\\n### Response:\\n\\n\"\ninputs\
          \ = tokenizer(prompt, return_tensors=\"pt\")\ninput_ids = inputs[\"input_ids\"\
          ].to(device)\n\ngeneration_config = GenerationConfig(\n    do_samples=True,\n\
          \    temperature=0.35,\n    top_p=0.9,\n)\n\nwith torch.no_grad():\n   \
          \ generation_output = model.generate(\n        input_ids=input_ids,\n  \
          \      generation_config=generation_config,\n        return_dict_in_generate=True,\n\
          \        output_scores=True,\n        max_new_tokens=1024,\n        pad_token_id=tokenizer.eos_token_id,\n\
          \        repetition_penalty=1.15,\n    )\noutput = generation_output[0]\n\
          response = tokenizer.decode(\n    output[:, input_ids.shape[-1] :][0], skip_special_tokens=True\n\
          ).strip()\n\nprint(response)\n\n\n\nWe hope these scripts will resolve the\
          \ issues you're facing. Your participation in testing Jellyfish is immensely\
          \ valuable to us, and we encourage you to reach out again should you have\
          \ any further questions or need additional assistance.\n\nBest regards,\n\
          \nHaochen, NECOUDBFM"
        updatedAt: '2024-01-24T05:24:07.508Z'
      numEdits: 0
      reactions: []
    id: 65b09ef725c7e48fd052164c
    type: comment
  author: HCZhang
  content: "Hello Zeyu,\n\nThank you for reaching out and expressing your interest\
    \ in our model. We apologize for the difficulties you've encountered.\n\nFrom\
    \ the code you've shared, it appears that you're attempting to load Jellyfish\
    \ directly via the Hugging Face API. Unfortunately, we don\u2019t have direct\
    \ control over how the model is handled during this process. Therefore, we strongly\
    \ recommend manually downloading the model files before attempting to use Jellyfish\
    \ for inference.\n\nTo assist you further, we are providing two scripts specifically\
    \ designed for inference. One is tailored for the vLLM module (https://github.com/vllm-project)\
    \ and the other for the Transformers module (https://huggingface.co/docs/transformers/index).\
    \  \nNote that vLLM has demonstrated significantly improved inference efficiency.\n\
    \n\n\nCode Snippet 1 for vLLM:\n\nfrom vllm import LLM, SamplingParams\n\nmodel\
    \ = LLM(model={path to Jellyfish files, e.g. \"/workspace/Jellyfish/\"})\nsampling_params\
    \ = SamplingParams(\n        temperature={temperature, e.g. 0.35},\n        top_p={top_p,\
    \ e.g. 0.9},\n        max_tokens={max new token, e.g. 1024},\n        stop=[\"\
    ### Instruction:\"],\n    )\n\nsystem_message = \"You are an AI assistant that\
    \ follows instruction extremely well. Help as much as you can.\"\nuser_message\
    \ = \"\"\"You are tasked with determining whether two records listed below are\
    \ the same based on the information provided.\nCarefully compare the {attribute\
    \ 1}, {attribute 2}... for each record before making your decision.  \nNote: Missing\
    \ values (N/A or \\\"nan\\\") should not be used as a basis for your decision.\
    \  \nRecord A: [{attribute 1}: {attribute 1 value}, {attribute 2}: {attribute\
    \ 2 value}, ...]\nRecord B: [{attribute 1}: {attribute 1 value}, {attribute 2}:\
    \ {attribute 2 value}, ...]  \nAre record A and record B the same entity? Choose\
    \ your answer from: [Yes, No].\n\"\"\"\n\nprompt = f\"{system_message}\\n\\n###\
    \ Instruction:\\n\\n{user_message}\\n\\n### Response:\\n\\n\"\noutputs = model.generate(prompt,\
    \ sampling_params)\nresponse = outputs[0].outputs[0].text.strip()\nprint(response)\n\
    \n\nCode Snippet 2 for Transformers:\n\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer, GenerationConfig\nimport torch\n\nif torch.cuda.is_available():\n\
    \    device = \"cuda\"\nelse:\n    device = \"cpu\"\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    {path_to_model},\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\
    ,\n)\ntokenizer = AutoTokenizer.from_pretrained({path_to_model})\n\nprompt = f\"\
    {system_message}\\n\\n### Instruction:\\n\\n{user_message}\\n\\n### Response:\\\
    n\\n\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\ninput_ids = inputs[\"\
    input_ids\"].to(device)\n\ngeneration_config = GenerationConfig(\n    do_samples=True,\n\
    \    temperature=0.35,\n    top_p=0.9,\n)\n\nwith torch.no_grad():\n    generation_output\
    \ = model.generate(\n        input_ids=input_ids,\n        generation_config=generation_config,\n\
    \        return_dict_in_generate=True,\n        output_scores=True,\n        max_new_tokens=1024,\n\
    \        pad_token_id=tokenizer.eos_token_id,\n        repetition_penalty=1.15,\n\
    \    )\noutput = generation_output[0]\nresponse = tokenizer.decode(\n    output[:,\
    \ input_ids.shape[-1] :][0], skip_special_tokens=True\n).strip()\n\nprint(response)\n\
    \n\n\nWe hope these scripts will resolve the issues you're facing. Your participation\
    \ in testing Jellyfish is immensely valuable to us, and we encourage you to reach\
    \ out again should you have any further questions or need additional assistance.\n\
    \nBest regards,\n\nHaochen, NECOUDBFM"
  created_at: 2024-01-24 05:24:07+00:00
  edited: false
  hidden: false
  id: 65b09ef725c7e48fd052164c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/070fb6d792ef5907dd62d10019cab50a.svg
      fullname: Haochen Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: HCZhang
      type: user
    createdAt: '2024-01-24T05:30:16.000Z'
    data:
      edited: false
      editors:
      - HCZhang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8865094184875488
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/070fb6d792ef5907dd62d10019cab50a.svg
          fullname: Haochen Zhang
          isHf: false
          isPro: false
          name: HCZhang
          type: user
        html: '<p>Additionally, we want to inform you that we will soon provide code
          examples for accessing the Jellyfish model directly using the Hugging Face
          API. These examples will be included in the model card. If you prefer this
          approach, we kindly ask for your patience as we prepare this documentation.</p>

          <p>Best regards,</p>

          <p>Haochen, NECOUDBFM</p>

          '
        raw: 'Additionally, we want to inform you that we will soon provide code examples
          for accessing the Jellyfish model directly using the Hugging Face API. These
          examples will be included in the model card. If you prefer this approach,
          we kindly ask for your patience as we prepare this documentation.


          Best regards,


          Haochen, NECOUDBFM'
        updatedAt: '2024-01-24T05:30:16.331Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ZeyuZhang
    id: 65b0a06838a0c8f7703677ef
    type: comment
  author: HCZhang
  content: 'Additionally, we want to inform you that we will soon provide code examples
    for accessing the Jellyfish model directly using the Hugging Face API. These examples
    will be included in the model card. If you prefer this approach, we kindly ask
    for your patience as we prepare this documentation.


    Best regards,


    Haochen, NECOUDBFM'
  created_at: 2024-01-24 05:30:16+00:00
  edited: false
  hidden: false
  id: 65b0a06838a0c8f7703677ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/89028c084d8012a23ec547b71f9aa5ba.svg
      fullname: Yuyang Dong
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yuyangdong
      type: user
    createdAt: '2024-01-24T13:09:25.000Z'
    data:
      edited: false
      editors:
      - yuyangdong
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7665351033210754
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/89028c084d8012a23ec547b71f9aa5ba.svg
          fullname: Yuyang Dong
          isHf: false
          isPro: false
          name: yuyangdong
          type: user
        html: '<p>Please see: <a href="https://huggingface.co/NECOUDBFM/Jellyfish#using-transformers-and-torch-modules">https://huggingface.co/NECOUDBFM/Jellyfish#using-transformers-and-torch-modules</a><br>for
          inference with the orginal huggingface transformers.</p>

          '
        raw: 'Please see: https://huggingface.co/NECOUDBFM/Jellyfish#using-transformers-and-torch-modules

          for inference with the orginal huggingface transformers.'
        updatedAt: '2024-01-24T13:09:25.505Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ZeyuZhang
    id: 65b10c056eaa79e3582f659e
    type: comment
  author: yuyangdong
  content: 'Please see: https://huggingface.co/NECOUDBFM/Jellyfish#using-transformers-and-torch-modules

    for inference with the orginal huggingface transformers.'
  created_at: 2024-01-24 13:09:25+00:00
  edited: false
  hidden: false
  id: 65b10c056eaa79e3582f659e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65b00730403a23a2fd765110/Uw-obs-VymyU9iMVKLURZ.jpeg?w=200&h=200&f=face
      fullname: Zeyu Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZeyuZhang
      type: user
    createdAt: '2024-01-24T13:27:24.000Z'
    data:
      edited: false
      editors:
      - ZeyuZhang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9348522424697876
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65b00730403a23a2fd765110/Uw-obs-VymyU9iMVKLURZ.jpeg?w=200&h=200&f=face
          fullname: Zeyu Zhang
          isHf: false
          isPro: false
          name: ZeyuZhang
          type: user
        html: '<p>Hi Haochen and Yuyang,<br>Thank you so much for your kind help!
          Actually I just managed to download the source models from Huggingface Repo
          as you suggested before, I will test the model in my upcoming endeavors.</p>

          <p>Thank you again.<br>Best,<br>Zeyu</p>

          '
        raw: 'Hi Haochen and Yuyang,

          Thank you so much for your kind help! Actually I just managed to download
          the source models from Huggingface Repo as you suggested before, I will
          test the model in my upcoming endeavors.


          Thank you again.

          Best,

          Zeyu'
        updatedAt: '2024-01-24T13:27:24.711Z'
      numEdits: 0
      reactions: []
    id: 65b1103ca46e8751c45afb33
    type: comment
  author: ZeyuZhang
  content: 'Hi Haochen and Yuyang,

    Thank you so much for your kind help! Actually I just managed to download the
    source models from Huggingface Repo as you suggested before, I will test the model
    in my upcoming endeavors.


    Thank you again.

    Best,

    Zeyu'
  created_at: 2024-01-24 13:27:24+00:00
  edited: false
  hidden: false
  id: 65b1103ca46e8751c45afb33
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: NECOUDBFM/Jellyfish-13B
repo_type: model
status: open
target_branch: null
title: How to apply the model for inferencing
