!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bilelomrani
conflicting_files: null
created_at: 2022-10-23 00:10:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650962849424-61fbe1550ec9c25531549928.jpeg?w=200&h=200&f=face
      fullname: Bilel Omrani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bilelomrani
      type: user
    createdAt: '2022-10-23T01:10:02.000Z'
    data:
      edited: false
      editors:
      - bilelomrani
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650962849424-61fbe1550ec9c25531549928.jpeg?w=200&h=200&f=face
          fullname: Bilel Omrani
          isHf: false
          isPro: false
          name: bilelomrani
          type: user
        html: '<p>The <code>truncation=True</code> parameter with camembert-large''s
          tokenizer does not seem to have any effect. When running this example:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"camembert/camembert-large"</span>)

          tokenizer([<span class="hljs-string">"Some long piece of text"</span>, <span
          class="hljs-string">"Some other long piece of text"</span>], padding=<span
          class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>,
          return_tensors=<span class="hljs-string">"pt"</span>)

          </code></pre>

          <p>the following warning is issued</p>

          <pre><code>Asking to truncate to max_length but no maximum length is provided
          and the model has no predefined maximum length. Default to no truncation.

          </code></pre>

          <p>The inference thus causes an exception on long sentences because the
          tokenizer fails to truncate the input to 512 tokens.</p>

          '
        raw: "The `truncation=True` parameter with camembert-large's tokenizer does\
          \ not seem to have any effect. When running this example:\r\n```python\r\
          \nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          camembert/camembert-large\")\r\ntokenizer([\"Some long piece of text\",\
          \ \"Some other long piece of text\"], padding=True, truncation=True, return_tensors=\"\
          pt\")\r\n```\r\nthe following warning is issued\r\n```\r\nAsking to truncate\
          \ to max_length but no maximum length is provided and the model has no predefined\
          \ maximum length. Default to no truncation.\r\n```\r\nThe inference thus\
          \ causes an exception on long sentences because the tokenizer fails to truncate\
          \ the input to 512 tokens."
        updatedAt: '2022-10-23T01:10:02.218Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Riep
        - AngledLuffa
        - bourdoiscatie
    id: 6354946a74026d8c3afd5c47
    type: comment
  author: bilelomrani
  content: "The `truncation=True` parameter with camembert-large's tokenizer does\
    \ not seem to have any effect. When running this example:\r\n```python\r\nfrom\
    \ transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    camembert/camembert-large\")\r\ntokenizer([\"Some long piece of text\", \"Some\
    \ other long piece of text\"], padding=True, truncation=True, return_tensors=\"\
    pt\")\r\n```\r\nthe following warning is issued\r\n```\r\nAsking to truncate to\
    \ max_length but no maximum length is provided and the model has no predefined\
    \ maximum length. Default to no truncation.\r\n```\r\nThe inference thus causes\
    \ an exception on long sentences because the tokenizer fails to truncate the input\
    \ to 512 tokens."
  created_at: 2022-10-23 00:10:02+00:00
  edited: false
  hidden: false
  id: 6354946a74026d8c3afd5c47
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
      fullname: John Bauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AngledLuffa
      type: user
    createdAt: '2023-05-17T23:40:25.000Z'
    data:
      edited: false
      editors:
      - AngledLuffa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
          fullname: John Bauer
          isHf: false
          isPro: false
          name: AngledLuffa
          type: user
        html: '<p>Agreed, it would be better if the tokenizer config included <code>512</code>
          as the limit</p>

          '
        raw: Agreed, it would be better if the tokenizer config included `512` as
          the limit
        updatedAt: '2023-05-17T23:40:25.842Z'
      numEdits: 0
      reactions: []
    id: 646565e986e668ad22e9aad7
    type: comment
  author: AngledLuffa
  content: Agreed, it would be better if the tokenizer config included `512` as the
    limit
  created_at: 2023-05-17 22:40:25+00:00
  edited: false
  hidden: false
  id: 646565e986e668ad22e9aad7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
      fullname: John Bauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AngledLuffa
      type: user
    createdAt: '2023-05-17T23:44:09.000Z'
    data:
      edited: false
      editors:
      - AngledLuffa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
          fullname: John Bauer
          isHf: false
          isPro: false
          name: AngledLuffa
          type: user
        html: "<p>however, <span data-props=\"{&quot;user&quot;:&quot;bilelomrani&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/bilelomrani\"\
          >@<span class=\"underline\">bilelomrani</span></a></span>\n\n\t</span></span>,\
          \ a workaround is to set it yourself after loading the tokenizer</p>\n<pre><code>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"camembert/camembert-large\")\ntokenizer.model_max_length\
          \ = 512\n</code></pre>\n"
        raw: 'however, @bilelomrani, a workaround is to set it yourself after loading
          the tokenizer


          ```

          tokenizer = AutoTokenizer.from_pretrained("camembert/camembert-large")

          tokenizer.model_max_length = 512

          ```'
        updatedAt: '2023-05-17T23:44:09.509Z'
      numEdits: 0
      reactions: []
    id: 646566c9a0748f9aa4cb18ef
    type: comment
  author: AngledLuffa
  content: 'however, @bilelomrani, a workaround is to set it yourself after loading
    the tokenizer


    ```

    tokenizer = AutoTokenizer.from_pretrained("camembert/camembert-large")

    tokenizer.model_max_length = 512

    ```'
  created_at: 2023-05-17 22:44:09+00:00
  edited: false
  hidden: false
  id: 646566c9a0748f9aa4cb18ef
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: almanach/camembert-large
repo_type: model
status: open
target_branch: null
title: No maximum length is provided
