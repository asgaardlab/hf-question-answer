!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CR2022
conflicting_files: null
created_at: 2023-06-25 00:58:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-06-25T01:58:13.000Z'
    data:
      edited: true
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3510094881057739
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>2023-06-25 03:55:27 INFO:Cache capacity is 0 bytes<br>llama.cpp:
          loading model from models/wizardlm-33b-v1.0-uncensored.ggmlv3.q5_K_S/wizardlm-33b-v1.0-uncensored.ggmlv3.q5_K_S.bin<br>llama_model_load_internal:
          format     = ggjt v3 (latest)<br>llama_model_load_internal: n_vocab    =
          32000<br>llama_model_load_internal: n_ctx      = 2048<br>llama_model_load_internal:
          n_embd     = 6656<br>llama_model_load_internal: n_mult     = 256<br>llama_model_load_internal:
          n_head     = 52<br>llama_model_load_internal: n_layer    = 60<br>llama_model_load_internal:
          n_rot      = 128<br>llama_model_load_internal: ftype      = 16 (mostly Q5_K
          - Small)<br>llama_model_load_internal: n_ff       = 17920<br>llama_model_load_internal:
          n_parts    = 1<br>llama_model_load_internal: model size = 30B<br>llama_model_load_internal:
          ggml ctx size =    0.13 MB<br>llama_model_load_internal: mem required  =
          23661.29 MB (+ 3124.00 MB per state)<br>....................................................................................................<br>llama_init_from_file:
          kv self size  = 3120.00 MB<br>AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI
          = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA
          = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |<br>2023-06-25 03:55:28
          INFO:Loaded the model in 0.60 seconds.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/F_fS4mGvQQOEj0rgEyWmm.png"><img
          alt="5266324524.png" src="https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/F_fS4mGvQQOEj0rgEyWmm.png"></a><br>Traceback
          (most recent call last):<br>  File "/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/routes.py",
          line 427, in run_predict<br>    output = await app.get_blocks().process_api(<br>  File
          "/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/blocks.py",
          line 1323, in process_api<br>    result = await self.call_function(<br>  File
          "/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/blocks.py",
          line 1067, in call_function<br>    prediction = await utils.async_iteration(iterator)<br>  File
          "/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/utils.py",
          line 336, in async_iteration<br>    return await iterator.<strong>anext</strong>()<br>  File
          "/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/utils.py",
          line 329, in <strong>anext</strong><br>    return await anyio.to_thread.run_sync(<br>  File
          "/root/miniconda3/envs/textgen/lib/python3.10/site-packages/anyio/to_thread.py",
          line 33, in run_sync<br>    return await get_asynclib().run_sync_in_worker_thread(<br>  File
          "/root/miniconda3/envs/textgen/lib/python3.10/site-packages/anyio/_backends/_asyncio.py",
          line 877, in run_sync_in_worker_thread<br>    return await future<br>  File
          "/root/miniconda3/envs/textgen/lib/python3.10/site-packages/anyio/_backends/_asyncio.py",
          line 807, in run<br>    result = context.run(func, *args)<br>  File "/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/utils.py",
          line 312, in run_sync_iterator_async<br>    return next(iterator)<br>  File
          "/root/text-generation-webui/modules/chat.py", line 295, in generate_chat_reply_wrapper<br>    for
          i, history in enumerate(generate_chat_reply(text, shared.history, state,
          regenerate, _continue, loading_message=True)):<br>  File "/root/text-generation-webui/modules/chat.py",
          line 280, in generate_chat_reply<br>    for history in chatbot_wrapper(text,
          history, state, regenerate=regenerate, _continue=_continue, loading_message=loading_message):<br>  File
          "/root/text-generation-webui/modules/chat.py", line 164, in chatbot_wrapper<br>    stopping_strings
          = get_stopping_strings(state)<br>  File "/root/text-generation-webui/modules/chat.py",
          line 128, in get_stopping_strings<br>    state[''turn_template''].split(''&lt;|user-message|&gt;'')[1].split(''&lt;|bot|&gt;'')[0]
          + ''&lt;|bot|&gt;'',<br>IndexError: list index out of range</p>

          '
        raw: "2023-06-25 03:55:27 INFO:Cache capacity is 0 bytes\nllama.cpp: loading\
          \ model from models/wizardlm-33b-v1.0-uncensored.ggmlv3.q5_K_S/wizardlm-33b-v1.0-uncensored.ggmlv3.q5_K_S.bin\n\
          llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
          \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
          \ n_embd     = 6656\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
          \ n_head     = 52\nllama_model_load_internal: n_layer    = 60\nllama_model_load_internal:\
          \ n_rot      = 128\nllama_model_load_internal: ftype      = 16 (mostly Q5_K\
          \ - Small)\nllama_model_load_internal: n_ff       = 17920\nllama_model_load_internal:\
          \ n_parts    = 1\nllama_model_load_internal: model size = 30B\nllama_model_load_internal:\
          \ ggml ctx size =    0.13 MB\nllama_model_load_internal: mem required  =\
          \ 23661.29 MB (+ 3124.00 MB per state)\n....................................................................................................\n\
          llama_init_from_file: kv self size  = 3120.00 MB\nAVX = 1 | AVX2 = 1 | AVX512\
          \ = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA\
          \ = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX\
          \ = 0 |\n2023-06-25 03:55:28 INFO:Loaded the model in 0.60 seconds.\n![5266324524.png](https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/F_fS4mGvQQOEj0rgEyWmm.png)\n\
          Traceback (most recent call last):\n  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/routes.py\"\
          , line 427, in run_predict\n    output = await app.get_blocks().process_api(\n\
          \  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/blocks.py\"\
          , line 1323, in process_api\n    result = await self.call_function(\n  File\
          \ \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/blocks.py\"\
          , line 1067, in call_function\n    prediction = await utils.async_iteration(iterator)\n\
          \  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/utils.py\"\
          , line 336, in async_iteration\n    return await iterator.__anext__()\n\
          \  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/utils.py\"\
          , line 329, in __anext__\n    return await anyio.to_thread.run_sync(\n \
          \ File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/anyio/to_thread.py\"\
          , line 33, in run_sync\n    return await get_asynclib().run_sync_in_worker_thread(\n\
          \  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\"\
          , line 877, in run_sync_in_worker_thread\n    return await future\n  File\
          \ \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\"\
          , line 807, in run\n    result = context.run(func, *args)\n  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/utils.py\"\
          , line 312, in run_sync_iterator_async\n    return next(iterator)\n  File\
          \ \"/root/text-generation-webui/modules/chat.py\", line 295, in generate_chat_reply_wrapper\n\
          \    for i, history in enumerate(generate_chat_reply(text, shared.history,\
          \ state, regenerate, _continue, loading_message=True)):\n  File \"/root/text-generation-webui/modules/chat.py\"\
          , line 280, in generate_chat_reply\n    for history in chatbot_wrapper(text,\
          \ history, state, regenerate=regenerate, _continue=_continue, loading_message=loading_message):\n\
          \  File \"/root/text-generation-webui/modules/chat.py\", line 164, in chatbot_wrapper\n\
          \    stopping_strings = get_stopping_strings(state)\n  File \"/root/text-generation-webui/modules/chat.py\"\
          , line 128, in get_stopping_strings\n    state['turn_template'].split('<|user-message|>')[1].split('<|bot|>')[0]\
          \ + '<|bot|>',\nIndexError: list index out of range\n"
        updatedAt: '2023-06-25T02:26:45.265Z'
      numEdits: 2
      reactions: []
    id: 64979f3588c7ac7d71842f38
    type: comment
  author: CR2022
  content: "2023-06-25 03:55:27 INFO:Cache capacity is 0 bytes\nllama.cpp: loading\
    \ model from models/wizardlm-33b-v1.0-uncensored.ggmlv3.q5_K_S/wizardlm-33b-v1.0-uncensored.ggmlv3.q5_K_S.bin\n\
    llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
    \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
    \ n_embd     = 6656\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
    \ n_head     = 52\nllama_model_load_internal: n_layer    = 60\nllama_model_load_internal:\
    \ n_rot      = 128\nllama_model_load_internal: ftype      = 16 (mostly Q5_K -\
    \ Small)\nllama_model_load_internal: n_ff       = 17920\nllama_model_load_internal:\
    \ n_parts    = 1\nllama_model_load_internal: model size = 30B\nllama_model_load_internal:\
    \ ggml ctx size =    0.13 MB\nllama_model_load_internal: mem required  = 23661.29\
    \ MB (+ 3124.00 MB per state)\n....................................................................................................\n\
    llama_init_from_file: kv self size  = 3120.00 MB\nAVX = 1 | AVX2 = 1 | AVX512\
    \ = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 |\
    \ F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\n2023-06-25\
    \ 03:55:28 INFO:Loaded the model in 0.60 seconds.\n![5266324524.png](https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/F_fS4mGvQQOEj0rgEyWmm.png)\n\
    Traceback (most recent call last):\n  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/routes.py\"\
    , line 427, in run_predict\n    output = await app.get_blocks().process_api(\n\
    \  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/blocks.py\"\
    , line 1323, in process_api\n    result = await self.call_function(\n  File \"\
    /root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/blocks.py\"\
    , line 1067, in call_function\n    prediction = await utils.async_iteration(iterator)\n\
    \  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/utils.py\"\
    , line 336, in async_iteration\n    return await iterator.__anext__()\n  File\
    \ \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/utils.py\"\
    , line 329, in __anext__\n    return await anyio.to_thread.run_sync(\n  File \"\
    /root/miniconda3/envs/textgen/lib/python3.10/site-packages/anyio/to_thread.py\"\
    , line 33, in run_sync\n    return await get_asynclib().run_sync_in_worker_thread(\n\
    \  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\"\
    , line 877, in run_sync_in_worker_thread\n    return await future\n  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\"\
    , line 807, in run\n    result = context.run(func, *args)\n  File \"/root/miniconda3/envs/textgen/lib/python3.10/site-packages/gradio/utils.py\"\
    , line 312, in run_sync_iterator_async\n    return next(iterator)\n  File \"/root/text-generation-webui/modules/chat.py\"\
    , line 295, in generate_chat_reply_wrapper\n    for i, history in enumerate(generate_chat_reply(text,\
    \ shared.history, state, regenerate, _continue, loading_message=True)):\n  File\
    \ \"/root/text-generation-webui/modules/chat.py\", line 280, in generate_chat_reply\n\
    \    for history in chatbot_wrapper(text, history, state, regenerate=regenerate,\
    \ _continue=_continue, loading_message=loading_message):\n  File \"/root/text-generation-webui/modules/chat.py\"\
    , line 164, in chatbot_wrapper\n    stopping_strings = get_stopping_strings(state)\n\
    \  File \"/root/text-generation-webui/modules/chat.py\", line 128, in get_stopping_strings\n\
    \    state['turn_template'].split('<|user-message|>')[1].split('<|bot|>')[0] +\
    \ '<|bot|>',\nIndexError: list index out of range\n"
  created_at: 2023-06-25 00:58:13+00:00
  edited: true
  hidden: false
  id: 64979f3588c7ac7d71842f38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-06-25T10:31:50.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8241762518882751
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>with llama.cpp works fine</p>

          '
        raw: with llama.cpp works fine
        updatedAt: '2023-06-25T10:31:50.568Z'
      numEdits: 0
      reactions: []
    id: 64981796e486365ca6bd8607
    type: comment
  author: mirek190
  content: with llama.cpp works fine
  created_at: 2023-06-25 09:31:50+00:00
  edited: false
  hidden: false
  id: 64981796e486365ca6bd8607
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-06-25T18:11:11.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8907073140144348
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>It should also work with the latest version of text generation according
          to the model card:</p>

          <p>"They are now also compatible with recent releases of text-generation-webui,
          KoboldCpp, llama-cpp-python and ctransformers. Other tools and libraries
          may or may not be compatible - check their documentation if in doubt."</p>

          '
        raw: 'It should also work with the latest version of text generation according
          to the model card:


          "They are now also compatible with recent releases of text-generation-webui,
          KoboldCpp, llama-cpp-python and ctransformers. Other tools and libraries
          may or may not be compatible - check their documentation if in doubt."'
        updatedAt: '2023-06-25T18:11:11.826Z'
      numEdits: 0
      reactions: []
    id: 6498833f289f760ae8ffc36e
    type: comment
  author: CR2022
  content: 'It should also work with the latest version of text generation according
    to the model card:


    "They are now also compatible with recent releases of text-generation-webui, KoboldCpp,
    llama-cpp-python and ctransformers. Other tools and libraries may or may not be
    compatible - check their documentation if in doubt."'
  created_at: 2023-06-25 17:11:11+00:00
  edited: false
  hidden: false
  id: 6498833f289f760ae8ffc36e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-25T23:17:29.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8680460453033447
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes works fine with text-generation-webui</p>

          <p>That error you''ve got is a bug with the prompt template in the UI, not
          related to the model itself. Try updating text-generation-webui if you haven''t
          already, it might be fixed by now.</p>

          '
        raw: 'Yes works fine with text-generation-webui


          That error you''ve got is a bug with the prompt template in the UI, not
          related to the model itself. Try updating text-generation-webui if you haven''t
          already, it might be fixed by now.'
        updatedAt: '2023-06-25T23:17:29.915Z'
      numEdits: 0
      reactions: []
    id: 6498cb093756a91498edf2de
    type: comment
  author: TheBloke
  content: 'Yes works fine with text-generation-webui


    That error you''ve got is a bug with the prompt template in the UI, not related
    to the model itself. Try updating text-generation-webui if you haven''t already,
    it might be fixed by now.'
  created_at: 2023-06-25 22:17:29+00:00
  edited: false
  hidden: false
  id: 6498cb093756a91498edf2de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-06-26T01:25:32.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8498210906982422
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<blockquote>

          <p>Yes works fine with text-generation-webui</p>

          <p>That error you''ve got is a bug with the prompt template in the UI, not
          related to the model itself. Try updating text-generation-webui if you haven''t
          already, it might be fixed by now.</p>

          </blockquote>

          <p>Ok thank you I will check it out later today when I can launch text-generation-webui
          and if it works I will close this discussion.</p>

          '
        raw: "> Yes works fine with text-generation-webui\n> \n> That error you've\
          \ got is a bug with the prompt template in the UI, not related to the model\
          \ itself. Try updating text-generation-webui if you haven't already, it\
          \ might be fixed by now.\n\nOk thank you I will check it out later today\
          \ when I can launch text-generation-webui and if it works I will close this\
          \ discussion."
        updatedAt: '2023-06-26T01:25:32.455Z'
      numEdits: 0
      reactions: []
    id: 6498e90c7c15c218be8df0d8
    type: comment
  author: CR2022
  content: "> Yes works fine with text-generation-webui\n> \n> That error you've got\
    \ is a bug with the prompt template in the UI, not related to the model itself.\
    \ Try updating text-generation-webui if you haven't already, it might be fixed\
    \ by now.\n\nOk thank you I will check it out later today when I can launch text-generation-webui\
    \ and if it works I will close this discussion."
  created_at: 2023-06-26 00:25:32+00:00
  edited: false
  hidden: false
  id: 6498e90c7c15c218be8df0d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-06-27T03:21:26.000Z'
    data:
      edited: true
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.85208660364151
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<blockquote>

          <p>Yes works fine with text-generation-webui</p>

          <p>That error you''ve got is a bug with the prompt template in the UI, not
          related to the model itself. Try updating text-generation-webui if you haven''t
          already, it might be fixed by now.</p>

          </blockquote>

          <p>Text generation web ui is all up to date but still the same error in
          terminal.</p>

          <p>When I check the chat settings/instruction template it gives errors in
          red. I tried the correct modes  chat-instruct and instruct but both will
          give red errors.</p>

          <p>The model works in chat function so the problem must be with text generation
          web ui and not the model.</p>

          '
        raw: "> Yes works fine with text-generation-webui\n> \n> That error you've\
          \ got is a bug with the prompt template in the UI, not related to the model\
          \ itself. Try updating text-generation-webui if you haven't already, it\
          \ might be fixed by now.\n\nText generation web ui is all up to date but\
          \ still the same error in terminal.\n\nWhen I check the chat settings/instruction\
          \ template it gives errors in red. I tried the correct modes  chat-instruct\
          \ and instruct but both will give red errors.\n\nThe model works in chat\
          \ function so the problem must be with text generation web ui and not the\
          \ model."
        updatedAt: '2023-06-27T03:38:35.985Z'
      numEdits: 2
      reactions: []
    id: 649a55b6f57df33a56a916f5
    type: comment
  author: CR2022
  content: "> Yes works fine with text-generation-webui\n> \n> That error you've got\
    \ is a bug with the prompt template in the UI, not related to the model itself.\
    \ Try updating text-generation-webui if you haven't already, it might be fixed\
    \ by now.\n\nText generation web ui is all up to date but still the same error\
    \ in terminal.\n\nWhen I check the chat settings/instruction template it gives\
    \ errors in red. I tried the correct modes  chat-instruct and instruct but both\
    \ will give red errors.\n\nThe model works in chat function so the problem must\
    \ be with text generation web ui and not the model."
  created_at: 2023-06-27 02:21:26+00:00
  edited: true
  hidden: false
  id: 649a55b6f57df33a56a916f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-06-27T03:38:37.000Z'
    data:
      status: closed
    id: 649a59bd994b2637aa501ad5
    type: status-change
  author: CR2022
  created_at: 2023-06-27 02:38:37+00:00
  id: 649a59bd994b2637aa501ad5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardLM-33B-V1.0-Uncensored-GGML
repo_type: model
status: closed
target_branch: null
title: 'IndexError: list index out of range'
