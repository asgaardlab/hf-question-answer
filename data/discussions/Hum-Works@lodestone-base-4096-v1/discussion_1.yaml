!!python/object:huggingface_hub.community.DiscussionWithDetails
author: megil-paxton
conflicting_files: null
created_at: 2023-09-21 13:41:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d90dd805104a640d9f99732e747c904c.svg
      fullname: Megil Gallant
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: megil-paxton
      type: user
    createdAt: '2023-09-21T14:41:26.000Z'
    data:
      edited: false
      editors:
      - megil-paxton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6038471460342407
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d90dd805104a640d9f99732e747c904c.svg
          fullname: Megil Gallant
          isHf: false
          isPro: false
          name: megil-paxton
          type: user
        html: '<p>Is there a way to run the model on a cpu? I''ve tried passing device="cpu"
          to the SenteceTransformer but am getting this error:</p>

          <p>File ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501,
          in Module._call_impl(self, *args, **kwargs)<br>   1496 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1497 # this function,
          and just call forward.<br>   1498 if not (self._backward_hooks or self._backward_pre_hooks
          or self._forward_hooks or self._forward_pre_hooks<br>...<br>--&gt; 781 assert
          q.is_cuda and k.is_cuda and v.is_cuda<br>    782 softmax_scale = softmax_scale
          or 1.0 / math.sqrt(d)<br>    784 has_bias = bias is not None</p>

          '
        raw: "Is there a way to run the model on a cpu? I've tried passing device=\"\
          cpu\" to the SenteceTransformer but am getting this error:\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\r\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\r\n   1497 # this\
          \ function, and just call forward.\r\n   1498 if not (self._backward_hooks\
          \ or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\
          \n...\r\n--> 781 assert q.is_cuda and k.is_cuda and v.is_cuda\r\n    782\
          \ softmax_scale = softmax_scale or 1.0 / math.sqrt(d)\r\n    784 has_bias\
          \ = bias is not None"
        updatedAt: '2023-09-21T14:41:26.707Z'
      numEdits: 0
      reactions: []
    id: 650c5616e063bef6d9a456a5
    type: comment
  author: megil-paxton
  content: "Is there a way to run the model on a cpu? I've tried passing device=\"\
    cpu\" to the SenteceTransformer but am getting this error:\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\r\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\r\n   1497 # this function,\
    \ and just call forward.\r\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\r\n...\r\n--> 781 assert q.is_cuda\
    \ and k.is_cuda and v.is_cuda\r\n    782 softmax_scale = softmax_scale or 1.0\
    \ / math.sqrt(d)\r\n    784 has_bias = bias is not None"
  created_at: 2023-09-21 13:41:26+00:00
  edited: false
  hidden: false
  id: 650c5616e063bef6d9a456a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d90dd805104a640d9f99732e747c904c.svg
      fullname: Megil Gallant
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: megil-paxton
      type: user
    createdAt: '2023-09-21T14:48:39.000Z'
    data:
      edited: false
      editors:
      - megil-paxton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.15471918880939484
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d90dd805104a640d9f99732e747c904c.svg
          fullname: Megil Gallant
          isHf: false
          isPro: false
          name: megil-paxton
          type: user
        html: '<p>As a follow up, when I remove the trust_from_remote_source and revision
          numbers, embeddings are returned but with a message that makes me unsure
          of what model is actually being used to generate them:</p>

          <p>from sentence_transformers import SentenceTransformer</p>

          <p>model = SentenceTransformer(''Hum-Works/lodestone-base-4096-v1'')<br>sentences
          = ["This is an example sentence", "Each sentence is converted"]</p>

          <p>Response:</p>

          <p>Some weights of the model checkpoint at /home//.cache/torch/sentence_transformers/Hum-Works_lodestone-base-4096-v1/
          were not used when initializing BertModel: [''encoder.layer.8.mlp.gated_layers.weight'',
          ''encoder.layer.0.attention.self.Wqkv.weight'', ''encoder.layer.2.mlp.layernorm.bias'',
          ''encoder.layer.7.mlp.wo.bias'', ''encoder.layer.10.attention.self.Wqkv.bias'',
          ''encoder.layer.2.mlp.wo.bias'', ''encoder.layer.7.mlp.wo.weight'', ''encoder.layer.1.attention.self.Wqkv.bias'',
          ''encoder.layer.9.mlp.wo.weight'', ''encoder.layer.4.mlp.layernorm.weight'',
          ''encoder.layer.1.mlp.gated_layers.weight'', ''encoder.layer.10.mlp.wo.bias'',
          ''encoder.layer.0.mlp.layernorm.bias'', ''encoder.layer.2.mlp.wo.weight'',
          ''encoder.layer.7.mlp.gated_layers.weight'', ''encoder.layer.11.mlp.layernorm.bias'',
          ''encoder.layer.6.mlp.layernorm.weight'', ''encoder.layer.8.attention.self.Wqkv.weight'',
          ''encoder.layer.0.attention.self.Wqkv.bias'', ''encoder.layer.10.mlp.layernorm.weight'',
          ''encoder.layer.1.mlp.layernorm.bias'', ''encoder.layer.3.mlp.wo.weight'',
          ''encoder.layer.10.mlp.wo.weight'', ''encoder.layer.8.mlp.wo.bias'', ''encoder.layer.3.attention.self.Wqkv.weight'',
          ''encoder.layer.10.mlp.layernorm.bias'', ''encoder.layer.9.attention.self.Wqkv.bias'',
          ''encoder.layer.9.attention.self.Wqkv.weight'', ''encoder.layer.11.mlp.gated_layers.weight'',
          ''encoder.layer.3.mlp.wo.bias'', ''encoder.layer.0.mlp.layernorm.weight'',
          ''encoder.layer.8.attention.self.Wqkv.bias'', ''encoder.layer.10.mlp.gated_layers.weight'',
          ''encoder.layer.7.attention.self.Wqkv.weight'', ''encoder.layer.6.mlp.gated_layers.weight'',
          ''encoder.layer.6.mlp.wo.bias'', ''encoder.layer.5.mlp.layernorm.weight'',
          ''encoder.layer.7.mlp.layernorm.bias'', ''encoder.layer.5.mlp.layernorm.bias'',
          ''encoder.layer.8.mlp.layernorm.weight'', ''encoder.layer.5.mlp.wo.weight'',
          ''encoder.layer.3.mlp.layernorm.weight'', ''encoder.layer.1.mlp.wo.weight'',
          ''encoder.layer.4.mlp.gated_layers.weight'', ''encoder.layer.4.attention.self.Wqkv.bias'',
          ''encoder.layer.11.attention.self.Wqkv.bias'', ''encoder.layer.5.mlp.gated_layers.weight'',
          ''encoder.layer.1.attention.self.Wqkv.weight'', ''encoder.layer.5.attention.self.Wqkv.bias'',
          ''encoder.layer.6.mlp.wo.weight'', ''encoder.layer.0.mlp.wo.bias'', ''encoder.layer.2.attention.self.Wqkv.bias'',
          ''encoder.layer.5.attention.self.Wqkv.weight'', ''encoder.layer.6.mlp.layernorm.bias'',
          ''encoder.layer.1.mlp.layernorm.weight'', ''encoder.layer.4.mlp.wo.weight'',
          ''encoder.layer.10.attention.self.Wqkv.weight'', ''encoder.layer.9.mlp.layernorm.weight'',
          ''encoder.layer.7.mlp.layernorm.weight'', ''encoder.layer.9.mlp.gated_layers.weight'',
          ''encoder.layer.4.mlp.layernorm.bias'', ''encoder.layer.3.attention.self.Wqkv.bias'',
          ''encoder.layer.11.mlp.layernorm.weight'', ''encoder.layer.4.attention.self.Wqkv.weight'',
          ''encoder.layer.2.attention.self.Wqkv.weight'', ''encoder.layer.2.mlp.layernorm.weight'',
          ''encoder.layer.0.mlp.wo.weight'', ''encoder.layer.9.mlp.wo.bias'', ''encoder.layer.0.mlp.gated_layers.weight'',
          ''encoder.layer.4.mlp.wo.bias'', ''encoder.layer.3.mlp.gated_layers.weight'',
          ''encoder.layer.9.mlp.layernorm.bias'', ''encoder.layer.6.attention.self.Wqkv.bias'',
          ''encoder.layer.11.mlp.wo.bias'', ''encoder.layer.8.mlp.wo.weight'', ''encoder.layer.6.attention.self.Wqkv.weight'',
          ''encoder.layer.3.mlp.layernorm.bias'', ''encoder.layer.11.attention.self.Wqkv.weight'',
          ''encoder.layer.8.mlp.layernorm.bias'', ''encoder.layer.2.mlp.gated_layers.weight'',
          ''encoder.layer.7.attention.self.Wqkv.bias'', ''encoder.layer.11.mlp.wo.weight'',
          ''encoder.layer.1.mlp.wo.bias'', ''encoder.layer.5.mlp.wo.bias'']</p>

          <ul>

          <li>This IS expected if you are initializing BertModel from the checkpoint
          of a model trained on another task or with another architecture (e.g. initializing
          a BertForSequenceClassification model from a BertForPreTraining model).</li>

          <li>This IS NOT expected if you are initializing BertModel from the checkpoint
          of a model that you expect to be exactly identical (initializing a BertForSequenceClassification
          model from a BertForSequenceClassification model).<br>Some weights of BertModel
          were not initialized from the model checkpoint at /home//.cache/torch/sentence_transformers/Hum-Works_lodestone-base-4096-v1/
          and are newly initialized: [''encoder.layer.5.output.LayerNorm.bias'', ''encoder.layer.11.intermediate.dense.bias'',
          ''encoder.layer.2.attention.self.value.weight'', ''encoder.layer.1.output.LayerNorm.bias'',
          ''encoder.layer.0.attention.self.query.bias'', ''encoder.layer.0.attention.self.value.bias'',
          ''encoder.layer.10.attention.self.value.weight'', ''encoder.layer.7.attention.self.query.bias'',
          ''encoder.layer.0.intermediate.dense.weight'', ''encoder.layer.11.attention.self.value.bias'',
          ''encoder.layer.11.output.LayerNorm.bias'', ''encoder.layer.7.attention.self.value.weight'',
          ''encoder.layer.4.attention.self.query.weight'', ''encoder.layer.10.attention.self.key.weight'',
          ''encoder.layer.0.attention.self.value.weight'', ''encoder.layer.3.attention.self.key.bias'',
          ''encoder.layer.11.attention.self.key.weight'', ''encoder.layer.6.output.LayerNorm.bias'',
          ''encoder.layer.5.attention.self.value.weight'', ''encoder.layer.8.intermediate.dense.weight'',
          ''encoder.layer.1.output.LayerNorm.weight'', ''encoder.layer.5.intermediate.dense.weight'',
          ''encoder.layer.11.output.LayerNorm.weight'', ''embeddings.position_embeddings.weight'',
          ''encoder.layer.4.output.LayerNorm.bias'', ''encoder.layer.6.attention.self.value.bias'',
          ''encoder.layer.4.attention.self.key.bias'', ''encoder.layer.3.intermediate.dense.weight'',
          ''encoder.layer.4.attention.self.key.weight'', ''encoder.layer.11.attention.self.query.weight'',
          ''encoder.layer.10.intermediate.dense.weight'', ''encoder.layer.9.attention.self.key.weight'',
          ''encoder.layer.11.attention.self.key.bias'', ''encoder.layer.5.attention.self.query.weight'',
          ''encoder.layer.11.intermediate.dense.weight'', ''encoder.layer.4.output.dense.weight'',
          ''encoder.layer.6.attention.self.key.bias'', ''encoder.layer.10.attention.self.query.bias'',
          ''encoder.layer.11.output.dense.weight'', ''encoder.layer.10.attention.self.value.bias'',
          ''encoder.layer.6.output.dense.bias'', ''encoder.layer.9.output.LayerNorm.weight'',
          ''encoder.layer.4.attention.self.query.bias'', ''encoder.layer.2.attention.self.key.weight'',
          ''encoder.layer.7.output.LayerNorm.weight'', ''encoder.layer.0.output.LayerNorm.weight'',
          ''encoder.layer.1.intermediate.dense.bias'', ''encoder.layer.9.attention.self.value.weight'',
          ''encoder.layer.10.intermediate.dense.bias'', ''encoder.layer.4.intermediate.dense.weight'',
          ''encoder.layer.9.attention.self.query.bias'', ''encoder.layer.1.attention.self.query.weight'',
          ''encoder.layer.9.output.dense.weight'', ''encoder.layer.10.attention.self.query.weight'',
          ''encoder.layer.3.attention.self.key.weight'', ''encoder.layer.6.output.dense.weight'',
          ''encoder.layer.3.output.dense.weight'', ''encoder.layer.7.attention.self.value.bias'',
          ''encoder.layer.8.attention.self.value.bias'', ''encoder.layer.6.intermediate.dense.bias'',
          ''encoder.layer.5.output.dense.weight'', ''encoder.layer.8.intermediate.dense.bias'',
          ''encoder.layer.9.attention.self.query.weight'', ''encoder.layer.1.attention.self.query.bias'',
          ''encoder.layer.6.attention.self.key.weight'', ''encoder.layer.4.output.LayerNorm.weight'',
          ''encoder.layer.2.attention.self.query.bias'', ''encoder.layer.4.output.dense.bias'',
          ''encoder.layer.1.attention.self.key.bias'', ''encoder.layer.8.output.dense.weight'',
          ''encoder.layer.0.intermediate.dense.bias'', ''encoder.layer.2.output.dense.weight'',
          ''encoder.layer.3.intermediate.dense.bias'', ''encoder.layer.3.attention.self.value.bias'',
          ''encoder.layer.4.attention.self.value.weight'', ''encoder.layer.1.intermediate.dense.weight'',
          ''encoder.layer.1.output.dense.bias'', ''encoder.layer.6.attention.self.query.weight'',
          ''encoder.layer.6.output.LayerNorm.weight'', ''encoder.layer.5.intermediate.dense.bias'',
          ''encoder.layer.0.attention.self.query.weight'', ''encoder.layer.9.attention.self.key.bias'',
          ''encoder.layer.10.output.dense.weight'', ''encoder.layer.3.output.LayerNorm.weight'',
          ''encoder.layer.9.output.dense.bias'', ''encoder.layer.6.attention.self.value.weight'',
          ''encoder.layer.1.attention.self.key.weight'', ''encoder.layer.8.output.LayerNorm.weight'',
          ''encoder.layer.7.attention.self.key.weight'', ''encoder.layer.5.attention.self.value.bias'',
          ''encoder.layer.11.output.dense.bias'', ''encoder.layer.0.attention.self.key.weight'',
          ''encoder.layer.8.output.dense.bias'', ''encoder.layer.2.output.LayerNorm.weight'',
          ''encoder.layer.8.attention.self.query.weight'', ''encoder.layer.11.attention.self.value.weight'',
          ''encoder.layer.2.output.dense.bias'', ''encoder.layer.5.output.dense.bias'',
          ''encoder.layer.10.output.LayerNorm.bias'', ''encoder.layer.8.attention.self.value.weight'',
          ''encoder.layer.2.attention.self.key.bias'', ''encoder.layer.6.attention.self.query.bias'',
          ''encoder.layer.2.attention.self.query.weight'', ''encoder.layer.7.intermediate.dense.weight'',
          ''encoder.layer.5.attention.self.key.bias'', ''encoder.layer.1.output.dense.weight'',
          ''encoder.layer.7.output.LayerNorm.bias'', ''encoder.layer.7.output.dense.bias'',
          ''encoder.layer.8.attention.self.key.bias'', ''encoder.layer.9.attention.self.value.bias'',
          ''encoder.layer.5.attention.self.query.bias'', ''encoder.layer.8.attention.self.query.bias'',
          ''encoder.layer.8.output.LayerNorm.bias'', ''encoder.layer.0.output.dense.bias'',
          ''encoder.layer.5.attention.self.key.weight'', ''encoder.layer.11.attention.self.query.bias'',
          ''encoder.layer.2.attention.self.value.bias'', ''encoder.layer.9.output.LayerNorm.bias'',
          ''encoder.layer.7.intermediate.dense.bias'', ''encoder.layer.10.attention.self.key.bias'',
          ''encoder.layer.9.intermediate.dense.weight'', ''encoder.layer.6.intermediate.dense.weight'',
          ''encoder.layer.1.attention.self.value.bias'', ''encoder.layer.4.intermediate.dense.bias'',
          ''encoder.layer.4.attention.self.value.bias'', ''encoder.layer.1.attention.self.value.weight'',
          ''encoder.layer.0.output.LayerNorm.bias'', ''encoder.layer.0.attention.self.key.bias'',
          ''encoder.layer.7.attention.self.query.weight'', ''encoder.layer.2.intermediate.dense.bias'',
          ''encoder.layer.3.output.dense.bias'', ''encoder.layer.0.output.dense.weight'',
          ''encoder.layer.5.output.LayerNorm.weight'', ''encoder.layer.2.output.LayerNorm.bias'',
          ''encoder.layer.3.attention.self.query.weight'', ''encoder.layer.2.intermediate.dense.weight'',
          ''encoder.layer.7.output.dense.weight'', ''encoder.layer.9.intermediate.dense.bias'',
          ''encoder.layer.8.attention.self.key.weight'', ''encoder.layer.7.attention.self.key.bias'',
          ''encoder.layer.10.output.dense.bias'', ''encoder.layer.10.output.LayerNorm.weight'',
          ''encoder.layer.3.attention.self.value.weight'', ''encoder.layer.3.output.LayerNorm.bias'',
          ''encoder.layer.3.attention.self.query.bias'']<br>You should probably TRAIN
          this model on a down-stream task to be able to use it for predictions and
          inference.</li>

          </ul>

          '
        raw: 'As a follow up, when I remove the trust_from_remote_source and revision
          numbers, embeddings are returned but with a message that makes me unsure
          of what model is actually being used to generate them:


          from sentence_transformers import SentenceTransformer


          model = SentenceTransformer(''Hum-Works/lodestone-base-4096-v1'')

          sentences = ["This is an example sentence", "Each sentence is converted"]


          Response:


          Some weights of the model checkpoint at /home/<me>/.cache/torch/sentence_transformers/Hum-Works_lodestone-base-4096-v1/
          were not used when initializing BertModel: [''encoder.layer.8.mlp.gated_layers.weight'',
          ''encoder.layer.0.attention.self.Wqkv.weight'', ''encoder.layer.2.mlp.layernorm.bias'',
          ''encoder.layer.7.mlp.wo.bias'', ''encoder.layer.10.attention.self.Wqkv.bias'',
          ''encoder.layer.2.mlp.wo.bias'', ''encoder.layer.7.mlp.wo.weight'', ''encoder.layer.1.attention.self.Wqkv.bias'',
          ''encoder.layer.9.mlp.wo.weight'', ''encoder.layer.4.mlp.layernorm.weight'',
          ''encoder.layer.1.mlp.gated_layers.weight'', ''encoder.layer.10.mlp.wo.bias'',
          ''encoder.layer.0.mlp.layernorm.bias'', ''encoder.layer.2.mlp.wo.weight'',
          ''encoder.layer.7.mlp.gated_layers.weight'', ''encoder.layer.11.mlp.layernorm.bias'',
          ''encoder.layer.6.mlp.layernorm.weight'', ''encoder.layer.8.attention.self.Wqkv.weight'',
          ''encoder.layer.0.attention.self.Wqkv.bias'', ''encoder.layer.10.mlp.layernorm.weight'',
          ''encoder.layer.1.mlp.layernorm.bias'', ''encoder.layer.3.mlp.wo.weight'',
          ''encoder.layer.10.mlp.wo.weight'', ''encoder.layer.8.mlp.wo.bias'', ''encoder.layer.3.attention.self.Wqkv.weight'',
          ''encoder.layer.10.mlp.layernorm.bias'', ''encoder.layer.9.attention.self.Wqkv.bias'',
          ''encoder.layer.9.attention.self.Wqkv.weight'', ''encoder.layer.11.mlp.gated_layers.weight'',
          ''encoder.layer.3.mlp.wo.bias'', ''encoder.layer.0.mlp.layernorm.weight'',
          ''encoder.layer.8.attention.self.Wqkv.bias'', ''encoder.layer.10.mlp.gated_layers.weight'',
          ''encoder.layer.7.attention.self.Wqkv.weight'', ''encoder.layer.6.mlp.gated_layers.weight'',
          ''encoder.layer.6.mlp.wo.bias'', ''encoder.layer.5.mlp.layernorm.weight'',
          ''encoder.layer.7.mlp.layernorm.bias'', ''encoder.layer.5.mlp.layernorm.bias'',
          ''encoder.layer.8.mlp.layernorm.weight'', ''encoder.layer.5.mlp.wo.weight'',
          ''encoder.layer.3.mlp.layernorm.weight'', ''encoder.layer.1.mlp.wo.weight'',
          ''encoder.layer.4.mlp.gated_layers.weight'', ''encoder.layer.4.attention.self.Wqkv.bias'',
          ''encoder.layer.11.attention.self.Wqkv.bias'', ''encoder.layer.5.mlp.gated_layers.weight'',
          ''encoder.layer.1.attention.self.Wqkv.weight'', ''encoder.layer.5.attention.self.Wqkv.bias'',
          ''encoder.layer.6.mlp.wo.weight'', ''encoder.layer.0.mlp.wo.bias'', ''encoder.layer.2.attention.self.Wqkv.bias'',
          ''encoder.layer.5.attention.self.Wqkv.weight'', ''encoder.layer.6.mlp.layernorm.bias'',
          ''encoder.layer.1.mlp.layernorm.weight'', ''encoder.layer.4.mlp.wo.weight'',
          ''encoder.layer.10.attention.self.Wqkv.weight'', ''encoder.layer.9.mlp.layernorm.weight'',
          ''encoder.layer.7.mlp.layernorm.weight'', ''encoder.layer.9.mlp.gated_layers.weight'',
          ''encoder.layer.4.mlp.layernorm.bias'', ''encoder.layer.3.attention.self.Wqkv.bias'',
          ''encoder.layer.11.mlp.layernorm.weight'', ''encoder.layer.4.attention.self.Wqkv.weight'',
          ''encoder.layer.2.attention.self.Wqkv.weight'', ''encoder.layer.2.mlp.layernorm.weight'',
          ''encoder.layer.0.mlp.wo.weight'', ''encoder.layer.9.mlp.wo.bias'', ''encoder.layer.0.mlp.gated_layers.weight'',
          ''encoder.layer.4.mlp.wo.bias'', ''encoder.layer.3.mlp.gated_layers.weight'',
          ''encoder.layer.9.mlp.layernorm.bias'', ''encoder.layer.6.attention.self.Wqkv.bias'',
          ''encoder.layer.11.mlp.wo.bias'', ''encoder.layer.8.mlp.wo.weight'', ''encoder.layer.6.attention.self.Wqkv.weight'',
          ''encoder.layer.3.mlp.layernorm.bias'', ''encoder.layer.11.attention.self.Wqkv.weight'',
          ''encoder.layer.8.mlp.layernorm.bias'', ''encoder.layer.2.mlp.gated_layers.weight'',
          ''encoder.layer.7.attention.self.Wqkv.bias'', ''encoder.layer.11.mlp.wo.weight'',
          ''encoder.layer.1.mlp.wo.bias'', ''encoder.layer.5.mlp.wo.bias'']

          - This IS expected if you are initializing BertModel from the checkpoint
          of a model trained on another task or with another architecture (e.g. initializing
          a BertForSequenceClassification model from a BertForPreTraining model).

          - This IS NOT expected if you are initializing BertModel from the checkpoint
          of a model that you expect to be exactly identical (initializing a BertForSequenceClassification
          model from a BertForSequenceClassification model).

          Some weights of BertModel were not initialized from the model checkpoint
          at /home/<me>/.cache/torch/sentence_transformers/Hum-Works_lodestone-base-4096-v1/
          and are newly initialized: [''encoder.layer.5.output.LayerNorm.bias'', ''encoder.layer.11.intermediate.dense.bias'',
          ''encoder.layer.2.attention.self.value.weight'', ''encoder.layer.1.output.LayerNorm.bias'',
          ''encoder.layer.0.attention.self.query.bias'', ''encoder.layer.0.attention.self.value.bias'',
          ''encoder.layer.10.attention.self.value.weight'', ''encoder.layer.7.attention.self.query.bias'',
          ''encoder.layer.0.intermediate.dense.weight'', ''encoder.layer.11.attention.self.value.bias'',
          ''encoder.layer.11.output.LayerNorm.bias'', ''encoder.layer.7.attention.self.value.weight'',
          ''encoder.layer.4.attention.self.query.weight'', ''encoder.layer.10.attention.self.key.weight'',
          ''encoder.layer.0.attention.self.value.weight'', ''encoder.layer.3.attention.self.key.bias'',
          ''encoder.layer.11.attention.self.key.weight'', ''encoder.layer.6.output.LayerNorm.bias'',
          ''encoder.layer.5.attention.self.value.weight'', ''encoder.layer.8.intermediate.dense.weight'',
          ''encoder.layer.1.output.LayerNorm.weight'', ''encoder.layer.5.intermediate.dense.weight'',
          ''encoder.layer.11.output.LayerNorm.weight'', ''embeddings.position_embeddings.weight'',
          ''encoder.layer.4.output.LayerNorm.bias'', ''encoder.layer.6.attention.self.value.bias'',
          ''encoder.layer.4.attention.self.key.bias'', ''encoder.layer.3.intermediate.dense.weight'',
          ''encoder.layer.4.attention.self.key.weight'', ''encoder.layer.11.attention.self.query.weight'',
          ''encoder.layer.10.intermediate.dense.weight'', ''encoder.layer.9.attention.self.key.weight'',
          ''encoder.layer.11.attention.self.key.bias'', ''encoder.layer.5.attention.self.query.weight'',
          ''encoder.layer.11.intermediate.dense.weight'', ''encoder.layer.4.output.dense.weight'',
          ''encoder.layer.6.attention.self.key.bias'', ''encoder.layer.10.attention.self.query.bias'',
          ''encoder.layer.11.output.dense.weight'', ''encoder.layer.10.attention.self.value.bias'',
          ''encoder.layer.6.output.dense.bias'', ''encoder.layer.9.output.LayerNorm.weight'',
          ''encoder.layer.4.attention.self.query.bias'', ''encoder.layer.2.attention.self.key.weight'',
          ''encoder.layer.7.output.LayerNorm.weight'', ''encoder.layer.0.output.LayerNorm.weight'',
          ''encoder.layer.1.intermediate.dense.bias'', ''encoder.layer.9.attention.self.value.weight'',
          ''encoder.layer.10.intermediate.dense.bias'', ''encoder.layer.4.intermediate.dense.weight'',
          ''encoder.layer.9.attention.self.query.bias'', ''encoder.layer.1.attention.self.query.weight'',
          ''encoder.layer.9.output.dense.weight'', ''encoder.layer.10.attention.self.query.weight'',
          ''encoder.layer.3.attention.self.key.weight'', ''encoder.layer.6.output.dense.weight'',
          ''encoder.layer.3.output.dense.weight'', ''encoder.layer.7.attention.self.value.bias'',
          ''encoder.layer.8.attention.self.value.bias'', ''encoder.layer.6.intermediate.dense.bias'',
          ''encoder.layer.5.output.dense.weight'', ''encoder.layer.8.intermediate.dense.bias'',
          ''encoder.layer.9.attention.self.query.weight'', ''encoder.layer.1.attention.self.query.bias'',
          ''encoder.layer.6.attention.self.key.weight'', ''encoder.layer.4.output.LayerNorm.weight'',
          ''encoder.layer.2.attention.self.query.bias'', ''encoder.layer.4.output.dense.bias'',
          ''encoder.layer.1.attention.self.key.bias'', ''encoder.layer.8.output.dense.weight'',
          ''encoder.layer.0.intermediate.dense.bias'', ''encoder.layer.2.output.dense.weight'',
          ''encoder.layer.3.intermediate.dense.bias'', ''encoder.layer.3.attention.self.value.bias'',
          ''encoder.layer.4.attention.self.value.weight'', ''encoder.layer.1.intermediate.dense.weight'',
          ''encoder.layer.1.output.dense.bias'', ''encoder.layer.6.attention.self.query.weight'',
          ''encoder.layer.6.output.LayerNorm.weight'', ''encoder.layer.5.intermediate.dense.bias'',
          ''encoder.layer.0.attention.self.query.weight'', ''encoder.layer.9.attention.self.key.bias'',
          ''encoder.layer.10.output.dense.weight'', ''encoder.layer.3.output.LayerNorm.weight'',
          ''encoder.layer.9.output.dense.bias'', ''encoder.layer.6.attention.self.value.weight'',
          ''encoder.layer.1.attention.self.key.weight'', ''encoder.layer.8.output.LayerNorm.weight'',
          ''encoder.layer.7.attention.self.key.weight'', ''encoder.layer.5.attention.self.value.bias'',
          ''encoder.layer.11.output.dense.bias'', ''encoder.layer.0.attention.self.key.weight'',
          ''encoder.layer.8.output.dense.bias'', ''encoder.layer.2.output.LayerNorm.weight'',
          ''encoder.layer.8.attention.self.query.weight'', ''encoder.layer.11.attention.self.value.weight'',
          ''encoder.layer.2.output.dense.bias'', ''encoder.layer.5.output.dense.bias'',
          ''encoder.layer.10.output.LayerNorm.bias'', ''encoder.layer.8.attention.self.value.weight'',
          ''encoder.layer.2.attention.self.key.bias'', ''encoder.layer.6.attention.self.query.bias'',
          ''encoder.layer.2.attention.self.query.weight'', ''encoder.layer.7.intermediate.dense.weight'',
          ''encoder.layer.5.attention.self.key.bias'', ''encoder.layer.1.output.dense.weight'',
          ''encoder.layer.7.output.LayerNorm.bias'', ''encoder.layer.7.output.dense.bias'',
          ''encoder.layer.8.attention.self.key.bias'', ''encoder.layer.9.attention.self.value.bias'',
          ''encoder.layer.5.attention.self.query.bias'', ''encoder.layer.8.attention.self.query.bias'',
          ''encoder.layer.8.output.LayerNorm.bias'', ''encoder.layer.0.output.dense.bias'',
          ''encoder.layer.5.attention.self.key.weight'', ''encoder.layer.11.attention.self.query.bias'',
          ''encoder.layer.2.attention.self.value.bias'', ''encoder.layer.9.output.LayerNorm.bias'',
          ''encoder.layer.7.intermediate.dense.bias'', ''encoder.layer.10.attention.self.key.bias'',
          ''encoder.layer.9.intermediate.dense.weight'', ''encoder.layer.6.intermediate.dense.weight'',
          ''encoder.layer.1.attention.self.value.bias'', ''encoder.layer.4.intermediate.dense.bias'',
          ''encoder.layer.4.attention.self.value.bias'', ''encoder.layer.1.attention.self.value.weight'',
          ''encoder.layer.0.output.LayerNorm.bias'', ''encoder.layer.0.attention.self.key.bias'',
          ''encoder.layer.7.attention.self.query.weight'', ''encoder.layer.2.intermediate.dense.bias'',
          ''encoder.layer.3.output.dense.bias'', ''encoder.layer.0.output.dense.weight'',
          ''encoder.layer.5.output.LayerNorm.weight'', ''encoder.layer.2.output.LayerNorm.bias'',
          ''encoder.layer.3.attention.self.query.weight'', ''encoder.layer.2.intermediate.dense.weight'',
          ''encoder.layer.7.output.dense.weight'', ''encoder.layer.9.intermediate.dense.bias'',
          ''encoder.layer.8.attention.self.key.weight'', ''encoder.layer.7.attention.self.key.bias'',
          ''encoder.layer.10.output.dense.bias'', ''encoder.layer.10.output.LayerNorm.weight'',
          ''encoder.layer.3.attention.self.value.weight'', ''encoder.layer.3.output.LayerNorm.bias'',
          ''encoder.layer.3.attention.self.query.bias'']

          You should probably TRAIN this model on a down-stream task to be able to
          use it for predictions and inference.'
        updatedAt: '2023-09-21T14:48:39.936Z'
      numEdits: 0
      reactions: []
    id: 650c57c749e89cb6747f324b
    type: comment
  author: megil-paxton
  content: 'As a follow up, when I remove the trust_from_remote_source and revision
    numbers, embeddings are returned but with a message that makes me unsure of what
    model is actually being used to generate them:


    from sentence_transformers import SentenceTransformer


    model = SentenceTransformer(''Hum-Works/lodestone-base-4096-v1'')

    sentences = ["This is an example sentence", "Each sentence is converted"]


    Response:


    Some weights of the model checkpoint at /home/<me>/.cache/torch/sentence_transformers/Hum-Works_lodestone-base-4096-v1/
    were not used when initializing BertModel: [''encoder.layer.8.mlp.gated_layers.weight'',
    ''encoder.layer.0.attention.self.Wqkv.weight'', ''encoder.layer.2.mlp.layernorm.bias'',
    ''encoder.layer.7.mlp.wo.bias'', ''encoder.layer.10.attention.self.Wqkv.bias'',
    ''encoder.layer.2.mlp.wo.bias'', ''encoder.layer.7.mlp.wo.weight'', ''encoder.layer.1.attention.self.Wqkv.bias'',
    ''encoder.layer.9.mlp.wo.weight'', ''encoder.layer.4.mlp.layernorm.weight'', ''encoder.layer.1.mlp.gated_layers.weight'',
    ''encoder.layer.10.mlp.wo.bias'', ''encoder.layer.0.mlp.layernorm.bias'', ''encoder.layer.2.mlp.wo.weight'',
    ''encoder.layer.7.mlp.gated_layers.weight'', ''encoder.layer.11.mlp.layernorm.bias'',
    ''encoder.layer.6.mlp.layernorm.weight'', ''encoder.layer.8.attention.self.Wqkv.weight'',
    ''encoder.layer.0.attention.self.Wqkv.bias'', ''encoder.layer.10.mlp.layernorm.weight'',
    ''encoder.layer.1.mlp.layernorm.bias'', ''encoder.layer.3.mlp.wo.weight'', ''encoder.layer.10.mlp.wo.weight'',
    ''encoder.layer.8.mlp.wo.bias'', ''encoder.layer.3.attention.self.Wqkv.weight'',
    ''encoder.layer.10.mlp.layernorm.bias'', ''encoder.layer.9.attention.self.Wqkv.bias'',
    ''encoder.layer.9.attention.self.Wqkv.weight'', ''encoder.layer.11.mlp.gated_layers.weight'',
    ''encoder.layer.3.mlp.wo.bias'', ''encoder.layer.0.mlp.layernorm.weight'', ''encoder.layer.8.attention.self.Wqkv.bias'',
    ''encoder.layer.10.mlp.gated_layers.weight'', ''encoder.layer.7.attention.self.Wqkv.weight'',
    ''encoder.layer.6.mlp.gated_layers.weight'', ''encoder.layer.6.mlp.wo.bias'',
    ''encoder.layer.5.mlp.layernorm.weight'', ''encoder.layer.7.mlp.layernorm.bias'',
    ''encoder.layer.5.mlp.layernorm.bias'', ''encoder.layer.8.mlp.layernorm.weight'',
    ''encoder.layer.5.mlp.wo.weight'', ''encoder.layer.3.mlp.layernorm.weight'', ''encoder.layer.1.mlp.wo.weight'',
    ''encoder.layer.4.mlp.gated_layers.weight'', ''encoder.layer.4.attention.self.Wqkv.bias'',
    ''encoder.layer.11.attention.self.Wqkv.bias'', ''encoder.layer.5.mlp.gated_layers.weight'',
    ''encoder.layer.1.attention.self.Wqkv.weight'', ''encoder.layer.5.attention.self.Wqkv.bias'',
    ''encoder.layer.6.mlp.wo.weight'', ''encoder.layer.0.mlp.wo.bias'', ''encoder.layer.2.attention.self.Wqkv.bias'',
    ''encoder.layer.5.attention.self.Wqkv.weight'', ''encoder.layer.6.mlp.layernorm.bias'',
    ''encoder.layer.1.mlp.layernorm.weight'', ''encoder.layer.4.mlp.wo.weight'', ''encoder.layer.10.attention.self.Wqkv.weight'',
    ''encoder.layer.9.mlp.layernorm.weight'', ''encoder.layer.7.mlp.layernorm.weight'',
    ''encoder.layer.9.mlp.gated_layers.weight'', ''encoder.layer.4.mlp.layernorm.bias'',
    ''encoder.layer.3.attention.self.Wqkv.bias'', ''encoder.layer.11.mlp.layernorm.weight'',
    ''encoder.layer.4.attention.self.Wqkv.weight'', ''encoder.layer.2.attention.self.Wqkv.weight'',
    ''encoder.layer.2.mlp.layernorm.weight'', ''encoder.layer.0.mlp.wo.weight'', ''encoder.layer.9.mlp.wo.bias'',
    ''encoder.layer.0.mlp.gated_layers.weight'', ''encoder.layer.4.mlp.wo.bias'',
    ''encoder.layer.3.mlp.gated_layers.weight'', ''encoder.layer.9.mlp.layernorm.bias'',
    ''encoder.layer.6.attention.self.Wqkv.bias'', ''encoder.layer.11.mlp.wo.bias'',
    ''encoder.layer.8.mlp.wo.weight'', ''encoder.layer.6.attention.self.Wqkv.weight'',
    ''encoder.layer.3.mlp.layernorm.bias'', ''encoder.layer.11.attention.self.Wqkv.weight'',
    ''encoder.layer.8.mlp.layernorm.bias'', ''encoder.layer.2.mlp.gated_layers.weight'',
    ''encoder.layer.7.attention.self.Wqkv.bias'', ''encoder.layer.11.mlp.wo.weight'',
    ''encoder.layer.1.mlp.wo.bias'', ''encoder.layer.5.mlp.wo.bias'']

    - This IS expected if you are initializing BertModel from the checkpoint of a
    model trained on another task or with another architecture (e.g. initializing
    a BertForSequenceClassification model from a BertForPreTraining model).

    - This IS NOT expected if you are initializing BertModel from the checkpoint of
    a model that you expect to be exactly identical (initializing a BertForSequenceClassification
    model from a BertForSequenceClassification model).

    Some weights of BertModel were not initialized from the model checkpoint at /home/<me>/.cache/torch/sentence_transformers/Hum-Works_lodestone-base-4096-v1/
    and are newly initialized: [''encoder.layer.5.output.LayerNorm.bias'', ''encoder.layer.11.intermediate.dense.bias'',
    ''encoder.layer.2.attention.self.value.weight'', ''encoder.layer.1.output.LayerNorm.bias'',
    ''encoder.layer.0.attention.self.query.bias'', ''encoder.layer.0.attention.self.value.bias'',
    ''encoder.layer.10.attention.self.value.weight'', ''encoder.layer.7.attention.self.query.bias'',
    ''encoder.layer.0.intermediate.dense.weight'', ''encoder.layer.11.attention.self.value.bias'',
    ''encoder.layer.11.output.LayerNorm.bias'', ''encoder.layer.7.attention.self.value.weight'',
    ''encoder.layer.4.attention.self.query.weight'', ''encoder.layer.10.attention.self.key.weight'',
    ''encoder.layer.0.attention.self.value.weight'', ''encoder.layer.3.attention.self.key.bias'',
    ''encoder.layer.11.attention.self.key.weight'', ''encoder.layer.6.output.LayerNorm.bias'',
    ''encoder.layer.5.attention.self.value.weight'', ''encoder.layer.8.intermediate.dense.weight'',
    ''encoder.layer.1.output.LayerNorm.weight'', ''encoder.layer.5.intermediate.dense.weight'',
    ''encoder.layer.11.output.LayerNorm.weight'', ''embeddings.position_embeddings.weight'',
    ''encoder.layer.4.output.LayerNorm.bias'', ''encoder.layer.6.attention.self.value.bias'',
    ''encoder.layer.4.attention.self.key.bias'', ''encoder.layer.3.intermediate.dense.weight'',
    ''encoder.layer.4.attention.self.key.weight'', ''encoder.layer.11.attention.self.query.weight'',
    ''encoder.layer.10.intermediate.dense.weight'', ''encoder.layer.9.attention.self.key.weight'',
    ''encoder.layer.11.attention.self.key.bias'', ''encoder.layer.5.attention.self.query.weight'',
    ''encoder.layer.11.intermediate.dense.weight'', ''encoder.layer.4.output.dense.weight'',
    ''encoder.layer.6.attention.self.key.bias'', ''encoder.layer.10.attention.self.query.bias'',
    ''encoder.layer.11.output.dense.weight'', ''encoder.layer.10.attention.self.value.bias'',
    ''encoder.layer.6.output.dense.bias'', ''encoder.layer.9.output.LayerNorm.weight'',
    ''encoder.layer.4.attention.self.query.bias'', ''encoder.layer.2.attention.self.key.weight'',
    ''encoder.layer.7.output.LayerNorm.weight'', ''encoder.layer.0.output.LayerNorm.weight'',
    ''encoder.layer.1.intermediate.dense.bias'', ''encoder.layer.9.attention.self.value.weight'',
    ''encoder.layer.10.intermediate.dense.bias'', ''encoder.layer.4.intermediate.dense.weight'',
    ''encoder.layer.9.attention.self.query.bias'', ''encoder.layer.1.attention.self.query.weight'',
    ''encoder.layer.9.output.dense.weight'', ''encoder.layer.10.attention.self.query.weight'',
    ''encoder.layer.3.attention.self.key.weight'', ''encoder.layer.6.output.dense.weight'',
    ''encoder.layer.3.output.dense.weight'', ''encoder.layer.7.attention.self.value.bias'',
    ''encoder.layer.8.attention.self.value.bias'', ''encoder.layer.6.intermediate.dense.bias'',
    ''encoder.layer.5.output.dense.weight'', ''encoder.layer.8.intermediate.dense.bias'',
    ''encoder.layer.9.attention.self.query.weight'', ''encoder.layer.1.attention.self.query.bias'',
    ''encoder.layer.6.attention.self.key.weight'', ''encoder.layer.4.output.LayerNorm.weight'',
    ''encoder.layer.2.attention.self.query.bias'', ''encoder.layer.4.output.dense.bias'',
    ''encoder.layer.1.attention.self.key.bias'', ''encoder.layer.8.output.dense.weight'',
    ''encoder.layer.0.intermediate.dense.bias'', ''encoder.layer.2.output.dense.weight'',
    ''encoder.layer.3.intermediate.dense.bias'', ''encoder.layer.3.attention.self.value.bias'',
    ''encoder.layer.4.attention.self.value.weight'', ''encoder.layer.1.intermediate.dense.weight'',
    ''encoder.layer.1.output.dense.bias'', ''encoder.layer.6.attention.self.query.weight'',
    ''encoder.layer.6.output.LayerNorm.weight'', ''encoder.layer.5.intermediate.dense.bias'',
    ''encoder.layer.0.attention.self.query.weight'', ''encoder.layer.9.attention.self.key.bias'',
    ''encoder.layer.10.output.dense.weight'', ''encoder.layer.3.output.LayerNorm.weight'',
    ''encoder.layer.9.output.dense.bias'', ''encoder.layer.6.attention.self.value.weight'',
    ''encoder.layer.1.attention.self.key.weight'', ''encoder.layer.8.output.LayerNorm.weight'',
    ''encoder.layer.7.attention.self.key.weight'', ''encoder.layer.5.attention.self.value.bias'',
    ''encoder.layer.11.output.dense.bias'', ''encoder.layer.0.attention.self.key.weight'',
    ''encoder.layer.8.output.dense.bias'', ''encoder.layer.2.output.LayerNorm.weight'',
    ''encoder.layer.8.attention.self.query.weight'', ''encoder.layer.11.attention.self.value.weight'',
    ''encoder.layer.2.output.dense.bias'', ''encoder.layer.5.output.dense.bias'',
    ''encoder.layer.10.output.LayerNorm.bias'', ''encoder.layer.8.attention.self.value.weight'',
    ''encoder.layer.2.attention.self.key.bias'', ''encoder.layer.6.attention.self.query.bias'',
    ''encoder.layer.2.attention.self.query.weight'', ''encoder.layer.7.intermediate.dense.weight'',
    ''encoder.layer.5.attention.self.key.bias'', ''encoder.layer.1.output.dense.weight'',
    ''encoder.layer.7.output.LayerNorm.bias'', ''encoder.layer.7.output.dense.bias'',
    ''encoder.layer.8.attention.self.key.bias'', ''encoder.layer.9.attention.self.value.bias'',
    ''encoder.layer.5.attention.self.query.bias'', ''encoder.layer.8.attention.self.query.bias'',
    ''encoder.layer.8.output.LayerNorm.bias'', ''encoder.layer.0.output.dense.bias'',
    ''encoder.layer.5.attention.self.key.weight'', ''encoder.layer.11.attention.self.query.bias'',
    ''encoder.layer.2.attention.self.value.bias'', ''encoder.layer.9.output.LayerNorm.bias'',
    ''encoder.layer.7.intermediate.dense.bias'', ''encoder.layer.10.attention.self.key.bias'',
    ''encoder.layer.9.intermediate.dense.weight'', ''encoder.layer.6.intermediate.dense.weight'',
    ''encoder.layer.1.attention.self.value.bias'', ''encoder.layer.4.intermediate.dense.bias'',
    ''encoder.layer.4.attention.self.value.bias'', ''encoder.layer.1.attention.self.value.weight'',
    ''encoder.layer.0.output.LayerNorm.bias'', ''encoder.layer.0.attention.self.key.bias'',
    ''encoder.layer.7.attention.self.query.weight'', ''encoder.layer.2.intermediate.dense.bias'',
    ''encoder.layer.3.output.dense.bias'', ''encoder.layer.0.output.dense.weight'',
    ''encoder.layer.5.output.LayerNorm.weight'', ''encoder.layer.2.output.LayerNorm.bias'',
    ''encoder.layer.3.attention.self.query.weight'', ''encoder.layer.2.intermediate.dense.weight'',
    ''encoder.layer.7.output.dense.weight'', ''encoder.layer.9.intermediate.dense.bias'',
    ''encoder.layer.8.attention.self.key.weight'', ''encoder.layer.7.attention.self.key.bias'',
    ''encoder.layer.10.output.dense.bias'', ''encoder.layer.10.output.LayerNorm.weight'',
    ''encoder.layer.3.attention.self.value.weight'', ''encoder.layer.3.output.LayerNorm.bias'',
    ''encoder.layer.3.attention.self.query.bias'']

    You should probably TRAIN this model on a down-stream task to be able to use it
    for predictions and inference.'
  created_at: 2023-09-21 13:48:39+00:00
  edited: false
  hidden: false
  id: 650c57c749e89cb6747f324b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64e8da2cf6d7c8bdfb64d76a/DsPY0-4Qyi5UOsyz5qKC3.jpeg?w=200&h=200&f=face
      fullname: Dylan DiGioia
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dylanAtHum
      type: user
    createdAt: '2023-09-21T16:00:11.000Z'
    data:
      edited: false
      editors:
      - dylanAtHum
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9770208597183228
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64e8da2cf6d7c8bdfb64d76a/DsPY0-4Qyi5UOsyz5qKC3.jpeg?w=200&h=200&f=face
          fullname: Dylan DiGioia
          isHf: false
          isPro: false
          name: dylanAtHum
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;megil-paxton&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/megil-paxton\"\
          >@<span class=\"underline\">megil-paxton</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>Good catch, there is a bug with the code from our base model. If you\
          \ have Triton installed it tries to use it, but Triton doesn't support running\
          \ on CPU.<br>I didn't catch this when I tested CPU usage on my machine as\
          \ I was on Windows (terrible I know) and therefore didn't have triton installed.</p>\n\
          <p>It's a pretty straightforward fix. I'll get it pushed for you in the\
          \ next couple days.</p>\n"
        raw: 'Hey @megil-paxton,


          Good catch, there is a bug with the code from our base model. If you have
          Triton installed it tries to use it, but Triton doesn''t support running
          on CPU.

          I didn''t catch this when I tested CPU usage on my machine as I was on Windows
          (terrible I know) and therefore didn''t have triton installed.


          It''s a pretty straightforward fix. I''ll get it pushed for you in the next
          couple days.'
        updatedAt: '2023-09-21T16:00:11.208Z'
      numEdits: 0
      reactions: []
    id: 650c688b473ddc90020cd855
    type: comment
  author: dylanAtHum
  content: 'Hey @megil-paxton,


    Good catch, there is a bug with the code from our base model. If you have Triton
    installed it tries to use it, but Triton doesn''t support running on CPU.

    I didn''t catch this when I tested CPU usage on my machine as I was on Windows
    (terrible I know) and therefore didn''t have triton installed.


    It''s a pretty straightforward fix. I''ll get it pushed for you in the next couple
    days.'
  created_at: 2023-09-21 15:00:11+00:00
  edited: false
  hidden: false
  id: 650c688b473ddc90020cd855
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Hum-Works/lodestone-base-4096-v1
repo_type: model
status: open
target_branch: null
title: Not able to run without GPU
