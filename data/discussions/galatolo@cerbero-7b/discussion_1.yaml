!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Federic
conflicting_files: null
created_at: 2023-12-05 09:49:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d0f3e6f0431e6df22857402fad333733.svg
      fullname: Canzoneri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Federic
      type: user
    createdAt: '2023-12-05T09:49:26.000Z'
    data:
      edited: false
      editors:
      - Federic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6541956663131714
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d0f3e6f0431e6df22857402fad333733.svg
          fullname: Canzoneri
          isHf: false
          isPro: false
          name: Federic
          type: user
        html: '<p>Hi I''m trying to play with this model,  but i cannot load it in
          gpu (T4 16GB provided by Colab), even if I specify device_map="cuda:0" it
          still loads in RAM. Any advice? I have another question why the model weights
          so much ~ 30GB despite having 7B parameters?</p>

          <p>import transformers<br>quantization_config = transformers.BitsAndBytesConfig(<br>          load_in_4bit=True,<br>          bnb_4bit_quant_type=''nf4'',<br>          bnb_4bit_use_double_quant=True,<br>          #bnb_4bit_compute_dtype=bfloat16<br>      )</p>

          <p>llm = AutoModelForCausalLM.from_pretrained(<br>    "galatolo/cerbero-7b",<br>     quantization_config
          = quantization_config,<br>     device_map="cuda:0"<br>) </p>

          '
        raw: "Hi I'm trying to play with this model,  but i cannot load it in gpu\
          \ (T4 16GB provided by Colab), even if I specify device_map=\"cuda:0\" it\
          \ still loads in RAM. Any advice? I have another question why the model\
          \ weights so much ~ 30GB despite having 7B parameters?\r\n\r\nimport transformers\r\
          \nquantization_config = transformers.BitsAndBytesConfig(\r\n          load_in_4bit=True,\r\
          \n          bnb_4bit_quant_type='nf4',\r\n          bnb_4bit_use_double_quant=True,\r\
          \n          #bnb_4bit_compute_dtype=bfloat16\r\n      )\r\n\r\nllm = AutoModelForCausalLM.from_pretrained(\r\
          \n    \"galatolo/cerbero-7b\",\r\n     quantization_config = quantization_config,\r\
          \n     device_map=\"cuda:0\"\r\n) \r\n\r\n\r\n\r\n"
        updatedAt: '2023-12-05T09:49:26.483Z'
      numEdits: 0
      reactions: []
    id: 656ef226a1f8b1d0d48ea6db
    type: comment
  author: Federic
  content: "Hi I'm trying to play with this model,  but i cannot load it in gpu (T4\
    \ 16GB provided by Colab), even if I specify device_map=\"cuda:0\" it still loads\
    \ in RAM. Any advice? I have another question why the model weights so much ~\
    \ 30GB despite having 7B parameters?\r\n\r\nimport transformers\r\nquantization_config\
    \ = transformers.BitsAndBytesConfig(\r\n          load_in_4bit=True,\r\n     \
    \     bnb_4bit_quant_type='nf4',\r\n          bnb_4bit_use_double_quant=True,\r\
    \n          #bnb_4bit_compute_dtype=bfloat16\r\n      )\r\n\r\nllm = AutoModelForCausalLM.from_pretrained(\r\
    \n    \"galatolo/cerbero-7b\",\r\n     quantization_config = quantization_config,\r\
    \n     device_map=\"cuda:0\"\r\n) \r\n\r\n\r\n\r\n"
  created_at: 2023-12-05 09:49:26+00:00
  edited: false
  hidden: false
  id: 656ef226a1f8b1d0d48ea6db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659104835639-noauth.png?w=200&h=200&f=face
      fullname: Federico Galatolo
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: galatolo
      type: user
    createdAt: '2023-12-05T10:04:37.000Z'
    data:
      edited: false
      editors:
      - galatolo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9535799622535706
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659104835639-noauth.png?w=200&h=200&f=face
          fullname: Federico Galatolo
          isHf: false
          isPro: false
          name: galatolo
          type: user
        html: '<p>Hi, it weighs that much because the weights are in the float32 format
          (rather than the more common float16).<br>I attempted to load the model
          using Google Colab, and it appears to crash due to insufficient RAM.<br>I
          will upload a <code>float16</code> variant, maybe it will solve this issue</p>

          '
        raw: "Hi, it weighs that much because the weights are in the float32 format\
          \ (rather than the more common float16). \nI attempted to load the model\
          \ using Google Colab, and it appears to crash due to insufficient RAM.\n\
          I will upload a `float16` variant, maybe it will solve this issue"
        updatedAt: '2023-12-05T10:04:37.366Z'
      numEdits: 0
      reactions: []
    id: 656ef5b58e7f2775ac08cf1d
    type: comment
  author: galatolo
  content: "Hi, it weighs that much because the weights are in the float32 format\
    \ (rather than the more common float16). \nI attempted to load the model using\
    \ Google Colab, and it appears to crash due to insufficient RAM.\nI will upload\
    \ a `float16` variant, maybe it will solve this issue"
  created_at: 2023-12-05 10:04:37+00:00
  edited: false
  hidden: false
  id: 656ef5b58e7f2775ac08cf1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659104835639-noauth.png?w=200&h=200&f=face
      fullname: Federico Galatolo
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: galatolo
      type: user
    createdAt: '2023-12-05T14:08:05.000Z'
    data:
      edited: true
      editors:
      - galatolo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9595407247543335
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659104835639-noauth.png?w=200&h=200&f=face
          fullname: Federico Galatolo
          isHf: false
          isPro: false
          name: galatolo
          type: user
        html: '<p>I uploaded the float16 variant, and you can load it using the following
          code:</p>

          <pre><code>model = AutoModelForCausalLM.from_pretrained("galatolo/cerbero-7b",
          revision="float16")

          </code></pre>

          <p>However, it appears that Colab does not have enough RAM to handle this.
          I believe the best option is to use the llama.cpp version, which I have
          already quantized to 4 bits.</p>

          '
        raw: 'I uploaded the float16 variant, and you can load it using the following
          code:


          ```

          model = AutoModelForCausalLM.from_pretrained("galatolo/cerbero-7b", revision="float16")

          ```


          However, it appears that Colab does not have enough RAM to handle this.
          I believe the best option is to use the llama.cpp version, which I have
          already quantized to 4 bits.'
        updatedAt: '2023-12-05T14:10:18.189Z'
      numEdits: 2
      reactions: []
    id: 656f2ec57e56bb875bfd69e6
    type: comment
  author: galatolo
  content: 'I uploaded the float16 variant, and you can load it using the following
    code:


    ```

    model = AutoModelForCausalLM.from_pretrained("galatolo/cerbero-7b", revision="float16")

    ```


    However, it appears that Colab does not have enough RAM to handle this. I believe
    the best option is to use the llama.cpp version, which I have already quantized
    to 4 bits.'
  created_at: 2023-12-05 14:08:05+00:00
  edited: true
  hidden: false
  id: 656f2ec57e56bb875bfd69e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659104835639-noauth.png?w=200&h=200&f=face
      fullname: Federico Galatolo
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: galatolo
      type: user
    createdAt: '2023-12-07T12:39:47.000Z'
    data:
      status: closed
    id: 6571bd13c2201bd157a28f52
    type: status-change
  author: galatolo
  created_at: 2023-12-07 12:39:47+00:00
  id: 6571bd13c2201bd157a28f52
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: galatolo/cerbero-7b
repo_type: model
status: closed
target_branch: null
title: Unable to load model in GPU
