!!python/object:huggingface_hub.community.DiscussionWithDetails
author: autobots
conflicting_files: null
created_at: 2023-09-03 14:00:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-09-03T15:00:29.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9693434834480286
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>Is there something wrong with the GGUF quants of this model? I have
          downloaded both the Q4KM and the Q5KM and the model starts repeating the
          same outputs around 3k context. The longer it goes, the harder this repetition
          is to break. I have to change parameters and edit the context and it still
          tries to pull up old sentences verbatim.</p>

          <p>I also downloaded the GPTQ but the same issue is not present. Most of
          my other llama.cpp models were d/l as GGML and converted via the script,
          not quantized to GGUF directly. When I use them, this issue definitely doesn''t
          occur whatsoever. I know there were some commits to the llama.cpp scripts
          post this conversion but not sure if that matters.</p>

          <p>I thought it could be the samplers but again, I don''t have this problem
          with other CPP models of the same size which were converted GGML-&gt;GGUF.
          I use the same settings.</p>

          <p>I am now tempted to get the GGML and re-convert  them to see if the issue
          is present but that''s another 40GB of downloading for an unknown result.</p>

          '
        raw: "Is there something wrong with the GGUF quants of this model? I have\
          \ downloaded both the Q4KM and the Q5KM and the model starts repeating the\
          \ same outputs around 3k context. The longer it goes, the harder this repetition\
          \ is to break. I have to change parameters and edit the context and it still\
          \ tries to pull up old sentences verbatim.\r\n\r\nI also downloaded the\
          \ GPTQ but the same issue is not present. Most of my other llama.cpp models\
          \ were d/l as GGML and converted via the script, not quantized to GGUF directly.\
          \ When I use them, this issue definitely doesn't occur whatsoever. I know\
          \ there were some commits to the llama.cpp scripts post this conversion\
          \ but not sure if that matters.\r\n\r\nI thought it could be the samplers\
          \ but again, I don't have this problem with other CPP models of the same\
          \ size which were converted GGML->GGUF. I use the same settings.\r\n\r\n\
          I am now tempted to get the GGML and re-convert  them to see if the issue\
          \ is present but that's another 40GB of downloading for an unknown result.\r\
          \n\r\n"
        updatedAt: '2023-09-03T15:00:29.067Z'
      numEdits: 0
      reactions: []
    id: 64f49f8d7ab56c5b6d8307ff
    type: comment
  author: autobots
  content: "Is there something wrong with the GGUF quants of this model? I have downloaded\
    \ both the Q4KM and the Q5KM and the model starts repeating the same outputs around\
    \ 3k context. The longer it goes, the harder this repetition is to break. I have\
    \ to change parameters and edit the context and it still tries to pull up old\
    \ sentences verbatim.\r\n\r\nI also downloaded the GPTQ but the same issue is\
    \ not present. Most of my other llama.cpp models were d/l as GGML and converted\
    \ via the script, not quantized to GGUF directly. When I use them, this issue\
    \ definitely doesn't occur whatsoever. I know there were some commits to the llama.cpp\
    \ scripts post this conversion but not sure if that matters.\r\n\r\nI thought\
    \ it could be the samplers but again, I don't have this problem with other CPP\
    \ models of the same size which were converted GGML->GGUF. I use the same settings.\r\
    \n\r\nI am now tempted to get the GGML and re-convert  them to see if the issue\
    \ is present but that's another 40GB of downloading for an unknown result.\r\n\
    \r\n"
  created_at: 2023-09-03 14:00:29+00:00
  edited: false
  hidden: false
  id: 64f49f8d7ab56c5b6d8307ff
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/fiction.live-Kimiko-V2-70B-GGUF
repo_type: model
status: open
target_branch: null
title: Repetition issues?
