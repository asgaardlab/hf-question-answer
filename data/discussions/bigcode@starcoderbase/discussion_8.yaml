!!python/object:huggingface_hub.community.DiscussionWithDetails
author: miraclezst
conflicting_files: null
created_at: 2023-05-10 05:54:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34c7f3f30759bbbbee0a77d0361e08a2.svg
      fullname: zhaositong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: miraclezst
      type: user
    createdAt: '2023-05-10T06:54:54.000Z'
    data:
      edited: false
      editors:
      - miraclezst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34c7f3f30759bbbbee0a77d0361e08a2.svg
          fullname: zhaositong
          isHf: false
          isPro: false
          name: miraclezst
          type: user
        html: '<h2 id="my-code-as-below">my code as below:</h2>

          <h1 id="pip-install--q-transformers">pip install -q transformers</h1>

          <p>from transformers import AutoModelForCausalLM, AutoTokenizer<br>import
          os</p>

          <p>checkpoint = "bigcode/starcoder"<br>device = "cuda" # for GPU usage or
          "cpu" for CPU usage</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(checkpoint,use_auth_token=True)<br>model
          = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True,load_in_8bit=True,device_map={"":
          0}) </p>

          <p>input_text = "def print_hello_world():\n    \n    print(''Hello world!'')"<br>inputs
          = tokenizer.encode(input_text, return_tensors="pt").to(device)<br>outputs
          = model.generate(inputs)<br>print(tokenizer.decode(outputs[0]))</p>

          <hr>

          <p>output:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64588eccead43697df2d6f1a/VGkFfrEQ1itUukph70Nrv.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64588eccead43697df2d6f1a/VGkFfrEQ1itUukph70Nrv.png"></a></p>

          <p>Does anyone know what is the reason for this?</p>

          '
        raw: "my code as below:\r\n-----------------------------------------------------------------------------------------------------------------\r\
          \n# pip install -q transformers\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\r\nimport os\r\n\r\ncheckpoint = \"bigcode/starcoder\"\r\
          \ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(checkpoint,use_auth_token=True)\r\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True,load_in_8bit=True,device_map={\"\
          \": 0}) \r\n\r\ninput_text = \"<fim_prefix>def print_hello_world():\\n \
          \   <fim_suffix>\\n    print('Hello world!')<fim_middle>\"\r\ninputs = tokenizer.encode(input_text,\
          \ return_tensors=\"pt\").to(device)\r\noutputs = model.generate(inputs)\r\
          \nprint(tokenizer.decode(outputs[0]))\r\n\r\n-----------------------------------------------------------------------------------------------------------------\r\
          \noutput:\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64588eccead43697df2d6f1a/VGkFfrEQ1itUukph70Nrv.png)\r\
          \n\r\nDoes anyone know what is the reason for this?"
        updatedAt: '2023-05-10T06:54:54.601Z'
      numEdits: 0
      reactions: []
    id: 645b3fbebccccb946f7f701e
    type: comment
  author: miraclezst
  content: "my code as below:\r\n-----------------------------------------------------------------------------------------------------------------\r\
    \n# pip install -q transformers\r\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\r\nimport os\r\n\r\ncheckpoint = \"bigcode/starcoder\"\r\ndevice\
    \ = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(checkpoint,use_auth_token=True)\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True,load_in_8bit=True,device_map={\"\
    \": 0}) \r\n\r\ninput_text = \"<fim_prefix>def print_hello_world():\\n    <fim_suffix>\\\
    n    print('Hello world!')<fim_middle>\"\r\ninputs = tokenizer.encode(input_text,\
    \ return_tensors=\"pt\").to(device)\r\noutputs = model.generate(inputs)\r\nprint(tokenizer.decode(outputs[0]))\r\
    \n\r\n-----------------------------------------------------------------------------------------------------------------\r\
    \noutput:\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64588eccead43697df2d6f1a/VGkFfrEQ1itUukph70Nrv.png)\r\
    \n\r\nDoes anyone know what is the reason for this?"
  created_at: 2023-05-10 05:54:54+00:00
  edited: false
  hidden: false
  id: 645b3fbebccccb946f7f701e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/aOsbtx6wAs-yQMRfyvvHT.jpeg?w=200&h=200&f=face
      fullname: Shihang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hodophile
      type: user
    createdAt: '2023-06-15T02:26:37.000Z'
    data:
      edited: false
      editors:
      - Hodophile
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9874938726425171
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/aOsbtx6wAs-yQMRfyvvHT.jpeg?w=200&h=200&f=face
          fullname: Shihang
          isHf: false
          isPro: false
          name: Hodophile
          type: user
        html: '<p>Can you leave the code you want to fill in blank? It seems the code
          has been completed and no further action is required in your snippet.</p>

          '
        raw: Can you leave the code you want to fill in blank? It seems the code has
          been completed and no further action is required in your snippet.
        updatedAt: '2023-06-15T02:26:37.705Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - noobmldude
    id: 648a76dd498fefb32ef43e47
    type: comment
  author: Hodophile
  content: Can you leave the code you want to fill in blank? It seems the code has
    been completed and no further action is required in your snippet.
  created_at: 2023-06-15 01:26:37+00:00
  edited: false
  hidden: false
  id: 648a76dd498fefb32ef43e47
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/nn8KQ64Z2N4owqACg6LSY.jpeg?w=200&h=200&f=face
      fullname: Scott L. Burson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: slburson
      type: user
    createdAt: '2024-01-11T23:04:53.000Z'
    data:
      edited: false
      editors:
      - slburson
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6788922548294067
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/nn8KQ64Z2N4owqACg6LSY.jpeg?w=200&h=200&f=face
          fullname: Scott L. Burson
          isHf: false
          isPro: false
          name: slburson
          type: user
        html: "<p>The warning can be eliminated by passing an additional argument\
          \ to <code>model.generate</code>:</p>\n<pre><code>outputs = model.generate(inputs,\
          \ pad_token_id=tokenizer.eos_token_id)\n</code></pre>\n<p>Also, I strongly\
          \ suspect that the example has <code>&lt;fim_suffix&gt;</code> and <code>&lt;fim_middle&gt;</code>\
          \ swapped.  When I do this:</p>\n<pre><code>input_text = \"&lt;fim_prefix&gt;def\
          \ print_hello_world():\\n    &lt;fim_middle&gt;\\n    print('Hello world!')&lt;fim_suffix&gt;\"\
          \ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\
          outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id)\n\
          print(tokenizer.decode(outputs[0]))\n</code></pre>\n<p>I get this output:</p>\n\
          <pre><code>&lt;fim_prefix&gt;def print_hello_world():\n    &lt;fim_middle&gt;\n\
          \    print('Hello world!')&lt;fim_suffix&gt;\n\nif\n</code></pre>\n<p>That\
          \ trailing <code>if</code> is a bit weird, but it seems not unusual for\
          \ these models to throw in a stray token at the end; I think I've seen another\
          \ model do it.  Except for that, I gather this output is as intended.</p>\n"
        raw: "The warning can be eliminated by passing an additional argument to `model.generate`:\n\
          ```\noutputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id)\n\
          ```\n\nAlso, I strongly suspect that the example has `<fim_suffix>` and\
          \ `<fim_middle>` swapped.  When I do this:\n```\ninput_text = \"<fim_prefix>def\
          \ print_hello_world():\\n    <fim_middle>\\n    print('Hello world!')<fim_suffix>\"\
          \ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\
          outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id)\n\
          print(tokenizer.decode(outputs[0]))\n```\nI get this output:\n```\n<fim_prefix>def\
          \ print_hello_world():\n    <fim_middle>\n    print('Hello world!')<fim_suffix>\n\
          \nif\n```\nThat trailing `if` is a bit weird, but it seems not unusual for\
          \ these models to throw in a stray token at the end; I think I've seen another\
          \ model do it.  Except for that, I gather this output is as intended.\n"
        updatedAt: '2024-01-11T23:04:53.396Z'
      numEdits: 0
      reactions: []
    id: 65a074157326ec30b60ef80b
    type: comment
  author: slburson
  content: "The warning can be eliminated by passing an additional argument to `model.generate`:\n\
    ```\noutputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id)\n```\n\
    \nAlso, I strongly suspect that the example has `<fim_suffix>` and `<fim_middle>`\
    \ swapped.  When I do this:\n```\ninput_text = \"<fim_prefix>def print_hello_world():\\\
    n    <fim_middle>\\n    print('Hello world!')<fim_suffix>\"\ninputs = tokenizer.encode(input_text,\
    \ return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id)\n\
    print(tokenizer.decode(outputs[0]))\n```\nI get this output:\n```\n<fim_prefix>def\
    \ print_hello_world():\n    <fim_middle>\n    print('Hello world!')<fim_suffix>\n\
    \nif\n```\nThat trailing `if` is a bit weird, but it seems not unusual for these\
    \ models to throw in a stray token at the end; I think I've seen another model\
    \ do it.  Except for that, I gather this output is as intended.\n"
  created_at: 2024-01-11 23:04:53+00:00
  edited: false
  hidden: false
  id: 65a074157326ec30b60ef80b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: bigcode/starcoderbase
repo_type: model
status: open
target_branch: null
title: 'can not generate with mode: Fill-in-the-middle'
