!!python/object:huggingface_hub.community.DiscussionWithDetails
author: leelichen
conflicting_files: null
created_at: 2023-06-01 02:24:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22f39c40343f5cc194e8c0f995a60296.svg
      fullname: Chen, Lee Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leelichen
      type: user
    createdAt: '2023-06-01T03:24:34.000Z'
    data:
      edited: false
      editors:
      - leelichen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22f39c40343f5cc194e8c0f995a60296.svg
          fullname: Chen, Lee Li
          isHf: false
          isPro: false
          name: leelichen
          type: user
        html: '<p>May I ask where the training code is and does the training data
          include ABAP language</p>

          '
        raw: May I ask where the training code is and does the training data include
          ABAP language
        updatedAt: '2023-06-01T03:24:34.508Z'
      numEdits: 0
      reactions: []
    id: 64780f72168cb428e0128886
    type: comment
  author: leelichen
  content: May I ask where the training code is and does the training data include
    ABAP language
  created_at: 2023-06-01 02:24:34+00:00
  edited: false
  hidden: false
  id: 64780f72168cb428e0128886
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-06-01T15:03:00.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>You can find the training codebase here: <a rel="nofollow" href="https://github.com/bigcode-project/Megatron-LM/tree/multi-query-attention">https://github.com/bigcode-project/Megatron-LM/tree/multi-query-attention</a>
          There''s also a repo for fine-tuning with PEFT or DeepSpeed: <a rel="nofollow"
          href="https://github.com/bigcode-project/starcoder">https://github.com/bigcode-project/starcoder</a><br>We
          didn''t include ABAP you can find the full list of languages included in
          the training in the <a rel="nofollow" href="https://arxiv.org/abs/2305.06161">paper</a>
          in table 1. But we do have ABAP in the <a href="https://huggingface.co/datasets/bigcode/the-stack/tree/main/data/abap">Stack</a>
          dataset if you want to try fine-tuning.</p>

          '
        raw: 'You can find the training codebase here: https://github.com/bigcode-project/Megatron-LM/tree/multi-query-attention
          There''s also a repo for fine-tuning with PEFT or DeepSpeed: https://github.com/bigcode-project/starcoder

          We didn''t include ABAP you can find the full list of languages included
          in the training in the [paper](https://arxiv.org/abs/2305.06161) in table
          1. But we do have ABAP in the [Stack](https://huggingface.co/datasets/bigcode/the-stack/tree/main/data/abap)
          dataset if you want to try fine-tuning.'
        updatedAt: '2023-06-01T15:03:00.796Z'
      numEdits: 0
      reactions: []
    id: 6478b324ad83f3939b500617
    type: comment
  author: loubnabnl
  content: 'You can find the training codebase here: https://github.com/bigcode-project/Megatron-LM/tree/multi-query-attention
    There''s also a repo for fine-tuning with PEFT or DeepSpeed: https://github.com/bigcode-project/starcoder

    We didn''t include ABAP you can find the full list of languages included in the
    training in the [paper](https://arxiv.org/abs/2305.06161) in table 1. But we do
    have ABAP in the [Stack](https://huggingface.co/datasets/bigcode/the-stack/tree/main/data/abap)
    dataset if you want to try fine-tuning.'
  created_at: 2023-06-01 14:03:00+00:00
  edited: false
  hidden: false
  id: 6478b324ad83f3939b500617
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22f39c40343f5cc194e8c0f995a60296.svg
      fullname: Chen, Lee Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leelichen
      type: user
    createdAt: '2023-06-02T09:18:55.000Z'
    data:
      edited: false
      editors:
      - leelichen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22f39c40343f5cc194e8c0f995a60296.svg
          fullname: Chen, Lee Li
          isHf: false
          isPro: false
          name: leelichen
          type: user
        html: "<blockquote>\n<p>You can find the training codebase here: <a rel=\"\
          nofollow\" href=\"https://github.com/bigcode-project/Megatron-LM/tree/multi-query-attention\"\
          >https://github.com/bigcode-project/Megatron-LM/tree/multi-query-attention</a>\
          \ There's also a repo for fine-tuning with PEFT or DeepSpeed: <a rel=\"\
          nofollow\" href=\"https://github.com/bigcode-project/starcoder\">https://github.com/bigcode-project/starcoder</a><br>We\
          \ didn't include ABAP you can find the full list of languages included in\
          \ the training in the <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2305.06161\"\
          >paper</a> in table 1. But we do have ABAP in the <a href=\"https://huggingface.co/datasets/bigcode/the-stack/tree/main/data/abap\"\
          >Stack</a> dataset if you want to try fine-tuning.</p>\n</blockquote>\n\
          <p>Thank you very much for your reply.<br>Do you have any suitable parameter\
          \ suggestions for fine-tuning\uFF1F</p>\n"
        raw: "> You can find the training codebase here: https://github.com/bigcode-project/Megatron-LM/tree/multi-query-attention\
          \ There's also a repo for fine-tuning with PEFT or DeepSpeed: https://github.com/bigcode-project/starcoder\n\
          > We didn't include ABAP you can find the full list of languages included\
          \ in the training in the [paper](https://arxiv.org/abs/2305.06161) in table\
          \ 1. But we do have ABAP in the [Stack](https://huggingface.co/datasets/bigcode/the-stack/tree/main/data/abap)\
          \ dataset if you want to try fine-tuning.\n\nThank you very much for your\
          \ reply.\nDo you have any suitable parameter suggestions for fine-tuning\uFF1F"
        updatedAt: '2023-06-02T09:18:55.783Z'
      numEdits: 0
      reactions: []
    id: 6479b3ff1a2aefceecd3487c
    type: comment
  author: leelichen
  content: "> You can find the training codebase here: https://github.com/bigcode-project/Megatron-LM/tree/multi-query-attention\
    \ There's also a repo for fine-tuning with PEFT or DeepSpeed: https://github.com/bigcode-project/starcoder\n\
    > We didn't include ABAP you can find the full list of languages included in the\
    \ training in the [paper](https://arxiv.org/abs/2305.06161) in table 1. But we\
    \ do have ABAP in the [Stack](https://huggingface.co/datasets/bigcode/the-stack/tree/main/data/abap)\
    \ dataset if you want to try fine-tuning.\n\nThank you very much for your reply.\n\
    Do you have any suitable parameter suggestions for fine-tuning\uFF1F"
  created_at: 2023-06-02 08:18:55+00:00
  edited: false
  hidden: false
  id: 6479b3ff1a2aefceecd3487c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-06-02T12:36:45.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>You can start from the default parameters in the repo and tune them
          if needed</p>

          '
        raw: You can start from the default parameters in the repo and tune them if
          needed
        updatedAt: '2023-06-02T12:36:45.074Z'
      numEdits: 0
      reactions: []
    id: 6479e25d1a2aefceecd7cee8
    type: comment
  author: loubnabnl
  content: You can start from the default parameters in the repo and tune them if
    needed
  created_at: 2023-06-02 11:36:45+00:00
  edited: false
  hidden: false
  id: 6479e25d1a2aefceecd7cee8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22f39c40343f5cc194e8c0f995a60296.svg
      fullname: Chen, Lee Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leelichen
      type: user
    createdAt: '2023-06-02T14:47:27.000Z'
    data:
      edited: false
      editors:
      - leelichen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22f39c40343f5cc194e8c0f995a60296.svg
          fullname: Chen, Lee Li
          isHf: false
          isPro: false
          name: leelichen
          type: user
        html: '<blockquote>

          <p>You can start from the default parameters in the repo and tune them if
          needed</p>

          </blockquote>

          <p>If using ABAP data from the Stack dataset, how long is the estimated
          fine-tuning time? I have an A100 80G machine.<br>I would like to do a budget
          assessment first</p>

          '
        raw: "> You can start from the default parameters in the repo and tune them\
          \ if needed\n\nIf using ABAP data from the Stack dataset, how long is the\
          \ estimated fine-tuning time? I have an A100 80G machine. \nI would like\
          \ to do a budget assessment first"
        updatedAt: '2023-06-02T14:47:27.329Z'
      numEdits: 0
      reactions: []
    id: 647a00fff518a860fbcae62c
    type: comment
  author: leelichen
  content: "> You can start from the default parameters in the repo and tune them\
    \ if needed\n\nIf using ABAP data from the Stack dataset, how long is the estimated\
    \ fine-tuning time? I have an A100 80G machine. \nI would like to do a budget\
    \ assessment first"
  created_at: 2023-06-02 13:47:27+00:00
  edited: false
  hidden: false
  id: 647a00fff518a860fbcae62c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22f39c40343f5cc194e8c0f995a60296.svg
      fullname: Chen, Lee Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leelichen
      type: user
    createdAt: '2023-06-05T02:44:17.000Z'
    data:
      edited: false
      editors:
      - leelichen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5553531050682068
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22f39c40343f5cc194e8c0f995a60296.svg
          fullname: Chen, Lee Li
          isHf: false
          isPro: false
          name: leelichen
          type: user
        html: '<p>I saw a file for fine-tuning starcoder. Is this file about fine-tuning
          starcoderbase to starcoder?</p>

          <pre><code>https://github.com/bigcode-project/Megatron-LM/blob/finetune-starcoder/examples/finetune_bigcode_model.slurm

          </code></pre>

          <p>Where can I see the paths contained in the file?</p>

          <pre><code>STARCODER_PATH=/fsx/boomcode/starcoder/

          CHECKPOINT_PATH=/fsx/boomcode/starcoderpy/$SLURM_JOB_ID

          TOKENIZER_FILE=/fsx/boomcode/tokenizer-starcoder/tokenizer.json

          WEIGHTS_TRAIN=/fsx/boomcode/datamix_python/train_data_paths.txt.tmp

          WEIGHTS_VALID=/fsx/boomcode/datamix_python/valid_data_paths.txt.tmp

          DATA_PATH=/fsx/boomcode/tokenized/python/

          </code></pre>

          '
        raw: 'I saw a file for fine-tuning starcoder. Is this file about fine-tuning
          starcoderbase to starcoder?

          ```

          https://github.com/bigcode-project/Megatron-LM/blob/finetune-starcoder/examples/finetune_bigcode_model.slurm

          ```

          Where can I see the paths contained in the file?

          ```

          STARCODER_PATH=/fsx/boomcode/starcoder/

          CHECKPOINT_PATH=/fsx/boomcode/starcoderpy/$SLURM_JOB_ID

          TOKENIZER_FILE=/fsx/boomcode/tokenizer-starcoder/tokenizer.json

          WEIGHTS_TRAIN=/fsx/boomcode/datamix_python/train_data_paths.txt.tmp

          WEIGHTS_VALID=/fsx/boomcode/datamix_python/valid_data_paths.txt.tmp

          DATA_PATH=/fsx/boomcode/tokenized/python/

          ```'
        updatedAt: '2023-06-05T02:44:17.776Z'
      numEdits: 0
      reactions: []
    id: 647d4c01c788767ab5e479cd
    type: comment
  author: leelichen
  content: 'I saw a file for fine-tuning starcoder. Is this file about fine-tuning
    starcoderbase to starcoder?

    ```

    https://github.com/bigcode-project/Megatron-LM/blob/finetune-starcoder/examples/finetune_bigcode_model.slurm

    ```

    Where can I see the paths contained in the file?

    ```

    STARCODER_PATH=/fsx/boomcode/starcoder/

    CHECKPOINT_PATH=/fsx/boomcode/starcoderpy/$SLURM_JOB_ID

    TOKENIZER_FILE=/fsx/boomcode/tokenizer-starcoder/tokenizer.json

    WEIGHTS_TRAIN=/fsx/boomcode/datamix_python/train_data_paths.txt.tmp

    WEIGHTS_VALID=/fsx/boomcode/datamix_python/valid_data_paths.txt.tmp

    DATA_PATH=/fsx/boomcode/tokenized/python/

    ```'
  created_at: 2023-06-05 01:44:17+00:00
  edited: false
  hidden: false
  id: 647d4c01c788767ab5e479cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c44e3ca36c410f975283a70da1a7dff.svg
      fullname: WWN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wwn
      type: user
    createdAt: '2023-06-06T16:57:42.000Z'
    data:
      edited: false
      editors:
      - wwn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5894034504890442
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c44e3ca36c410f975283a70da1a7dff.svg
          fullname: WWN
          isHf: false
          isPro: false
          name: wwn
          type: user
        html: '<blockquote>

          <p>I saw a file for fine-tuning starcoder. Is this file about fine-tuning
          starcoderbase to starcoder?</p>

          <pre><code>https://github.com/bigcode-project/Megatron-LM/blob/finetune-starcoder/examples/finetune_bigcode_model.slurm

          </code></pre>

          <p>Where can I see the paths contained in the file?</p>

          <pre><code>STARCODER_PATH=/fsx/boomcode/starcoder/

          CHECKPOINT_PATH=/fsx/boomcode/starcoderpy/$SLURM_JOB_ID

          TOKENIZER_FILE=/fsx/boomcode/tokenizer-starcoder/tokenizer.json

          WEIGHTS_TRAIN=/fsx/boomcode/datamix_python/train_data_paths.txt.tmp

          WEIGHTS_VALID=/fsx/boomcode/datamix_python/valid_data_paths.txt.tmp

          DATA_PATH=/fsx/boomcode/tokenized/python/

          </code></pre>

          </blockquote>

          <p>I have the same questions? where to get or generate these files? </p>

          <blockquote>

          <p>WEIGHTS_TRAIN=/fsx/boomcode/datamix_python/train_data_paths.txt.tmp<br>WEIGHTS_VALID=/fsx/boomcode/datamix_python/valid_data_paths.txt.tmp</p>

          </blockquote>

          '
        raw: "> I saw a file for fine-tuning starcoder. Is this file about fine-tuning\
          \ starcoderbase to starcoder?\n> ```\n> https://github.com/bigcode-project/Megatron-LM/blob/finetune-starcoder/examples/finetune_bigcode_model.slurm\n\
          > ```\n> Where can I see the paths contained in the file?\n> ```\n> STARCODER_PATH=/fsx/boomcode/starcoder/\n\
          > CHECKPOINT_PATH=/fsx/boomcode/starcoderpy/$SLURM_JOB_ID\n> TOKENIZER_FILE=/fsx/boomcode/tokenizer-starcoder/tokenizer.json\n\
          > WEIGHTS_TRAIN=/fsx/boomcode/datamix_python/train_data_paths.txt.tmp\n\
          > WEIGHTS_VALID=/fsx/boomcode/datamix_python/valid_data_paths.txt.tmp\n\
          > DATA_PATH=/fsx/boomcode/tokenized/python/\n> ```\n\nI have the same questions?\
          \ where to get or generate these files? \n> WEIGHTS_TRAIN=/fsx/boomcode/datamix_python/train_data_paths.txt.tmp\n\
          > WEIGHTS_VALID=/fsx/boomcode/datamix_python/valid_data_paths.txt.tmp"
        updatedAt: '2023-06-06T16:57:42.230Z'
      numEdits: 0
      reactions: []
    id: 647f65869c31024457a920f5
    type: comment
  author: wwn
  content: "> I saw a file for fine-tuning starcoder. Is this file about fine-tuning\
    \ starcoderbase to starcoder?\n> ```\n> https://github.com/bigcode-project/Megatron-LM/blob/finetune-starcoder/examples/finetune_bigcode_model.slurm\n\
    > ```\n> Where can I see the paths contained in the file?\n> ```\n> STARCODER_PATH=/fsx/boomcode/starcoder/\n\
    > CHECKPOINT_PATH=/fsx/boomcode/starcoderpy/$SLURM_JOB_ID\n> TOKENIZER_FILE=/fsx/boomcode/tokenizer-starcoder/tokenizer.json\n\
    > WEIGHTS_TRAIN=/fsx/boomcode/datamix_python/train_data_paths.txt.tmp\n> WEIGHTS_VALID=/fsx/boomcode/datamix_python/valid_data_paths.txt.tmp\n\
    > DATA_PATH=/fsx/boomcode/tokenized/python/\n> ```\n\nI have the same questions?\
    \ where to get or generate these files? \n> WEIGHTS_TRAIN=/fsx/boomcode/datamix_python/train_data_paths.txt.tmp\n\
    > WEIGHTS_VALID=/fsx/boomcode/datamix_python/valid_data_paths.txt.tmp"
  created_at: 2023-06-06 15:57:42+00:00
  edited: false
  hidden: false
  id: 647f65869c31024457a920f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-06-06T19:08:40.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8888298273086548
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>To generate the data weights you can use this repo: <a rel="nofollow"
          href="https://github.com/bigcode-project/bigcode-data-mix#2---substitute-the-data-path">https://github.com/bigcode-project/bigcode-data-mix#2---substitute-the-data-path</a>.
          </p>

          <p>For short trainings, or non distributed (1 A100 in your case) using PEFT
          indicated here: <a rel="nofollow" href="https://github.com/bigcode-project/starcoder">https://github.com/bigcode-project/starcoder</a>  would
          be faster and easier to setup. Otherwise full fine-tuning could be expensive,
          for reference the fine-tuning of StarCoderBase on 35B of Python tokens to
          get StarCoder took ~2 days on 512 GPUs (in your case ABAP has much less
          data than Python so it would take much less time, but full-finetuning could
          be slow for one A100).</p>

          '
        raw: "To generate the data weights you can use this repo: https://github.com/bigcode-project/bigcode-data-mix#2---substitute-the-data-path.\
          \ \n\nFor short trainings, or non distributed (1 A100 in your case) using\
          \ PEFT indicated here: https://github.com/bigcode-project/starcoder  would\
          \ be faster and easier to setup. Otherwise full fine-tuning could be expensive,\
          \ for reference the fine-tuning of StarCoderBase on 35B of Python tokens\
          \ to get StarCoder took ~2 days on 512 GPUs (in your case ABAP has much\
          \ less data than Python so it would take much less time, but full-finetuning\
          \ could be slow for one A100)."
        updatedAt: '2023-06-06T19:08:40.910Z'
      numEdits: 0
      reactions: []
    id: 647f8438c785bbc0bc0405c0
    type: comment
  author: loubnabnl
  content: "To generate the data weights you can use this repo: https://github.com/bigcode-project/bigcode-data-mix#2---substitute-the-data-path.\
    \ \n\nFor short trainings, or non distributed (1 A100 in your case) using PEFT\
    \ indicated here: https://github.com/bigcode-project/starcoder  would be faster\
    \ and easier to setup. Otherwise full fine-tuning could be expensive, for reference\
    \ the fine-tuning of StarCoderBase on 35B of Python tokens to get StarCoder took\
    \ ~2 days on 512 GPUs (in your case ABAP has much less data than Python so it\
    \ would take much less time, but full-finetuning could be slow for one A100)."
  created_at: 2023-06-06 18:08:40+00:00
  edited: false
  hidden: false
  id: 647f8438c785bbc0bc0405c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c44e3ca36c410f975283a70da1a7dff.svg
      fullname: WWN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wwn
      type: user
    createdAt: '2023-06-07T03:54:08.000Z'
    data:
      edited: false
      editors:
      - wwn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.857746422290802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c44e3ca36c410f975283a70da1a7dff.svg
          fullname: WWN
          isHf: false
          isPro: false
          name: wwn
          type: user
        html: '<blockquote>

          <p>To generate the data weights you can use this repo: <a rel="nofollow"
          href="https://github.com/bigcode-project/bigcode-data-mix#2---substitute-the-data-path">https://github.com/bigcode-project/bigcode-data-mix#2---substitute-the-data-path</a>.
          </p>

          <p>For short trainings, or non distributed (1 A100 in your case) using PEFT
          indicated here: <a rel="nofollow" href="https://github.com/bigcode-project/starcoder">https://github.com/bigcode-project/starcoder</a>  would
          be faster and easier to setup. Otherwise full fine-tuning could be expensive,
          for reference the fine-tuning of StarCoderBase on 35B of Python tokens to
          get StarCoder took ~2 days on 512 GPUs (in your case ABAP has much less
          data than Python so it would take much less time, but full-finetuning could
          be slow for one A100).</p>

          </blockquote>

          <p>thx,  but how to generate the files in "gpt2-preprocessed_content_with_meta_document"
          folder from raw .parquet files</p>

          '
        raw: "> To generate the data weights you can use this repo: https://github.com/bigcode-project/bigcode-data-mix#2---substitute-the-data-path.\
          \ \n> \n> For short trainings, or non distributed (1 A100 in your case)\
          \ using PEFT indicated here: https://github.com/bigcode-project/starcoder\
          \  would be faster and easier to setup. Otherwise full fine-tuning could\
          \ be expensive, for reference the fine-tuning of StarCoderBase on 35B of\
          \ Python tokens to get StarCoder took ~2 days on 512 GPUs (in your case\
          \ ABAP has much less data than Python so it would take much less time, but\
          \ full-finetuning could be slow for one A100).\n\nthx,  but how to generate\
          \ the files in \"gpt2-preprocessed_content_with_meta_document\" folder from\
          \ raw .parquet files"
        updatedAt: '2023-06-07T03:54:08.959Z'
      numEdits: 0
      reactions: []
    id: 647fff6086888bbffbe68ec5
    type: comment
  author: wwn
  content: "> To generate the data weights you can use this repo: https://github.com/bigcode-project/bigcode-data-mix#2---substitute-the-data-path.\
    \ \n> \n> For short trainings, or non distributed (1 A100 in your case) using\
    \ PEFT indicated here: https://github.com/bigcode-project/starcoder  would be\
    \ faster and easier to setup. Otherwise full fine-tuning could be expensive, for\
    \ reference the fine-tuning of StarCoderBase on 35B of Python tokens to get StarCoder\
    \ took ~2 days on 512 GPUs (in your case ABAP has much less data than Python so\
    \ it would take much less time, but full-finetuning could be slow for one A100).\n\
    \nthx,  but how to generate the files in \"gpt2-preprocessed_content_with_meta_document\"\
    \ folder from raw .parquet files"
  created_at: 2023-06-07 02:54:08+00:00
  edited: false
  hidden: false
  id: 647fff6086888bbffbe68ec5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-06-07T07:45:39.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9500114321708679
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>you need to tokenize the data with Megatron-LM, see their readme</p>

          '
        raw: you need to tokenize the data with Megatron-LM, see their readme
        updatedAt: '2023-06-07T07:45:39.965Z'
      numEdits: 0
      reactions: []
    id: 648035a33aaeb1651714328a
    type: comment
  author: loubnabnl
  content: you need to tokenize the data with Megatron-LM, see their readme
  created_at: 2023-06-07 06:45:39+00:00
  edited: false
  hidden: false
  id: 648035a33aaeb1651714328a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22f39c40343f5cc194e8c0f995a60296.svg
      fullname: Chen, Lee Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leelichen
      type: user
    createdAt: '2023-06-08T01:45:20.000Z'
    data:
      edited: false
      editors:
      - leelichen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8927767872810364
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22f39c40343f5cc194e8c0f995a60296.svg
          fullname: Chen, Lee Li
          isHf: false
          isPro: false
          name: leelichen
          type: user
        html: '<blockquote>

          <p>To generate the data weights you can use this repo: <a rel="nofollow"
          href="https://github.com/bigcode-project/bigcode-data-mix#2---substitute-the-data-path">https://github.com/bigcode-project/bigcode-data-mix#2---substitute-the-data-path</a>.
          </p>

          <p>For short trainings, or non distributed (1 A100 in your case) using PEFT
          indicated here: <a rel="nofollow" href="https://github.com/bigcode-project/starcoder">https://github.com/bigcode-project/starcoder</a>  would
          be faster and easier to setup. Otherwise full fine-tuning could be expensive,
          for reference the fine-tuning of StarCoderBase on 35B of Python tokens to
          get StarCoder took ~2 days on 512 GPUs (in your case ABAP has much less
          data than Python so it would take much less time, but full-finetuning could
          be slow for one A100).</p>

          </blockquote>

          <p>What directory does CHECKPOINT_PATH refer to? Where can I find CHECKPOINT_PATH
          for starcoder?</p>

          '
        raw: "> To generate the data weights you can use this repo: https://github.com/bigcode-project/bigcode-data-mix#2---substitute-the-data-path.\
          \ \n> \n> For short trainings, or non distributed (1 A100 in your case)\
          \ using PEFT indicated here: https://github.com/bigcode-project/starcoder\
          \  would be faster and easier to setup. Otherwise full fine-tuning could\
          \ be expensive, for reference the fine-tuning of StarCoderBase on 35B of\
          \ Python tokens to get StarCoder took ~2 days on 512 GPUs (in your case\
          \ ABAP has much less data than Python so it would take much less time, but\
          \ full-finetuning could be slow for one A100).\n\nWhat directory does CHECKPOINT_PATH\
          \ refer to? Where can I find CHECKPOINT_PATH for starcoder?"
        updatedAt: '2023-06-08T01:45:20.817Z'
      numEdits: 0
      reactions: []
    id: 648132b040facadc55785cb2
    type: comment
  author: leelichen
  content: "> To generate the data weights you can use this repo: https://github.com/bigcode-project/bigcode-data-mix#2---substitute-the-data-path.\
    \ \n> \n> For short trainings, or non distributed (1 A100 in your case) using\
    \ PEFT indicated here: https://github.com/bigcode-project/starcoder  would be\
    \ faster and easier to setup. Otherwise full fine-tuning could be expensive, for\
    \ reference the fine-tuning of StarCoderBase on 35B of Python tokens to get StarCoder\
    \ took ~2 days on 512 GPUs (in your case ABAP has much less data than Python so\
    \ it would take much less time, but full-finetuning could be slow for one A100).\n\
    \nWhat directory does CHECKPOINT_PATH refer to? Where can I find CHECKPOINT_PATH\
    \ for starcoder?"
  created_at: 2023-06-08 00:45:20+00:00
  edited: false
  hidden: false
  id: 648132b040facadc55785cb2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-06-08T09:18:00.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9868812561035156
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>It''s the paths where the checkpoints are saved, for us it was specific
          to our cluster</p>

          '
        raw: It's the paths where the checkpoints are saved, for us it was specific
          to our cluster
        updatedAt: '2023-06-08T09:18:00.552Z'
      numEdits: 0
      reactions: []
    id: 64819cc8ea57ed6f5a367cd2
    type: comment
  author: loubnabnl
  content: It's the paths where the checkpoints are saved, for us it was specific
    to our cluster
  created_at: 2023-06-08 08:18:00+00:00
  edited: false
  hidden: false
  id: 64819cc8ea57ed6f5a367cd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2023-06-08T13:40:45.000Z'
    data:
      status: closed
    id: 6481da5d6f283a7468667cc1
    type: status-change
  author: cakiki
  created_at: 2023-06-08 12:40:45+00:00
  id: 6481da5d6f283a7468667cc1
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: bigcode/starcoderbase
repo_type: model
status: closed
target_branch: null
title: "May I ask where the training code is and does the training data include ABAP\
  \ language\uFF1F"
