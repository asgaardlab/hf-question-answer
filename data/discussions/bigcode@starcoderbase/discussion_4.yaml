!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Bilibili
conflicting_files: null
created_at: 2023-05-06 01:22:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640162547139-61c2e44c39245e7bf62def6f.jpeg?w=200&h=200&f=face
      fullname: Bilibili
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bilibili
      type: user
    createdAt: '2023-05-06T02:22:26.000Z'
    data:
      edited: true
      editors:
      - Bilibili
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640162547139-61c2e44c39245e7bf62def6f.jpeg?w=200&h=200&f=face
          fullname: Bilibili
          isHf: false
          isPro: false
          name: Bilibili
          type: user
        html: "<p>Hi, I am trying to run the given inference code in the Model Card,\
          \ but it gives me such error after downloading the files:</p>\n<pre><code>Downloading\
          \ (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 677/677 [00:00&lt;00:00,\
          \ 44.4kB/s]\nDownloading (\u2026)olve/main/vocab.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 777k/777k\
          \ [00:00&lt;00:00, 1.02MB/s]                                           \
          \                                                                      \
          \                           \nDownloading (\u2026)olve/main/merges.txt:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 442k/442k [00:00&lt;00:00, 774kB/s]\nDownloading (\u2026)/main/tokenizer.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.06M/2.06M\
          \ [00:00&lt;00:00, 2.09MB/s]\nDownloading (\u2026)cial_tokens_map.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 532/532 [00:00&lt;00:00, 99.9kB/s]\nDownloading (\u2026\
          )lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 1.04k/1.04k [00:00&lt;00:00, 271kB/s]\nTraceback (most\
          \ recent call last):                                   \n  File \"/home/workspace//starcoder/starcoder.py\"\
          , line 7, in &lt;module&gt;                                            \
          \                   \n    model = AutoModelForCausalLM.from_pretrained(checkpoint,\
          \ trust_remote_code=True).to(device)\n  File \"/root/miniconda3/envs/starcoder/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 441, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\
          \                     \n  File \"/root/miniconda3/envs/starcoder/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 917, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]         \n  File \"/root/miniconda3/envs/starcoder/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 623, in __getitem__\n    raise KeyError(key)                    \
          \                          \nKeyError: 'gpt_bigcode' \n</code></pre>\n<p>Seems\
          \ like error in config.json, right?</p>\n"
        raw: "Hi, I am trying to run the given inference code in the Model Card, but\
          \ it gives me such error after downloading the files:\n\n```\nDownloading\
          \ (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 677/677 [00:00<00:00,\
          \ 44.4kB/s]\nDownloading (\u2026)olve/main/vocab.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 777k/777k\
          \ [00:00<00:00, 1.02MB/s]                                              \
          \                                                                      \
          \                        \nDownloading (\u2026)olve/main/merges.txt: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 442k/442k [00:00<00:00, 774kB/s]\nDownloading (\u2026)/main/tokenizer.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.06M/2.06M\
          \ [00:00<00:00, 2.09MB/s]\nDownloading (\u2026)cial_tokens_map.json: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 532/532 [00:00<00:00, 99.9kB/s]\nDownloading (\u2026)lve/main/config.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 1.04k/1.04k [00:00<00:00, 271kB/s]\nTraceback (most recent call last):\
          \                                   \n  File \"/home/workspace//starcoder/starcoder.py\"\
          , line 7, in <module>                                                  \
          \             \n    model = AutoModelForCausalLM.from_pretrained(checkpoint,\
          \ trust_remote_code=True).to(device)\n  File \"/root/miniconda3/envs/starcoder/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 441, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\
          \                     \n  File \"/root/miniconda3/envs/starcoder/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 917, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]         \n  File \"/root/miniconda3/envs/starcoder/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 623, in __getitem__\n    raise KeyError(key)                    \
          \                          \nKeyError: 'gpt_bigcode' \n```\nSeems like error\
          \ in config.json, right?"
        updatedAt: '2023-05-06T02:25:45.711Z'
      numEdits: 1
      reactions: []
    id: 6455b9e2d0a574bca477fa79
    type: comment
  author: Bilibili
  content: "Hi, I am trying to run the given inference code in the Model Card, but\
    \ it gives me such error after downloading the files:\n\n```\nDownloading (\u2026\
    )okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 677/677 [00:00<00:00, 44.4kB/s]\nDownloading (\u2026)olve/main/vocab.json:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 777k/777k [00:00<00:00,\
    \ 1.02MB/s]                                                                  \
    \                                                                          \n\
    Downloading (\u2026)olve/main/merges.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 442k/442k [00:00<00:00, 774kB/s]\nDownloading (\u2026)/main/tokenizer.json:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.06M/2.06M [00:00<00:00, 2.09MB/s]\n\
    Downloading (\u2026)cial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 532/532 [00:00<00:00, 99.9kB/s]\nDownloading (\u2026\
    )lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.04k/1.04k\
    \ [00:00<00:00, 271kB/s]\nTraceback (most recent call last):                 \
    \                  \n  File \"/home/workspace//starcoder/starcoder.py\", line\
    \ 7, in <module>                                                             \
    \  \n    model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n\
    \  File \"/root/miniconda3/envs/starcoder/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 441, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\
    \                     \n  File \"/root/miniconda3/envs/starcoder/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 917, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
    model_type\"]]         \n  File \"/root/miniconda3/envs/starcoder/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 623, in __getitem__\n    raise KeyError(key)                          \
    \                    \nKeyError: 'gpt_bigcode' \n```\nSeems like error in config.json,\
    \ right?"
  created_at: 2023-05-06 01:22:26+00:00
  edited: true
  hidden: false
  id: 6455b9e2d0a574bca477fa79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640162547139-61c2e44c39245e7bf62def6f.jpeg?w=200&h=200&f=face
      fullname: Bilibili
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bilibili
      type: user
    createdAt: '2023-05-06T14:21:31.000Z'
    data:
      status: closed
    id: 6456626bcd6567f52fb544ff
    type: status-change
  author: Bilibili
  created_at: 2023-05-06 13:21:31+00:00
  id: 6456626bcd6567f52fb544ff
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2023-05-07T00:23:47.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: '<p>you need to be on transformers&gt;=4.28.1</p>

          '
        raw: you need to be on transformers>=4.28.1
        updatedAt: '2023-05-07T00:23:47.843Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Bilibili
    id: 6456ef9378c059b099ba7f3d
    type: comment
  author: mayank31398
  content: you need to be on transformers>=4.28.1
  created_at: 2023-05-06 23:23:47+00:00
  edited: false
  hidden: false
  id: 6456ef9378c059b099ba7f3d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: bigcode/starcoderbase
repo_type: model
status: closed
target_branch: null
title: 'KeyError: ''gpt_bigcode'''
