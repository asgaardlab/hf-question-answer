!!python/object:huggingface_hub.community.DiscussionWithDetails
author: iammano
conflicting_files: null
created_at: 2024-01-07 17:56:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64849ae2a2a4c9d5cb678cf8/PBZcJdkNV8RNAVnTdlUNT.jpeg?w=200&h=200&f=face
      fullname: Mano Ranjan N
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iammano
      type: user
    createdAt: '2024-01-07T17:56:40.000Z'
    data:
      edited: true
      editors:
      - iammano
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5346381068229675
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64849ae2a2a4c9d5cb678cf8/PBZcJdkNV8RNAVnTdlUNT.jpeg?w=200&h=200&f=face
          fullname: Mano Ranjan N
          isHf: false
          isPro: false
          name: iammano
          type: user
        html: "<p>Hi there, </p>\n<p>I'm getting this error when I try to pass the\
          \ model to the llm parameter in langchain's RetrievalQASourceChain parameter,\
          \ Below is my code.</p>\n<pre><code>def __init__(self, **kwargs):\n    \
          \    \"\"\"\n        Initializes the QA bot with a Qdrant client, embeddings,\
          \ and a language model.\n\n        Parameters:\n        - **kwargs: Additional\
          \ keyword arguments for configuration.\n        \"\"\"\n        \n     \
          \   self.client = QdrantClient(url=HOST_NAME,api_key=API_KEY)\n        self.embedding\
          \ = get_embedding()\n        self.device = \"cuda:0\" \n        if torch.cuda.is_available():\n\
          \            print(\"Inside GPU\")\n            \n            self.llm =\
          \ AutoModelForCausalLM.from_pretrained(\n                              \
          \  HF_MODEL,\n                                low_cpu_mem_usage=True,\n\
          \                                device_map=self.device\n              \
          \              )\n        else:\n            self.llm = CTransformers(model=MODEL_PATH,\n\
          \                                    model_type=MODEL_TYPE, \n         \
          \                           config=MODEL_PARAMETERS)\n\n\ndef answer(query):\n\
          qa = RetrievalQAWithSourcesChain.from_chain_type(\n                    \
          \            llm=self.llm,\n                                retriever=retriever,\n\
          \                                chain_type_kwargs={\n                 \
          \                   \"prompt\": my_prompt\n                            \
          \    },\n                                device_map=self.device,\n     \
          \                           reduce_k_below_max_tokens=True,\n          \
          \                      return_source_documents=True)\n        \n</code></pre>\n\
          <p>These are methods of a class that I have shared partial code here. Thanks\
          \ for your understanding. </p>\n<p>Error Message:</p>\n<pre><code>Traceback\
          \ (most recent call last):\n  File \"/home/manoranjan.n/doc_test/backend/main.py\"\
          , line 68, in ask_model\n    result = qabot.answer(data['question'])\n \
          \ File \"/home/manoranjan.n/doc_test/backend/qa_bot.py\", line 227, in answer\n\
          \    qa = RetrievalQAWithSourcesChain.from_chain_type(\n  File \"/home/manoranjan.n/.local/lib/python3.8/site-packages/langchain/chains/qa_with_sources/base.py\"\
          , line 85, in from_chain_type\n    combine_documents_chain = load_qa_with_sources_chain(\n\
          \  File \"/home/manoranjan.n/.local/lib/python3.8/site-packages/langchain/chains/qa_with_sources/loading.py\"\
          , line 183, in load_qa_with_sources_chain\n    return _func(llm, verbose=verbose,\
          \ **kwargs)\n  File \"/home/manoranjan.n/.local/lib/python3.8/site-packages/langchain/chains/qa_with_sources/loading.py\"\
          , line 62, in _load_stuff_chain\n    llm_chain = LLMChain(llm=llm, prompt=prompt,\
          \ verbose=verbose)\n  File \"/home/manoranjan.n/.local/lib/python3.8/site-packages/langchain_core/load/serializable.py\"\
          , line 107, in __init__\n    super().__init__(**kwargs)\n  File \"pydantic/main.py\"\
          , line 341, in pydantic.main.BaseModel.__init__\npydantic.error_wrappers.ValidationError:\
          \ 2 validation errors for LLMChain\nllm\n  instance of Runnable expected\
          \ (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n\
          \  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\n\
          </code></pre>\n<p>Somekind of validation error, please help me on this.</p>\n"
        raw: "Hi there, \n\nI'm getting this error when I try to pass the model to\
          \ the llm parameter in langchain's RetrievalQASourceChain parameter, Below\
          \ is my code.\n```\ndef __init__(self, **kwargs):\n        \"\"\"\n    \
          \    Initializes the QA bot with a Qdrant client, embeddings, and a language\
          \ model.\n\n        Parameters:\n        - **kwargs: Additional keyword\
          \ arguments for configuration.\n        \"\"\"\n        \n        self.client\
          \ = QdrantClient(url=HOST_NAME,api_key=API_KEY)\n        self.embedding\
          \ = get_embedding()\n        self.device = \"cuda:0\" \n        if torch.cuda.is_available():\n\
          \            print(\"Inside GPU\")\n            \n            self.llm =\
          \ AutoModelForCausalLM.from_pretrained(\n                              \
          \  HF_MODEL,\n                                low_cpu_mem_usage=True,\n\
          \                                device_map=self.device\n              \
          \              )\n        else:\n            self.llm = CTransformers(model=MODEL_PATH,\n\
          \                                    model_type=MODEL_TYPE, \n         \
          \                           config=MODEL_PARAMETERS)\n\n\ndef answer(query):\n\
          qa = RetrievalQAWithSourcesChain.from_chain_type(\n                    \
          \            llm=self.llm,\n                                retriever=retriever,\n\
          \                                chain_type_kwargs={\n                 \
          \                   \"prompt\": my_prompt\n                            \
          \    },\n                                device_map=self.device,\n     \
          \                           reduce_k_below_max_tokens=True,\n          \
          \                      return_source_documents=True)\n        \n```\nThese\
          \ are methods of a class that I have shared partial code here. Thanks for\
          \ your understanding. \n\nError Message:\n\n```\nTraceback (most recent\
          \ call last):\n  File \"/home/manoranjan.n/doc_test/backend/main.py\", line\
          \ 68, in ask_model\n    result = qabot.answer(data['question'])\n  File\
          \ \"/home/manoranjan.n/doc_test/backend/qa_bot.py\", line 227, in answer\n\
          \    qa = RetrievalQAWithSourcesChain.from_chain_type(\n  File \"/home/manoranjan.n/.local/lib/python3.8/site-packages/langchain/chains/qa_with_sources/base.py\"\
          , line 85, in from_chain_type\n    combine_documents_chain = load_qa_with_sources_chain(\n\
          \  File \"/home/manoranjan.n/.local/lib/python3.8/site-packages/langchain/chains/qa_with_sources/loading.py\"\
          , line 183, in load_qa_with_sources_chain\n    return _func(llm, verbose=verbose,\
          \ **kwargs)\n  File \"/home/manoranjan.n/.local/lib/python3.8/site-packages/langchain/chains/qa_with_sources/loading.py\"\
          , line 62, in _load_stuff_chain\n    llm_chain = LLMChain(llm=llm, prompt=prompt,\
          \ verbose=verbose)\n  File \"/home/manoranjan.n/.local/lib/python3.8/site-packages/langchain_core/load/serializable.py\"\
          , line 107, in __init__\n    super().__init__(**kwargs)\n  File \"pydantic/main.py\"\
          , line 341, in pydantic.main.BaseModel.__init__\npydantic.error_wrappers.ValidationError:\
          \ 2 validation errors for LLMChain\nllm\n  instance of Runnable expected\
          \ (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n\
          \  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\n\
          ```\nSomekind of validation error, please help me on this."
        updatedAt: '2024-01-07T17:58:08.949Z'
      numEdits: 1
      reactions: []
    id: 659ae5d8be7822d24dde37f9
    type: comment
  author: iammano
  content: "Hi there, \n\nI'm getting this error when I try to pass the model to the\
    \ llm parameter in langchain's RetrievalQASourceChain parameter, Below is my code.\n\
    ```\ndef __init__(self, **kwargs):\n        \"\"\"\n        Initializes the QA\
    \ bot with a Qdrant client, embeddings, and a language model.\n\n        Parameters:\n\
    \        - **kwargs: Additional keyword arguments for configuration.\n       \
    \ \"\"\"\n        \n        self.client = QdrantClient(url=HOST_NAME,api_key=API_KEY)\n\
    \        self.embedding = get_embedding()\n        self.device = \"cuda:0\" \n\
    \        if torch.cuda.is_available():\n            print(\"Inside GPU\")\n  \
    \          \n            self.llm = AutoModelForCausalLM.from_pretrained(\n  \
    \                              HF_MODEL,\n                                low_cpu_mem_usage=True,\n\
    \                                device_map=self.device\n                    \
    \        )\n        else:\n            self.llm = CTransformers(model=MODEL_PATH,\n\
    \                                    model_type=MODEL_TYPE, \n               \
    \                     config=MODEL_PARAMETERS)\n\n\ndef answer(query):\nqa = RetrievalQAWithSourcesChain.from_chain_type(\n\
    \                                llm=self.llm,\n                             \
    \   retriever=retriever,\n                                chain_type_kwargs={\n\
    \                                    \"prompt\": my_prompt\n                 \
    \               },\n                                device_map=self.device,\n\
    \                                reduce_k_below_max_tokens=True,\n           \
    \                     return_source_documents=True)\n        \n```\nThese are\
    \ methods of a class that I have shared partial code here. Thanks for your understanding.\
    \ \n\nError Message:\n\n```\nTraceback (most recent call last):\n  File \"/home/manoranjan.n/doc_test/backend/main.py\"\
    , line 68, in ask_model\n    result = qabot.answer(data['question'])\n  File \"\
    /home/manoranjan.n/doc_test/backend/qa_bot.py\", line 227, in answer\n    qa =\
    \ RetrievalQAWithSourcesChain.from_chain_type(\n  File \"/home/manoranjan.n/.local/lib/python3.8/site-packages/langchain/chains/qa_with_sources/base.py\"\
    , line 85, in from_chain_type\n    combine_documents_chain = load_qa_with_sources_chain(\n\
    \  File \"/home/manoranjan.n/.local/lib/python3.8/site-packages/langchain/chains/qa_with_sources/loading.py\"\
    , line 183, in load_qa_with_sources_chain\n    return _func(llm, verbose=verbose,\
    \ **kwargs)\n  File \"/home/manoranjan.n/.local/lib/python3.8/site-packages/langchain/chains/qa_with_sources/loading.py\"\
    , line 62, in _load_stuff_chain\n    llm_chain = LLMChain(llm=llm, prompt=prompt,\
    \ verbose=verbose)\n  File \"/home/manoranjan.n/.local/lib/python3.8/site-packages/langchain_core/load/serializable.py\"\
    , line 107, in __init__\n    super().__init__(**kwargs)\n  File \"pydantic/main.py\"\
    , line 341, in pydantic.main.BaseModel.__init__\npydantic.error_wrappers.ValidationError:\
    \ 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type;\
    \ expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type;\
    \ expected_arbitrary_type=Runnable)\n```\nSomekind of validation error, please\
    \ help me on this."
  created_at: 2024-01-07 17:56:40+00:00
  edited: true
  hidden: false
  id: 659ae5d8be7822d24dde37f9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Mistral-7B-Instruct-v0.2-AWQ
repo_type: model
status: open
target_branch: null
title: RAG issue |  loading the model in RetrievalQASourceChain
