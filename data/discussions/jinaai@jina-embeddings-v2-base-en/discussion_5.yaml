!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lysandre
conflicting_files: null
created_at: 2023-10-24 11:24:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-24T12:24:42.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8918273448944092
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: '<p>Hey! Transformers developer here, really love how you leverage the
          <code>jinaai/jina-bert-implementation</code> custom code within this specific
          repository.</p>

          <p>Do you have any feedback regarding this feature? Could anything make
          great contributions such as this one simpler for you and your team?</p>

          '
        raw: "Hey! Transformers developer here, really love how you leverage the `jinaai/jina-bert-implementation`\
          \ custom code within this specific repository.\r\n\r\nDo you have any feedback\
          \ regarding this feature? Could anything make great contributions such as\
          \ this one simpler for you and your team?"
        updatedAt: '2023-10-24T12:24:42.904Z'
      numEdits: 0
      reactions: []
    id: 6537b78a434f0b412ac2ad3a
    type: comment
  author: lysandre
  content: "Hey! Transformers developer here, really love how you leverage the `jinaai/jina-bert-implementation`\
    \ custom code within this specific repository.\r\n\r\nDo you have any feedback\
    \ regarding this feature? Could anything make great contributions such as this\
    \ one simpler for you and your team?"
  created_at: 2023-10-24 11:24:42+00:00
  edited: false
  hidden: false
  id: 6537b78a434f0b412ac2ad3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2023-10-24T17:57:42.000Z'
    data:
      edited: true
      editors:
      - Jackmin108
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9272876977920532
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;lysandre&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lysandre\"\
          >@<span class=\"underline\">lysandre</span></a></span>\n\n\t</span></span>!\
          \ Thank you and the HF team for implementing such a useful feature! Overall,\
          \ we are quite satisfied with the feature but we do have some minor feedback.</p>\n\
          <ol>\n<li><p>Documentation<br>The clarity of the transformers <a href=\"\
          https://huggingface.co/docs/transformers/custom_models\">documentation on\
          \ custom models</a> could be improved by describing the syntax of <code>auto_map</code>\
          \ in <em>config.json</em>. We never used the <code>register</code> functions\
          \ and would just directly modify the <em>config.json</em>. The behaviour\
          \ that allowed one to use code on the Hub from another repo using \"--\"\
          \ doesn't seem to be documented anywhere but we figured out that it was\
          \ possible because <code>save_pretrained</code> saves in this format when\
          \ using a custom model. The feature does seem to be pretty new though (I\
          \ believe ~6 months ago?) so maybe that is why it hasn't been too well documented\
          \ yet. But we think that if it was better communicated to users that it\
          \ was possible to do this, more people would develop on the Hub as we did.</p>\n\
          </li>\n<li><p>Promote <code>trust_remote_code</code> to environment variable<br>Some\
          \ downstream libraries currently do not support passing the <code>trust_remote_code</code>\
          \ argument. Notable to our work was <code>sentence_transformers</code>,\
          \ despite quite a few requests for this [<a rel=\"nofollow\" href=\"https://github.com/UKPLab/sentence-transformers/issues/1473\"\
          >1</a>, <a rel=\"nofollow\" href=\"https://github.com/UKPLab/sentence-transformers/issues/2272\"\
          >2</a>, <a rel=\"nofollow\" href=\"https://github.com/UKPLab/sentence-transformers/pull/2274\"\
          >3</a>]. This leads to us needing to <a rel=\"nofollow\" href=\"https://github.com/simonw/llm-sentence-transformers/pull/10/files\"\
          >monkeypatching the model loading logic in the libraries</a> to be able\
          \ to use our model. If <code>trust_remote_code</code> could be read from\
          \ an environment variable e.g. <code>HUGGINGFACE_TRUST_REMOTE_CODE</code>,\
          \ it would make it such that one only need set the environment variable\
          \ to enable loading custom models. This would make the use of custom models\
          \ much easier to adopt throughout the ecosystem.</p>\n</li>\n<li><p>Silent\
          \ failure when <code>trust_remote_code</code> is not set to True.<br>When\
          \ <code>trust_remote_code</code> is not set to True for our model, the behaviour\
          \ seems to be that it loads the classic BERT implementation from transformers\
          \ and <a rel=\"nofollow\" href=\"https://x.com/simonw/status/1716644983917392330?s=20\"\
          >throws a bunch of warnings from re-initialised weights</a>. This is not\
          \ ideal because if a downstream evaluation script forgot to set the arg,\
          \ it would generate inaccurate results and the only way of knowing that\
          \ something was wrong was to scroll through the output logs and see if this\
          \ warning appeared or print the model and see if the model has the right\
          \ name. If instead, it would error and ask the user to set the <code>trust_remote_code</code>\
          \ arg, it would be more easily caught and save us quite some head scratching\
          \ and communication overhead in the team.</p>\n</li>\n</ol>\n"
        raw: 'Hey @lysandre! Thank you and the HF team for implementing such a useful
          feature! Overall, we are quite satisfied with the feature but we do have
          some minor feedback.


          1. Documentation

          The clarity of the transformers [documentation on custom models](https://huggingface.co/docs/transformers/custom_models)
          could be improved by describing the syntax of `auto_map` in *config.json*.
          We never used the `register` functions and would just directly modify the
          *config.json*. The behaviour that allowed one to use code on the Hub from
          another repo using "--" doesn''t seem to be documented anywhere but we figured
          out that it was possible because `save_pretrained` saves in this format
          when using a custom model. The feature does seem to be pretty new though
          (I believe ~6 months ago?) so maybe that is why it hasn''t been too well
          documented yet. But we think that if it was better communicated to users
          that it was possible to do this, more people would develop on the Hub as
          we did.


          2. Promote `trust_remote_code` to environment variable

          Some downstream libraries currently do not support passing the `trust_remote_code`
          argument. Notable to our work was `sentence_transformers`, despite quite
          a few requests for this [[1](https://github.com/UKPLab/sentence-transformers/issues/1473),
          [2](https://github.com/UKPLab/sentence-transformers/issues/2272), [3](https://github.com/UKPLab/sentence-transformers/pull/2274)].
          This leads to us needing to [monkeypatching the model loading logic in the
          libraries](https://github.com/simonw/llm-sentence-transformers/pull/10/files)
          to be able to use our model. If `trust_remote_code` could be read from an
          environment variable e.g. `HUGGINGFACE_TRUST_REMOTE_CODE`, it would make
          it such that one only need set the environment variable to enable loading
          custom models. This would make the use of custom models much easier to adopt
          throughout the ecosystem.


          3. Silent failure when `trust_remote_code` is not set to True.

          When `trust_remote_code` is not set to True for our model, the behaviour
          seems to be that it loads the classic BERT implementation from transformers
          and [throws a bunch of warnings from re-initialised weights](https://x.com/simonw/status/1716644983917392330?s=20).
          This is not ideal because if a downstream evaluation script forgot to set
          the arg, it would generate inaccurate results and the only way of knowing
          that something was wrong was to scroll through the output logs and see if
          this warning appeared or print the model and see if the model has the right
          name. If instead, it would error and ask the user to set the `trust_remote_code`
          arg, it would be more easily caught and save us quite some head scratching
          and communication overhead in the team.'
        updatedAt: '2023-10-25T19:43:23.099Z'
      numEdits: 1
      reactions:
      - count: 6
        reaction: "\U0001F917"
        users:
        - bwang0911
        - lysandre
        - lewtun
        - joaogante
        - julien-c
        - ydshieh
      - count: 4
        reaction: "\U0001F44D"
        users:
        - michaelfeil
        - ydshieh
        - guyaglionby
        - kpriyanshu256
    id: 65380596ffe3e0513108cd3a
    type: comment
  author: Jackmin108
  content: 'Hey @lysandre! Thank you and the HF team for implementing such a useful
    feature! Overall, we are quite satisfied with the feature but we do have some
    minor feedback.


    1. Documentation

    The clarity of the transformers [documentation on custom models](https://huggingface.co/docs/transformers/custom_models)
    could be improved by describing the syntax of `auto_map` in *config.json*. We
    never used the `register` functions and would just directly modify the *config.json*.
    The behaviour that allowed one to use code on the Hub from another repo using
    "--" doesn''t seem to be documented anywhere but we figured out that it was possible
    because `save_pretrained` saves in this format when using a custom model. The
    feature does seem to be pretty new though (I believe ~6 months ago?) so maybe
    that is why it hasn''t been too well documented yet. But we think that if it was
    better communicated to users that it was possible to do this, more people would
    develop on the Hub as we did.


    2. Promote `trust_remote_code` to environment variable

    Some downstream libraries currently do not support passing the `trust_remote_code`
    argument. Notable to our work was `sentence_transformers`, despite quite a few
    requests for this [[1](https://github.com/UKPLab/sentence-transformers/issues/1473),
    [2](https://github.com/UKPLab/sentence-transformers/issues/2272), [3](https://github.com/UKPLab/sentence-transformers/pull/2274)].
    This leads to us needing to [monkeypatching the model loading logic in the libraries](https://github.com/simonw/llm-sentence-transformers/pull/10/files)
    to be able to use our model. If `trust_remote_code` could be read from an environment
    variable e.g. `HUGGINGFACE_TRUST_REMOTE_CODE`, it would make it such that one
    only need set the environment variable to enable loading custom models. This would
    make the use of custom models much easier to adopt throughout the ecosystem.


    3. Silent failure when `trust_remote_code` is not set to True.

    When `trust_remote_code` is not set to True for our model, the behaviour seems
    to be that it loads the classic BERT implementation from transformers and [throws
    a bunch of warnings from re-initialised weights](https://x.com/simonw/status/1716644983917392330?s=20).
    This is not ideal because if a downstream evaluation script forgot to set the
    arg, it would generate inaccurate results and the only way of knowing that something
    was wrong was to scroll through the output logs and see if this warning appeared
    or print the model and see if the model has the right name. If instead, it would
    error and ask the user to set the `trust_remote_code` arg, it would be more easily
    caught and save us quite some head scratching and communication overhead in the
    team.'
  created_at: 2023-10-24 16:57:42+00:00
  edited: true
  hidden: false
  id: 65380596ffe3e0513108cd3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2023-10-24T18:21:05.000Z'
    data:
      edited: true
      editors:
      - Jackmin108
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9049056768417358
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: '<p>On an unrelated note, we really loved developing on the Hugging
          Face Hub and would love to be able to stay in the Hub ecosystem for a larger
          portion of our workflow. However, a big blocker for us is the rate limit
          (we hit the commit limit and artifact download limit quite a few times).
          The behaviour of the rate limit seems to be a pretty well asked question
          in the huggingface forums [<a rel="nofollow" href="https://discuss.huggingface.co/t/httperror-429-client-error-too-many-requests-for-url/29460">1</a>,
          <a rel="nofollow" href="https://discuss.huggingface.co/t/recent-critical-issue-with-hub-links-and-429-error/48452">2</a>,
          <a rel="nofollow" href="https://discuss.huggingface.co/t/facing-rate-limit-issues-on-the-inference-api/22412">3</a>,
          <a rel="nofollow" href="https://discuss.huggingface.co/t/subscription-tiers-descriptions-unclear-e-g-higher-rate-limits-doesnt-specify-what-the-rate-limit-is/43709">4</a>,
          <a rel="nofollow" href="https://discuss.huggingface.co/t/hugging-face-api-rate-limits/16746">5</a>].</p>

          <p>We understand that storing and transferring data at scale is not cheap
          and we would be happy to consider having an Enterprise subscription for
          our needs. However, it is not clear to us based on the pricing page and
          HF communications that it will solve our rate limit issue. Perhaps this
          can be more clearly communicated on the pricing page as well as the forums
          to allow more companies and users to adopt workflows that utilise the Hub.</p>

          '
        raw: 'On an unrelated note, we really loved developing on the Hugging Face
          Hub and would love to be able to stay in the Hub ecosystem for a larger
          portion of our workflow. However, a big blocker for us is the rate limit
          (we hit the commit limit and artifact download limit quite a few times).
          The behaviour of the rate limit seems to be a pretty well asked question
          in the huggingface forums [[1](https://discuss.huggingface.co/t/httperror-429-client-error-too-many-requests-for-url/29460),
          [2](https://discuss.huggingface.co/t/recent-critical-issue-with-hub-links-and-429-error/48452),
          [3](https://discuss.huggingface.co/t/facing-rate-limit-issues-on-the-inference-api/22412),
          [4](https://discuss.huggingface.co/t/subscription-tiers-descriptions-unclear-e-g-higher-rate-limits-doesnt-specify-what-the-rate-limit-is/43709),
          [5](https://discuss.huggingface.co/t/hugging-face-api-rate-limits/16746)].


          We understand that storing and transferring data at scale is not cheap and
          we would be happy to consider having an Enterprise subscription for our
          needs. However, it is not clear to us based on the pricing page and HF communications
          that it will solve our rate limit issue. Perhaps this can be more clearly
          communicated on the pricing page as well as the forums to allow more companies
          and users to adopt workflows that utilise the Hub.'
        updatedAt: '2023-10-24T18:27:01.256Z'
      numEdits: 5
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - bwang0911
        - nateraw
        - jupyterjazz
        - julien-c
        - ydshieh
    id: 65380b1152d82b887702fbbf
    type: comment
  author: Jackmin108
  content: 'On an unrelated note, we really loved developing on the Hugging Face Hub
    and would love to be able to stay in the Hub ecosystem for a larger portion of
    our workflow. However, a big blocker for us is the rate limit (we hit the commit
    limit and artifact download limit quite a few times). The behaviour of the rate
    limit seems to be a pretty well asked question in the huggingface forums [[1](https://discuss.huggingface.co/t/httperror-429-client-error-too-many-requests-for-url/29460),
    [2](https://discuss.huggingface.co/t/recent-critical-issue-with-hub-links-and-429-error/48452),
    [3](https://discuss.huggingface.co/t/facing-rate-limit-issues-on-the-inference-api/22412),
    [4](https://discuss.huggingface.co/t/subscription-tiers-descriptions-unclear-e-g-higher-rate-limits-doesnt-specify-what-the-rate-limit-is/43709),
    [5](https://discuss.huggingface.co/t/hugging-face-api-rate-limits/16746)].


    We understand that storing and transferring data at scale is not cheap and we
    would be happy to consider having an Enterprise subscription for our needs. However,
    it is not clear to us based on the pricing page and HF communications that it
    will solve our rate limit issue. Perhaps this can be more clearly communicated
    on the pricing page as well as the forums to allow more companies and users to
    adopt workflows that utilise the Hub.'
  created_at: 2023-10-24 17:21:05+00:00
  edited: true
  hidden: false
  id: 65380b1152d82b887702fbbf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-31T13:34:31.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9088916778564453
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Jackmin108&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Jackmin108\"\
          >@<span class=\"underline\">Jackmin108</span></a></span>\n\n\t</span></span>,\
          \ that's great feedback! Thanks a lot for coming back to us.</p>\n<p>I'm\
          \ circulating this internally and we'll see how to fix the abovementioned\
          \ issues best.</p>\n<hr>\n<p>Regarding rate limits, there is a higher rate\
          \ limit for pro users; let me loop <span data-props=\"{&quot;user&quot;:&quot;jeffboudier&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jeffboudier\"\
          >@<span class=\"underline\">jeffboudier</span></a></span>\n\n\t</span></span>\
          \ in regarding enterprise subscriptions.</p>\n"
        raw: 'Hey @Jackmin108, that''s great feedback! Thanks a lot for coming back
          to us.


          I''m circulating this internally and we''ll see how to fix the abovementioned
          issues best.


          ---


          Regarding rate limits, there is a higher rate limit for pro users; let me
          loop @jeffboudier in regarding enterprise subscriptions.'
        updatedAt: '2023-10-31T13:34:31.128Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - bwang0911
        - Jackmin108
    id: 654102670a2101c338f3d737
    type: comment
  author: lysandre
  content: 'Hey @Jackmin108, that''s great feedback! Thanks a lot for coming back
    to us.


    I''m circulating this internally and we''ll see how to fix the abovementioned
    issues best.


    ---


    Regarding rate limits, there is a higher rate limit for pro users; let me loop
    @jeffboudier in regarding enterprise subscriptions.'
  created_at: 2023-10-31 12:34:31+00:00
  edited: false
  hidden: false
  id: 654102670a2101c338f3d737
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1605114051380-noauth.jpeg?w=200&h=200&f=face
      fullname: Jeff Boudier
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeffboudier
      type: user
    createdAt: '2023-11-01T05:27:22.000Z'
    data:
      edited: false
      editors:
      - jeffboudier
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8936796188354492
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1605114051380-noauth.jpeg?w=200&h=200&f=face
          fullname: Jeff Boudier
          isHf: true
          isPro: false
          name: jeffboudier
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;lysandre&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lysandre\"\
          >@<span class=\"underline\">lysandre</span></a></span>\n\n\t</span></span>\
          \  - hi <span data-props=\"{&quot;user&quot;:&quot;Jackmin108&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Jackmin108\">@<span class=\"\
          underline\">Jackmin108</span></a></span>\n\n\t</span></span>, congrats on\
          \ the impactful open source release!</p>\n<p>Enterprise subscriptions for\
          \ Hub organizations grant PRO privileges to all their members. So members\
          \ of an Enterprise organization get their rate limits lifted to higher values.\
          \ The reason we don't give actual numbers for rate limits is that the values\
          \ depend on how much load we are getting and subject to change - the bottom\
          \ line is PRO users and Enterprise organization members get priority. I\
          \ agree we can document this better - eventually we will. For clarity, Inference\
          \ Endpoints and Spaces usage is not subject to rate limits.</p>\n"
        raw: 'Thanks @lysandre  - hi @Jackmin108, congrats on the impactful open source
          release!


          Enterprise subscriptions for Hub organizations grant PRO privileges to all
          their members. So members of an Enterprise organization get their rate limits
          lifted to higher values. The reason we don''t give actual numbers for rate
          limits is that the values depend on how much load we are getting and subject
          to change - the bottom line is PRO users and Enterprise organization members
          get priority. I agree we can document this better - eventually we will.
          For clarity, Inference Endpoints and Spaces usage is not subject to rate
          limits.'
        updatedAt: '2023-11-01T05:27:22.012Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - bwang0911
    id: 6541e1baaa727b1a95b8ef98
    type: comment
  author: jeffboudier
  content: 'Thanks @lysandre  - hi @Jackmin108, congrats on the impactful open source
    release!


    Enterprise subscriptions for Hub organizations grant PRO privileges to all their
    members. So members of an Enterprise organization get their rate limits lifted
    to higher values. The reason we don''t give actual numbers for rate limits is
    that the values depend on how much load we are getting and subject to change -
    the bottom line is PRO users and Enterprise organization members get priority.
    I agree we can document this better - eventually we will. For clarity, Inference
    Endpoints and Spaces usage is not subject to rate limits.'
  created_at: 2023-11-01 04:27:22+00:00
  edited: false
  hidden: false
  id: 6541e1baaa727b1a95b8ef98
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63491dc83d8dc83a55cb749c/IoqJrOIaEnYO_S7si4KGp.jpeg?w=200&h=200&f=face
      fullname: Bo Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bwang0911
      type: user
    createdAt: '2023-12-22T11:53:02.000Z'
    data:
      status: closed
    id: 6585789e0e68ae8756206ca5
    type: status-change
  author: bwang0911
  created_at: 2023-12-22 11:53:02+00:00
  id: 6585789e0e68ae8756206ca5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: jinaai/jina-embeddings-v2-base-en
repo_type: model
status: closed
target_branch: null
title: Love the Hub implementation
