!!python/object:huggingface_hub.community.DiscussionWithDetails
author: davidefiocco
conflicting_files: null
created_at: 2024-01-24 23:23:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb0ec5880c3c3b93976e4ae6968ad467.svg
      fullname: Davide Fiocco
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: davidefiocco
      type: user
    createdAt: '2024-01-24T23:23:32.000Z'
    data:
      edited: true
      editors:
      - davidefiocco
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6528728604316711
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb0ec5880c3c3b93976e4ae6968ad467.svg
          fullname: Davide Fiocco
          isHf: false
          isPro: false
          name: davidefiocco
          type: user
        html: "<p>I am computing Jina v2 embeddings via the <code>transformers</code>\
          \ Python libraries and via the API (see <a rel=\"nofollow\" href=\"https://jina.ai/embeddings/\"\
          >https://jina.ai/embeddings/</a>).</p>\n<p>With <code>transformers</code>\
          \ I can run code along the lines of the model card</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModel\n\nsentences = [<span\
          \ class=\"hljs-string\">'How is the weather today?'</span>]\n\nmodel = AutoModel.from_pretrained(<span\
          \ class=\"hljs-string\">'jinaai/jina-embeddings-v2-base-en'</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\nembeddings_1 = model.encode(sentences)\n\
          </code></pre>\n<p>or </p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> sentence_transformers <span class=\"hljs-keyword\"\
          >import</span> SentenceTransformer\n\nmodel = SentenceTransformer(<span\
          \ class=\"hljs-string\">'jinaai/jina-embeddings-v2-base-en'</span>)\nembeddings_2\
          \ = model.encode(sentences)\n</code></pre>\n<p>and the resulting <code>embeddings_1</code>\
          \ and <code>embeddings_2</code> match.</p>\n<p>However if I use the Jina\
          \ API e.g. via</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> requests\n\nurl = <span class=\"hljs-string\"\
          >'https://api.jina.ai/v1/embeddings'</span>\n\nheaders = {\n  <span class=\"\
          hljs-string\">'Content-Type'</span>: <span class=\"hljs-string\">'application/json'</span>,\n\
          \  <span class=\"hljs-string\">'Authorization'</span>: <span class=\"hljs-string\"\
          >'Bearer jina_123456...'</span> <span class=\"hljs-comment\"># visit https://jina.ai/embeddings/\
          \ for an API key</span>\n}\n\ndata = {\n  <span class=\"hljs-string\">'input'</span>:\
          \ sentences,\n  <span class=\"hljs-string\">'model'</span>: <span class=\"\
          hljs-string\">'jina-embeddings-v2-base-en'</span> <span class=\"hljs-comment\"\
          ># note that the model name matches</span>\n}\n\nresponse = requests.post(url,\
          \ headers=headers, json=data)\nembeddings_3 = <span class=\"hljs-built_in\"\
          >eval</span>(response.content)[<span class=\"hljs-string\">\"data\"</span>][<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">\"embedding\"\
          </span>]\n</code></pre>\n<p><code>embeddings_3</code> differ from the other\
          \ two arrays by a small difference, around 2e-4 in absolute value on average.\
          \ I see this discrepancy both with CPU and GPU runtimes.</p>\n<p>What am\
          \ I doing wrong? I also posted this very question on <a rel=\"nofollow\"\
          \ href=\"https://stackoverflow.com/questions/77875253/why-does-local-inference-differ-from-the-api-when-computing-jina-embeddings\"\
          >https://stackoverflow.com/questions/77875253/why-does-local-inference-differ-from-the-api-when-computing-jina-embeddings</a>\
          \ </p>\n"
        raw: "I am computing Jina v2 embeddings via the `transformers` Python libraries\
          \ and via the API (see https://jina.ai/embeddings/).\n\nWith `transformers`\
          \ I can run code along the lines of the model card\n\n```python\nfrom transformers\
          \ import AutoModel\n\nsentences = ['How is the weather today?']\n\nmodel\
          \ = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n\
          embeddings_1 = model.encode(sentences)\n```\n\nor \n\n\n```python\nfrom\
          \ sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('jinaai/jina-embeddings-v2-base-en')\n\
          embeddings_2 = model.encode(sentences)\n```\n\nand the resulting `embeddings_1`\
          \ and `embeddings_2` match.\n\nHowever if I use the Jina API e.g. via\n\n\
          ```python\nimport requests\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\
          \nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization':\
          \ 'Bearer jina_123456...' # visit https://jina.ai/embeddings/ for an API\
          \ key\n}\n\ndata = {\n  'input': sentences,\n  'model': 'jina-embeddings-v2-base-en'\
          \ # note that the model name matches\n}\n\nresponse = requests.post(url,\
          \ headers=headers, json=data)\nembeddings_3 = eval(response.content)[\"\
          data\"][0][\"embedding\"]\n```\n\n`embeddings_3` differ from the other two\
          \ arrays by a small difference, around 2e-4 in absolute value on average.\
          \ I see this discrepancy both with CPU and GPU runtimes.\n\nWhat am I doing\
          \ wrong? I also posted this very question on https://stackoverflow.com/questions/77875253/why-does-local-inference-differ-from-the-api-when-computing-jina-embeddings "
        updatedAt: '2024-01-25T10:47:22.152Z'
      numEdits: 3
      reactions: []
    id: 65b19bf41dcf354c00da5157
    type: comment
  author: davidefiocco
  content: "I am computing Jina v2 embeddings via the `transformers` Python libraries\
    \ and via the API (see https://jina.ai/embeddings/).\n\nWith `transformers` I\
    \ can run code along the lines of the model card\n\n```python\nfrom transformers\
    \ import AutoModel\n\nsentences = ['How is the weather today?']\n\nmodel = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en',\
    \ trust_remote_code=True)\nembeddings_1 = model.encode(sentences)\n```\n\nor \n\
    \n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel\
    \ = SentenceTransformer('jinaai/jina-embeddings-v2-base-en')\nembeddings_2 = model.encode(sentences)\n\
    ```\n\nand the resulting `embeddings_1` and `embeddings_2` match.\n\nHowever if\
    \ I use the Jina API e.g. via\n\n```python\nimport requests\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\
    \nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer\
    \ jina_123456...' # visit https://jina.ai/embeddings/ for an API key\n}\n\ndata\
    \ = {\n  'input': sentences,\n  'model': 'jina-embeddings-v2-base-en' # note that\
    \ the model name matches\n}\n\nresponse = requests.post(url, headers=headers,\
    \ json=data)\nembeddings_3 = eval(response.content)[\"data\"][0][\"embedding\"\
    ]\n```\n\n`embeddings_3` differ from the other two arrays by a small difference,\
    \ around 2e-4 in absolute value on average. I see this discrepancy both with CPU\
    \ and GPU runtimes.\n\nWhat am I doing wrong? I also posted this very question\
    \ on https://stackoverflow.com/questions/77875253/why-does-local-inference-differ-from-the-api-when-computing-jina-embeddings "
  created_at: 2024-01-24 23:23:32+00:00
  edited: true
  hidden: false
  id: 65b19bf41dcf354c00da5157
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/26a5e536b47ab91ef5ad50e40cc2e91f.svg
      fullname: Felix
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: numb3r3
      type: user
    createdAt: '2024-01-25T03:21:22.000Z'
    data:
      edited: false
      editors:
      - numb3r3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.95159512758255
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/26a5e536b47ab91ef5ad50e40cc2e91f.svg
          fullname: Felix
          isHf: false
          isPro: false
          name: numb3r3
          type: user
        html: '<p>A good catch. Actually, in our API, we are running the model forward
          with half-precision <code>fp16</code> for higher cost-efficiency. I believe
          this is the source of the inconsistence. </p>

          '
        raw: 'A good catch. Actually, in our API, we are running the model forward
          with half-precision `fp16` for higher cost-efficiency. I believe this is
          the source of the inconsistence. '
        updatedAt: '2024-01-25T03:21:22.709Z'
      numEdits: 0
      reactions: []
    id: 65b1d3b2f32c79fea7229a7e
    type: comment
  author: numb3r3
  content: 'A good catch. Actually, in our API, we are running the model forward with
    half-precision `fp16` for higher cost-efficiency. I believe this is the source
    of the inconsistence. '
  created_at: 2024-01-25 03:21:22+00:00
  edited: false
  hidden: false
  id: 65b1d3b2f32c79fea7229a7e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb0ec5880c3c3b93976e4ae6968ad467.svg
      fullname: Davide Fiocco
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: davidefiocco
      type: user
    createdAt: '2024-01-25T10:43:35.000Z'
    data:
      edited: true
      editors:
      - davidefiocco
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9676352739334106
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb0ec5880c3c3b93976e4ae6968ad467.svg
          fullname: Davide Fiocco
          isHf: false
          isPro: false
          name: davidefiocco
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;numb3r3&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/numb3r3\">@<span class=\"\
          underline\">numb3r3</span></a></span>\n\n\t</span></span>, thanks for your\
          \ prompt reply, that would explain!</p>\n<p>Just to be 100% sure, is there\
          \ a way to prove this, e.g. is there code I can run with available model\
          \ weights to match API results exactly?</p>\n<p>Having half-precision models\
          \ running locally would be neat, as we'd get</p>\n<ol>\n<li>a local equivalent\
          \ of the Jina API</li>\n<li>the same performance/efficiency gains that you\
          \ are enjoying at Jina</li>\n</ol>\n<p>Hopefully it's not a cheeky ask :)\
          \ </p>\n"
        raw: 'Hi @numb3r3, thanks for your prompt reply, that would explain!


          Just to be 100% sure, is there a way to prove this, e.g. is there code I
          can run with available model weights to match API results exactly?


          Having half-precision models running locally would be neat, as we''d get


          1) a local equivalent of the Jina API

          2) the same performance/efficiency gains that you are enjoying at Jina


          Hopefully it''s not a cheeky ask :) '
        updatedAt: '2024-01-25T11:04:32.492Z'
      numEdits: 5
      reactions: []
    id: 65b23b57a73e98a1e94a88fb
    type: comment
  author: davidefiocco
  content: 'Hi @numb3r3, thanks for your prompt reply, that would explain!


    Just to be 100% sure, is there a way to prove this, e.g. is there code I can run
    with available model weights to match API results exactly?


    Having half-precision models running locally would be neat, as we''d get


    1) a local equivalent of the Jina API

    2) the same performance/efficiency gains that you are enjoying at Jina


    Hopefully it''s not a cheeky ask :) '
  created_at: 2024-01-25 10:43:35+00:00
  edited: true
  hidden: false
  id: 65b23b57a73e98a1e94a88fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/26a5e536b47ab91ef5ad50e40cc2e91f.svg
      fullname: Felix
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: numb3r3
      type: user
    createdAt: '2024-01-25T12:10:29.000Z'
    data:
      edited: false
      editors:
      - numb3r3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8522810935974121
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/26a5e536b47ab91ef5ad50e40cc2e91f.svg
          fullname: Felix
          isHf: false
          isPro: false
          name: numb3r3
          type: user
        html: '<p>You can easily enjoy the fp16 optimization by</p>

          <pre><code>from transformers import AutoModel


          sentences = [''How is the weather today?'']


          model = AutoModel.from_pretrained(''jinaai/jina-embeddings-v2-base-en'',
          trust_remote_code=True,  dtype=torch.float16)

          embeddings_1 = model.encode(sentences)

          </code></pre>

          <p>By the way, if you are interested in a private deployment of our API
          with all related optimizations such as fp16 support, I wanted to let you
          know that we also offer Jina AI through AWS SageMaker. This allows for optimized,
          on-premises deployment of our 8k embedding models. You can find more details
          here: <a rel="nofollow" href="https://jina.ai/news/jina-ai-8k-embedding-models-hit-aws-marketplace-for-on-prem-deployment/">https://jina.ai/news/jina-ai-8k-embedding-models-hit-aws-marketplace-for-on-prem-deployment/</a></p>

          '
        raw: 'You can easily enjoy the fp16 optimization by


          ```

          from transformers import AutoModel


          sentences = [''How is the weather today?'']


          model = AutoModel.from_pretrained(''jinaai/jina-embeddings-v2-base-en'',
          trust_remote_code=True,  dtype=torch.float16)

          embeddings_1 = model.encode(sentences)

          ```


          By the way, if you are interested in a private deployment of our API with
          all related optimizations such as fp16 support, I wanted to let you know
          that we also offer Jina AI through AWS SageMaker. This allows for optimized,
          on-premises deployment of our 8k embedding models. You can find more details
          here: https://jina.ai/news/jina-ai-8k-embedding-models-hit-aws-marketplace-for-on-prem-deployment/'
        updatedAt: '2024-01-25T12:10:29.712Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - davidefiocco
    id: 65b24fb5b389ca2de16907ce
    type: comment
  author: numb3r3
  content: 'You can easily enjoy the fp16 optimization by


    ```

    from transformers import AutoModel


    sentences = [''How is the weather today?'']


    model = AutoModel.from_pretrained(''jinaai/jina-embeddings-v2-base-en'', trust_remote_code=True,  dtype=torch.float16)

    embeddings_1 = model.encode(sentences)

    ```


    By the way, if you are interested in a private deployment of our API with all
    related optimizations such as fp16 support, I wanted to let you know that we also
    offer Jina AI through AWS SageMaker. This allows for optimized, on-premises deployment
    of our 8k embedding models. You can find more details here: https://jina.ai/news/jina-ai-8k-embedding-models-hit-aws-marketplace-for-on-prem-deployment/'
  created_at: 2024-01-25 12:10:29+00:00
  edited: false
  hidden: false
  id: 65b24fb5b389ca2de16907ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb0ec5880c3c3b93976e4ae6968ad467.svg
      fullname: Davide Fiocco
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: davidefiocco
      type: user
    createdAt: '2024-01-25T12:55:14.000Z'
    data:
      edited: true
      editors:
      - davidefiocco
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7834439277648926
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb0ec5880c3c3b93976e4ae6968ad467.svg
          fullname: Davide Fiocco
          isHf: false
          isPro: false
          name: davidefiocco
          type: user
        html: '<p>Hey, thanks again (and sorry for cross-posting on StackOverflow,
          will make sure that the result/conclusion of this discussion will be there
          as well).</p>

          <p>I tried this following your advice:</p>

          <pre><code>from transformers import AutoModel

          import torch


          sentences = [''How is the weather today?'']


          model = AutoModel.from_pretrained(''jinaai/jina-embeddings-v2-base-en'',
          trust_remote_code=True, token = "hf_my_authorized_token", torch_dtype =
          torch.float16)

          embeddings_1 = model.encode(sentences)

          </code></pre>

          <p>i.e. slightly modifying the <code>from_pretrained</code> call because
          <code>dtype</code> (as in your original response) is not an expected kwarg
          for <code>JinaBertModel.__init__()</code>.</p>

          <p>However, the snipped above throws a <code>RuntimeError: "LayerNormKernelImpl"
          not implemented for ''Half''</code>, which is the same error I get with</p>

          <pre><code>model = AutoModel.from_pretrained(''jinaai/jina-embeddings-v2-base-en'',
          trust_remote_code=True, token = "hf_my_authorized_token")

          model.half()

          embeddings_1 = model.encode(sentences)

          </code></pre>

          <p>using <code>transformers==4.35.2</code>. So this approach doesn''t seem
          to work for me :/</p>

          <p>Thanks a ton also for the AWS suggestion, it''s not the cloud provider
          I am using atm but it''s good to know!</p>

          '
        raw: 'Hey, thanks again (and sorry for cross-posting on StackOverflow, will
          make sure that the result/conclusion of this discussion will be there as
          well).


          I tried this following your advice:


          ```

          from transformers import AutoModel

          import torch


          sentences = [''How is the weather today?'']


          model = AutoModel.from_pretrained(''jinaai/jina-embeddings-v2-base-en'',
          trust_remote_code=True, token = "hf_my_authorized_token", torch_dtype =
          torch.float16)

          embeddings_1 = model.encode(sentences)

          ```


          i.e. slightly modifying the `from_pretrained` call because `dtype` (as in
          your original response) is not an expected kwarg for `JinaBertModel.__init__()`.


          However, the snipped above throws a `RuntimeError: "LayerNormKernelImpl"
          not implemented for ''Half''`, which is the same error I get with


          ```

          model = AutoModel.from_pretrained(''jinaai/jina-embeddings-v2-base-en'',
          trust_remote_code=True, token = "hf_my_authorized_token")

          model.half()

          embeddings_1 = model.encode(sentences)

          ```


          using `transformers==4.35.2`. So this approach doesn''t seem to work for
          me :/


          Thanks a ton also for the AWS suggestion, it''s not the cloud provider I
          am using atm but it''s good to know!'
        updatedAt: '2024-01-25T13:29:33.361Z'
      numEdits: 6
      reactions: []
    id: 65b25a32ce74220b83377547
    type: comment
  author: davidefiocco
  content: 'Hey, thanks again (and sorry for cross-posting on StackOverflow, will
    make sure that the result/conclusion of this discussion will be there as well).


    I tried this following your advice:


    ```

    from transformers import AutoModel

    import torch


    sentences = [''How is the weather today?'']


    model = AutoModel.from_pretrained(''jinaai/jina-embeddings-v2-base-en'', trust_remote_code=True,
    token = "hf_my_authorized_token", torch_dtype = torch.float16)

    embeddings_1 = model.encode(sentences)

    ```


    i.e. slightly modifying the `from_pretrained` call because `dtype` (as in your
    original response) is not an expected kwarg for `JinaBertModel.__init__()`.


    However, the snipped above throws a `RuntimeError: "LayerNormKernelImpl" not implemented
    for ''Half''`, which is the same error I get with


    ```

    model = AutoModel.from_pretrained(''jinaai/jina-embeddings-v2-base-en'', trust_remote_code=True,
    token = "hf_my_authorized_token")

    model.half()

    embeddings_1 = model.encode(sentences)

    ```


    using `transformers==4.35.2`. So this approach doesn''t seem to work for me :/


    Thanks a ton also for the AWS suggestion, it''s not the cloud provider I am using
    atm but it''s good to know!'
  created_at: 2024-01-25 12:55:14+00:00
  edited: true
  hidden: false
  id: 65b25a32ce74220b83377547
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 38
repo_id: jinaai/jina-embeddings-v2-base-en
repo_type: model
status: open
target_branch: null
title: Why does local inference differ from the API?
