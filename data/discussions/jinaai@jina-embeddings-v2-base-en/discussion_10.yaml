!!python/object:huggingface_hub.community.DiscussionWithDetails
author: do-me
conflicting_files: null
created_at: 2023-10-26 17:40:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IiercF_qxHWize2kitl9X.jpeg?w=200&h=200&f=face
      fullname: "Dominik Weckm\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: do-me
      type: user
    createdAt: '2023-10-26T18:40:49.000Z'
    data:
      edited: false
      editors:
      - do-me
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7778787612915039
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IiercF_qxHWize2kitl9X.jpeg?w=200&h=200&f=face
          fullname: "Dominik Weckm\xFCller"
          isHf: false
          isPro: false
          name: do-me
          type: user
        html: '<p>Great models!</p>

          <p>I just quantized the base and small model for onnx usage e.g. in transformers.js:</p>

          <ul>

          <li><a href="https://huggingface.co/do-me/jina-embeddings-v2-small-en">https://huggingface.co/do-me/jina-embeddings-v2-small-en</a></li>

          <li><a href="https://huggingface.co/do-me/jina-embeddings-v2-base-en">https://huggingface.co/do-me/jina-embeddings-v2-base-en</a></li>

          </ul>

          <p>I use these models in <a rel="nofollow" href="https://do-me.github.io/SemanticFinder/">SemanticFinder</a>
          for semantic similarity; to the right you can see the similarity score.
          </p>

          <p>While the small one yields expected good results i.e. food-related chunks
          are most similar...</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/eOQosflZH76WWrAbbrYlW.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/eOQosflZH76WWrAbbrYlW.png"></a></p>

          <p>... the base one for some reason returns complete random, unrelated results.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/QtqAbGRfvxo51Wzii0CI9.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/QtqAbGRfvxo51Wzii0CI9.png"></a></p>

          <p>I just used the standard onnx conversion script of the transformers.js
          repo with default settings. Is there any kind of setting that needs some
          tweaking maybe?<br>I also noticed that if I use longer chunks (e.g. 400
          instead of 40 chars), the model behaves as expected again.</p>

          <p>In any case it would be cool if you could add the onnx versions to this
          repo too :)</p>

          '
        raw: "Great models!\r\n\r\nI just quantized the base and small model for onnx\
          \ usage e.g. in transformers.js:\r\n\r\n- https://huggingface.co/do-me/jina-embeddings-v2-small-en\r\
          \n- https://huggingface.co/do-me/jina-embeddings-v2-base-en\r\n\r\nI use\
          \ these models in [SemanticFinder](https://do-me.github.io/SemanticFinder/)\
          \ for semantic similarity; to the right you can see the similarity score.\
          \ \r\n\r\nWhile the small one yields expected good results i.e. food-related\
          \ chunks are most similar...\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/eOQosflZH76WWrAbbrYlW.png)\r\
          \n\r\n... the base one for some reason returns complete random, unrelated\
          \ results.\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/QtqAbGRfvxo51Wzii0CI9.png)\r\
          \n\r\nI just used the standard onnx conversion script of the transformers.js\
          \ repo with default settings. Is there any kind of setting that needs some\
          \ tweaking maybe? \r\nI also noticed that if I use longer chunks (e.g. 400\
          \ instead of 40 chars), the model behaves as expected again.\r\n\r\nIn any\
          \ case it would be cool if you could add the onnx versions to this repo\
          \ too :)"
        updatedAt: '2023-10-26T18:40:49.685Z'
      numEdits: 0
      reactions: []
    id: 653ab2b1025e5d01724481fe
    type: comment
  author: do-me
  content: "Great models!\r\n\r\nI just quantized the base and small model for onnx\
    \ usage e.g. in transformers.js:\r\n\r\n- https://huggingface.co/do-me/jina-embeddings-v2-small-en\r\
    \n- https://huggingface.co/do-me/jina-embeddings-v2-base-en\r\n\r\nI use these\
    \ models in [SemanticFinder](https://do-me.github.io/SemanticFinder/) for semantic\
    \ similarity; to the right you can see the similarity score. \r\n\r\nWhile the\
    \ small one yields expected good results i.e. food-related chunks are most similar...\r\
    \n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/eOQosflZH76WWrAbbrYlW.png)\r\
    \n\r\n... the base one for some reason returns complete random, unrelated results.\r\
    \n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/QtqAbGRfvxo51Wzii0CI9.png)\r\
    \n\r\nI just used the standard onnx conversion script of the transformers.js repo\
    \ with default settings. Is there any kind of setting that needs some tweaking\
    \ maybe? \r\nI also noticed that if I use longer chunks (e.g. 400 instead of 40\
    \ chars), the model behaves as expected again.\r\n\r\nIn any case it would be\
    \ cool if you could add the onnx versions to this repo too :)"
  created_at: 2023-10-26 17:40:49+00:00
  edited: false
  hidden: false
  id: 653ab2b1025e5d01724481fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2023-10-26T22:20:07.000Z'
    data:
      edited: true
      editors:
      - Jackmin108
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9691688418388367
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: '<p>Hrmmm that is weird. Maybe the model was converted wrongly?<br>Can
          you try the <a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-en/resolve/main/model.onnx">ONNX
          weights we uploaded in fp32</a>?</p>

          <p>If it still has this weird behavior for you, maybe it is the quantization
          to int8.<br>Though I cant think of a reason why the quantization would make
          the model worse for short sequence (when it does fine at longer).</p>

          '
        raw: 'Hrmmm that is weird. Maybe the model was converted wrongly?

          Can you try the [ONNX weights we uploaded in fp32](https://huggingface.co/jinaai/jina-embeddings-v2-base-en/resolve/main/model.onnx)?


          If it still has this weird behavior for you, maybe it is the quantization
          to int8.

          Though I cant think of a reason why the quantization would make the model
          worse for short sequence (when it does fine at longer).


          '
        updatedAt: '2023-10-26T22:20:45.849Z'
      numEdits: 1
      reactions: []
    id: 653ae61733574f20f7fb1102
    type: comment
  author: Jackmin108
  content: 'Hrmmm that is weird. Maybe the model was converted wrongly?

    Can you try the [ONNX weights we uploaded in fp32](https://huggingface.co/jinaai/jina-embeddings-v2-base-en/resolve/main/model.onnx)?


    If it still has this weird behavior for you, maybe it is the quantization to int8.

    Though I cant think of a reason why the quantization would make the model worse
    for short sequence (when it does fine at longer).


    '
  created_at: 2023-10-26 21:20:07+00:00
  edited: true
  hidden: false
  id: 653ae61733574f20f7fb1102
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IiercF_qxHWize2kitl9X.jpeg?w=200&h=200&f=face
      fullname: "Dominik Weckm\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: do-me
      type: user
    createdAt: '2023-10-27T06:47:52.000Z'
    data:
      edited: true
      editors:
      - do-me
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9349944591522217
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IiercF_qxHWize2kitl9X.jpeg?w=200&h=200&f=face
          fullname: "Dominik Weckm\xFCller"
          isHf: false
          isPro: false
          name: do-me
          type: user
        html: '<p>Your fp32 weights work just fine on short sequences, just as expected.
          You can test yourself if you like with <code>do-me/test</code>:<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/Ck7UeuBeSHqRtCVT05oGa.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/Ck7UeuBeSHqRtCVT05oGa.png"></a></p>

          <p>However, on a closer look again on the quantized versions, even on longer
          sequences the results seem to be bit arbitrary (though better overall).
          I''m pretty sure there was something wrong with the conversion. Will head
          over to transformers.js and ask the experts there. See <a rel="nofollow"
          href="https://github.com/xenova/transformers.js/issues/371">issue here</a>.</p>

          '
        raw: 'Your fp32 weights work just fine on short sequences, just as expected.
          You can test yourself if you like with `do-me/test`:

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/Ck7UeuBeSHqRtCVT05oGa.png)


          However, on a closer look again on the quantized versions, even on longer
          sequences the results seem to be bit arbitrary (though better overall).
          I''m pretty sure there was something wrong with the conversion. Will head
          over to transformers.js and ask the experts there. See [issue here](https://github.com/xenova/transformers.js/issues/371).'
        updatedAt: '2023-10-27T06:55:49.581Z'
      numEdits: 1
      reactions: []
    id: 653b5d18ae155b92ba0caa1f
    type: comment
  author: do-me
  content: 'Your fp32 weights work just fine on short sequences, just as expected.
    You can test yourself if you like with `do-me/test`:

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/Ck7UeuBeSHqRtCVT05oGa.png)


    However, on a closer look again on the quantized versions, even on longer sequences
    the results seem to be bit arbitrary (though better overall). I''m pretty sure
    there was something wrong with the conversion. Will head over to transformers.js
    and ask the experts there. See [issue here](https://github.com/xenova/transformers.js/issues/371).'
  created_at: 2023-10-27 05:47:52+00:00
  edited: true
  hidden: false
  id: 653b5d18ae155b92ba0caa1f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-10-27T13:21:25.000Z'
    data:
      edited: true
      editors:
      - Xenova
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8428066968917847
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
          fullname: Joshua
          isHf: true
          isPro: false
          name: Xenova
          type: user
        html: '<p>Hi all - creator of Transformers.js here. I just wanted to ask the
          authors how they converted the weights to ONNX? When I try do it with optimum,
          I get the following error:</p>

          <pre><code>torch.onnx.errors.UnsupportedOperatorError: Exporting the operator
          ''aten::scaled_dot_product_attention'' to ONNX opset version 11 is not supported.
          Please feel free to request support or submit a pull request on PyTorch
          GitHub: https://github.com/pytorch/pytorch/issues.

          </code></pre>

          <p>However, inspecting your model in netron suggests it''s also using v11.</p>

          <p>I will upload a Transformers.js-compatible version using your model (w/
          quantization) shortly, but I would love to be able to convert it myself
          too!</p>

          '
        raw: 'Hi all - creator of Transformers.js here. I just wanted to ask the authors
          how they converted the weights to ONNX? When I try do it with optimum, I
          get the following error:

          ```

          torch.onnx.errors.UnsupportedOperatorError: Exporting the operator ''aten::scaled_dot_product_attention''
          to ONNX opset version 11 is not supported. Please feel free to request support
          or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.

          ```


          However, inspecting your model in netron suggests it''s also using v11.


          I will upload a Transformers.js-compatible version using your model (w/
          quantization) shortly, but I would love to be able to convert it myself
          too!'
        updatedAt: '2023-10-27T13:21:49.615Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - do-me
    id: 653bb955165096faae199518
    type: comment
  author: Xenova
  content: 'Hi all - creator of Transformers.js here. I just wanted to ask the authors
    how they converted the weights to ONNX? When I try do it with optimum, I get the
    following error:

    ```

    torch.onnx.errors.UnsupportedOperatorError: Exporting the operator ''aten::scaled_dot_product_attention''
    to ONNX opset version 11 is not supported. Please feel free to request support
    or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.

    ```


    However, inspecting your model in netron suggests it''s also using v11.


    I will upload a Transformers.js-compatible version using your model (w/ quantization)
    shortly, but I would love to be able to convert it myself too!'
  created_at: 2023-10-27 12:21:25+00:00
  edited: true
  hidden: false
  id: 653bb955165096faae199518
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-10-27T13:51:30.000Z'
    data:
      edited: false
      editors:
      - Xenova
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5879406332969666
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
          fullname: Joshua
          isHf: true
          isPro: false
          name: Xenova
          type: user
        html: '<p>Uploaded: <a href="https://huggingface.co/Xenova/jina-embeddings-v2-base-en">https://huggingface.co/Xenova/jina-embeddings-v2-base-en</a><br>(and
          <a href="https://huggingface.co/Xenova/jina-embeddings-v2-small-en">https://huggingface.co/Xenova/jina-embeddings-v2-small-en</a>)</p>

          '
        raw: 'Uploaded: https://huggingface.co/Xenova/jina-embeddings-v2-base-en

          (and https://huggingface.co/Xenova/jina-embeddings-v2-small-en)

          '
        updatedAt: '2023-10-27T13:51:30.751Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - do-me
        - Jackmin108
        - Felladrin
    id: 653bc0622c81c4adc8f28318
    type: comment
  author: Xenova
  content: 'Uploaded: https://huggingface.co/Xenova/jina-embeddings-v2-base-en

    (and https://huggingface.co/Xenova/jina-embeddings-v2-small-en)

    '
  created_at: 2023-10-27 12:51:30+00:00
  edited: false
  hidden: false
  id: 653bc0622c81c4adc8f28318
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-10-27T14:08:04.000Z'
    data:
      edited: false
      editors:
      - Xenova
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5720839500427246
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
          fullname: Joshua
          isHf: true
          isPro: false
          name: Xenova
          type: user
        html: "<p>And the quantized versions seem to perform relatively well too:</p>\n\
          <pre><code class=\"language-js\"><span class=\"hljs-comment\">// npm i <span\
          \ data-props=\"{&quot;user&quot;:&quot;xenova&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/xenova\">@<span class=\"underline\"\
          >xenova</span></a></span>\n\n\t</span></span>/transformers</span>\n<span\
          \ class=\"hljs-keyword\">import</span> { pipeline, cos_sim } <span class=\"\
          hljs-keyword\">from</span> <span class=\"hljs-string\">'@xenova/transformers'</span>;\n\
          \n<span class=\"hljs-comment\">// Create feature extraction pipeline</span>\n\
          <span class=\"hljs-keyword\">const</span> extractor = <span class=\"hljs-keyword\"\
          >await</span> <span class=\"hljs-title function_\">pipeline</span>(<span\
          \ class=\"hljs-string\">'feature-extraction'</span>, <span class=\"hljs-string\"\
          >'Xenova/jina-embeddings-v2-base-en'</span>,\n    <span class=\"hljs-comment\"\
          >// { quantized: false } // &lt;-- uncomment to use the unquantized versions</span>\n\
          );\n\n<span class=\"hljs-comment\">// Generate embeddings</span>\n<span\
          \ class=\"hljs-keyword\">const</span> output = <span class=\"hljs-keyword\"\
          >await</span> <span class=\"hljs-title function_\">extractor</span>(\n \
          \   [<span class=\"hljs-string\">'How is the weather today?'</span>, <span\
          \ class=\"hljs-string\">'What is the current weather like today?'</span>],\n\
          \    { <span class=\"hljs-attr\">pooling</span>: <span class=\"hljs-string\"\
          >'mean'</span> }\n);\n\n<span class=\"hljs-comment\">// Compute cosine similarity</span>\n\
          <span class=\"hljs-variable language_\">console</span>.<span class=\"hljs-title\
          \ function_\">log</span>(<span class=\"hljs-title function_\">cos_sim</span>(output[<span\
          \ class=\"hljs-number\">0</span>].<span class=\"hljs-property\">data</span>,\
          \ output[<span class=\"hljs-number\">1</span>].<span class=\"hljs-property\"\
          >data</span>)); \n<span class=\"hljs-comment\">// quantized (0.9022937687830741)\
          \ vs unquantized (0.9341313949712492)</span>\n</code></pre>\n"
        raw: "And the quantized versions seem to perform relatively well too:\n\n\
          ```js\n// npm i @xenova/transformers\nimport { pipeline, cos_sim } from\
          \ '@xenova/transformers';\n\n// Create feature extraction pipeline\nconst\
          \ extractor = await pipeline('feature-extraction', 'Xenova/jina-embeddings-v2-base-en',\n\
          \    // { quantized: false } // <-- uncomment to use the unquantized versions\n\
          );\n\n// Generate embeddings\nconst output = await extractor(\n    ['How\
          \ is the weather today?', 'What is the current weather like today?'],\n\
          \    { pooling: 'mean' }\n);\n\n// Compute cosine similarity\nconsole.log(cos_sim(output[0].data,\
          \ output[1].data)); \n// quantized (0.9022937687830741) vs unquantized (0.9341313949712492)\n\
          ```"
        updatedAt: '2023-10-27T14:08:04.074Z'
      numEdits: 0
      reactions: []
    id: 653bc44471278c21dcd97395
    type: comment
  author: Xenova
  content: "And the quantized versions seem to perform relatively well too:\n\n```js\n\
    // npm i @xenova/transformers\nimport { pipeline, cos_sim } from '@xenova/transformers';\n\
    \n// Create feature extraction pipeline\nconst extractor = await pipeline('feature-extraction',\
    \ 'Xenova/jina-embeddings-v2-base-en',\n    // { quantized: false } // <-- uncomment\
    \ to use the unquantized versions\n);\n\n// Generate embeddings\nconst output\
    \ = await extractor(\n    ['How is the weather today?', 'What is the current weather\
    \ like today?'],\n    { pooling: 'mean' }\n);\n\n// Compute cosine similarity\n\
    console.log(cos_sim(output[0].data, output[1].data)); \n// quantized (0.9022937687830741)\
    \ vs unquantized (0.9341313949712492)\n```"
  created_at: 2023-10-27 13:08:04+00:00
  edited: false
  hidden: false
  id: 653bc44471278c21dcd97395
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-10-27T14:08:14.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
          fullname: Joshua
          isHf: true
          isPro: false
          name: Xenova
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-10-27T14:08:58.129Z'
      numEdits: 1
      reactions: []
    id: 653bc44e045212d531b1171b
    type: comment
  author: Xenova
  content: This comment has been hidden
  created_at: 2023-10-27 13:08:14+00:00
  edited: true
  hidden: true
  id: 653bc44e045212d531b1171b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IiercF_qxHWize2kitl9X.jpeg?w=200&h=200&f=face
      fullname: "Dominik Weckm\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: do-me
      type: user
    createdAt: '2023-10-27T20:22:07.000Z'
    data:
      edited: true
      editors:
      - do-me
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8320150375366211
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IiercF_qxHWize2kitl9X.jpeg?w=200&h=200&f=face
          fullname: "Dominik Weckm\xFCller"
          isHf: false
          isPro: false
          name: do-me
          type: user
        html: '<p>Work like a charm now, just indexed the whole bible once with 200
          words and 4000 chars - both work great also with short search queries! :)<br><a
          rel="nofollow" href="https://twitter.com/DomeGIS/status/1717999491427495958">https://twitter.com/DomeGIS/status/1717999491427495958</a></p>

          '
        raw: 'Work like a charm now, just indexed the whole bible once with 200 words
          and 4000 chars - both work great also with short search queries! :)

          https://twitter.com/DomeGIS/status/1717999491427495958'
        updatedAt: '2023-10-27T20:22:25.076Z'
      numEdits: 1
      reactions: []
    id: 653c1befcfe482d11903863b
    type: comment
  author: do-me
  content: 'Work like a charm now, just indexed the whole bible once with 200 words
    and 4000 chars - both work great also with short search queries! :)

    https://twitter.com/DomeGIS/status/1717999491427495958'
  created_at: 2023-10-27 19:22:07+00:00
  edited: true
  hidden: false
  id: 653c1befcfe482d11903863b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2023-10-27T20:59:41.000Z'
    data:
      edited: true
      editors:
      - Jackmin108
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7072616219520569
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: '<p>Glad you like the model!</p>

          <p>We made 2 modifications to fix 2 issues with the ONNX export:</p>

          <ol>

          <li><p><code>scaled_dot_product_attention</code> (SDPA)<br>As you have observed,
          the ONNX 11 opset does not contain <code>scaled_dot_product_attention</code>.
          For our ONNX 11 export, we disabled the <code>scaled_dot_product_attention</code>
          path by setting <code>attn_implementation=None</code> in the model config.
          This causes the tracer to go through the standard python implementation
          of the SDPA operation which is able to be represented in ONNX 11.</p>

          </li>

          <li><p>Protobuf serialization limit (dynamic alibi tensor alloc)<br>Once
          you solve issue 1, you will encounter a 2nd issue:</p>

          </li>

          </ol>

          <pre><code class="language-bash">[libprotobuf ERROR /Users/runner/work/pytorch/pytorch/pytorch/third_party/protobuf/src/google/protobuf/message_lite.cc:457]
          onnx_torch.ModelProto exceeded maximum protobuf size of 2GB: 3768614404

          </code></pre>

          <p>ONNX uses protobuf which has a serialized file size limit of 2GB. The
          limit is hit because of the temporary alibi buffer we allocate at model
          creation (to save some malloc operations and to prevent memory fragmentation).<br>In
          order to reduce the file size, we allocate the alibi tensor on the fly instead.</p>

          <p>The required code changes are in <a href="https://huggingface.co/jinaai/jina-bert-implementation/discussions/7/files">this
          PR</a>.<br>With these changes, we are able to export the model using the
          optimum exporter:</p>

          <pre><code class="language-bash">pip install <span class="hljs-string">''optimum[exporters]''</span>

          optimum-cli <span class="hljs-built_in">export</span> onnx --model jina-embeddings-v2-base-en-airgap
          --task feature-extraction --trust-remote-code --opset 11 jina_onnx

          </code></pre>

          '
        raw: 'Glad you like the model!


          We made 2 modifications to fix 2 issues with the ONNX export:

          1. `scaled_dot_product_attention` (SDPA)

          As you have observed, the ONNX 11 opset does not contain `scaled_dot_product_attention`.
          For our ONNX 11 export, we disabled the `scaled_dot_product_attention` path
          by setting `attn_implementation=None` in the model config. This causes the
          tracer to go through the standard python implementation of the SDPA operation
          which is able to be represented in ONNX 11.


          2. Protobuf serialization limit (dynamic alibi tensor alloc)

          Once you solve issue 1, you will encounter a 2nd issue:

          ```bash

          [libprotobuf ERROR /Users/runner/work/pytorch/pytorch/pytorch/third_party/protobuf/src/google/protobuf/message_lite.cc:457]
          onnx_torch.ModelProto exceeded maximum protobuf size of 2GB: 3768614404

          ```

          ONNX uses protobuf which has a serialized file size limit of 2GB. The limit
          is hit because of the temporary alibi buffer we allocate at model creation
          (to save some malloc operations and to prevent memory fragmentation).

          In order to reduce the file size, we allocate the alibi tensor on the fly
          instead.


          The required code changes are in [this PR](https://huggingface.co/jinaai/jina-bert-implementation/discussions/7/files).

          With these changes, we are able to export the model using the optimum exporter:

          ```bash

          pip install ''optimum[exporters]''

          optimum-cli export onnx --model jina-embeddings-v2-base-en-airgap --task
          feature-extraction --trust-remote-code --opset 11 jina_onnx

          ```'
        updatedAt: '2023-10-27T21:01:13.697Z'
      numEdits: 3
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Xenova
        - kousun12
    id: 653c24bdaa1f487614a1fdaf
    type: comment
  author: Jackmin108
  content: 'Glad you like the model!


    We made 2 modifications to fix 2 issues with the ONNX export:

    1. `scaled_dot_product_attention` (SDPA)

    As you have observed, the ONNX 11 opset does not contain `scaled_dot_product_attention`.
    For our ONNX 11 export, we disabled the `scaled_dot_product_attention` path by
    setting `attn_implementation=None` in the model config. This causes the tracer
    to go through the standard python implementation of the SDPA operation which is
    able to be represented in ONNX 11.


    2. Protobuf serialization limit (dynamic alibi tensor alloc)

    Once you solve issue 1, you will encounter a 2nd issue:

    ```bash

    [libprotobuf ERROR /Users/runner/work/pytorch/pytorch/pytorch/third_party/protobuf/src/google/protobuf/message_lite.cc:457]
    onnx_torch.ModelProto exceeded maximum protobuf size of 2GB: 3768614404

    ```

    ONNX uses protobuf which has a serialized file size limit of 2GB. The limit is
    hit because of the temporary alibi buffer we allocate at model creation (to save
    some malloc operations and to prevent memory fragmentation).

    In order to reduce the file size, we allocate the alibi tensor on the fly instead.


    The required code changes are in [this PR](https://huggingface.co/jinaai/jina-bert-implementation/discussions/7/files).

    With these changes, we are able to export the model using the optimum exporter:

    ```bash

    pip install ''optimum[exporters]''

    optimum-cli export onnx --model jina-embeddings-v2-base-en-airgap --task feature-extraction
    --trust-remote-code --opset 11 jina_onnx

    ```'
  created_at: 2023-10-27 19:59:41+00:00
  edited: true
  hidden: false
  id: 653c24bdaa1f487614a1fdaf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2023-10-27T21:05:44.000Z'
    data:
      edited: false
      editors:
      - Jackmin108
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6792561411857605
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: '<p><code>scaled_dot_product_attention</code> was actually added in
          ONNX 14. I have exported the models with <code>scaled_dot_product_attention</code>
          in ONNX 14 in a PR.<br>Can you guys help to test if it provides any speed
          improvement?</p>

          <p>Small: <a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en/resolve/e79175ccfa7251b6bd7eb5bbbeaabe93c44b417f/model.onnx">https://huggingface.co/jinaai/jina-embeddings-v2-small-en/resolve/e79175ccfa7251b6bd7eb5bbbeaabe93c44b417f/model.onnx</a><br>Base:
          <a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-en/resolve/d78362089ac649604c57978b78184611f2e72772/model.onnx">https://huggingface.co/jinaai/jina-embeddings-v2-base-en/resolve/d78362089ac649604c57978b78184611f2e72772/model.onnx</a></p>

          '
        raw: '`scaled_dot_product_attention` was actually added in ONNX 14. I have
          exported the models with `scaled_dot_product_attention` in ONNX 14 in a
          PR.

          Can you guys help to test if it provides any speed improvement?


          Small: https://huggingface.co/jinaai/jina-embeddings-v2-small-en/resolve/e79175ccfa7251b6bd7eb5bbbeaabe93c44b417f/model.onnx

          Base: https://huggingface.co/jinaai/jina-embeddings-v2-base-en/resolve/d78362089ac649604c57978b78184611f2e72772/model.onnx

          '
        updatedAt: '2023-10-27T21:05:44.732Z'
      numEdits: 0
      reactions: []
    id: 653c2628059fdd2587a99334
    type: comment
  author: Jackmin108
  content: '`scaled_dot_product_attention` was actually added in ONNX 14. I have exported
    the models with `scaled_dot_product_attention` in ONNX 14 in a PR.

    Can you guys help to test if it provides any speed improvement?


    Small: https://huggingface.co/jinaai/jina-embeddings-v2-small-en/resolve/e79175ccfa7251b6bd7eb5bbbeaabe93c44b417f/model.onnx

    Base: https://huggingface.co/jinaai/jina-embeddings-v2-base-en/resolve/d78362089ac649604c57978b78184611f2e72772/model.onnx

    '
  created_at: 2023-10-27 20:05:44+00:00
  edited: false
  hidden: false
  id: 653c2628059fdd2587a99334
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IiercF_qxHWize2kitl9X.jpeg?w=200&h=200&f=face
      fullname: "Dominik Weckm\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: do-me
      type: user
    createdAt: '2023-10-27T21:55:14.000Z'
    data:
      edited: false
      editors:
      - do-me
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.884388267993927
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IiercF_qxHWize2kitl9X.jpeg?w=200&h=200&f=face
          fullname: "Dominik Weckm\xFCller"
          isHf: false
          isPro: false
          name: do-me
          type: user
        html: "<p>Thank you for looking into this. Sure, we can help with testing!<br><span\
          \ data-props=\"{&quot;user&quot;:&quot;Xenova&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/Xenova\">@<span class=\"underline\"\
          >Xenova</span></a></span>\n\n\t</span></span> if I understand correctly\
          \ the models should work even better with these modifications, so could\
          \ you quantize them again? </p>\n<p>Meanwhile <a rel=\"nofollow\" href=\"\
          https://do-me.github.io/SemanticFinder/\">I added your unquantized versions</a>\
          \ if you want to try them. There are Xenova's first quantized versions and\
          \ your current unquantized ones <span data-props=\"{&quot;user&quot;:&quot;Jackmin108&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Jackmin108\"\
          >@<span class=\"underline\">Jackmin108</span></a></span>\n\n\t</span></span>\
          \ on my test repos below:<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/yko5_Ll9dknt7_ra2mi-9.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/yko5_Ll9dknt7_ra2mi-9.png\"\
          ></a></p>\n<p>Hope this helps in some way. From what I see, the new unquantized\
          \ models are now even faster for indexing than the quantized ones! </p>\n"
        raw: "Thank you for looking into this. Sure, we can help with testing! \n\
          @Xenova if I understand correctly the models should work even better with\
          \ these modifications, so could you quantize them again? \n\nMeanwhile [I\
          \ added your unquantized versions](https://do-me.github.io/SemanticFinder/)\
          \ if you want to try them. There are Xenova's first quantized versions and\
          \ your current unquantized ones @Jackmin108 on my test repos below:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/yko5_Ll9dknt7_ra2mi-9.png)\n\
          \nHope this helps in some way. From what I see, the new unquantized models\
          \ are now even faster for indexing than the quantized ones! "
        updatedAt: '2023-10-27T21:55:14.674Z'
      numEdits: 0
      reactions: []
    id: 653c31c29c7bf38da274a4c7
    type: comment
  author: do-me
  content: "Thank you for looking into this. Sure, we can help with testing! \n@Xenova\
    \ if I understand correctly the models should work even better with these modifications,\
    \ so could you quantize them again? \n\nMeanwhile [I added your unquantized versions](https://do-me.github.io/SemanticFinder/)\
    \ if you want to try them. There are Xenova's first quantized versions and your\
    \ current unquantized ones @Jackmin108 on my test repos below:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c4da8719565937fb268b32/yko5_Ll9dknt7_ra2mi-9.png)\n\
    \nHope this helps in some way. From what I see, the new unquantized models are\
    \ now even faster for indexing than the quantized ones! "
  created_at: 2023-10-27 20:55:14+00:00
  edited: false
  hidden: false
  id: 653c31c29c7bf38da274a4c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594936097363-noauth.jpeg?w=200&h=200&f=face
      fullname: Nate Raw
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nateraw
      type: user
    createdAt: '2023-11-06T07:50:46.000Z'
    data:
      edited: false
      editors:
      - nateraw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8577895164489746
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594936097363-noauth.jpeg?w=200&h=200&f=face
          fullname: Nate Raw
          isHf: false
          isPro: false
          name: nateraw
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Jackmin108&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Jackmin108\">@<span class=\"\
          underline\">Jackmin108</span></a></span>\n\n\t</span></span> can you provide\
          \ an example of using these onnx weights you've contributed to compute embeddings,\
          \ please? :)</p>\n"
        raw: '@Jackmin108 can you provide an example of using these onnx weights you''ve
          contributed to compute embeddings, please? :)'
        updatedAt: '2023-11-06T07:50:46.750Z'
      numEdits: 0
      reactions: []
    id: 65489ad6b8ac1a89ff2f0b17
    type: comment
  author: nateraw
  content: '@Jackmin108 can you provide an example of using these onnx weights you''ve
    contributed to compute embeddings, please? :)'
  created_at: 2023-11-06 07:50:46+00:00
  edited: false
  hidden: false
  id: 65489ad6b8ac1a89ff2f0b17
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4eb94144545a3944a650fc0ca69b006a.svg
      fullname: Duc M. Truong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ductruong
      type: user
    createdAt: '2023-11-13T01:27:23.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/4eb94144545a3944a650fc0ca69b006a.svg
          fullname: Duc M. Truong
          isHf: false
          isPro: false
          name: ductruong
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-11-14T10:58:37.517Z'
      numEdits: 0
      reactions: []
    id: 65517b7b8cc59d5b4959cfd1
    type: comment
  author: ductruong
  content: This comment has been hidden
  created_at: 2023-11-13 01:27:23+00:00
  edited: true
  hidden: true
  id: 65517b7b8cc59d5b4959cfd1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63491dc83d8dc83a55cb749c/IoqJrOIaEnYO_S7si4KGp.jpeg?w=200&h=200&f=face
      fullname: Bo Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bwang0911
      type: user
    createdAt: '2023-11-19T10:31:04.000Z'
    data:
      edited: false
      editors:
      - bwang0911
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3421502709388733
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63491dc83d8dc83a55cb749c/IoqJrOIaEnYO_S7si4KGp.jpeg?w=200&h=200&f=face
          fullname: Bo Wang
          isHf: false
          isPro: false
          name: bwang0911
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;nateraw&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nateraw\">@<span class=\"\
          underline\">nateraw</span></a></span>\n\n\t</span></span> please check the\
          \ following code:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ AutoTokenizer\n<span class=\"hljs-keyword\">from</span> onnxruntime <span\
          \ class=\"hljs-keyword\">import</span> InferenceSession\n\u200B\ntokenizer\
          \ = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">'jinaai/jina-embeddings-v2-base-en'</span>,\
          \ trust_remote_code=<span class=\"hljs-literal\">True</span>)\nsession =\
          \ InferenceSession(<span class=\"hljs-string\">'YOUR-FOLDER/jina_embeddings_v2_small_onnx_w_mean_pooling/model.onnx'</span>)\n\
          \u200B\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\
          \ function_\">embed_onnx</span>(<span class=\"hljs-params\">text</span>):\n\
          \    <span class=\"hljs-comment\"># ONNX Runtime expects NumPy arrays as\
          \ input</span>\n    inputs = tokenizer(text, return_tensors=<span class=\"\
          hljs-string\">\"np\"</span>)\n    outputs = session.run(output_names=[<span\
          \ class=\"hljs-string\">\"last_hidden_state\"</span>], input_feed=<span\
          \ class=\"hljs-built_in\">dict</span>(inputs))\n\u200B\n    <span class=\"\
          hljs-keyword\">return</span> outputs\n\u200B\n\u200B\n<span class=\"hljs-built_in\"\
          >print</span>(embed_onnx(<span class=\"hljs-string\">'hello world'</span>))\n\
          </code></pre>\n"
        raw: "hi @nateraw please check the following code:\n\n```python\nfrom transformers\
          \ import AutoTokenizer\nfrom onnxruntime import InferenceSession\n\u200B\
          \ntokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-base-en',\
          \ trust_remote_code=True)\nsession = InferenceSession('YOUR-FOLDER/jina_embeddings_v2_small_onnx_w_mean_pooling/model.onnx')\n\
          \u200B\ndef embed_onnx(text):\n    # ONNX Runtime expects NumPy arrays as\
          \ input\n    inputs = tokenizer(text, return_tensors=\"np\")\n    outputs\
          \ = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n\
          \u200B\n    return outputs\n\u200B\n\u200B\nprint(embed_onnx('hello world'))\n\
          ```"
        updatedAt: '2023-11-19T10:31:04.835Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - do-me
    id: 6559e3e8ab0644b5318c3ced
    type: comment
  author: bwang0911
  content: "hi @nateraw please check the following code:\n\n```python\nfrom transformers\
    \ import AutoTokenizer\nfrom onnxruntime import InferenceSession\n\u200B\ntokenizer\
    \ = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n\
    session = InferenceSession('YOUR-FOLDER/jina_embeddings_v2_small_onnx_w_mean_pooling/model.onnx')\n\
    \u200B\ndef embed_onnx(text):\n    # ONNX Runtime expects NumPy arrays as input\n\
    \    inputs = tokenizer(text, return_tensors=\"np\")\n    outputs = session.run(output_names=[\"\
    last_hidden_state\"], input_feed=dict(inputs))\n\u200B\n    return outputs\n\u200B\
    \n\u200B\nprint(embed_onnx('hello world'))\n```"
  created_at: 2023-11-19 10:31:04+00:00
  edited: false
  hidden: false
  id: 6559e3e8ab0644b5318c3ced
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63491dc83d8dc83a55cb749c/IoqJrOIaEnYO_S7si4KGp.jpeg?w=200&h=200&f=face
      fullname: Bo Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bwang0911
      type: user
    createdAt: '2023-12-22T11:52:27.000Z'
    data:
      status: closed
    id: 6585787b14c8922f1480ed7b
    type: status-change
  author: bwang0911
  created_at: 2023-12-22 11:52:27+00:00
  id: 6585787b14c8922f1480ed7b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: jinaai/jina-embeddings-v2-base-en
repo_type: model
status: closed
target_branch: null
title: Potential quality loss on short input? (quantized onnx version)
