!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hiiamsid
conflicting_files: null
created_at: 2023-11-23 18:28:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1632562296929-614c3bace0ee7bff0f3bdc57.jpeg?w=200&h=200&f=face
      fullname: Siddhartha Shrestha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiiamsid
      type: user
    createdAt: '2023-11-23T18:28:36.000Z'
    data:
      edited: false
      editors:
      - hiiamsid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9096084833145142
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1632562296929-614c3bace0ee7bff0f3bdc57.jpeg?w=200&h=200&f=face
          fullname: Siddhartha Shrestha
          isHf: false
          isPro: false
          name: hiiamsid
          type: user
        html: '<p>I used sft trainer for finetuning this model, but to my surprise
          while pushing finetuned model 3 files of smaller size is save in total around
          15GB but original model is around 25 GB. This is surprising</p>

          '
        raw: I used sft trainer for finetuning this model, but to my surprise while
          pushing finetuned model 3 files of smaller size is save in total around
          15GB but original model is around 25 GB. This is surprising
        updatedAt: '2023-11-23T18:28:36.126Z'
      numEdits: 0
      reactions: []
    id: 655f99d43beaa281e5434c64
    type: comment
  author: hiiamsid
  content: I used sft trainer for finetuning this model, but to my surprise while
    pushing finetuned model 3 files of smaller size is save in total around 15GB but
    original model is around 25 GB. This is surprising
  created_at: 2023-11-23 18:28:36+00:00
  edited: false
  hidden: false
  id: 655f99d43beaa281e5434c64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-11-23T21:26:26.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9595760107040405
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;hiiamsid&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/hiiamsid\">@<span class=\"\
          underline\">hiiamsid</span></a></span>\n\n\t</span></span> the most likely\
          \ reason is that this model is stored in fp32 format while the output you\
          \ got is in fp16 format.</p>\n<p>fp32 is pretty useless since its 2x the\
          \ size of fp16 but fp32 is 2x slower then fp16. there is no quality difference\
          \ as well.<br>Thats why 99% of the time everyone uses fp16 since it increases\
          \ speed in inference and training, decreases vram/ram usage in inference\
          \ and training and is much smaller then fp32.</p>\n"
        raw: '@hiiamsid the most likely reason is that this model is stored in fp32
          format while the output you got is in fp16 format.


          fp32 is pretty useless since its 2x the size of fp16 but fp32 is 2x slower
          then fp16. there is no quality difference as well.

          Thats why 99% of the time everyone uses fp16 since it increases speed in
          inference and training, decreases vram/ram usage in inference and training
          and is much smaller then fp32.'
        updatedAt: '2023-11-23T21:26:26.753Z'
      numEdits: 0
      reactions: []
    id: 655fc3824ab6f7cf94c6899e
    type: comment
  author: YaTharThShaRma999
  content: '@hiiamsid the most likely reason is that this model is stored in fp32
    format while the output you got is in fp16 format.


    fp32 is pretty useless since its 2x the size of fp16 but fp32 is 2x slower then
    fp16. there is no quality difference as well.

    Thats why 99% of the time everyone uses fp16 since it increases speed in inference
    and training, decreases vram/ram usage in inference and training and is much smaller
    then fp32.'
  created_at: 2023-11-23 21:26:26+00:00
  edited: false
  hidden: false
  id: 655fc3824ab6f7cf94c6899e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: microsoft/Orca-2-7b
repo_type: model
status: open
target_branch: null
title: Finetuned model is less in size than original model
