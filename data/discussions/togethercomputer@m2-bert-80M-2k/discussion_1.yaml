!!python/object:huggingface_hub.community.DiscussionWithDetails
author: quincyqiang
conflicting_files: null
created_at: 2024-01-18 06:53:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611624993098-noauth.jpeg?w=200&h=200&f=face
      fullname: quincyqiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: quincyqiang
      type: user
    createdAt: '2024-01-18T06:53:19.000Z'
    data:
      edited: false
      editors:
      - quincyqiang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2269933521747589
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611624993098-noauth.jpeg?w=200&h=200&f=face
          fullname: quincyqiang
          isHf: false
          isPro: false
          name: quincyqiang
          type: user
        html: '<p>AutoModelForTokenClassification</p>

          '
        raw: AutoModelForTokenClassification
        updatedAt: '2024-01-18T06:53:19.054Z'
      numEdits: 0
      reactions: []
    id: 65a8cadf73a327a31dd13205
    type: comment
  author: quincyqiang
  content: AutoModelForTokenClassification
  created_at: 2024-01-18 06:53:19+00:00
  edited: false
  hidden: false
  id: 65a8cadf73a327a31dd13205
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/32157bf89dbe9c7385b4816ea15ec240.svg
      fullname: Dan Fu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: danfu09
      type: user
    createdAt: '2024-01-18T07:30:41.000Z'
    data:
      edited: false
      editors:
      - danfu09
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7819493412971497
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/32157bf89dbe9c7385b4816ea15ec240.svg
          fullname: Dan Fu
          isHf: false
          isPro: false
          name: danfu09
          type: user
        html: "<p>Hi, I\u2019m not familiar with this AutoModel. Can you give an example\
          \ of a use case?</p>\n"
        raw: "Hi, I\u2019m not familiar with this AutoModel. Can you give an example\
          \ of a use case?"
        updatedAt: '2024-01-18T07:30:41.163Z'
      numEdits: 0
      reactions: []
    id: 65a8d3a19db09e1fc15d10aa
    type: comment
  author: danfu09
  content: "Hi, I\u2019m not familiar with this AutoModel. Can you give an example\
    \ of a use case?"
  created_at: 2024-01-18 07:30:41+00:00
  edited: false
  hidden: false
  id: 65a8d3a19db09e1fc15d10aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611624993098-noauth.jpeg?w=200&h=200&f=face
      fullname: quincyqiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: quincyqiang
      type: user
    createdAt: '2024-01-18T13:47:31.000Z'
    data:
      edited: false
      editors:
      - quincyqiang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4266852140426636
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611624993098-noauth.jpeg?w=200&h=200&f=face
          fullname: quincyqiang
          isHf: false
          isPro: false
          name: quincyqiang
          type: user
        html: "<p><a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py\"\
          >https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py</a></p>\n\
          <pre><code>class BertForTokenClassification(BertPreTrainedModel):\n    def\
          \ __init__(self, config):\n        super().__init__(config)\n        self.num_labels\
          \ = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n\
          \        classifier_dropout = (\n            config.classifier_dropout if\
          \ config.classifier_dropout is not None else config.hidden_dropout_prob\n\
          \        )\n        self.dropout = nn.Dropout(classifier_dropout)\n    \
          \    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\
          \n        # Initialize weights and apply final processing\n        self.post_init()\n\
          \n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"\
          batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n    \
          \    checkpoint=_CHECKPOINT_FOR_TOKEN_CLASSIFICATION,\n        output_type=TokenClassifierOutput,\n\
          \        config_class=_CONFIG_FOR_DOC,\n        expected_output=_TOKEN_CLASS_EXPECTED_OUTPUT,\n\
          \        expected_loss=_TOKEN_CLASS_EXPECTED_LOSS,\n    )\n    def forward(\n\
          \        self,\n        input_ids: Optional[torch.Tensor] = None,\n    \
          \    attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids:\
          \ Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor]\
          \ = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds:\
          \ Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor]\
          \ = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states:\
          \ Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n\
          \    ) -&gt; Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n      \
          \  r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`,\
          \ *optional*):\n            Labels for computing the token classification\
          \ loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n      \
          \  \"\"\"\n        return_dict = return_dict if return_dict is not None\
          \ else self.config.use_return_dict\n\n        outputs = self.bert(\n   \
          \         input_ids,\n            attention_mask=attention_mask,\n     \
          \       token_type_ids=token_type_ids,\n            position_ids=position_ids,\n\
          \            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n\
          \            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n\
          \            return_dict=return_dict,\n        )\n\n        sequence_output\
          \ = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n\
          \        logits = self.classifier(sequence_output)\n\n        loss = None\n\
          \        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n\
          \            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\
          \n        if not return_dict:\n            output = (logits,) + outputs[2:]\n\
          \            return ((loss,) + output) if loss is not None else output\n\
          \n        return TokenClassifierOutput(\n            loss=loss,\n      \
          \      logits=logits,\n            hidden_states=outputs.hidden_states,\n\
          \            attentions=outputs.attentions,\n        )\n</code></pre>\n"
        raw: "https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py\n\
          ```\nclass BertForTokenClassification(BertPreTrainedModel):\n    def __init__(self,\
          \ config):\n        super().__init__(config)\n        self.num_labels =\
          \ config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n\
          \        classifier_dropout = (\n            config.classifier_dropout if\
          \ config.classifier_dropout is not None else config.hidden_dropout_prob\n\
          \        )\n        self.dropout = nn.Dropout(classifier_dropout)\n    \
          \    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\
          \n        # Initialize weights and apply final processing\n        self.post_init()\n\
          \n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"\
          batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n    \
          \    checkpoint=_CHECKPOINT_FOR_TOKEN_CLASSIFICATION,\n        output_type=TokenClassifierOutput,\n\
          \        config_class=_CONFIG_FOR_DOC,\n        expected_output=_TOKEN_CLASS_EXPECTED_OUTPUT,\n\
          \        expected_loss=_TOKEN_CLASS_EXPECTED_LOSS,\n    )\n    def forward(\n\
          \        self,\n        input_ids: Optional[torch.Tensor] = None,\n    \
          \    attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids:\
          \ Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor]\
          \ = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds:\
          \ Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor]\
          \ = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states:\
          \ Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n\
          \    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n        r\"\
          \"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`,\
          \ *optional*):\n            Labels for computing the token classification\
          \ loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n      \
          \  \"\"\"\n        return_dict = return_dict if return_dict is not None\
          \ else self.config.use_return_dict\n\n        outputs = self.bert(\n   \
          \         input_ids,\n            attention_mask=attention_mask,\n     \
          \       token_type_ids=token_type_ids,\n            position_ids=position_ids,\n\
          \            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n\
          \            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n\
          \            return_dict=return_dict,\n        )\n\n        sequence_output\
          \ = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n\
          \        logits = self.classifier(sequence_output)\n\n        loss = None\n\
          \        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n\
          \            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\
          \n        if not return_dict:\n            output = (logits,) + outputs[2:]\n\
          \            return ((loss,) + output) if loss is not None else output\n\
          \n        return TokenClassifierOutput(\n            loss=loss,\n      \
          \      logits=logits,\n            hidden_states=outputs.hidden_states,\n\
          \            attentions=outputs.attentions,\n        )\n```"
        updatedAt: '2024-01-18T13:47:31.643Z'
      numEdits: 0
      reactions: []
    id: 65a92bf3532284b19b32520c
    type: comment
  author: quincyqiang
  content: "https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py\n\
    ```\nclass BertForTokenClassification(BertPreTrainedModel):\n    def __init__(self,\
    \ config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\
    \n        self.bert = BertModel(config, add_pooling_layer=False)\n        classifier_dropout\
    \ = (\n            config.classifier_dropout if config.classifier_dropout is not\
    \ None else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\n\
    \        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n\
    \        # Initialize weights and apply final processing\n        self.post_init()\n\
    \n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size,\
    \ sequence_length\"))\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_TOKEN_CLASSIFICATION,\n\
    \        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n\
    \        expected_output=_TOKEN_CLASS_EXPECTED_OUTPUT,\n        expected_loss=_TOKEN_CLASS_EXPECTED_LOSS,\n\
    \    )\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor]\
    \ = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids:\
    \ Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor]\
    \ = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds:\
    \ Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n\
    \        output_attentions: Optional[bool] = None,\n        output_hidden_states:\
    \ Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) ->\
    \ Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n        r\"\"\"\n      \
    \  labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n\
    \            Labels for computing the token classification loss. Indices should\
    \ be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict\
    \ = return_dict if return_dict is not None else self.config.use_return_dict\n\n\
    \        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n\
    \            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n\
    \            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n\
    \            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n\
    \            return_dict=return_dict,\n        )\n\n        sequence_output =\
    \ outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n    \
    \    logits = self.classifier(sequence_output)\n\n        loss = None\n      \
    \  if labels is not None:\n            loss_fct = CrossEntropyLoss()\n       \
    \     loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n \
    \       if not return_dict:\n            output = (logits,) + outputs[2:]\n  \
    \          return ((loss,) + output) if loss is not None else output\n\n     \
    \   return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n\
    \            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n\
    \        )\n```"
  created_at: 2024-01-18 13:47:31+00:00
  edited: false
  hidden: false
  id: 65a92bf3532284b19b32520c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611624993098-noauth.jpeg?w=200&h=200&f=face
      fullname: quincyqiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: quincyqiang
      type: user
    createdAt: '2024-01-18T13:48:52.000Z'
    data:
      edited: false
      editors:
      - quincyqiang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7074679732322693
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611624993098-noauth.jpeg?w=200&h=200&f=face
          fullname: quincyqiang
          isHf: false
          isPro: false
          name: quincyqiang
          type: user
        html: '<p><a href="https://huggingface.co/togethercomputer/m2-bert-80M-2k/blob/main/bert_layers.py">https://huggingface.co/togethercomputer/m2-bert-80M-2k/blob/main/bert_layers.py</a></p>

          <p>the file doesnt have the class <code>BertForTokenClassification</code>,I
          want use to ner tasks</p>

          '
        raw: 'https://huggingface.co/togethercomputer/m2-bert-80M-2k/blob/main/bert_layers.py


          the file doesnt have the class `BertForTokenClassification`,I want use to
          ner tasks'
        updatedAt: '2024-01-18T13:48:52.403Z'
      numEdits: 0
      reactions: []
    id: 65a92c443e7c8dcbbe337bc3
    type: comment
  author: quincyqiang
  content: 'https://huggingface.co/togethercomputer/m2-bert-80M-2k/blob/main/bert_layers.py


    the file doesnt have the class `BertForTokenClassification`,I want use to ner
    tasks'
  created_at: 2024-01-18 13:48:52+00:00
  edited: false
  hidden: false
  id: 65a92c443e7c8dcbbe337bc3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: togethercomputer/m2-bert-80M-2k
repo_type: model
status: open
target_branch: null
title: no token classification
