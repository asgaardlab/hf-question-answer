!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zoejones
conflicting_files: null
created_at: 2023-10-20 01:26:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad7d9e735ad3d638d0509ab646152d4c.svg
      fullname: zoejones
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zoejones
      type: user
    createdAt: '2023-10-20T02:26:28.000Z'
    data:
      edited: false
      editors:
      - zoejones
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.15555186569690704
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad7d9e735ad3d638d0509ab646152d4c.svg
          fullname: zoejones
          isHf: false
          isPro: false
          name: zoejones
          type: user
        html: "<p>\u8FD9\u4E2A1.3B\u7684\u4E8C\u90CE\u795Eembedding\u662F\u4E0D\u662F\
          \u4E0D\u5168\uFF0C\u6211\u52A0\u8F7D\u7684\u65F6\u5019\u6709\u5F88\u591A\
          missing keys</p>\n<p>\u4EE3\u7801\uFF1A<br>MODEL_NAME_OR_PATH = \"/home/inspur/nas_data/pretrain/Erlangshen-TCBert-1.3B-Sentence-Embedding-Chinese\"\
          <br>self.tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)<br>self.model\
          \ = BertForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH)</p>\n<p>\u62A5\u9519\
          \uFF1A<br>missing_keys ['bert.encoder.layer.9.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias',\
          \ 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight',\
          \ 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.weight',\
          \ 'cls.predictions.bias', 'bert.embeddings.position_ids', 'bert.encoder.layer.16.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.21.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.14.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.16.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight']<br>Traceback\
          \ (most recent call last):<br>  File \"/home/inspur/test_llms/m3e/erlangshen_embedding.py\"\
          , line 83, in <br>    sentence_trans = ErLangShenEmbedding()<br>  File \"\
          /home/inspur/test_llms/m3e/erlangshen_embedding.py\", line 12, in <strong>init</strong><br>\
          \    self.model = BertForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH) #.cuda().eval()<br>\
          \  File \"/home/inspur/anaconda3/envs/rlhf_py39/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 2777, in from_pretrained<br>    ) = cls._load_pretrained_model(<br>\
          \  File \"/home/inspur/anaconda3/envs/rlhf_py39/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 2931, in _load_pretrained_model<br>    group.remove(k)<br>AttributeError:\
          \ 'str' object has no attribute 'remove'</p>\n"
        raw: "\u8FD9\u4E2A1.3B\u7684\u4E8C\u90CE\u795Eembedding\u662F\u4E0D\u662F\u4E0D\
          \u5168\uFF0C\u6211\u52A0\u8F7D\u7684\u65F6\u5019\u6709\u5F88\u591Amissing\
          \ keys\r\n\r\n\u4EE3\u7801\uFF1A\r\nMODEL_NAME_OR_PATH = \"/home/inspur/nas_data/pretrain/Erlangshen-TCBert-1.3B-Sentence-Embedding-Chinese\"\
          \r\nself.tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\r\
          \nself.model = BertForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH)\r\n\r\
          \n\r\n\u62A5\u9519\uFF1A\r\nmissing_keys ['bert.encoder.layer.9.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias',\
          \ 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight',\
          \ 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.weight',\
          \ 'cls.predictions.bias', 'bert.embeddings.position_ids', 'bert.encoder.layer.16.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.21.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.14.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.16.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight']\r\
          \nTraceback (most recent call last):\r\n  File \"/home/inspur/test_llms/m3e/erlangshen_embedding.py\"\
          , line 83, in <module>\r\n    sentence_trans = ErLangShenEmbedding()\r\n\
          \  File \"/home/inspur/test_llms/m3e/erlangshen_embedding.py\", line 12,\
          \ in __init__\r\n    self.model = BertForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH)\
          \ #.cuda().eval()\r\n  File \"/home/inspur/anaconda3/envs/rlhf_py39/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 2777, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
          \n  File \"/home/inspur/anaconda3/envs/rlhf_py39/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 2931, in _load_pretrained_model\r\n    group.remove(k)\r\nAttributeError:\
          \ 'str' object has no attribute 'remove'\r\n\r\n"
        updatedAt: '2023-10-20T02:26:28.360Z'
      numEdits: 0
      reactions: []
    id: 6531e55427561a21c8dc4435
    type: comment
  author: zoejones
  content: "\u8FD9\u4E2A1.3B\u7684\u4E8C\u90CE\u795Eembedding\u662F\u4E0D\u662F\u4E0D\
    \u5168\uFF0C\u6211\u52A0\u8F7D\u7684\u65F6\u5019\u6709\u5F88\u591Amissing keys\r\
    \n\r\n\u4EE3\u7801\uFF1A\r\nMODEL_NAME_OR_PATH = \"/home/inspur/nas_data/pretrain/Erlangshen-TCBert-1.3B-Sentence-Embedding-Chinese\"\
    \r\nself.tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\r\nself.model\
    \ = BertForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH)\r\n\r\n\r\n\u62A5\u9519\
    \uFF1A\r\nmissing_keys ['bert.encoder.layer.9.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias',\
    \ 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight',\
    \ 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.weight',\
    \ 'cls.predictions.bias', 'bert.embeddings.position_ids', 'bert.encoder.layer.16.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.21.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.14.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.16.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight']\r\
    \nTraceback (most recent call last):\r\n  File \"/home/inspur/test_llms/m3e/erlangshen_embedding.py\"\
    , line 83, in <module>\r\n    sentence_trans = ErLangShenEmbedding()\r\n  File\
    \ \"/home/inspur/test_llms/m3e/erlangshen_embedding.py\", line 12, in __init__\r\
    \n    self.model = BertForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH) #.cuda().eval()\r\
    \n  File \"/home/inspur/anaconda3/envs/rlhf_py39/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
    , line 2777, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File\
    \ \"/home/inspur/anaconda3/envs/rlhf_py39/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
    , line 2931, in _load_pretrained_model\r\n    group.remove(k)\r\nAttributeError:\
    \ 'str' object has no attribute 'remove'\r\n\r\n"
  created_at: 2023-10-20 01:26:28+00:00
  edited: false
  hidden: false
  id: 6531e55427561a21c8dc4435
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666695219265-635262337d071f23d084d8af.jpeg?w=200&h=200&f=face
      fullname: Kunhao Pan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pskun
      type: user
    createdAt: '2023-10-23T02:14:32.000Z'
    data:
      edited: false
      editors:
      - pskun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.15132907032966614
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666695219265-635262337d071f23d084d8af.jpeg?w=200&h=200&f=face
          fullname: Kunhao Pan
          isHf: false
          isPro: false
          name: pskun
          type: user
        html: "<blockquote>\n<p>\u8FD9\u4E2A1.3B\u7684\u4E8C\u90CE\u795Eembedding\u662F\
          \u4E0D\u662F\u4E0D\u5168\uFF0C\u6211\u52A0\u8F7D\u7684\u65F6\u5019\u6709\
          \u5F88\u591Amissing keys</p>\n<p>\u4EE3\u7801\uFF1A<br>MODEL_NAME_OR_PATH\
          \ = \"/home/inspur/nas_data/pretrain/Erlangshen-TCBert-1.3B-Sentence-Embedding-Chinese\"\
          <br>self.tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)<br>self.model\
          \ = BertForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH)</p>\n<p>\u62A5\u9519\
          \uFF1A<br>missing_keys ['bert.encoder.layer.9.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias',\
          \ 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight',\
          \ 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.weight',\
          \ 'cls.predictions.bias', 'bert.embeddings.position_ids', 'bert.encoder.layer.16.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.21.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.14.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.16.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight']<br>Traceback\
          \ (most recent call last):<br>  File \"/home/inspur/test_llms/m3e/erlangshen_embedding.py\"\
          , line 83, in <br>    sentence_trans = ErLangShenEmbedding()<br>  File \"\
          /home/inspur/test_llms/m3e/erlangshen_embedding.py\", line 12, in <strong>init</strong><br>\
          \    self.model = BertForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH) #.cuda().eval()<br>\
          \  File \"/home/inspur/anaconda3/envs/rlhf_py39/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 2777, in from_pretrained<br>    ) = cls._load_pretrained_model(<br>\
          \  File \"/home/inspur/anaconda3/envs/rlhf_py39/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 2931, in _load_pretrained_model<br>    group.remove(k)<br>AttributeError:\
          \ 'str' object has no attribute 'remove'</p>\n</blockquote>\n<p>\u8BF7\u4F7F\
          \u7528MegatronBertForMaskedLM\u6216\u8005AutoModelForMaskedLM\uFF0C\u6211\
          \u4EEC\u4F1A\u5C3D\u5FEB\u4FEE\u6539\u793A\u4F8B\u3002</p>\n"
        raw: "> \u8FD9\u4E2A1.3B\u7684\u4E8C\u90CE\u795Eembedding\u662F\u4E0D\u662F\
          \u4E0D\u5168\uFF0C\u6211\u52A0\u8F7D\u7684\u65F6\u5019\u6709\u5F88\u591A\
          missing keys\n> \n> \u4EE3\u7801\uFF1A\n> MODEL_NAME_OR_PATH = \"/home/inspur/nas_data/pretrain/Erlangshen-TCBert-1.3B-Sentence-Embedding-Chinese\"\
          \n> self.tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n\
          > self.model = BertForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH)\n> \n\
          > \n> \u62A5\u9519\uFF1A\n> missing_keys ['bert.encoder.layer.9.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias',\
          \ 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight',\
          \ 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.weight',\
          \ 'cls.predictions.bias', 'bert.embeddings.position_ids', 'bert.encoder.layer.16.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.21.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.14.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.16.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight']\n\
          > Traceback (most recent call last):\n>   File \"/home/inspur/test_llms/m3e/erlangshen_embedding.py\"\
          , line 83, in <module>\n>     sentence_trans = ErLangShenEmbedding()\n>\
          \   File \"/home/inspur/test_llms/m3e/erlangshen_embedding.py\", line 12,\
          \ in __init__\n>     self.model = BertForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH)\
          \ #.cuda().eval()\n>   File \"/home/inspur/anaconda3/envs/rlhf_py39/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 2777, in from_pretrained\n>     ) = cls._load_pretrained_model(\n\
          >   File \"/home/inspur/anaconda3/envs/rlhf_py39/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 2931, in _load_pretrained_model\n>     group.remove(k)\n> AttributeError:\
          \ 'str' object has no attribute 'remove'\n\n\u8BF7\u4F7F\u7528MegatronBertForMaskedLM\u6216\
          \u8005AutoModelForMaskedLM\uFF0C\u6211\u4EEC\u4F1A\u5C3D\u5FEB\u4FEE\u6539\
          \u793A\u4F8B\u3002"
        updatedAt: '2023-10-23T02:14:32.362Z'
      numEdits: 0
      reactions: []
    id: 6535d70823e0af0e0da03606
    type: comment
  author: pskun
  content: "> \u8FD9\u4E2A1.3B\u7684\u4E8C\u90CE\u795Eembedding\u662F\u4E0D\u662F\u4E0D\
    \u5168\uFF0C\u6211\u52A0\u8F7D\u7684\u65F6\u5019\u6709\u5F88\u591Amissing keys\n\
    > \n> \u4EE3\u7801\uFF1A\n> MODEL_NAME_OR_PATH = \"/home/inspur/nas_data/pretrain/Erlangshen-TCBert-1.3B-Sentence-Embedding-Chinese\"\
    \n> self.tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n> self.model\
    \ = BertForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH)\n> \n> \n> \u62A5\u9519\
    \uFF1A\n> missing_keys ['bert.encoder.layer.9.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias',\
    \ 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight',\
    \ 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.weight',\
    \ 'cls.predictions.bias', 'bert.embeddings.position_ids', 'bert.encoder.layer.16.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.21.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.14.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.16.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight']\n\
    > Traceback (most recent call last):\n>   File \"/home/inspur/test_llms/m3e/erlangshen_embedding.py\"\
    , line 83, in <module>\n>     sentence_trans = ErLangShenEmbedding()\n>   File\
    \ \"/home/inspur/test_llms/m3e/erlangshen_embedding.py\", line 12, in __init__\n\
    >     self.model = BertForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH) #.cuda().eval()\n\
    >   File \"/home/inspur/anaconda3/envs/rlhf_py39/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
    , line 2777, in from_pretrained\n>     ) = cls._load_pretrained_model(\n>   File\
    \ \"/home/inspur/anaconda3/envs/rlhf_py39/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
    , line 2931, in _load_pretrained_model\n>     group.remove(k)\n> AttributeError:\
    \ 'str' object has no attribute 'remove'\n\n\u8BF7\u4F7F\u7528MegatronBertForMaskedLM\u6216\
    \u8005AutoModelForMaskedLM\uFF0C\u6211\u4EEC\u4F1A\u5C3D\u5FEB\u4FEE\u6539\u793A\
    \u4F8B\u3002"
  created_at: 2023-10-23 01:14:32+00:00
  edited: false
  hidden: false
  id: 6535d70823e0af0e0da03606
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad7d9e735ad3d638d0509ab646152d4c.svg
      fullname: zoejones
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zoejones
      type: user
    createdAt: '2023-10-23T02:50:49.000Z'
    data:
      edited: false
      editors:
      - zoejones
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.1387690007686615
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad7d9e735ad3d638d0509ab646152d4c.svg
          fullname: zoejones
          isHf: false
          isPro: false
          name: zoejones
          type: user
        html: "<p>\u4F7F\u7528MegatronBertForMaskedLM\uFF1Amissing_keys ['cls.predictions.bias',\
          \ 'bert.embeddings.position_ids']</p>\n<p>\u4F7F\u7528AutoModelForMaskedLM\uFF1A\
          missing_keys ['bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.bias', 'cls.predictions.bias',\
          \ 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.17.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.18.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.15.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.21.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.22.attention.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.12.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.19.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.16.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.21.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.12.attention.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.12.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.16.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.weight',\
          \ 'bert.embeddings.position_ids', 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.bias',\
          \ 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.15.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.21.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.19.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.20.attention.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.15.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight',\
          \ 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.output.LayerNorm.weight']</p>\n\
          <p>\u770B\u4E86\u4E00\u4E0B\uFF0C\u52A0\u8F7D\u7684bin\u53C2\u6570\u6587\
          \u4EF6\u6709\u5F88\u591A\u540D\u4E3A.ln\u7684\u53C2\u6570\u800C\u4E0D\u662F\
          LayerNorm\u3002<br>cls.predictions\u53C2\u6570\u6587\u4EF6\u4E2D\u662F\uFF1A\
          <br>cls.predictions.transform.dense.weight torch.Size([2048, 2048])<br>cls.predictions.transform.dense.bias\
          \ torch.Size([2048])<br>cls.predictions.transform.LayerNorm.weight torch.Size([2048])<br>cls.predictions.transform.LayerNorm.bias\
          \ torch.Size([2048])<br>cls.predictions.decoder.weight torch.Size([21248,\
          \ 2048])<br>cls.predictions.decoder.bias torch.Size([21248])</p>\n"
        raw: "\u4F7F\u7528MegatronBertForMaskedLM\uFF1Amissing_keys ['cls.predictions.bias',\
          \ 'bert.embeddings.position_ids']\n\n\u4F7F\u7528AutoModelForMaskedLM\uFF1A\
          missing_keys ['bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.bias', 'cls.predictions.bias',\
          \ 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.17.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.18.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.15.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.21.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.22.attention.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.12.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.19.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.16.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.21.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.12.attention.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.12.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.16.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.weight',\
          \ 'bert.embeddings.position_ids', 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.bias',\
          \ 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.15.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.21.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.19.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.20.attention.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.15.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight',\
          \ 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.output.LayerNorm.weight']\n\
          \n\u770B\u4E86\u4E00\u4E0B\uFF0C\u52A0\u8F7D\u7684bin\u53C2\u6570\u6587\u4EF6\
          \u6709\u5F88\u591A\u540D\u4E3A.ln\u7684\u53C2\u6570\u800C\u4E0D\u662FLayerNorm\u3002\
          \ncls.predictions\u53C2\u6570\u6587\u4EF6\u4E2D\u662F\uFF1A\ncls.predictions.transform.dense.weight\
          \ torch.Size([2048, 2048])\ncls.predictions.transform.dense.bias torch.Size([2048])\n\
          cls.predictions.transform.LayerNorm.weight torch.Size([2048])\ncls.predictions.transform.LayerNorm.bias\
          \ torch.Size([2048])\ncls.predictions.decoder.weight torch.Size([21248,\
          \ 2048])\ncls.predictions.decoder.bias torch.Size([21248])"
        updatedAt: '2023-10-23T02:50:49.212Z'
      numEdits: 0
      reactions: []
    id: 6535df898bde2fae19ba7495
    type: comment
  author: zoejones
  content: "\u4F7F\u7528MegatronBertForMaskedLM\uFF1Amissing_keys ['cls.predictions.bias',\
    \ 'bert.embeddings.position_ids']\n\n\u4F7F\u7528AutoModelForMaskedLM\uFF1Amissing_keys\
    \ ['bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.10.output.LayerNorm.bias', 'cls.predictions.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.16.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.13.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.15.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.21.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.22.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.23.attention.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.16.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.21.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.14.output.LayerNorm.bias', 'bert.encoder.layer.12.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.12.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.embeddings.position_ids',\
    \ 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.attention.output.LayerNorm.bias',\
    \ 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.20.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.22.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.19.attention.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.13.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight',\
    \ 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.13.output.LayerNorm.weight']\n\n\u770B\u4E86\u4E00\u4E0B\
    \uFF0C\u52A0\u8F7D\u7684bin\u53C2\u6570\u6587\u4EF6\u6709\u5F88\u591A\u540D\u4E3A\
    .ln\u7684\u53C2\u6570\u800C\u4E0D\u662FLayerNorm\u3002\ncls.predictions\u53C2\u6570\
    \u6587\u4EF6\u4E2D\u662F\uFF1A\ncls.predictions.transform.dense.weight torch.Size([2048,\
    \ 2048])\ncls.predictions.transform.dense.bias torch.Size([2048])\ncls.predictions.transform.LayerNorm.weight\
    \ torch.Size([2048])\ncls.predictions.transform.LayerNorm.bias torch.Size([2048])\n\
    cls.predictions.decoder.weight torch.Size([21248, 2048])\ncls.predictions.decoder.bias\
    \ torch.Size([21248])"
  created_at: 2023-10-23 01:50:49+00:00
  edited: false
  hidden: false
  id: 6535df898bde2fae19ba7495
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666695219265-635262337d071f23d084d8af.jpeg?w=200&h=200&f=face
      fullname: Kunhao Pan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pskun
      type: user
    createdAt: '2023-10-23T05:19:22.000Z'
    data:
      edited: false
      editors:
      - pskun
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.15219083428382874
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666695219265-635262337d071f23d084d8af.jpeg?w=200&h=200&f=face
          fullname: Kunhao Pan
          isHf: false
          isPro: false
          name: pskun
          type: user
        html: "<blockquote>\n<p>\u4F7F\u7528MegatronBertForMaskedLM\uFF1Amissing_keys\
          \ ['cls.predictions.bias', 'bert.embeddings.position_ids']</p>\n<p>\u4F7F\
          \u7528AutoModelForMaskedLM\uFF1Amissing_keys ['bert.encoder.layer.13.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.16.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.14.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias',\
          \ 'cls.predictions.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.16.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.13.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.15.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.21.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.22.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.23.attention.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.16.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.21.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.14.output.LayerNorm.bias', 'bert.encoder.layer.12.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.12.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.embeddings.position_ids',\
          \ 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.attention.output.LayerNorm.bias',\
          \ 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.20.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.22.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.19.attention.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.13.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight',\
          \ 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.13.output.LayerNorm.weight']</p>\n<p>\u770B\u4E86\u4E00\
          \u4E0B\uFF0C\u52A0\u8F7D\u7684bin\u53C2\u6570\u6587\u4EF6\u6709\u5F88\u591A\
          \u540D\u4E3A.ln\u7684\u53C2\u6570\u800C\u4E0D\u662FLayerNorm\u3002<br>cls.predictions\u53C2\
          \u6570\u6587\u4EF6\u4E2D\u662F\uFF1A<br>cls.predictions.transform.dense.weight\
          \ torch.Size([2048, 2048])<br>cls.predictions.transform.dense.bias torch.Size([2048])<br>cls.predictions.transform.LayerNorm.weight\
          \ torch.Size([2048])<br>cls.predictions.transform.LayerNorm.bias torch.Size([2048])<br>cls.predictions.decoder.weight\
          \ torch.Size([21248, 2048])<br>cls.predictions.decoder.bias torch.Size([21248])</p>\n\
          </blockquote>\n<p>missing_keys ['cls.predictions.bias', 'bert.embeddings.position_ids']\uFF0C\
          \u8FD9\u4E24\u4E2A\u53C2\u6570\u53EF\u80FD\u662Ftransformers\u7684\u7248\
          \u672C\u95EE\u9898\uFF0C\u5BF9\u7ED3\u679C\u5E94\u8BE5\u4E0D\u5F71\u54CD\
          \u3002\u53EF\u4EE5\u63D0\u4F9Btransformers\u7684\u7248\u672C\u4F9B\u53C2\
          \u8003\u3002</p>\n"
        raw: "> \u4F7F\u7528MegatronBertForMaskedLM\uFF1Amissing_keys ['cls.predictions.bias',\
          \ 'bert.embeddings.position_ids']\n> \n> \u4F7F\u7528AutoModelForMaskedLM\uFF1A\
          missing_keys ['bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.output.LayerNorm.bias', 'cls.predictions.bias',\
          \ 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.17.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.18.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.15.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.21.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.22.attention.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.12.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.19.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.16.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.21.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.12.attention.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.12.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.16.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.weight',\
          \ 'bert.embeddings.position_ids', 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.bias',\
          \ 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.15.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.21.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.17.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.19.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.20.attention.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.15.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight',\
          \ 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.output.LayerNorm.weight']\n\
          > \n> \u770B\u4E86\u4E00\u4E0B\uFF0C\u52A0\u8F7D\u7684bin\u53C2\u6570\u6587\
          \u4EF6\u6709\u5F88\u591A\u540D\u4E3A.ln\u7684\u53C2\u6570\u800C\u4E0D\u662F\
          LayerNorm\u3002\n> cls.predictions\u53C2\u6570\u6587\u4EF6\u4E2D\u662F\uFF1A\
          \n> cls.predictions.transform.dense.weight torch.Size([2048, 2048])\n> cls.predictions.transform.dense.bias\
          \ torch.Size([2048])\n> cls.predictions.transform.LayerNorm.weight torch.Size([2048])\n\
          > cls.predictions.transform.LayerNorm.bias torch.Size([2048])\n> cls.predictions.decoder.weight\
          \ torch.Size([21248, 2048])\n> cls.predictions.decoder.bias torch.Size([21248])\n\
          \nmissing_keys ['cls.predictions.bias', 'bert.embeddings.position_ids']\uFF0C\
          \u8FD9\u4E24\u4E2A\u53C2\u6570\u53EF\u80FD\u662Ftransformers\u7684\u7248\
          \u672C\u95EE\u9898\uFF0C\u5BF9\u7ED3\u679C\u5E94\u8BE5\u4E0D\u5F71\u54CD\
          \u3002\u53EF\u4EE5\u63D0\u4F9Btransformers\u7684\u7248\u672C\u4F9B\u53C2\
          \u8003\u3002"
        updatedAt: '2023-10-23T05:19:22.862Z'
      numEdits: 0
      reactions: []
    id: 6536025ad34e9f02b9dc5567
    type: comment
  author: pskun
  content: "> \u4F7F\u7528MegatronBertForMaskedLM\uFF1Amissing_keys ['cls.predictions.bias',\
    \ 'bert.embeddings.position_ids']\n> \n> \u4F7F\u7528AutoModelForMaskedLM\uFF1A\
    missing_keys ['bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.10.output.LayerNorm.bias', 'cls.predictions.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.16.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.13.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.15.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.21.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.22.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.23.attention.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.16.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.21.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.14.output.LayerNorm.bias', 'bert.encoder.layer.12.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.12.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.embeddings.position_ids',\
    \ 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.attention.output.LayerNorm.bias',\
    \ 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.20.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.22.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.19.attention.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.13.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight',\
    \ 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.13.output.LayerNorm.weight']\n> \n> \u770B\u4E86\u4E00\u4E0B\
    \uFF0C\u52A0\u8F7D\u7684bin\u53C2\u6570\u6587\u4EF6\u6709\u5F88\u591A\u540D\u4E3A\
    .ln\u7684\u53C2\u6570\u800C\u4E0D\u662FLayerNorm\u3002\n> cls.predictions\u53C2\
    \u6570\u6587\u4EF6\u4E2D\u662F\uFF1A\n> cls.predictions.transform.dense.weight\
    \ torch.Size([2048, 2048])\n> cls.predictions.transform.dense.bias torch.Size([2048])\n\
    > cls.predictions.transform.LayerNorm.weight torch.Size([2048])\n> cls.predictions.transform.LayerNorm.bias\
    \ torch.Size([2048])\n> cls.predictions.decoder.weight torch.Size([21248, 2048])\n\
    > cls.predictions.decoder.bias torch.Size([21248])\n\nmissing_keys ['cls.predictions.bias',\
    \ 'bert.embeddings.position_ids']\uFF0C\u8FD9\u4E24\u4E2A\u53C2\u6570\u53EF\u80FD\
    \u662Ftransformers\u7684\u7248\u672C\u95EE\u9898\uFF0C\u5BF9\u7ED3\u679C\u5E94\
    \u8BE5\u4E0D\u5F71\u54CD\u3002\u53EF\u4EE5\u63D0\u4F9Btransformers\u7684\u7248\
    \u672C\u4F9B\u53C2\u8003\u3002"
  created_at: 2023-10-23 04:19:22+00:00
  edited: false
  hidden: false
  id: 6536025ad34e9f02b9dc5567
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad7d9e735ad3d638d0509ab646152d4c.svg
      fullname: zoejones
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zoejones
      type: user
    createdAt: '2023-10-24T01:35:15.000Z'
    data:
      edited: false
      editors:
      - zoejones
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9674999713897705
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad7d9e735ad3d638d0509ab646152d4c.svg
          fullname: zoejones
          isHf: false
          isPro: false
          name: zoejones
          type: user
        html: "<p>\u6211\u7684transformers\u7684\u7248\u672C\u662F4.29.2\u3002\u8BF7\
          \u95EE\u53EF\u4EE5\u63D0\u95EE\u4E00\u4E2A\u5B98\u65B9\u7684transformers\u7248\
          \u672C\u6216\u73AF\u5883\u5417\uFF1F</p>\n"
        raw: "\u6211\u7684transformers\u7684\u7248\u672C\u662F4.29.2\u3002\u8BF7\u95EE\
          \u53EF\u4EE5\u63D0\u95EE\u4E00\u4E2A\u5B98\u65B9\u7684transformers\u7248\
          \u672C\u6216\u73AF\u5883\u5417\uFF1F"
        updatedAt: '2023-10-24T01:35:15.698Z'
      numEdits: 0
      reactions: []
    id: 65371f538cc9d7c02400c125
    type: comment
  author: zoejones
  content: "\u6211\u7684transformers\u7684\u7248\u672C\u662F4.29.2\u3002\u8BF7\u95EE\
    \u53EF\u4EE5\u63D0\u95EE\u4E00\u4E2A\u5B98\u65B9\u7684transformers\u7248\u672C\
    \u6216\u73AF\u5883\u5417\uFF1F"
  created_at: 2023-10-24 00:35:15+00:00
  edited: false
  hidden: false
  id: 65371f538cc9d7c02400c125
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666695219265-635262337d071f23d084d8af.jpeg?w=200&h=200&f=face
      fullname: Kunhao Pan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pskun
      type: user
    createdAt: '2023-10-24T04:05:52.000Z'
    data:
      edited: false
      editors:
      - pskun
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9127061367034912
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666695219265-635262337d071f23d084d8af.jpeg?w=200&h=200&f=face
          fullname: Kunhao Pan
          isHf: false
          isPro: false
          name: pskun
          type: user
        html: "<blockquote>\n<p>\u6211\u7684transformers\u7684\u7248\u672C\u662F4.29.2\u3002\
          \u8BF7\u95EE\u53EF\u4EE5\u63D0\u95EE\u4E00\u4E2A\u5B98\u65B9\u7684transformers\u7248\
          \u672C\u6216\u73AF\u5883\u5417\uFF1F</p>\n</blockquote>\n<p>\u60A8\u53EF\
          \u4EE5\u5C1D\u8BD5transformers==4.18.0</p>\n"
        raw: "> \u6211\u7684transformers\u7684\u7248\u672C\u662F4.29.2\u3002\u8BF7\
          \u95EE\u53EF\u4EE5\u63D0\u95EE\u4E00\u4E2A\u5B98\u65B9\u7684transformers\u7248\
          \u672C\u6216\u73AF\u5883\u5417\uFF1F\n\n\u60A8\u53EF\u4EE5\u5C1D\u8BD5transformers==4.18.0"
        updatedAt: '2023-10-24T04:05:52.173Z'
      numEdits: 0
      reactions: []
    id: 653742a00d973d3fee44ad7b
    type: comment
  author: pskun
  content: "> \u6211\u7684transformers\u7684\u7248\u672C\u662F4.29.2\u3002\u8BF7\u95EE\
    \u53EF\u4EE5\u63D0\u95EE\u4E00\u4E2A\u5B98\u65B9\u7684transformers\u7248\u672C\
    \u6216\u73AF\u5883\u5417\uFF1F\n\n\u60A8\u53EF\u4EE5\u5C1D\u8BD5transformers==4.18.0"
  created_at: 2023-10-24 03:05:52+00:00
  edited: false
  hidden: false
  id: 653742a00d973d3fee44ad7b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: IDEA-CCNL/Erlangshen-TCBert-1.3B-Sentence-Embedding-Chinese
repo_type: model
status: open
target_branch: null
title: "\u6A21\u578B\u53C2\u6570\u4E0D\u5168"
