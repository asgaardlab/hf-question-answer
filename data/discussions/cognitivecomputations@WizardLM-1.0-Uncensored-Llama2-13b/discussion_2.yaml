!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheYuriLover
conflicting_files: null
created_at: 2023-08-06 15:25:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-08-06T16:25:13.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.980914831161499
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>Hello,</p>

          <p>I noticed your model had some abrupt stops during generations, like the
          sentence stop right away without even finishing. I noticed a similar issue
          with the older model vicuna-free, I guess this same training method still
          has this bug.</p>

          '
        raw: "Hello,\r\n\r\nI noticed your model had some abrupt stops during generations,\
          \ like the sentence stop right away without even finishing. I noticed a\
          \ similar issue with the older model vicuna-free, I guess this same training\
          \ method still has this bug."
        updatedAt: '2023-08-06T16:25:13.500Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - sandboarder
        - kroonen
    id: 64cfc96959503263d9e9dde4
    type: comment
  author: TheYuriLover
  content: "Hello,\r\n\r\nI noticed your model had some abrupt stops during generations,\
    \ like the sentence stop right away without even finishing. I noticed a similar\
    \ issue with the older model vicuna-free, I guess this same training method still\
    \ has this bug."
  created_at: 2023-08-06 15:25:13+00:00
  edited: false
  hidden: false
  id: 64cfc96959503263d9e9dde4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63037b895c70c21d0ea80b0e/myu35DuQn9io_HhxNLYR4.png?w=200&h=200&f=face
      fullname: Pooodle Shmith
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: Tom9000
      type: user
    createdAt: '2023-09-03T21:36:14.000Z'
    data:
      edited: false
      editors:
      - Tom9000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.947107195854187
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63037b895c70c21d0ea80b0e/myu35DuQn9io_HhxNLYR4.png?w=200&h=200&f=face
          fullname: Pooodle Shmith
          isHf: false
          isPro: true
          name: Tom9000
          type: user
        html: '<p>I get that too, rather often.<br>I wonder if there is some kind
          of contamination in the dataset and it''s coming from the model, or if some
          bug in our inference engines.</p>

          '
        raw: 'I get that too, rather often.

          I wonder if there is some kind of contamination in the dataset and it''s
          coming from the model, or if some bug in our inference engines.'
        updatedAt: '2023-09-03T21:36:14.600Z'
      numEdits: 0
      reactions: []
    id: 64f4fc4e9bc8dd9451626bb8
    type: comment
  author: Tom9000
  content: 'I get that too, rather often.

    I wonder if there is some kind of contamination in the dataset and it''s coming
    from the model, or if some bug in our inference engines.'
  created_at: 2023-09-03 20:36:14+00:00
  edited: false
  hidden: false
  id: 64f4fc4e9bc8dd9451626bb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-09-03T22:03:26.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.968192458152771
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>must be a fastchat thing.  I''m on axolotl now, and not seeing that
          kind of problem</p>

          '
        raw: must be a fastchat thing.  I'm on axolotl now, and not seeing that kind
          of problem
        updatedAt: '2023-09-03T22:03:26.019Z'
      numEdits: 0
      reactions: []
    id: 64f502ae9907c21102e21140
    type: comment
  author: ehartford
  content: must be a fastchat thing.  I'm on axolotl now, and not seeing that kind
    of problem
  created_at: 2023-09-03 21:03:26+00:00
  edited: false
  hidden: false
  id: 64f502ae9907c21102e21140
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63037b895c70c21d0ea80b0e/myu35DuQn9io_HhxNLYR4.png?w=200&h=200&f=face
      fullname: Pooodle Shmith
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: Tom9000
      type: user
    createdAt: '2023-09-03T22:44:16.000Z'
    data:
      edited: false
      editors:
      - Tom9000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.972699761390686
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63037b895c70c21d0ea80b0e/myu35DuQn9io_HhxNLYR4.png?w=200&h=200&f=face
          fullname: Pooodle Shmith
          isHf: false
          isPro: true
          name: Tom9000
          type: user
        html: '<p>That would suggest issue with the tools used for inference, rather
          than model.<br>I''ll test more with different loaders, engines and quantizations.
          </p>

          '
        raw: 'That would suggest issue with the tools used for inference, rather than
          model.

          I''ll test more with different loaders, engines and quantizations. '
        updatedAt: '2023-09-03T22:44:16.474Z'
      numEdits: 0
      reactions: []
    id: 64f50c4008175bde9698a34c
    type: comment
  author: Tom9000
  content: 'That would suggest issue with the tools used for inference, rather than
    model.

    I''ll test more with different loaders, engines and quantizations. '
  created_at: 2023-09-03 21:44:16+00:00
  edited: false
  hidden: false
  id: 64f50c4008175bde9698a34c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63037b895c70c21d0ea80b0e/myu35DuQn9io_HhxNLYR4.png?w=200&h=200&f=face
      fullname: Pooodle Shmith
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: Tom9000
      type: user
    createdAt: '2023-09-05T10:21:55.000Z'
    data:
      edited: false
      editors:
      - Tom9000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9615992307662964
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63037b895c70c21d0ea80b0e/myu35DuQn9io_HhxNLYR4.png?w=200&h=200&f=face
          fullname: Pooodle Shmith
          isHf: false
          isPro: true
          name: Tom9000
          type: user
        html: '<p>I couldn''t reproduce it yesterday, on the fresh install on my obabooga
          (I reinstall it every few days).<br>So either something got fixed, or it
          was due to something stupid on my part, like forgetting to change settings
          after installing it, for max tokens limit etc.</p>

          '
        raw: 'I couldn''t reproduce it yesterday, on the fresh install on my obabooga
          (I reinstall it every few days).

          So either something got fixed, or it was due to something stupid on my part,
          like forgetting to change settings after installing it, for max tokens limit
          etc.'
        updatedAt: '2023-09-05T10:21:55.909Z'
      numEdits: 0
      reactions: []
    id: 64f70143098581ab15d8453f
    type: comment
  author: Tom9000
  content: 'I couldn''t reproduce it yesterday, on the fresh install on my obabooga
    (I reinstall it every few days).

    So either something got fixed, or it was due to something stupid on my part, like
    forgetting to change settings after installing it, for max tokens limit etc.'
  created_at: 2023-09-05 09:21:55+00:00
  edited: false
  hidden: false
  id: 64f70143098581ab15d8453f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6f01c4bef23210049a0199a6f454665.svg
      fullname: Nic H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Aceallus
      type: user
    createdAt: '2023-10-26T04:32:02.000Z'
    data:
      edited: false
      editors:
      - Aceallus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9747462868690491
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6f01c4bef23210049a0199a6f454665.svg
          fullname: Nic H
          isHf: false
          isPro: false
          name: Aceallus
          type: user
        html: '<p>I get the same problem - the responses are truncated at some point
          regardless of how many of the context is used. Banning the EOS token solves
          the problem, but then he just goes on forever. I think I am using the correct
          format for all the prompts, but it seems like there is some problem with
          it. Specifically adding the EOS token prematurely. Don''t know how to solve
          it though - maybe some of you guys have an idea?</p>

          '
        raw: I get the same problem - the responses are truncated at some point regardless
          of how many of the context is used. Banning the EOS token solves the problem,
          but then he just goes on forever. I think I am using the correct format
          for all the prompts, but it seems like there is some problem with it. Specifically
          adding the EOS token prematurely. Don't know how to solve it though - maybe
          some of you guys have an idea?
        updatedAt: '2023-10-26T04:32:02.872Z'
      numEdits: 0
      reactions: []
    id: 6539ebc2a693d907fad732ae
    type: comment
  author: Aceallus
  content: I get the same problem - the responses are truncated at some point regardless
    of how many of the context is used. Banning the EOS token solves the problem,
    but then he just goes on forever. I think I am using the correct format for all
    the prompts, but it seems like there is some problem with it. Specifically adding
    the EOS token prematurely. Don't know how to solve it though - maybe some of you
    guys have an idea?
  created_at: 2023-10-26 03:32:02+00:00
  edited: false
  hidden: false
  id: 6539ebc2a693d907fad732ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-10-26T04:57:45.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6004780530929565
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>you should use a newer model.  LLaMA2-13B-Tiefighter maybe, or OpenHermes-2-Mistral-7B</p>

          '
        raw: you should use a newer model.  LLaMA2-13B-Tiefighter maybe, or OpenHermes-2-Mistral-7B
        updatedAt: '2023-10-26T04:57:45.090Z'
      numEdits: 0
      reactions: []
    id: 6539f1c99d66a6c304183b3a
    type: comment
  author: ehartford
  content: you should use a newer model.  LLaMA2-13B-Tiefighter maybe, or OpenHermes-2-Mistral-7B
  created_at: 2023-10-26 03:57:45+00:00
  edited: false
  hidden: false
  id: 6539f1c99d66a6c304183b3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6f01c4bef23210049a0199a6f454665.svg
      fullname: Nic H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Aceallus
      type: user
    createdAt: '2023-10-26T07:54:03.000Z'
    data:
      edited: false
      editors:
      - Aceallus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9529919624328613
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6f01c4bef23210049a0199a6f454665.svg
          fullname: Nic H
          isHf: false
          isPro: false
          name: Aceallus
          type: user
        html: '<p>Yeah probably, but I want to use it as a roleplay chat and so far
          your WizardLM model really hits the tone best. The other ones I tried out
          either talk like children or come around the corner with some weird suggestions
          like "I can''t harm innocents" and "We cant risk losing our humanity" for
          a character with extremely psychopathic tendencies. That kinda ruins the
          illusion^^</p>

          <p>I think I kinda found a way around it by modifying the prompt and banning
          the EOS token. Now it mostly works but sometimes the model still overshoots
          with statements like "End of Roleplay". Do you know if it''s possible to
          introduce a special token that reliably gets put at the end of the reply?
          Or custom-stopping strings like the Users'' character name?</p>

          '
        raw: 'Yeah probably, but I want to use it as a roleplay chat and so far your
          WizardLM model really hits the tone best. The other ones I tried out either
          talk like children or come around the corner with some weird suggestions
          like "I can''t harm innocents" and "We cant risk losing our humanity" for
          a character with extremely psychopathic tendencies. That kinda ruins the
          illusion^^


          I think I kinda found a way around it by modifying the prompt and banning
          the EOS token. Now it mostly works but sometimes the model still overshoots
          with statements like "End of Roleplay". Do you know if it''s possible to
          introduce a special token that reliably gets put at the end of the reply?
          Or custom-stopping strings like the Users'' character name?'
        updatedAt: '2023-10-26T07:54:03.906Z'
      numEdits: 0
      reactions: []
    id: 653a1b1b2a101a151dba3350
    type: comment
  author: Aceallus
  content: 'Yeah probably, but I want to use it as a roleplay chat and so far your
    WizardLM model really hits the tone best. The other ones I tried out either talk
    like children or come around the corner with some weird suggestions like "I can''t
    harm innocents" and "We cant risk losing our humanity" for a character with extremely
    psychopathic tendencies. That kinda ruins the illusion^^


    I think I kinda found a way around it by modifying the prompt and banning the
    EOS token. Now it mostly works but sometimes the model still overshoots with statements
    like "End of Roleplay". Do you know if it''s possible to introduce a special token
    that reliably gets put at the end of the reply? Or custom-stopping strings like
    the Users'' character name?'
  created_at: 2023-10-26 06:54:03+00:00
  edited: false
  hidden: false
  id: 653a1b1b2a101a151dba3350
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63037b895c70c21d0ea80b0e/myu35DuQn9io_HhxNLYR4.png?w=200&h=200&f=face
      fullname: Pooodle Shmith
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: Tom9000
      type: user
    createdAt: '2023-10-26T11:27:09.000Z'
    data:
      edited: false
      editors:
      - Tom9000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9801977872848511
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63037b895c70c21d0ea80b0e/myu35DuQn9io_HhxNLYR4.png?w=200&h=200&f=face
          fullname: Pooodle Shmith
          isHf: false
          isPro: true
          name: Tom9000
          type: user
        html: '<p>While I didn''t do enough testing to put my finger on anything confidently,
          I''ve seen that issue more often surfacing with quantized models at 4bit,
          more so than at 16bit or even 8bits.<br>Almost as if during quantization,
          there''s some non-zero chance for models to forget EOS tokens, or for those
          to lose some weight, enough cause issues.<br>On the other hand, I''ve not
          ran in to similar issues with models fine-tuned using ChatML format, yet.
          At least those few I did test a bit, all been following proper "grammar"
          without losing plot, even when quantized.<br>So I''m hoping that''s going
          to be less of an issue in the future, with more groups transitioning to
          stricter formats in their future fine-tunes.</p>

          '
        raw: 'While I didn''t do enough testing to put my finger on anything confidently,
          I''ve seen that issue more often surfacing with quantized models at 4bit,
          more so than at 16bit or even 8bits.

          Almost as if during quantization, there''s some non-zero chance for models
          to forget EOS tokens, or for those to lose some weight, enough cause issues.

          On the other hand, I''ve not ran in to similar issues with models fine-tuned
          using ChatML format, yet. At least those few I did test a bit, all been
          following proper "grammar" without losing plot, even when quantized.

          So I''m hoping that''s going to be less of an issue in the future, with
          more groups transitioning to stricter formats in their future fine-tunes.'
        updatedAt: '2023-10-26T11:27:09.404Z'
      numEdits: 0
      reactions: []
    id: 653a4d0d2ebce9b87d6f2c37
    type: comment
  author: Tom9000
  content: 'While I didn''t do enough testing to put my finger on anything confidently,
    I''ve seen that issue more often surfacing with quantized models at 4bit, more
    so than at 16bit or even 8bits.

    Almost as if during quantization, there''s some non-zero chance for models to
    forget EOS tokens, or for those to lose some weight, enough cause issues.

    On the other hand, I''ve not ran in to similar issues with models fine-tuned using
    ChatML format, yet. At least those few I did test a bit, all been following proper
    "grammar" without losing plot, even when quantized.

    So I''m hoping that''s going to be less of an issue in the future, with more groups
    transitioning to stricter formats in their future fine-tunes.'
  created_at: 2023-10-26 10:27:09+00:00
  edited: false
  hidden: false
  id: 653a4d0d2ebce9b87d6f2c37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6f01c4bef23210049a0199a6f454665.svg
      fullname: Nic H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Aceallus
      type: user
    createdAt: '2023-10-26T12:06:51.000Z'
    data:
      edited: false
      editors:
      - Aceallus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9766951203346252
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6f01c4bef23210049a0199a6f454665.svg
          fullname: Nic H
          isHf: false
          isPro: false
          name: Aceallus
          type: user
        html: '<p>Could be - I only briefly tested it by loading a different model
          in the same chat. The other ones seemed to be able to continue without problems
          - they were all 4bit quantized models. Doesn''t mean much though, could
          be that their specific error would surface under different circumstances.
          Other than that I noticed that it doesn''t seem to make a difference whether
          you choose a GPTQ or GGUF format - same error occurs.</p>

          '
        raw: Could be - I only briefly tested it by loading a different model in the
          same chat. The other ones seemed to be able to continue without problems
          - they were all 4bit quantized models. Doesn't mean much though, could be
          that their specific error would surface under different circumstances. Other
          than that I noticed that it doesn't seem to make a difference whether you
          choose a GPTQ or GGUF format - same error occurs.
        updatedAt: '2023-10-26T12:06:51.993Z'
      numEdits: 0
      reactions: []
    id: 653a565b9a19650c9ad62766
    type: comment
  author: Aceallus
  content: Could be - I only briefly tested it by loading a different model in the
    same chat. The other ones seemed to be able to continue without problems - they
    were all 4bit quantized models. Doesn't mean much though, could be that their
    specific error would surface under different circumstances. Other than that I
    noticed that it doesn't seem to make a difference whether you choose a GPTQ or
    GGUF format - same error occurs.
  created_at: 2023-10-26 11:06:51+00:00
  edited: false
  hidden: false
  id: 653a565b9a19650c9ad62766
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6f01c4bef23210049a0199a6f454665.svg
      fullname: Nic H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Aceallus
      type: user
    createdAt: '2023-10-26T13:13:35.000Z'
    data:
      edited: false
      editors:
      - Aceallus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.983608067035675
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6f01c4bef23210049a0199a6f454665.svg
          fullname: Nic H
          isHf: false
          isPro: false
          name: Aceallus
          type: user
        html: '<p>I tried out something with modifying the prompt and it seems like
          it''s working - Eos token is banned and I instructed the model to end each
          thought with the generation of "add end_of_thought_N after each paragraph
          with N counting upwards". Combined with a custom stopping string at End_of_thought_2
          it''s at least working as intended. Granted, it''s a dirty workaround but
          it seems to work for the time being.</p>

          '
        raw: I tried out something with modifying the prompt and it seems like it's
          working - Eos token is banned and I instructed the model to end each thought
          with the generation of "add end_of_thought_N after each paragraph with N
          counting upwards". Combined with a custom stopping string at End_of_thought_2
          it's at least working as intended. Granted, it's a dirty workaround but
          it seems to work for the time being.
        updatedAt: '2023-10-26T13:13:35.468Z'
      numEdits: 0
      reactions: []
    id: 653a65ff0b5b891a003031ed
    type: comment
  author: Aceallus
  content: I tried out something with modifying the prompt and it seems like it's
    working - Eos token is banned and I instructed the model to end each thought with
    the generation of "add end_of_thought_N after each paragraph with N counting upwards".
    Combined with a custom stopping string at End_of_thought_2 it's at least working
    as intended. Granted, it's a dirty workaround but it seems to work for the time
    being.
  created_at: 2023-10-26 12:13:35+00:00
  edited: false
  hidden: false
  id: 653a65ff0b5b891a003031ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63037b895c70c21d0ea80b0e/myu35DuQn9io_HhxNLYR4.png?w=200&h=200&f=face
      fullname: Pooodle Shmith
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: Tom9000
      type: user
    createdAt: '2023-10-26T13:26:18.000Z'
    data:
      edited: false
      editors:
      - Tom9000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9664289951324463
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63037b895c70c21d0ea80b0e/myu35DuQn9io_HhxNLYR4.png?w=200&h=200&f=face
          fullname: Pooodle Shmith
          isHf: false
          isPro: true
          name: Tom9000
          type: user
        html: '<p>I imagine models that suffer for that can be forced to work with
          some additional prompt crafting.<br>Or alternatively, toning down quantization,
          to 5-6 bits, rather than 4 and bellow, might be another workaround. Both
          GPTQ and GGUF formats are efficient, but a bit brute, in the way models
          get quantized.<br>This week I''m away from home and from my machines, so
          can''t test it until at least mid next month, but I''ll do more testing
          when I''m back home.</p>

          '
        raw: 'I imagine models that suffer for that can be forced to work with some
          additional prompt crafting.

          Or alternatively, toning down quantization, to 5-6 bits, rather than 4 and
          bellow, might be another workaround. Both GPTQ and GGUF formats are efficient,
          but a bit brute, in the way models get quantized.

          This week I''m away from home and from my machines, so can''t test it until
          at least mid next month, but I''ll do more testing when I''m back home.'
        updatedAt: '2023-10-26T13:26:18.822Z'
      numEdits: 0
      reactions: []
    id: 653a68fa9d53a3f8513c4c77
    type: comment
  author: Tom9000
  content: 'I imagine models that suffer for that can be forced to work with some
    additional prompt crafting.

    Or alternatively, toning down quantization, to 5-6 bits, rather than 4 and bellow,
    might be another workaround. Both GPTQ and GGUF formats are efficient, but a bit
    brute, in the way models get quantized.

    This week I''m away from home and from my machines, so can''t test it until at
    least mid next month, but I''ll do more testing when I''m back home.'
  created_at: 2023-10-26 12:26:18+00:00
  edited: false
  hidden: false
  id: 653a68fa9d53a3f8513c4c77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6f01c4bef23210049a0199a6f454665.svg
      fullname: Nic H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Aceallus
      type: user
    createdAt: '2023-10-27T03:14:27.000Z'
    data:
      edited: false
      editors:
      - Aceallus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9696035981178284
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6f01c4bef23210049a0199a6f454665.svg
          fullname: Nic H
          isHf: false
          isPro: false
          name: Aceallus
          type: user
        html: '<p>That sounds good - after your suggestion I tried it with the 8bit
          version. Got an error upon loading it though, so I can''t say whether that
          makes a difference yet. I noticed though that the model seems to perform
          very well for the initial part of the conversation and only later got off
          the rails. I think at a certain point the context transcends what the model
          is able to meaningfully interpret.</p>

          <p>I was thinking of diving a bit more into AutoGen and memGPT to basically
          relay the task of building a suitable prompt to different agents. Different
          agents could then factor in different aspects of the character like abilities,
          past memories and evolving relationship to the User Character in addition
          to the chat history. In combination the agents would then craft a specific
          prompt for each interaction and I hope that would make things a bit tidier
          and less taxing for the model to have to handle.<br>If I find something
          out that works, I''ll be sure to put it here - would be a shame if this
          model would be passed by the delevopment of time - cause personally I really
          like it.</p>

          <p>Have fun on your trip! Safe travels!</p>

          '
        raw: 'That sounds good - after your suggestion I tried it with the 8bit version.
          Got an error upon loading it though, so I can''t say whether that makes
          a difference yet. I noticed though that the model seems to perform very
          well for the initial part of the conversation and only later got off the
          rails. I think at a certain point the context transcends what the model
          is able to meaningfully interpret.


          I was thinking of diving a bit more into AutoGen and memGPT to basically
          relay the task of building a suitable prompt to different agents. Different
          agents could then factor in different aspects of the character like abilities,
          past memories and evolving relationship to the User Character in addition
          to the chat history. In combination the agents would then craft a specific
          prompt for each interaction and I hope that would make things a bit tidier
          and less taxing for the model to have to handle.

          If I find something out that works, I''ll be sure to put it here - would
          be a shame if this model would be passed by the delevopment of time - cause
          personally I really like it.


          Have fun on your trip! Safe travels!'
        updatedAt: '2023-10-27T03:14:27.293Z'
      numEdits: 0
      reactions: []
    id: 653b2b139b855c3b5286da79
    type: comment
  author: Aceallus
  content: 'That sounds good - after your suggestion I tried it with the 8bit version.
    Got an error upon loading it though, so I can''t say whether that makes a difference
    yet. I noticed though that the model seems to perform very well for the initial
    part of the conversation and only later got off the rails. I think at a certain
    point the context transcends what the model is able to meaningfully interpret.


    I was thinking of diving a bit more into AutoGen and memGPT to basically relay
    the task of building a suitable prompt to different agents. Different agents could
    then factor in different aspects of the character like abilities, past memories
    and evolving relationship to the User Character in addition to the chat history.
    In combination the agents would then craft a specific prompt for each interaction
    and I hope that would make things a bit tidier and less taxing for the model to
    have to handle.

    If I find something out that works, I''ll be sure to put it here - would be a
    shame if this model would be passed by the delevopment of time - cause personally
    I really like it.


    Have fun on your trip! Safe travels!'
  created_at: 2023-10-27 02:14:27+00:00
  edited: false
  hidden: false
  id: 653b2b139b855c3b5286da79
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: cognitivecomputations/WizardLM-1.0-Uncensored-Llama2-13b
repo_type: model
status: open
target_branch: null
title: 'The generation stops abruptly sometimes '
