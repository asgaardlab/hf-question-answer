!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yehiaserag
conflicting_files: null
created_at: 2023-03-18 04:30:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-03-18T05:30:49.000Z'
    data:
      edited: false
      editors:
      - yehiaserag
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: '<p>Shouldn''t this model be the same exact size as the model here <a
          href="https://huggingface.co/decapoda-research/llama-13b-hf-int4">https://huggingface.co/decapoda-research/llama-13b-hf-int4</a>
          ?</p>

          '
        raw: Shouldn't this model be the same exact size as the model here https://huggingface.co/decapoda-research/llama-13b-hf-int4
          ?
        updatedAt: '2023-03-18T05:30:49.832Z'
      numEdits: 0
      reactions: []
    id: 64154c89be815ecb88cf8e6d
    type: comment
  author: yehiaserag
  content: Shouldn't this model be the same exact size as the model here https://huggingface.co/decapoda-research/llama-13b-hf-int4
    ?
  created_at: 2023-03-18 04:30:49+00:00
  edited: false
  hidden: false
  id: 64154c89be815ecb88cf8e6d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-03-18T05:53:02.000Z'
    data:
      edited: false
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<p>Maybe he used LoRA which adds a few parameters.</p>

          '
        raw: Maybe he used LoRA which adds a few parameters.
        updatedAt: '2023-03-18T05:53:02.576Z'
      numEdits: 0
      reactions: []
    id: 641551beac9be91e9975b98a
    type: comment
  author: wassname
  content: Maybe he used LoRA which adds a few parameters.
  created_at: 2023-03-18 04:53:02+00:00
  edited: false
  hidden: false
  id: 641551beac9be91e9975b98a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-03-18T05:56:17.000Z'
    data:
      edited: false
      editors:
      - yehiaserag
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: '<p>still doesn''t explain about 500mb extra size, since the lora is
          about 37mb<br>also <a href="https://huggingface.co/elinas/alpaca-13b-int4">https://huggingface.co/elinas/alpaca-13b-int4</a>
          got the same exact size compared to <a href="https://huggingface.co/decapoda-research/llama-13b-hf-int4">https://huggingface.co/decapoda-research/llama-13b-hf-int4</a><br>so
          still wondering</p>

          '
        raw: 'still doesn''t explain about 500mb extra size, since the lora is about
          37mb

          also https://huggingface.co/elinas/alpaca-13b-int4 got the same exact size
          compared to https://huggingface.co/decapoda-research/llama-13b-hf-int4

          so still wondering'
        updatedAt: '2023-03-18T05:56:17.686Z'
      numEdits: 0
      reactions: []
    id: 641552817abeec7ff2a3f186
    type: comment
  author: yehiaserag
  content: 'still doesn''t explain about 500mb extra size, since the lora is about
    37mb

    also https://huggingface.co/elinas/alpaca-13b-int4 got the same exact size compared
    to https://huggingface.co/decapoda-research/llama-13b-hf-int4

    so still wondering'
  created_at: 2023-03-18 04:56:17+00:00
  edited: false
  hidden: false
  id: 641552817abeec7ff2a3f186
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/168969944b2772fed6c599f3047fb833.svg
      fullname: Neal Chandra
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nealchandra
      type: user
    createdAt: '2023-03-18T20:42:20.000Z'
    data:
      edited: false
      editors:
      - nealchandra
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/168969944b2772fed6c599f3047fb833.svg
          fullname: Neal Chandra
          isHf: false
          isPro: false
          name: nealchandra
          type: user
        html: '<p>Hi -- didn''t realize I had set this to public already, was going
          to add a few details. I did use LoRA to finetune this myself, but I agree
          that doesn''t clearly explain the difference in size. I''m not 100% sure
          what accounts for it, but the workflow I used was to start with the unquantized
          <a href="https://huggingface.co/decapoda-research/llama-13b-hf">https://huggingface.co/decapoda-research/llama-13b-hf</a>,
          then finetune it using peft, merge the adapter in and use GPTQ to convert
          it to 4bit quantization. So I didn''t rely on any public alpaca lora adapters
          or base models, it''s possible that my result differed due to some of the
          params I used along the way. </p>

          <p>I also haven''t tested this model extensively, I can''t guarantee the
          quality of inference will be on par with the Stanford example or even alpaca-lora.
          In fact, I would expect it not to be as I only finetuned on 1 epoch and
          my lowest learning rate was ~.85. </p>

          <p>My main goal was to test the workflow, and I tried to minimize GPU costs,
          I suspect someone else will produce a better quantized model. If you do
          end up playing around with the model feel free to share how it performs
          and if you have any suggestions for tuning or generation params!</p>

          '
        raw: "Hi -- didn't realize I had set this to public already, was going to\
          \ add a few details. I did use LoRA to finetune this myself, but I agree\
          \ that doesn't clearly explain the difference in size. I'm not 100% sure\
          \ what accounts for it, but the workflow I used was to start with the unquantized\
          \ https://huggingface.co/decapoda-research/llama-13b-hf, then finetune it\
          \ using peft, merge the adapter in and use GPTQ to convert it to 4bit quantization.\
          \ So I didn't rely on any public alpaca lora adapters or base models, it's\
          \ possible that my result differed due to some of the params I used along\
          \ the way. \n\nI also haven't tested this model extensively, I can't guarantee\
          \ the quality of inference will be on par with the Stanford example or even\
          \ alpaca-lora. In fact, I would expect it not to be as I only finetuned\
          \ on 1 epoch and my lowest learning rate was ~.85. \n\nMy main goal was\
          \ to test the workflow, and I tried to minimize GPU costs, I suspect someone\
          \ else will produce a better quantized model. If you do end up playing around\
          \ with the model feel free to share how it performs and if you have any\
          \ suggestions for tuning or generation params!"
        updatedAt: '2023-03-18T20:42:20.618Z'
      numEdits: 0
      reactions: []
    id: 6416222cbd9a427d965aeb22
    type: comment
  author: nealchandra
  content: "Hi -- didn't realize I had set this to public already, was going to add\
    \ a few details. I did use LoRA to finetune this myself, but I agree that doesn't\
    \ clearly explain the difference in size. I'm not 100% sure what accounts for\
    \ it, but the workflow I used was to start with the unquantized https://huggingface.co/decapoda-research/llama-13b-hf,\
    \ then finetune it using peft, merge the adapter in and use GPTQ to convert it\
    \ to 4bit quantization. So I didn't rely on any public alpaca lora adapters or\
    \ base models, it's possible that my result differed due to some of the params\
    \ I used along the way. \n\nI also haven't tested this model extensively, I can't\
    \ guarantee the quality of inference will be on par with the Stanford example\
    \ or even alpaca-lora. In fact, I would expect it not to be as I only finetuned\
    \ on 1 epoch and my lowest learning rate was ~.85. \n\nMy main goal was to test\
    \ the workflow, and I tried to minimize GPU costs, I suspect someone else will\
    \ produce a better quantized model. If you do end up playing around with the model\
    \ feel free to share how it performs and if you have any suggestions for tuning\
    \ or generation params!"
  created_at: 2023-03-18 19:42:20+00:00
  edited: false
  hidden: false
  id: 6416222cbd9a427d965aeb22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-03-18T23:39:32.000Z'
    data:
      edited: false
      editors:
      - yehiaserag
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: '<p>Thanks a lot for the explanation<br>I''ll update you if I find anything
          after testing</p>

          '
        raw: 'Thanks a lot for the explanation

          I''ll update you if I find anything after testing'
        updatedAt: '2023-03-18T23:39:32.359Z'
      numEdits: 0
      reactions: []
    id: 64164bb4b558cd39f1552cc2
    type: comment
  author: yehiaserag
  content: 'Thanks a lot for the explanation

    I''ll update you if I find anything after testing'
  created_at: 2023-03-18 22:39:32+00:00
  edited: false
  hidden: false
  id: 64164bb4b558cd39f1552cc2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: nealchandra/alpaca-13b-hf-int4
repo_type: model
status: open
target_branch: null
title: Size mismatch...
