!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HR1777
conflicting_files: null
created_at: 2023-07-06 06:29:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
      fullname: H R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HR1777
      type: user
    createdAt: '2023-07-06T07:29:41.000Z'
    data:
      edited: false
      editors:
      - HR1777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8861766457557678
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
          fullname: H R
          isHf: false
          isPro: false
          name: HR1777
          type: user
        html: '<p>Thank you for all your efforts you do for quantizing models. I have
          a request from you. chatglm2-6b is the <a href="/xzuyn/LLaVa-v1.1-13B-GGML/discussions/1">#1</a>
          trending model on huggingface.co (<a href="https://huggingface.co/THUDM/chatglm2-6b">https://huggingface.co/THUDM/chatglm2-6b</a>)
          but there is not any GGML version of it yet.<br>There is one GGML version
          here (<a href="https://huggingface.co/Xorbits/chatglm2-6B-GGML">https://huggingface.co/Xorbits/chatglm2-6B-GGML</a>)
          but it doesn''t work with popular interfaces like koboldcpp and it needs
          special interface for running it made by the same developer which is not
          released yet.<br>If its possible for you, please quantize chatglm2-6b (<a
          href="https://huggingface.co/THUDM/chatglm2-6b">https://huggingface.co/THUDM/chatglm2-6b</a>)
          and release the q6_k version of it.<br>Thank you so much in advance.</p>

          '
        raw: "Thank you for all your efforts you do for quantizing models. I have\
          \ a request from you. chatglm2-6b is the #1 trending model on huggingface.co\
          \ (https://huggingface.co/THUDM/chatglm2-6b) but there is not any GGML version\
          \ of it yet. \r\nThere is one GGML version here (https://huggingface.co/Xorbits/chatglm2-6B-GGML)\
          \ but it doesn't work with popular interfaces like koboldcpp and it needs\
          \ special interface for running it made by the same developer which is not\
          \ released yet.\r\nIf its possible for you, please quantize chatglm2-6b\
          \ (https://huggingface.co/THUDM/chatglm2-6b) and release the q6_k version\
          \ of it.\r\nThank you so much in advance."
        updatedAt: '2023-07-06T07:29:41.482Z'
      numEdits: 0
      reactions: []
    id: 64a66d65063f497473e028db
    type: comment
  author: HR1777
  content: "Thank you for all your efforts you do for quantizing models. I have a\
    \ request from you. chatglm2-6b is the #1 trending model on huggingface.co (https://huggingface.co/THUDM/chatglm2-6b)\
    \ but there is not any GGML version of it yet. \r\nThere is one GGML version here\
    \ (https://huggingface.co/Xorbits/chatglm2-6B-GGML) but it doesn't work with popular\
    \ interfaces like koboldcpp and it needs special interface for running it made\
    \ by the same developer which is not released yet.\r\nIf its possible for you,\
    \ please quantize chatglm2-6b (https://huggingface.co/THUDM/chatglm2-6b) and release\
    \ the q6_k version of it.\r\nThank you so much in advance."
  created_at: 2023-07-06 06:29:41+00:00
  edited: false
  hidden: false
  id: 64a66d65063f497473e028db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
      fullname: xzuyn
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: xzuyn
      type: user
    createdAt: '2023-07-07T04:01:18.000Z'
    data:
      edited: true
      editors:
      - xzuyn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5515433549880981
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
          fullname: xzuyn
          isHf: false
          isPro: false
          name: xzuyn
          type: user
        html: '<p><del>ChatGLM2 isn''t able to be quantized yet. <a href="https://huggingface.co/THUDM/chatglm2-6b/blob/0ecfe0b857efd00836a4851b3dd2ed04bd4b197f/config.json#L5">https://huggingface.co/THUDM/chatglm2-6b/blob/0ecfe0b857efd00836a4851b3dd2ed04bd4b197f/config.json#L5</a></del>
          Nevermind, I missed the part where someone already did one. You''ll need
          to wait a bit for it to be supported in llama.cpp so it can be merged into
          KoboldCPP.</p>

          <p>AFAIK the only supported architectures for GGML are; </p>

          <ol>

          <li>GPT-2</li>

          <li>GPT-J</li>

          <li>GPT-NeoX</li>

          <li>LLaMa</li>

          <li>MPT</li>

          <li>RWKV</li>

          </ol>

          '
        raw: "~~ChatGLM2 isn't able to be quantized yet. https://huggingface.co/THUDM/chatglm2-6b/blob/0ecfe0b857efd00836a4851b3dd2ed04bd4b197f/config.json#L5~~\
          \ Nevermind, I missed the part where someone already did one. You'll need\
          \ to wait a bit for it to be supported in llama.cpp so it can be merged\
          \ into KoboldCPP.\n\nAFAIK the only supported architectures for GGML are;\
          \ \n1. GPT-2\n2. GPT-J\n3. GPT-NeoX\n4. LLaMa\n5. MPT\n6. RWKV"
        updatedAt: '2023-07-07T04:04:18.040Z'
      numEdits: 1
      reactions: []
    id: 64a78e0e1ce0239e0f818463
    type: comment
  author: xzuyn
  content: "~~ChatGLM2 isn't able to be quantized yet. https://huggingface.co/THUDM/chatglm2-6b/blob/0ecfe0b857efd00836a4851b3dd2ed04bd4b197f/config.json#L5~~\
    \ Nevermind, I missed the part where someone already did one. You'll need to wait\
    \ a bit for it to be supported in llama.cpp so it can be merged into KoboldCPP.\n\
    \nAFAIK the only supported architectures for GGML are; \n1. GPT-2\n2. GPT-J\n\
    3. GPT-NeoX\n4. LLaMa\n5. MPT\n6. RWKV"
  created_at: 2023-07-07 03:01:18+00:00
  edited: true
  hidden: false
  id: 64a78e0e1ce0239e0f818463
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: xzuyn/LLaVa-v1.1-13B-GGML
repo_type: model
status: open
target_branch: null
title: A quantizing request
