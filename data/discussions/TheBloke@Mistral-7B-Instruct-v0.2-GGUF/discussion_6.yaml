!!python/object:huggingface_hub.community.DiscussionWithDetails
author: byunal
conflicting_files: null
created_at: 2024-01-17 16:01:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8bce5c7d47f613d1c90b1ec93c36b63.svg
      fullname: Cihat Unal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: byunal
      type: user
    createdAt: '2024-01-17T16:01:13.000Z'
    data:
      edited: false
      editors:
      - byunal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9780849814414978
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8bce5c7d47f613d1c90b1ec93c36b63.svg
          fullname: Cihat Unal
          isHf: false
          isPro: false
          name: byunal
          type: user
        html: '<p>I think this still suffers from sequences that have higher length
          than 512. However, mistral should be solving this by using SWA. How to tackle
          this issue ? or are there any similar or lightweight model for this ?</p>

          '
        raw: I think this still suffers from sequences that have higher length than
          512. However, mistral should be solving this by using SWA. How to tackle
          this issue ? or are there any similar or lightweight model for this ?
        updatedAt: '2024-01-17T16:01:13.590Z'
      numEdits: 0
      reactions: []
    id: 65a7f9c9de63b063e3338e83
    type: comment
  author: byunal
  content: I think this still suffers from sequences that have higher length than
    512. However, mistral should be solving this by using SWA. How to tackle this
    issue ? or are there any similar or lightweight model for this ?
  created_at: 2024-01-17 16:01:13+00:00
  edited: false
  hidden: false
  id: 65a7f9c9de63b063e3338e83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2024-01-17T16:43:43.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9231296181678772
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;byunal&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/byunal\">@<span class=\"\
          underline\">byunal</span></a></span>\n\n\t</span></span> hmm no it has a\
          \ much higher context length then 512. Infact all llama, qwen, mistral models\
          \ have higher context then 2048?</p>\n<p>I think you are using something\
          \ like ctransformers or llama cpp python which sets the context limit as\
          \ 512, you have to change it to your desired length.</p>\n"
        raw: '@byunal hmm no it has a much higher context length then 512. Infact
          all llama, qwen, mistral models have higher context then 2048?


          I think you are using something like ctransformers or llama cpp python which
          sets the context limit as 512, you have to change it to your desired length.'
        updatedAt: '2024-01-17T16:43:43.639Z'
      numEdits: 0
      reactions: []
    id: 65a803bfbdfe6bc8fd6db6d0
    type: comment
  author: YaTharThShaRma999
  content: '@byunal hmm no it has a much higher context length then 512. Infact all
    llama, qwen, mistral models have higher context then 2048?


    I think you are using something like ctransformers or llama cpp python which sets
    the context limit as 512, you have to change it to your desired length.'
  created_at: 2024-01-17 16:43:43+00:00
  edited: false
  hidden: false
  id: 65a803bfbdfe6bc8fd6db6d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8bce5c7d47f613d1c90b1ec93c36b63.svg
      fullname: Cihat Unal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: byunal
      type: user
    createdAt: '2024-01-18T11:41:30.000Z'
    data:
      edited: true
      editors:
      - byunal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9664772748947144
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8bce5c7d47f613d1c90b1ec93c36b63.svg
          fullname: Cihat Unal
          isHf: false
          isPro: false
          name: byunal
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;YaTharThShaRma999&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/YaTharThShaRma999\"\
          >@<span class=\"underline\">YaTharThShaRma999</span></a></span>\n\n\t</span></span>\
          \  Actually yes. I'm trying to use this model for text summarization on\
          \ CPU over ctransformers. Currently, I have no access to any GPU so I have\
          \ to do inference on CPU. Frankly, I didn't know that ctansformers limits\
          \ the context length. How can I neglect this limit on CPU ? I'd appreciated\
          \ if you can help.</p>\n"
        raw: '@YaTharThShaRma999  Actually yes. I''m trying to use this model for
          text summarization on CPU over ctransformers. Currently, I have no access
          to any GPU so I have to do inference on CPU. Frankly, I didn''t know that
          ctansformers limits the context length. How can I neglect this limit on
          CPU ? I''d appreciated if you can help.'
        updatedAt: '2024-01-18T11:42:14.716Z'
      numEdits: 2
      reactions: []
    id: 65a90e6a5e49cc9fdc931177
    type: comment
  author: byunal
  content: '@YaTharThShaRma999  Actually yes. I''m trying to use this model for text
    summarization on CPU over ctransformers. Currently, I have no access to any GPU
    so I have to do inference on CPU. Frankly, I didn''t know that ctansformers limits
    the context length. How can I neglect this limit on CPU ? I''d appreciated if
    you can help.'
  created_at: 2024-01-18 11:41:30+00:00
  edited: true
  hidden: false
  id: 65a90e6a5e49cc9fdc931177
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
repo_type: model
status: open
target_branch: null
title: No SWA ?
