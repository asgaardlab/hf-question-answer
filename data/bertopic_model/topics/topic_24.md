### Documents:
- Can we do lora fine-tuning on this model?

Does Mamba support Lora finetuning?
- How to build a lora model?

I built my own lora model and a base model. I have connected my lora model with my base model but still i'm not getting the desired results. The images generated by my lora models comes out be as same as the images generated by my base model. Can you please help?

Lora model link:


Base model link:


Note: 
Resolution of my base model is 256.
My lora model works perfectly on my machine
- The combination of lcm-lora with other LoRa seems to diminish the effectiveness of LoRa.

I fused the lcm-lora into the model in Civitai, then fused my own trained LoRa. There is a noticeable difference in the effect of fusing my own LoRa directly on Civitai compared to fusing it after lcm-lora. The original direct fusion of my own LoRa has a more distinct style, while the difference in styles between different LoRas seems to decrease after fusing with lcm-lora. Is this due to the combination of multiple LoRas, or is it a problem with lcm itself?
### Keywords: lora, loras, fine, lora model, lcm, training, peft, model, lora fine, tuning